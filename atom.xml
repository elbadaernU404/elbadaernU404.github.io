<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>青域</title>
  
  
  <link href="http://yoursite.com/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2023-10-24T03:09:59.359Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>tianL.R</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>LangChain + ChatGLM2-6B的本地知识问答库</title>
    <link href="http://yoursite.com/2023/10/24/LangChain%20+%20ChatGLM2-6B%E7%9A%84%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%BA%93/"/>
    <id>http://yoursite.com/2023/10/24/LangChain%20+%20ChatGLM2-6B%E7%9A%84%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%BA%93/</id>
    <published>2023-10-24T01:14:34.000Z</published>
    <updated>2023-10-24T03:09:59.359Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p>原项目Github：<a href="https://github.com/imClumsyPanda/langchain-ChatGLM" target="_blank" rel="noopener">https://github.com/imClumsyPanda/langchain-ChatGLM</a></p><h2 id="项目部署"><a href="#项目部署" class="headerlink" title="项目部署"></a>项目部署</h2><h3 id="机器配置："><a href="#机器配置：" class="headerlink" title="机器配置："></a>机器配置：</h3><p><strong>·</strong> <font color="gold">python 环境：anaconda3 + python3.10.12</font></p><p><strong>·</strong> <font color="gold">GPU：RTX3090*2 + CUDA11.7</font></p><p><strong>·</strong> <font color="gold">torch：2.0.1（CUDA未升至12）</font></p><h3 id="模型下载"><a href="#模型下载" class="headerlink" title="模型下载"></a>模型下载</h3><p><strong>·</strong> <font color="gold">m3e</font> <a href="https://huggingface.co/moka-ai/m3e-base/tree/main" target="_blank" rel="noopener">https://huggingface.co/moka-ai/m3e-base/tree/main</a></p><p><strong>·</strong> <font color="gold">chatglm2-6b</font> <a href="https://huggingface.co/THUDM/chatglm2-6b/tree/main" target="_blank" rel="noopener">https://huggingface.co/THUDM/chatglm2-6b/tree/main</a></p><p>chatglm清华源 <a href="https://cloud.tsinghua.edu.cn/d/674208019e314311ab5c/?p=%2F&amp;mode=list" target="_blank" rel="noopener">https://cloud.tsinghua.edu.cn/d/674208019e314311ab5c/?p=%2F&amp;mode=list</a></p><p>(这里将模型全部下载至<code>/root/huggingface</code>下)</p><h3 id="创建虚拟环境，安装依赖"><a href="#创建虚拟环境，安装依赖" class="headerlink" title="创建虚拟环境，安装依赖"></a>创建虚拟环境，安装依赖</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">conda create -n py310_dtglm python=3.10.12</span><br><span class="line">conda activate py310_dtglm</span><br><span class="line"></span><br><span class="line">pip install --use-pep517 -r requirements.txt -i https://mirror.baidu.com/pypi/simple</span><br><span class="line">pip install --use-pep517 -r requirements_api.txt -i https://mirror.baidu.com/pypi/simple</span><br><span class="line">pip install --use-pep517 -r requirements_webui.txt -i https://mirror.baidu.com/pypi/simple</span><br></pre></td></tr></table></figure><h3 id="修改配置、模型路径"><a href="#修改配置、模型路径" class="headerlink" title="修改配置、模型路径"></a>修改配置、模型路径</h3><p>复制配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python copy_config_example.py</span><br></pre></td></tr></table></figure><p>修改配置文件</p><p><strong>·</strong> <font color="gold">model_config.py</font></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">MODEL_ROOT_PATH = <span class="string">"/root/huggingface"</span></span><br><span class="line"></span><br><span class="line">MODEL_PATH = &#123;</span><br><span class="line">    <span class="string">"embed_model"</span>: &#123;</span><br><span class="line">...</span><br><span class="line">        <span class="string">"m3e-base"</span>: <span class="string">"/root/huggingface/m3e-base"</span>, <span class="comment"># 修改m3e模型路径</span></span><br><span class="line">...</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> add all supported llm models</span></span><br><span class="line">    <span class="string">"llm_model"</span>: &#123;</span><br><span class="line">...</span><br><span class="line">        <span class="string">"chatglm2-6b"</span>: <span class="string">"/root/huggingface/chatglm2-6b"</span>, <span class="comment"># 修改chatglm2-6b模型路径</span></span><br><span class="line">...</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">EMBEDDING_MODEL = <span class="string">"m3e-base"</span> <span class="comment"># 可以尝试最新的嵌入式sota模型：bge-large-zh-v1.5</span></span><br><span class="line">LLM_MODEL = <span class="string">"chatglm2-6b"</span></span><br></pre></td></tr></table></figure><a id="more"></a><p><strong>·</strong> <font color="gold">server_config.py</font></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># webui.py server</span></span><br><span class="line">WEBUI_SERVER = &#123;</span><br><span class="line">    <span class="string">"host"</span>: DEFAULT_BIND_HOST,</span><br><span class="line">    <span class="string">"port"</span>: <span class="number">8501</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># api.py server</span></span><br><span class="line">API_SERVER = &#123;</span><br><span class="line">    <span class="string">"host"</span>: DEFAULT_BIND_HOST,</span><br><span class="line">    <span class="string">"port"</span>: <span class="number">7861</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># fastchat openai_api server</span></span><br><span class="line">FSCHAT_OPENAI_API = &#123;</span><br><span class="line">    <span class="string">"host"</span>: DEFAULT_BIND_HOST,</span><br><span class="line">    <span class="string">"port"</span>: <span class="number">20000</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">FSCHAT_MODEL_WORKERS = &#123;</span><br><span class="line">    <span class="string">"default"</span>: &#123;</span><br><span class="line">        <span class="string">"host"</span>: DEFAULT_BIND_HOST,</span><br><span class="line">        <span class="string">"port"</span>: <span class="number">20002</span>,</span><br><span class="line">        <span class="string">"device"</span>: LLM_DEVICE,</span><br><span class="line">        <span class="string">"infer_turbo"</span>: <span class="literal">False</span>,</span><br><span class="line"></span><br><span class="line">        <span class="comment"># model_worker多卡加载需要配置的参数</span></span><br><span class="line">        <span class="string">"gpus"</span>: <span class="string">"0,1"</span>, <span class="comment"># 使用的GPU，以str的格式指定，如"0,1"，如失效请使用CUDA_VISIBLE_DEVICES="0,1"等形式指定</span></span><br><span class="line">        <span class="string">"num_gpus"</span>: <span class="number">2</span>, <span class="comment"># 使用GPU的数量</span></span><br><span class="line">        <span class="string">"max_gpu_memory"</span>: <span class="string">"20GiB"</span>, <span class="comment"># 每个GPU占用的最大显存</span></span><br><span class="line"></span><br><span class="line">    &#125;,</span><br><span class="line"></span><br><span class="line">    <span class="string">"zhipu-api"</span>: &#123; <span class="comment"># 请为每个要运行的在线API设置不同的端口</span></span><br><span class="line">        <span class="string">"port"</span>: <span class="number">21001</span>,</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># fastchat controller server</span></span><br><span class="line">FSCHAT_CONTROLLER = &#123;</span><br><span class="line">    <span class="string">"host"</span>: DEFAULT_BIND_HOST,</span><br><span class="line">    <span class="string">"port"</span>: <span class="number">20001</span>,</span><br><span class="line">    <span class="string">"dispatch_method"</span>: <span class="string">"shortest_queue"</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="初始化默认知识库"><a href="#初始化默认知识库" class="headerlink" title="初始化默认知识库"></a>初始化默认知识库</h3><p>样例知识库文件位置：<code>knowledge_base/samples/content/test.txt</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python init_database.py --recreate-vs</span><br></pre></td></tr></table></figure><h3 id="启动项目"><a href="#启动项目" class="headerlink" title="启动项目"></a>启动项目</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python startup.py -a</span><br></pre></td></tr></table></figure><p><img src="/2023/10/24/LangChain + ChatGLM2-6B的本地知识问答库/A.png" alt></p><h3 id="通过fastapi接口添加知识库"><a href="#通过fastapi接口添加知识库" class="headerlink" title="通过fastapi接口添加知识库"></a>通过fastapi接口添加知识库</h3><p><code>http://host:7861/knowledge_base/upload_docs</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">curl -X 'POST' \</span><br><span class="line">  'http://host:7861/knowledge_base/upload_docs' \</span><br><span class="line">  -H 'accept: application/json' \</span><br><span class="line">  -H 'Content-Type: multipart/form-data' \</span><br><span class="line">  -F 'to_vector_store=true' \</span><br><span class="line">  -F 'override=false' \</span><br><span class="line">  -F 'not_refresh_vs_cache=false' \</span><br><span class="line">  -F 'chunk_size=250' \</span><br><span class="line">  -F 'chunk_overlap=50' \</span><br><span class="line">  -F 'zh_title_enhance=true' \</span><br><span class="line">  -F 'files=@分体式M录AI智能分析设备建设方案.docx;type=application/vnd.openxmlformats-officedocument.wordprocessingml.document' \</span><br><span class="line">  -F 'knowledge_base_name=琅琊' \</span><br><span class="line">  -F 'docs='</span><br></pre></td></tr></table></figure><p><img src="/2023/10/24/LangChain + ChatGLM2-6B的本地知识问答库/B.png" alt></p><h3 id="选择知识库问答"><a href="#选择知识库问答" class="headerlink" title="选择知识库问答"></a>选择知识库问答</h3><p><img src="/2023/10/24/LangChain + ChatGLM2-6B的本地知识问答库/C.png" alt></p><p><img src="/2023/10/24/LangChain + ChatGLM2-6B的本地知识问答库/D.png" alt></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;原项目Github：&lt;a href=&quot;https://github.com/imClumsyPanda/langchain-ChatGLM&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/imClumsyPanda/langchain-ChatGLM&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;项目部署&quot;&gt;&lt;a href=&quot;#项目部署&quot; class=&quot;headerlink&quot; title=&quot;项目部署&quot;&gt;&lt;/a&gt;项目部署&lt;/h2&gt;&lt;h3 id=&quot;机器配置：&quot;&gt;&lt;a href=&quot;#机器配置：&quot; class=&quot;headerlink&quot; title=&quot;机器配置：&quot;&gt;&lt;/a&gt;机器配置：&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;·&lt;/strong&gt; &lt;font color=&quot;gold&quot;&gt;python 环境：anaconda3 + python3.10.12&lt;/font&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;·&lt;/strong&gt; &lt;font color=&quot;gold&quot;&gt;GPU：RTX3090*2 + CUDA11.7&lt;/font&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;·&lt;/strong&gt; &lt;font color=&quot;gold&quot;&gt;torch：2.0.1（CUDA未升至12）&lt;/font&gt;&lt;/p&gt;
&lt;h3 id=&quot;模型下载&quot;&gt;&lt;a href=&quot;#模型下载&quot; class=&quot;headerlink&quot; title=&quot;模型下载&quot;&gt;&lt;/a&gt;模型下载&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;·&lt;/strong&gt; &lt;font color=&quot;gold&quot;&gt;m3e&lt;/font&gt; &lt;a href=&quot;https://huggingface.co/moka-ai/m3e-base/tree/main&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://huggingface.co/moka-ai/m3e-base/tree/main&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;·&lt;/strong&gt; &lt;font color=&quot;gold&quot;&gt;chatglm2-6b&lt;/font&gt; &lt;a href=&quot;https://huggingface.co/THUDM/chatglm2-6b/tree/main&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://huggingface.co/THUDM/chatglm2-6b/tree/main&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;chatglm清华源 &lt;a href=&quot;https://cloud.tsinghua.edu.cn/d/674208019e314311ab5c/?p=%2F&amp;amp;mode=list&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://cloud.tsinghua.edu.cn/d/674208019e314311ab5c/?p=%2F&amp;amp;mode=list&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;(这里将模型全部下载至&lt;code&gt;/root/huggingface&lt;/code&gt;下)&lt;/p&gt;
&lt;h3 id=&quot;创建虚拟环境，安装依赖&quot;&gt;&lt;a href=&quot;#创建虚拟环境，安装依赖&quot; class=&quot;headerlink&quot; title=&quot;创建虚拟环境，安装依赖&quot;&gt;&lt;/a&gt;创建虚拟环境，安装依赖&lt;/h3&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;conda create -n py310_dtglm python=3.10.12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;conda activate py310_dtglm&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;pip install --use-pep517 -r requirements.txt -i https://mirror.baidu.com/pypi/simple&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;pip install --use-pep517 -r requirements_api.txt -i https://mirror.baidu.com/pypi/simple&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;pip install --use-pep517 -r requirements_webui.txt -i https://mirror.baidu.com/pypi/simple&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;修改配置、模型路径&quot;&gt;&lt;a href=&quot;#修改配置、模型路径&quot; class=&quot;headerlink&quot; title=&quot;修改配置、模型路径&quot;&gt;&lt;/a&gt;修改配置、模型路径&lt;/h3&gt;&lt;p&gt;复制配置文件&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;python copy_config_example.py&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;修改配置文件&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;·&lt;/strong&gt; &lt;font color=&quot;gold&quot;&gt;model_config.py&lt;/font&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;MODEL_ROOT_PATH = &lt;span class=&quot;string&quot;&gt;&quot;/root/huggingface&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;MODEL_PATH = &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;embed_model&quot;&lt;/span&gt;: &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		...&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;string&quot;&gt;&quot;m3e-base&quot;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&quot;/root/huggingface/m3e-base&quot;&lt;/span&gt;, &lt;span class=&quot;comment&quot;&gt;# 修改m3e模型路径&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		...&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#125;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;# &lt;span class=&quot;doctag&quot;&gt;TODO:&lt;/span&gt; add all supported llm models&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;llm_model&quot;&lt;/span&gt;: &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		...&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;string&quot;&gt;&quot;chatglm2-6b&quot;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&quot;/root/huggingface/chatglm2-6b&quot;&lt;/span&gt;, &lt;span class=&quot;comment&quot;&gt;# 修改chatglm2-6b模型路径&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		...&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#125;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;EMBEDDING_MODEL = &lt;span class=&quot;string&quot;&gt;&quot;m3e-base&quot;&lt;/span&gt; &lt;span class=&quot;comment&quot;&gt;# 可以尝试最新的嵌入式sota模型：bge-large-zh-v1.5&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;LLM_MODEL = &lt;span class=&quot;string&quot;&gt;&quot;chatglm2-6b&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Nebula3单机版快速安装</title>
    <link href="http://yoursite.com/2023/09/27/Nebula3%E5%8D%95%E6%9C%BA%E7%89%88%E5%BF%AB%E9%80%9F%E5%AE%89%E8%A3%85/"/>
    <id>http://yoursite.com/2023/09/27/Nebula3%E5%8D%95%E6%9C%BA%E7%89%88%E5%BF%AB%E9%80%9F%E5%AE%89%E8%A3%85/</id>
    <published>2023-09-27T08:12:31.000Z</published>
    <updated>2023-10-24T02:28:41.838Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><font color="gold"><strong>·</strong> 系统：CentOS7</font><font color="gold"><strong>·</strong> nebula版本：3.6.0（开源社区版）</font><font color="gold"><strong>·</strong> nebula-graph-studio版本：3.2.3</font><h2 id="单机部署"><a href="#单机部署" class="headerlink" title="单机部署"></a>单机部署</h2><h3 id="tar包源码下载"><a href="#tar包源码下载" class="headerlink" title="tar包源码下载"></a>tar包源码下载</h3><p><code>wget https://oss-cdn.nebula-graph.com.cn/package/3.6.0/nebula-graph-3.6.0.el7.x86_64.tar.gz</code></p><h3 id="解压并重命名"><a href="#解压并重命名" class="headerlink" title="解压并重命名"></a>解压并重命名</h3><p><code>tar -xvzf nebula-graph-3.6.0.el7.x86_64.tar.gz</code></p><p><code>mv nebula-graph-3.6.0.el7.x86_64 nebula</code></p><h3 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h3><p><code>cd nebula/etc</code></p><p><code>mv nebula-graphd.conf.default nebula-graphd.conf</code></p><p><code>mv nebula-metad.conf.default nebula-metad.conf</code></p><p><code>mv nebula-storaged.conf nebula-storaged.conf</code></p><p>修改对应文件存储位置、节点ip地址，集群同理</p><a id="more"></a><p><strong>示例：</strong></p><p><strong>·</strong> <code>nebula-graphd.conf</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line">########## basics ##########</span><br><span class="line"># Whether to run as a daemon process</span><br><span class="line">--daemonize=true</span><br><span class="line"># The file to host the process id</span><br><span class="line">--pid_file=pids/nebula-graphd.pid</span><br><span class="line"># Whether to enable optimizer</span><br><span class="line">--enable_optimizer=true</span><br><span class="line"># The default charset when a space is created</span><br><span class="line">--default_charset=utf8</span><br><span class="line"># The default collate when a space is created</span><br><span class="line">--default_collate=utf8_bin</span><br><span class="line"># Whether to use the configuration obtained from the configuration file</span><br><span class="line">--local_config=true</span><br><span class="line"></span><br><span class="line">########## logging ##########</span><br><span class="line"># The directory to host logging files</span><br><span class="line">--log_dir=/home/nebula/graph/logs</span><br><span class="line"># Log level, 0, 1, 2, 3 for INFO, WARNING, ERROR, FATAL respectively</span><br><span class="line">--minloglevel=0</span><br><span class="line"># Verbose log level, 1, 2, 3, 4, the higher of the level, the more verbose of the logging</span><br><span class="line">--v=0</span><br><span class="line"># Maximum seconds to buffer the log messages</span><br><span class="line">--logbufsecs=0</span><br><span class="line"># Whether to redirect stdout and stderr to separate output files</span><br><span class="line">--redirect_stdout=true</span><br><span class="line"># Destination filename of stdout and stderr, which will also reside in log_dir.</span><br><span class="line">--stdout_log_file=graphd-stdout.log</span><br><span class="line">--stderr_log_file=graphd-stderr.log</span><br><span class="line"># Copy log messages at or above this level to stderr in addition to logfiles. The numbers of severity levels INFO, WARNING, ERROR, and FATAL are 0, 1, 2, and 3, respectively.</span><br><span class="line">--stderrthreshold=3</span><br><span class="line"># wether logging files&apos; name contain time stamp.</span><br><span class="line">--timestamp_in_logfile_name=true</span><br><span class="line">########## query ##########</span><br><span class="line"># Whether to treat partial success as an error.</span><br><span class="line"># This flag is only used for Read-only access, and Modify access always treats partial success as an error.</span><br><span class="line">--accept_partial_success=false</span><br><span class="line"># Maximum sentence length, unit byte</span><br><span class="line">--max_allowed_query_size=4194304</span><br><span class="line"></span><br><span class="line">########## networking ##########</span><br><span class="line"># Comma separated Meta Server Addresses</span><br><span class="line">--meta_server_addrs=192.168.9.103:9559</span><br><span class="line"># Local IP used to identify the nebula-graphd process.</span><br><span class="line"># Change it to an address other than loopback if the service is distributed or</span><br><span class="line"># will be accessed remotely.</span><br><span class="line">--local_ip=192.168.9.103</span><br><span class="line"># Network device to listen on</span><br><span class="line">--listen_netdev=any</span><br><span class="line"># Port to listen on</span><br><span class="line">--port=9669</span><br><span class="line"># To turn on SO_REUSEPORT or not</span><br><span class="line">--reuse_port=false</span><br><span class="line"># Backlog of the listen socket, adjust this together with net.core.somaxconn</span><br><span class="line">--listen_backlog=1024</span><br><span class="line"># The number of seconds Nebula service waits before closing the idle connections</span><br><span class="line">--client_idle_timeout_secs=28800</span><br><span class="line"># The number of seconds before idle sessions expire</span><br><span class="line"># The range should be in [1, 604800]</span><br><span class="line">--session_idle_timeout_secs=28800</span><br><span class="line"># The number of threads to accept incoming connections</span><br><span class="line">--num_accept_threads=1</span><br><span class="line"># The number of networking IO threads, 0 for # of CPU cores</span><br><span class="line">--num_netio_threads=0</span><br><span class="line"># Max active connections for all networking threads. 0 means no limit.</span><br><span class="line"># Max connections for each networking thread = num_max_connections / num_netio_threads</span><br><span class="line">--num_max_connections=0</span><br><span class="line"># The number of threads to execute user queries, 0 for # of CPU cores</span><br><span class="line">--num_worker_threads=0</span><br><span class="line"># HTTP service ip</span><br><span class="line">--ws_ip=0.0.0.0</span><br><span class="line"># HTTP service port</span><br><span class="line">--ws_http_port=19669</span><br><span class="line"># storage client timeout</span><br><span class="line">--storage_client_timeout_ms=60000</span><br><span class="line"># slow query threshold in us</span><br><span class="line">--slow_query_threshold_us=200000</span><br><span class="line"># Port to listen on Meta with HTTP protocol, it corresponds to ws_http_port in metad&apos;s configuration file</span><br><span class="line">--ws_meta_http_port=19559</span><br><span class="line"></span><br><span class="line">########## authentication ##########</span><br><span class="line"># Enable authorization</span><br><span class="line">--enable_authorize=false</span><br><span class="line"># User login authentication type, password for nebula authentication, ldap for ldap authentication, cloud for cloud authentication</span><br><span class="line">--auth_type=password</span><br><span class="line"></span><br><span class="line">########## memory ##########</span><br><span class="line"># System memory high watermark ratio, cancel the memory checking when the ratio greater than 1.0</span><br><span class="line">--system_memory_high_watermark_ratio=0.8</span><br><span class="line"></span><br><span class="line">########## metrics ##########</span><br><span class="line">--enable_space_level_metrics=false</span><br><span class="line"></span><br><span class="line">########## experimental feature ##########</span><br><span class="line"># if use experimental features</span><br><span class="line">--enable_experimental_feature=false</span><br><span class="line"></span><br><span class="line"># if use balance data feature, only work if enable_experimental_feature is true</span><br><span class="line">--enable_data_balance=true</span><br><span class="line"></span><br><span class="line"># enable udf, written in c++ only for now</span><br><span class="line">--enable_udf=true</span><br><span class="line"></span><br><span class="line"># set the directory where the .so files of udf are stored, when enable_udf is true</span><br><span class="line">--udf_path=/home/nebula/dev/nebula/udf/</span><br><span class="line"></span><br><span class="line">########## session ##########</span><br><span class="line"># Maximum number of sessions that can be created per IP and per user</span><br><span class="line">--max_sessions_per_ip_per_user=300</span><br><span class="line"></span><br><span class="line">########## memory tracker ##########</span><br><span class="line"># trackable memory ratio (trackable_memory / (total_memory - untracked_reserved_memory) )</span><br><span class="line">--memory_tracker_limit_ratio=0.8</span><br><span class="line"># untracked reserved memory in Mib</span><br><span class="line">--memory_tracker_untracked_reserved_memory_mb=50</span><br><span class="line"></span><br><span class="line"># enable log memory tracker stats periodically</span><br><span class="line">--memory_tracker_detail_log=false</span><br><span class="line"># log memory tacker stats interval in milliseconds</span><br><span class="line">--memory_tracker_detail_log_interval_ms=60000</span><br><span class="line"></span><br><span class="line"># enable memory background purge (if jemalloc is used)</span><br><span class="line">--memory_purge_enabled=true</span><br><span class="line"># memory background purge interval in seconds</span><br><span class="line">--memory_purge_interval_seconds=10</span><br><span class="line"></span><br><span class="line">########## performance optimization ##########</span><br><span class="line"># The max job size in multi job mode</span><br><span class="line">--max_job_size=1</span><br><span class="line"># The min batch size for handling dataset in multi job mode, only enabled when max_job_size is greater than 1</span><br><span class="line">--min_batch_size=8192</span><br><span class="line"># if true, return directly without go through RPC</span><br><span class="line">--optimize_appendvertices=false</span><br><span class="line"># number of paths constructed by each thread</span><br><span class="line">--path_batch_size=10000</span><br></pre></td></tr></table></figure><p><strong>·</strong> <code>nebula-metad.conf</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">########## basics ##########</span><br><span class="line"># Whether to run as a daemon process</span><br><span class="line">--daemonize=true</span><br><span class="line"># The file to host the process id</span><br><span class="line">--pid_file=pids/nebula-metad.pid</span><br><span class="line"></span><br><span class="line">########## logging ##########</span><br><span class="line"># The directory to host logging files</span><br><span class="line">--log_dir=/home/nebula/meta/logs</span><br><span class="line"># Log level, 0, 1, 2, 3 for INFO, WARNING, ERROR, FATAL respectively</span><br><span class="line">--minloglevel=0</span><br><span class="line"># Verbose log level, 1, 2, 3, 4, the higher of the level, the more verbose of the logging</span><br><span class="line">--v=0</span><br><span class="line"># Maximum seconds to buffer the log messages</span><br><span class="line">--logbufsecs=0</span><br><span class="line"># Whether to redirect stdout and stderr to separate output files</span><br><span class="line">--redirect_stdout=true</span><br><span class="line"># Destination filename of stdout and stderr, which will also reside in log_dir.</span><br><span class="line">--stdout_log_file=metad-stdout.log</span><br><span class="line">--stderr_log_file=metad-stderr.log</span><br><span class="line"># Copy log messages at or above this level to stderr in addition to logfiles. The numbers of severity levels INFO, WARNING, ERROR, and FATAL are 0, 1, 2, and 3, respectively.</span><br><span class="line">--stderrthreshold=3</span><br><span class="line"># wether logging files&apos; name contain time stamp, If Using logrotate to rotate logging files, than should set it to true.</span><br><span class="line">--timestamp_in_logfile_name=true</span><br><span class="line"></span><br><span class="line">########## networking ##########</span><br><span class="line"># Comma separated Meta Server addresses</span><br><span class="line">--meta_server_addrs=192.168.9.103:9559</span><br><span class="line"># Local IP used to identify the nebula-metad process.</span><br><span class="line"># Change it to an address other than loopback if the service is distributed or</span><br><span class="line"># will be accessed remotely.</span><br><span class="line">--local_ip=192.168.9.103</span><br><span class="line"># Meta daemon listening port</span><br><span class="line">--port=9559</span><br><span class="line"># HTTP service ip</span><br><span class="line">--ws_ip=0.0.0.0</span><br><span class="line"># HTTP service port</span><br><span class="line">--ws_http_port=19559</span><br><span class="line"># Port to listen on Storage with HTTP protocol, it corresponds to ws_http_port in storage&apos;s configuration file</span><br><span class="line">--ws_storage_http_port=19779</span><br><span class="line"></span><br><span class="line">########## storage ##########</span><br><span class="line"># Root data path, here should be only single path for metad</span><br><span class="line">--data_path=/home/nebula/data/meta</span><br><span class="line"></span><br><span class="line">########## Misc #########</span><br><span class="line"># The default number of parts when a space is created</span><br><span class="line">--default_parts_num=100</span><br><span class="line"># The default replica factor when a space is created</span><br><span class="line">--default_replica_factor=1</span><br><span class="line"></span><br><span class="line">--heartbeat_interval_secs=10</span><br><span class="line">--agent_heartbeat_interval_secs=60</span><br></pre></td></tr></table></figure><p><strong>·</strong> <code>nebula-metad.conf</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line">########## basics ##########</span><br><span class="line"># Whether to run as a daemon process</span><br><span class="line">--daemonize=true</span><br><span class="line"># The file to host the process id</span><br><span class="line">--pid_file=pids/nebula-storaged.pid</span><br><span class="line"># Whether to use the configuration obtained from the configuration file</span><br><span class="line">--local_config=true</span><br><span class="line"></span><br><span class="line">########## logging ##########</span><br><span class="line"># The directory to host logging files</span><br><span class="line">--log_dir=/home/nebula/storage/logs</span><br><span class="line"># Log level, 0, 1, 2, 3 for INFO, WARNING, ERROR, FATAL respectively</span><br><span class="line">--minloglevel=0</span><br><span class="line"># Verbose log level, 1, 2, 3, 4, the higher of the level, the more verbose of the logging</span><br><span class="line">--v=0</span><br><span class="line"># Maximum seconds to buffer the log messages</span><br><span class="line">--logbufsecs=0</span><br><span class="line"># Whether to redirect stdout and stderr to separate output files</span><br><span class="line">--redirect_stdout=true</span><br><span class="line"># Destination filename of stdout and stderr, which will also reside in log_dir.</span><br><span class="line">--stdout_log_file=storaged-stdout.log</span><br><span class="line">--stderr_log_file=storaged-stderr.log</span><br><span class="line"># Copy log messages at or above this level to stderr in addition to logfiles. The numbers of severity levels INFO, WARNING, ERROR, and FATAL are 0, 1, 2, and 3, respectively.</span><br><span class="line">--stderrthreshold=3</span><br><span class="line"># Wether logging files&apos; name contain time stamp.</span><br><span class="line">--timestamp_in_logfile_name=true</span><br><span class="line"></span><br><span class="line">########## networking ##########</span><br><span class="line"># Comma separated Meta server addresses</span><br><span class="line">--meta_server_addrs=192.168.9.103:9559</span><br><span class="line"># Local IP used to identify the nebula-storaged process.</span><br><span class="line"># Change it to an address other than loopback if the service is distributed or</span><br><span class="line"># will be accessed remotely.</span><br><span class="line">--local_ip=192.168.9.103</span><br><span class="line"># Storage daemon listening port</span><br><span class="line">--port=9779</span><br><span class="line"># HTTP service ip</span><br><span class="line">--ws_ip=0.0.0.0</span><br><span class="line"># HTTP service port</span><br><span class="line">--ws_http_port=19779</span><br><span class="line"># heartbeat with meta service</span><br><span class="line">--heartbeat_interval_secs=10</span><br><span class="line"></span><br><span class="line">######### Raft #########</span><br><span class="line"># Raft election timeout</span><br><span class="line">--raft_heartbeat_interval_secs=30</span><br><span class="line"># RPC timeout for raft client (ms)</span><br><span class="line">--raft_rpc_timeout_ms=500</span><br><span class="line">## recycle Raft WAL</span><br><span class="line">--wal_ttl=14400</span><br><span class="line"></span><br><span class="line">########## Disk ##########</span><br><span class="line"># Root data path. Split by comma. e.g. --data_path=/disk1/path1/,/disk2/path2/</span><br><span class="line"># One path per Rocksdb instance.</span><br><span class="line">--data_path=/home/nebula/data/storage</span><br><span class="line"></span><br><span class="line"># Minimum reserved bytes of each data path</span><br><span class="line">--minimum_reserved_bytes=268435456</span><br><span class="line"></span><br><span class="line"># The default reserved bytes for one batch operation</span><br><span class="line">--rocksdb_batch_size=4096</span><br><span class="line"># The default block cache size used in BlockBasedTable.</span><br><span class="line"># The unit is MB.</span><br><span class="line">--rocksdb_block_cache=4</span><br><span class="line"># The type of storage engine, `rocksdb&apos;, `memory&apos;, etc.</span><br><span class="line">--engine_type=rocksdb</span><br><span class="line"></span><br><span class="line"># Compression algorithm, options: no,snappy,lz4,lz4hc,zlib,bzip2,zstd</span><br><span class="line"># For the sake of binary compatibility, the default value is snappy.</span><br><span class="line"># Recommend to use:</span><br><span class="line">#   * lz4 to gain more CPU performance, with the same compression ratio with snappy</span><br><span class="line">#   * zstd to occupy less disk space</span><br><span class="line">#   * lz4hc for the read-heavy write-light scenario</span><br><span class="line">--rocksdb_compression=lz4</span><br><span class="line"></span><br><span class="line"># Set different compressions for different levels</span><br><span class="line"># For example, if --rocksdb_compression is snappy,</span><br><span class="line"># &quot;no:no:lz4:lz4::zstd&quot; is identical to &quot;no:no:lz4:lz4:snappy:zstd:snappy&quot;</span><br><span class="line"># In order to disable compression for level 0/1, set it to &quot;no:no&quot;</span><br><span class="line">--rocksdb_compression_per_level=</span><br><span class="line"></span><br><span class="line"># Whether or not to enable rocksdb&apos;s statistics, disabled by default</span><br><span class="line">--enable_rocksdb_statistics=false</span><br><span class="line"></span><br><span class="line"># Statslevel used by rocksdb to collection statistics, optional values are</span><br><span class="line">#   * kExceptHistogramOrTimers, disable timer stats, and skip histogram stats</span><br><span class="line">#   * kExceptTimers, Skip timer stats</span><br><span class="line">#   * kExceptDetailedTimers, Collect all stats except time inside mutex lock AND time spent on compression.</span><br><span class="line">#   * kExceptTimeForMutex, Collect all stats except the counters requiring to get time inside the mutex lock.</span><br><span class="line">#   * kAll, Collect all stats</span><br><span class="line">--rocksdb_stats_level=kExceptHistogramOrTimers</span><br><span class="line"></span><br><span class="line"># Whether or not to enable rocksdb&apos;s prefix bloom filter, enabled by default.</span><br><span class="line">--enable_rocksdb_prefix_filtering=true</span><br><span class="line"># Whether or not to enable rocksdb&apos;s whole key bloom filter, disabled by default.</span><br><span class="line">--enable_rocksdb_whole_key_filtering=false</span><br><span class="line"></span><br><span class="line">############## rocksdb Options ##############</span><br><span class="line"># rocksdb DBOptions in json, each name and value of option is a string, given as &quot;option_name&quot;:&quot;option_value&quot; separated by comma</span><br><span class="line">--rocksdb_db_options=&#123;&#125;</span><br><span class="line"># rocksdb ColumnFamilyOptions in json, each name and value of option is string, given as &quot;option_name&quot;:&quot;option_value&quot; separated by comma</span><br><span class="line">--rocksdb_column_family_options=&#123;&quot;write_buffer_size&quot;:&quot;67108864&quot;,&quot;max_write_buffer_number&quot;:&quot;4&quot;,&quot;max_bytes_for_level_base&quot;:&quot;268435456&quot;&#125;</span><br><span class="line"># rocksdb BlockBasedTableOptions in json, each name and value of option is string, given as &quot;option_name&quot;:&quot;option_value&quot; separated by comma</span><br><span class="line">--rocksdb_block_based_table_options=&#123;&quot;block_size&quot;:&quot;8192&quot;&#125;</span><br><span class="line"></span><br><span class="line">############### misc ####################</span><br><span class="line"># Whether turn on query in multiple thread</span><br><span class="line">--query_concurrently=true</span><br><span class="line"># Whether remove outdated space data</span><br><span class="line">--auto_remove_invalid_space=true</span><br><span class="line"># Network IO threads number</span><br><span class="line">--num_io_threads=16</span><br><span class="line"># Max active connections for all networking threads. 0 means no limit.</span><br><span class="line"># Max connections for each networking thread = num_max_connections / num_netio_threads</span><br><span class="line">--num_max_connections=0</span><br><span class="line"># Worker threads number to handle request</span><br><span class="line">--num_worker_threads=32</span><br><span class="line"># Maximum subtasks to run admin jobs concurrently</span><br><span class="line">--max_concurrent_subtasks=10</span><br><span class="line"># The rate limit in bytes when leader synchronizes snapshot data</span><br><span class="line">--snapshot_part_rate_limit=10485760</span><br><span class="line"># The amount of data sent in each batch when leader synchronizes snapshot data</span><br><span class="line">--snapshot_batch_size=1048576</span><br><span class="line"># The rate limit in bytes when leader synchronizes rebuilding index</span><br><span class="line">--rebuild_index_part_rate_limit=4194304</span><br><span class="line"># The amount of data sent in each batch when leader synchronizes rebuilding index</span><br><span class="line">--rebuild_index_batch_size=1048576</span><br><span class="line"></span><br><span class="line">########## memory tracker ##########</span><br><span class="line"># trackable memory ratio (trackable_memory / (total_memory - untracked_reserved_memory) )</span><br><span class="line">--memory_tracker_limit_ratio=0.8</span><br><span class="line"># untracked reserved memory in Mib</span><br><span class="line">--memory_tracker_untracked_reserved_memory_mb=50</span><br><span class="line"></span><br><span class="line"># enable log memory tracker stats periodically</span><br><span class="line">--memory_tracker_detail_log=false</span><br><span class="line"># log memory tacker stats interval in milliseconds</span><br><span class="line">--memory_tracker_detail_log_interval_ms=60000</span><br><span class="line"></span><br><span class="line"># enable memory background purge (if jemalloc is used)</span><br><span class="line">--memory_purge_enabled=true</span><br><span class="line"># memory background purge interval in seconds</span><br><span class="line">--memory_purge_interval_seconds=10</span><br></pre></td></tr></table></figure><h3 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h3><p>回到nebula根目录</p><p><code>cd ..</code></p><p>执行启动脚本</p><p><code>./scripts/nebula.service start all</code></p><p>查看服务状态</p><p><code>./scripts/nebula.service status all</code></p><p>重启服务</p><p><code>./scripts/nebula.service restart all</code></p><p>关闭服务</p><p><code>./scripts/nebula.service stop all</code></p><h2 id="nebula-graph-studio部署"><a href="#nebula-graph-studio部署" class="headerlink" title="nebula-graph-studio部署"></a>nebula-graph-studio部署</h2><p>v3.2.3下载地址（免费使用图探索的最高社区版本）</p><p><a href="https://github.com/vesoft-inc/nebula-studio/releases/tag/v3.2.3" target="_blank" rel="noopener">https://github.com/vesoft-inc/nebula-studio/releases/tag/v3.2.3</a></p><p>源码安装后，进入对应目录，执行启动脚本</p><p><code>./service</code></p><p><img src="/2023/09/27/Nebula3单机版快速安装/A.png" alt></p><p><img src="/2023/09/27/Nebula3单机版快速安装/B.png" alt></p>]]></content>
    
    
    <summary type="html">&lt;font color=&quot;gold&quot;&gt;&lt;strong&gt;·&lt;/strong&gt; 系统：CentOS7&lt;/font&gt;

&lt;font color=&quot;gold&quot;&gt;&lt;strong&gt;·&lt;/strong&gt; nebula版本：3.6.0（开源社区版）&lt;/font&gt;

&lt;font color=&quot;gold&quot;&gt;&lt;strong&gt;·&lt;/strong&gt; nebula-graph-studio版本：3.2.3&lt;/font&gt;

&lt;h2 id=&quot;单机部署&quot;&gt;&lt;a href=&quot;#单机部署&quot; class=&quot;headerlink&quot; title=&quot;单机部署&quot;&gt;&lt;/a&gt;单机部署&lt;/h2&gt;&lt;h3 id=&quot;tar包源码下载&quot;&gt;&lt;a href=&quot;#tar包源码下载&quot; class=&quot;headerlink&quot; title=&quot;tar包源码下载&quot;&gt;&lt;/a&gt;tar包源码下载&lt;/h3&gt;&lt;p&gt;&lt;code&gt;wget https://oss-cdn.nebula-graph.com.cn/package/3.6.0/nebula-graph-3.6.0.el7.x86_64.tar.gz&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&quot;解压并重命名&quot;&gt;&lt;a href=&quot;#解压并重命名&quot; class=&quot;headerlink&quot; title=&quot;解压并重命名&quot;&gt;&lt;/a&gt;解压并重命名&lt;/h3&gt;&lt;p&gt;&lt;code&gt;tar -xvzf nebula-graph-3.6.0.el7.x86_64.tar.gz&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;mv nebula-graph-3.6.0.el7.x86_64 nebula&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&quot;修改配置文件&quot;&gt;&lt;a href=&quot;#修改配置文件&quot; class=&quot;headerlink&quot; title=&quot;修改配置文件&quot;&gt;&lt;/a&gt;修改配置文件&lt;/h3&gt;&lt;p&gt;&lt;code&gt;cd nebula/etc&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;mv nebula-graphd.conf.default nebula-graphd.conf&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;mv nebula-metad.conf.default nebula-metad.conf&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;mv nebula-storaged.conf nebula-storaged.conf&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;修改对应文件存储位置、节点ip地址，集群同理&lt;/p&gt;</summary>
    
    
    
    <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
    
    <category term="database" scheme="http://yoursite.com/tags/database/"/>
    
    <category term="nebula" scheme="http://yoursite.com/tags/nebula/"/>
    
  </entry>
  
  <entry>
    <title>LiveGBS国标GB/T28181视频流媒体平台</title>
    <link href="http://yoursite.com/2023/08/18/LiveGBS%E5%9B%BD%E6%A0%87GB-T28181%E8%A7%86%E9%A2%91%E6%B5%81%E5%AA%92%E4%BD%93%E5%B9%B3%E5%8F%B0/"/>
    <id>http://yoursite.com/2023/08/18/LiveGBS%E5%9B%BD%E6%A0%87GB-T28181%E8%A7%86%E9%A2%91%E6%B5%81%E5%AA%92%E4%BD%93%E5%B9%B3%E5%8F%B0/</id>
    <published>2023-08-18T08:34:45.000Z</published>
    <updated>2023-08-18T08:35:21.653Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><h2 id="软件包下载"><a href="#软件包下载" class="headerlink" title="软件包下载"></a>软件包下载</h2><p>LiveGBS GB28181流媒体服务下载地址：<a href="https://www.liveqing.com/docs/download/LiveGBS.html#%E7%89%88%E6%9C%AC%E4%B8%8B%E8%BD%BD" target="_blank" rel="noopener">https://www.liveqing.com/docs/download/LiveGBS.html#%E7%89%88%E6%9C%AC%E4%B8%8B%E8%BD%BD</a></p><p>选择windows版本的<code>LiveGBS 信令服务</code>和<code>LiveGBS</code>流媒体服务，免费版授权周期为26天，届时需要手动更新软件服务</p><p><img src="/2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/A.png" style="zoom:80%;"></p><h2 id="安装LiveGBS-GB28281"><a href="#安装LiveGBS-GB28281" class="headerlink" title="安装LiveGBS GB28281"></a>安装LiveGBS GB28281</h2><p>解压下载好的软件包，分别启动<code>LiveCMS.exe</code>和<code>LiveSMS.exe</code>，如果有默认端口被占用的情况可以修改对应的<code>livecms.ini</code>或<code>livesms.ini</code>配置文件，这里我将LiveGBS的默认端口从10000修改为10005</p><p>成功启动后后台出现livecms和livesms的图标</p><p><img src="/2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/B.png" alt></p><a id="more"></a><h3 id="配置LiveGBS"><a href="#配置LiveGBS" class="headerlink" title="配置LiveGBS"></a>配置LiveGBS</h3><p>进入<code>&lt;host&gt;:10005</code>，点击基础配置，修改信令服务配置和流媒体服务配置</p><p><img src="/2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/C.png" alt></p><p><img src="/2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/D.png" alt></p><h3 id="配置视频设备"><a href="#配置视频设备" class="headerlink" title="配置视频设备"></a>配置视频设备</h3><p>进入网络连接，选择对应的以太网，右键<code>属性</code>，选择<code>Internet 协议版本 4 (TCP/IPv4)</code>，选择<code>高级</code>，添加近端设备和摄像头的网段</p><p><img src="/2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/E.png" alt></p><p>进入设备IP，修改对应的网络配置、近端配置和摄像头配置</p><p><img src="/2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/F.png" alt></p><p><img src="/2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/G.png" alt></p><p><img src="/2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/H.png" alt></p><p>配置完成以后点击用户<code>admin</code>，选择重启设备</p><p><img src="/2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/I.png" alt></p><h2 id="视频拉流"><a href="#视频拉流" class="headerlink" title="视频拉流"></a>视频拉流</h2><p>回到<code>&lt;host&gt;:10005</code>，此时已经可以访问摄像头</p><p><img src="/2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/J.png" alt></p><p>点击国标设备，查看通道</p><p><img src="/2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/K.png" alt></p><p>看到已经配置好的摄像头信息</p><p><img src="/2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/L.png" alt></p><p><img src="/2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/M.png" alt></p><p>可从右下角获取视频拉流</p><p><img src="/2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/N.png" alt></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;软件包下载&quot;&gt;&lt;a href=&quot;#软件包下载&quot; class=&quot;headerlink&quot; title=&quot;软件包下载&quot;&gt;&lt;/a&gt;软件包下载&lt;/h2&gt;&lt;p&gt;LiveGBS GB28181流媒体服务下载地址：&lt;a href=&quot;https://www.liveqing.com/docs/download/LiveGBS.html#%E7%89%88%E6%9C%AC%E4%B8%8B%E8%BD%BD&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.liveqing.com/docs/download/LiveGBS.html#%E7%89%88%E6%9C%AC%E4%B8%8B%E8%BD%BD&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;选择windows版本的&lt;code&gt;LiveGBS 信令服务&lt;/code&gt;和&lt;code&gt;LiveGBS&lt;/code&gt;流媒体服务，免费版授权周期为26天，届时需要手动更新软件服务&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/A.png&quot; style=&quot;zoom:80%;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;安装LiveGBS-GB28281&quot;&gt;&lt;a href=&quot;#安装LiveGBS-GB28281&quot; class=&quot;headerlink&quot; title=&quot;安装LiveGBS GB28281&quot;&gt;&lt;/a&gt;安装LiveGBS GB28281&lt;/h2&gt;&lt;p&gt;解压下载好的软件包，分别启动&lt;code&gt;LiveCMS.exe&lt;/code&gt;和&lt;code&gt;LiveSMS.exe&lt;/code&gt;，如果有默认端口被占用的情况可以修改对应的&lt;code&gt;livecms.ini&lt;/code&gt;或&lt;code&gt;livesms.ini&lt;/code&gt;配置文件，这里我将LiveGBS的默认端口从10000修改为10005&lt;/p&gt;
&lt;p&gt;成功启动后后台出现livecms和livesms的图标&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/B.png&quot; alt&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="计算机视觉" scheme="http://yoursite.com/categories/计算机视觉/"/>
    
    
    <category term="LiveGBS" scheme="http://yoursite.com/tags/LiveGBS/"/>
    
  </entry>
  
  <entry>
    <title>paddleDetection-视频OCR</title>
    <link href="http://yoursite.com/2023/08/14/paddleDetection-%E8%A7%86%E9%A2%91OCR/"/>
    <id>http://yoursite.com/2023/08/14/paddleDetection-%E8%A7%86%E9%A2%91OCR/</id>
    <published>2023-08-14T03:15:55.000Z</published>
    <updated>2023-08-29T09:18:54.671Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><h2 id="PPOCR-V4"><a href="#PPOCR-V4" class="headerlink" title="PPOCR_V4"></a>PPOCR_V4</h2><p>安装百度最新ppocr_v4库，使用虚拟环境为py39_vio，本虚拟环境不可与人脸识别（py38_arcface）兼容（opencv版本不兼容）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install paddleocr --user -i https://mirror.baidu.com/pypi/simple</span><br></pre></td></tr></table></figure><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p><code>cfg_utils.py</code>新增cfg<code>--ocr</code>，设置True为开启，默认False</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parser.add_argument(</span><br><span class="line">    <span class="string">"--ocr"</span>,</span><br><span class="line">    type=bool,</span><br><span class="line">    default=<span class="literal">False</span>,</span><br><span class="line">    help=<span class="string">"use paddlepaddle-ocr"</span>)</span><br></pre></td></tr></table></figure><p><code>pipeline.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> python.visualize <span class="keyword">import</span> visualize_box_mask, visualize_attr, visualize_pose, visualize_action, visualize_vehicleplate, visualize_vehiclepress, visualize_lane, visualize_vehicle_retrograde, visualize_ocr</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PipePredictor</span><span class="params">(object)</span>:</span>  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, args, cfg, is_video=True, multi_camera=False)</span>:</span></span><br><span class="line">    self.ocr = args.ocr</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_video</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                    image_rgb,</span></span></span><br><span class="line"><span class="function"><span class="params">                    result,</span></span></span><br><span class="line"><span class="function"><span class="params">                    collector,</span></span></span><br><span class="line"><span class="function"><span class="params">                    frame_id,</span></span></span><br><span class="line"><span class="function"><span class="params">                    fps,</span></span></span><br><span class="line"><span class="function"><span class="params">                    entrance=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    records=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    center_traj=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    do_illegal_parking_recognition=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                    illegal_parking_dict=None)</span>:</span></span><br><span class="line">    image = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)</span><br><span class="line">    mot_res = copy.deepcopy(result.get(<span class="string">'mot'</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.ocr:</span><br><span class="line">        lock.acquire() <span class="comment"># 加锁，paddleOCR是线程不安全的</span></span><br><span class="line">        ocr_result = ocr.ocr(image, cls=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line">        lock.release()</span><br><span class="line">        ocr_boxes = [line[<span class="number">0</span>] <span class="keyword">for</span> line <span class="keyword">in</span> ocr_result]</span><br><span class="line">        ocr_txts = [line[<span class="number">1</span>][<span class="number">0</span>] <span class="keyword">for</span> line <span class="keyword">in</span> ocr_result]</span><br><span class="line">        ocr_scores = [line[<span class="number">1</span>][<span class="number">1</span>] <span class="keyword">for</span> line <span class="keyword">in</span> ocr_result]</span><br><span class="line">        </span><br><span class="line">        image = visualize_ocr(image, ocr_boxes, ocr_txts, ocr_scores)</span><br></pre></td></tr></table></figure><p><code>visualize.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_ocr</span><span class="params">(im, boxes, texts, score)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(im, str):</span><br><span class="line">        im = Image.open(im)</span><br><span class="line">        im = np.ascontiguousarray(np.copy(im))</span><br><span class="line">        im = cv2.cvtColor(im, cv2.COLOR_RGB2BGR)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        im = np.ascontiguousarray(np.copy(im))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建透明图层，为图像添加文字水印</span></span><br><span class="line">    im = Image.fromarray(im)</span><br><span class="line">    im = im.convert(<span class="string">'RGBA'</span>)</span><br><span class="line">    im_canvas = Image.new(<span class="string">'RGBA'</span>, im.size, (<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, res <span class="keyword">in</span> enumerate(texts):</span><br><span class="line">        <span class="keyword">if</span> boxes <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            box = boxes[i]</span><br><span class="line">            text = res</span><br><span class="line">            <span class="keyword">if</span> text == <span class="string">""</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            text_scale = max(<span class="number">1.0</span>, int(box[<span class="number">2</span>][<span class="number">1</span>] - box[<span class="number">1</span>][<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">            draw = ImageDraw.Draw(im_canvas)</span><br><span class="line">            draw.text(</span><br><span class="line">                (box[<span class="number">0</span>][<span class="number">0</span>], box[<span class="number">0</span>][<span class="number">1</span>]),</span><br><span class="line">                text,</span><br><span class="line">                font=ImageFont.truetype(font_file, size=int(text_scale)),</span><br><span class="line">                fill=(<span class="number">255</span>, <span class="number">255</span>, <span class="number">0</span>, <span class="number">85</span>)) <span class="comment"># 第四位是透明度</span></span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                draw.rectangle(</span><br><span class="line">                    ((box[<span class="number">0</span>][<span class="number">0</span>], box[<span class="number">0</span>][<span class="number">1</span>]), (box[<span class="number">2</span>][<span class="number">0</span>], box[<span class="number">2</span>][<span class="number">1</span>])),</span><br><span class="line">                    fill=<span class="literal">None</span>,</span><br><span class="line">                    outline=(<span class="number">255</span>, <span class="number">255</span>, <span class="number">0</span>),</span><br><span class="line">                    width=<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">except</span> ValueError:</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 复合图层</span></span><br><span class="line">    im = Image.alpha_composite(im, im_canvas)</span><br><span class="line">    im = im.convert(<span class="string">'RGB'</span>)</span><br><span class="line">    <span class="comment"># 还原连续存储数组</span></span><br><span class="line">    im = np.ascontiguousarray(np.copy(im))</span><br><span class="line">    <span class="keyword">return</span> im</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><p>在<code>deploy/pipeline/config</code>下创建视频ocr的yml文件<code>infer_cfg_ppocr.yml</code>，写入基本参数</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">crop_thresh:</span> <span class="number">0.5</span></span><br><span class="line"><span class="attr">visual:</span> <span class="literal">True</span></span><br><span class="line"><span class="attr">warmup_frame:</span> <span class="number">50</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python deploy/pipeline/pipeline.py --config deploy/pipeline/config/infer_cfg_ppocr.yml --device=gpu --video_file=demo_input/car_t1.mp4 --output_dir=demo_output --ocr=True</span><br></pre></td></tr></table></figure><p><img src="/2023/08/14/paddleDetection-视频OCR/A.png" alt></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;PPOCR-V4&quot;&gt;&lt;a href=&quot;#PPOCR-V4&quot; class=&quot;headerlink&quot; title=&quot;PPOCR_V4&quot;&gt;&lt;/a&gt;PPOCR_V4&lt;/h2&gt;&lt;p&gt;安装百度最新ppocr_v4库，使用虚拟环境为py39_vio，本虚拟环境不可与人脸识别（py38_arcface）兼容（opencv版本不兼容）&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;pip install paddleocr --user -i https://mirror.baidu.com/pypi/simple&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;代码&quot;&gt;&lt;a href=&quot;#代码&quot; class=&quot;headerlink&quot; title=&quot;代码&quot;&gt;&lt;/a&gt;代码&lt;/h2&gt;&lt;p&gt;&lt;code&gt;cfg_utils.py&lt;/code&gt;新增cfg&lt;code&gt;--ocr&lt;/code&gt;，设置True为开启，默认False&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;parser.add_argument(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;--ocr&quot;&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    type=bool,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    default=&lt;span class=&quot;literal&quot;&gt;False&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    help=&lt;span class=&quot;string&quot;&gt;&quot;use paddlepaddle-ocr&quot;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;pipeline.py&lt;/code&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; python.visualize &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; visualize_box_mask, visualize_attr, visualize_pose, visualize_action, visualize_vehicleplate, visualize_vehiclepress, visualize_lane, visualize_vehicle_retrograde, visualize_ocr&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;PipePredictor&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(object)&lt;/span&gt;:&lt;/span&gt;  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, args, cfg, is_video=True, multi_camera=False)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    	self.ocr = args.ocr&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;visualize_video&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self,&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;                    image_rgb,&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;                    result,&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;                    collector,&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;                    frame_id,&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;                    fps,&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;                    entrance=None,&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;                    records=None,&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;                    center_traj=None,&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;                    do_illegal_parking_recognition=False,&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;                    illegal_parking_dict=None)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    image = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    mot_res = copy.deepcopy(result.get(&lt;span class=&quot;string&quot;&gt;&#39;mot&#39;&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; self.ocr:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        lock.acquire() &lt;span class=&quot;comment&quot;&gt;# 加锁，paddleOCR是线程不安全的&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ocr_result = ocr.ocr(image, cls=&lt;span class=&quot;literal&quot;&gt;True&lt;/span&gt;)[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        lock.release()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ocr_boxes = [line[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;] &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; line &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; ocr_result]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ocr_txts = [line[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;] &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; line &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; ocr_result]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ocr_scores = [line[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;] &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; line &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; ocr_result]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        image = visualize_ocr(image, ocr_boxes, ocr_txts, ocr_scores)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;visualize.py&lt;/code&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;33&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;34&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;35&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;36&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;37&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;38&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;39&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;40&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;41&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;42&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;43&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;visualize_ocr&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(im, boxes, texts, score)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; isinstance(im, str):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        im = Image.open(im)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        im = np.ascontiguousarray(np.copy(im))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        im = cv2.cvtColor(im, cv2.COLOR_RGB2BGR)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        im = np.ascontiguousarray(np.copy(im))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;# 创建透明图层，为图像添加文字水印&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    im = Image.fromarray(im)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    im = im.convert(&lt;span class=&quot;string&quot;&gt;&#39;RGBA&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    im_canvas = Image.new(&lt;span class=&quot;string&quot;&gt;&#39;RGBA&#39;&lt;/span&gt;, im.size, (&lt;span class=&quot;number&quot;&gt;255&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;255&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;255&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; i, res &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; enumerate(texts):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; boxes &lt;span class=&quot;keyword&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;literal&quot;&gt;None&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            box = boxes[i]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            text = res&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; text == &lt;span class=&quot;string&quot;&gt;&quot;&quot;&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                &lt;span class=&quot;keyword&quot;&gt;continue&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            text_scale = max(&lt;span class=&quot;number&quot;&gt;1.0&lt;/span&gt;, int(box[&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;] - box[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;]))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            draw = ImageDraw.Draw(im_canvas)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            draw.text(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                (box[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;], box[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;]),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                text,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                font=ImageFont.truetype(font_file, size=int(text_scale)),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                fill=(&lt;span class=&quot;number&quot;&gt;255&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;255&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;85&lt;/span&gt;)) &lt;span class=&quot;comment&quot;&gt;# 第四位是透明度&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;try&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                draw.rectangle(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                    ((box[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;], box[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;]), (box[&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;], box[&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;])),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                    fill=&lt;span class=&quot;literal&quot;&gt;None&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                    outline=(&lt;span class=&quot;number&quot;&gt;255&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;255&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                    width=&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;except&lt;/span&gt; ValueError:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                &lt;span class=&quot;keyword&quot;&gt;pass&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;# 复合图层&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    im = Image.alpha_composite(im, im_canvas)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    im = im.convert(&lt;span class=&quot;string&quot;&gt;&#39;RGB&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;# 还原连续存储数组&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    im = np.ascontiguousarray(np.copy(im))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; im&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    
    <category term="OCR" scheme="http://yoursite.com/tags/OCR/"/>
    
    <category term="paddlepaddle" scheme="http://yoursite.com/tags/paddlepaddle/"/>
    
  </entry>
  
  <entry>
    <title>paddleDetection:OpenCV检测框转中文</title>
    <link href="http://yoursite.com/2023/08/09/paddleDetection-OpenCV%E6%A3%80%E6%B5%8B%E6%A1%86%E8%BD%AC%E4%B8%AD%E6%96%87/"/>
    <id>http://yoursite.com/2023/08/09/paddleDetection-OpenCV%E6%A3%80%E6%B5%8B%E6%A1%86%E8%BD%AC%E4%B8%AD%E6%96%87/</id>
    <published>2023-08-09T08:29:45.000Z</published>
    <updated>2023-08-10T02:50:16.225Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><font color="gold">注：OpenCV不能直接显示中文，通过PIL转换会损失一部分算力性能</font><h2 id="修改源码（可视化）"><a href="#修改源码（可视化）" class="headerlink" title="修改源码（可视化）"></a>修改源码（可视化）</h2><p><code>./deploy/python/visualize.py</code></p><h3 id="增加导入字体库和字体文件"><a href="#增加导入字体库和字体文件" class="headerlink" title="增加导入字体库和字体文件"></a>增加导入字体库和字体文件</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image, ImageDraw, ImageFile, ImageFont</span><br><span class="line"></span><br><span class="line">font_file = <span class="string">'/exp/work/video/PaddleDetection/deploy/pipeline/SourceHanSansCN-Medium.otf'</span></span><br></pre></td></tr></table></figure><h3 id="visualize-attr"><a href="#visualize-attr" class="headerlink" title="visualize_attr"></a>visualize_attr</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_attr</span><span class="params">(im, results, boxes=None, is_mtmct=False)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(im, str):</span><br><span class="line">        im = Image.open(im)</span><br><span class="line">        im = np.ascontiguousarray(np.copy(im))</span><br><span class="line">        im = cv2.cvtColor(im, cv2.COLOR_RGB2BGR)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        im = np.ascontiguousarray(np.copy(im))</span><br><span class="line"></span><br><span class="line">    line_inter = im.shape[<span class="number">0</span>] / <span class="number">40.</span></span><br><span class="line">    text_scale = max(<span class="number">0.5</span>, im.shape[<span class="number">0</span>] / <span class="number">100.</span>)</span><br><span class="line">    <span class="comment"># 将nparray图像转PIL图像</span></span><br><span class="line">    im = Image.fromarray(im)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, res <span class="keyword">in</span> enumerate(results):</span><br><span class="line">        print(i, res)</span><br><span class="line">        <span class="keyword">if</span> boxes <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            text_w = <span class="number">3</span></span><br><span class="line">            text_h = <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> is_mtmct:</span><br><span class="line">            box = boxes[i]  <span class="comment"># multi camera, bbox shape is x,y, w,h</span></span><br><span class="line">            text_w = int(box[<span class="number">0</span>]) + <span class="number">3</span></span><br><span class="line">            text_h = int(box[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            box = boxes[i]  <span class="comment"># single camera, bbox shape is 0, 0, x,y, w,h</span></span><br><span class="line">            text_w = int(box[<span class="number">2</span>]) + <span class="number">3</span></span><br><span class="line">            text_h = int(box[<span class="number">3</span>])</span><br><span class="line">        <span class="keyword">for</span> text <span class="keyword">in</span> res:</span><br><span class="line">            text_h += int(line_inter)</span><br><span class="line">            text_loc = (text_w, text_h)</span><br><span class="line">            <span class="comment"># 写入</span></span><br><span class="line">            draw = ImageDraw.Draw(im)</span><br><span class="line">            draw.text(</span><br><span class="line">                text_loc,</span><br><span class="line">                text,</span><br><span class="line">                font=ImageFont.truetype(font_file, size=int(text_scale)), <span class="comment"># 字体位置</span></span><br><span class="line">                fill=(<span class="number">0</span>, <span class="number">255</span>, <span class="number">255</span>))</span><br><span class="line">    <span class="comment"># 还原连续存储数组</span></span><br><span class="line">    im = np.ascontiguousarray(np.copy(im))</span><br><span class="line">    <span class="keyword">return</span> im</span><br></pre></td></tr></table></figure><h3 id="visualize-vehicleplate"><a href="#visualize-vehicleplate" class="headerlink" title="visualize_vehicleplate"></a>visualize_vehicleplate</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_vehicleplate</span><span class="params">(im, results, boxes=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(im, str):</span><br><span class="line">        im = Image.open(im)</span><br><span class="line">        im = np.ascontiguousarray(np.copy(im))</span><br><span class="line">        im = cv2.cvtColor(im, cv2.COLOR_RGB2BGR)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        im = np.ascontiguousarray(np.copy(im))</span><br><span class="line"></span><br><span class="line">    text_scale = max(<span class="number">1.0</span>, im.shape[<span class="number">0</span>] / <span class="number">400.</span>)</span><br><span class="line">    line_inter = im.shape[<span class="number">0</span>] / <span class="number">40.</span></span><br><span class="line">    <span class="comment"># 将nparray图像转PIL图像</span></span><br><span class="line">    im = Image.fromarray(im)</span><br><span class="line">    <span class="keyword">for</span> i, res <span class="keyword">in</span> enumerate(results):</span><br><span class="line">        <span class="keyword">if</span> boxes <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            text_w = <span class="number">3</span></span><br><span class="line">            text_h = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            box = boxes[i]</span><br><span class="line">            text = res</span><br><span class="line">            <span class="keyword">if</span> text == <span class="string">""</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            text_w = int(box[<span class="number">2</span>])</span><br><span class="line">            text_h = int(box[<span class="number">5</span>] + box[<span class="number">3</span>])</span><br><span class="line">            text_loc = (text_w, text_h)</span><br><span class="line">            <span class="comment"># 写入</span></span><br><span class="line">            draw = ImageDraw.Draw(im)</span><br><span class="line">            draw.text(</span><br><span class="line">                text_loc,</span><br><span class="line">                text,</span><br><span class="line">                font=ImageFont.truetype(font_file, size=int(text_scale)), <span class="comment"># 字体位置</span></span><br><span class="line">                fill=(<span class="number">0</span>, <span class="number">255</span>, <span class="number">255</span>))</span><br><span class="line">    <span class="comment"># 还原连续存储数组</span></span><br><span class="line">    im = np.ascontiguousarray(np.copy(im))</span><br><span class="line">    <span class="keyword">return</span> im</span><br></pre></td></tr></table></figure><h3 id="visualize-action"><a href="#visualize-action" class="headerlink" title="visualize_action"></a>visualize_action</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_action</span><span class="params">(im,</span></span></span><br><span class="line"><span class="function"><span class="params">                     mot_boxes,</span></span></span><br><span class="line"><span class="function"><span class="params">                     action_visual_collector=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                     action_text=<span class="string">""</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                     video_action_score=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                     video_action_text=<span class="string">""</span>)</span>:</span></span><br><span class="line">    im = cv2.imread(im) <span class="keyword">if</span> isinstance(im, str) <span class="keyword">else</span> im</span><br><span class="line">    im_h, im_w = im.shape[:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    text_scale = max(<span class="number">1</span>, im.shape[<span class="number">1</span>] / <span class="number">40.</span>)</span><br><span class="line">    text_thickness = <span class="number">2</span></span><br><span class="line">    <span class="comment"># 将nparray图像转PIL图像</span></span><br><span class="line">    im = Image.fromarray(im)</span><br><span class="line">    <span class="keyword">if</span> action_visual_collector:</span><br><span class="line">        id_action_dict = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> collector, action_type <span class="keyword">in</span> zip(action_visual_collector, action_text):</span><br><span class="line">            id_detected = collector.get_visualize_ids()</span><br><span class="line">            <span class="keyword">for</span> pid <span class="keyword">in</span> id_detected:</span><br><span class="line">                id_action_dict[pid] = id_action_dict.get(pid, [])</span><br><span class="line">                id_action_dict[pid].append(action_type)</span><br><span class="line">        <span class="keyword">for</span> mot_box <span class="keyword">in</span> mot_boxes:</span><br><span class="line">            <span class="comment"># mot_box is a format with [mot_id, class, score, xmin, ymin, w, h] </span></span><br><span class="line">            <span class="keyword">if</span> mot_box[<span class="number">0</span>] <span class="keyword">in</span> id_action_dict:</span><br><span class="line">                text_position = (int(mot_box[<span class="number">3</span>] + mot_box[<span class="number">5</span>] * <span class="number">0.75</span>),</span><br><span class="line">                                 int(mot_box[<span class="number">4</span>] - <span class="number">10</span>))</span><br><span class="line">                display_text = <span class="string">', '</span>.join(id_action_dict[mot_box[<span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line">                draw = ImageDraw.Draw(im)</span><br><span class="line">                draw.text(</span><br><span class="line">                    text_position,</span><br><span class="line">                    display_text,</span><br><span class="line">                    font=ImageFont.truetype(size=int(text_scale)),  <span class="comment"># 字体位置</span></span><br><span class="line">                    fill=(<span class="number">0</span>, <span class="number">0</span>, <span class="number">255</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> video_action_score:</span><br><span class="line">        draw = ImageDraw.Draw(im)</span><br><span class="line">        draw.text(</span><br><span class="line">            (int(im_w / <span class="number">2</span>), int(<span class="number">15</span> * text_scale) + <span class="number">5</span>),</span><br><span class="line">            video_action_text + <span class="string">': %.2f'</span> % video_action_score,</span><br><span class="line">            font=ImageFont.truetype(font_file, size=int(text_scale)),  <span class="comment"># 字体位置</span></span><br><span class="line">            fill=(<span class="number">0</span>, <span class="number">0</span>, <span class="number">255</span>))</span><br><span class="line">    <span class="comment"># 还原连续存储数组</span></span><br><span class="line">    im = np.ascontiguousarray(np.copy(im))</span><br><span class="line">    <span class="keyword">return</span> im</span><br></pre></td></tr></table></figure><h2 id="修改源码（属性）"><a href="#修改源码（属性）" class="headerlink" title="修改源码（属性）"></a>修改源码（属性）</h2><p>修改对应中文模块</p><p><code>./deploy/pipeline/</code></p><ul><li>pipeline.py</li></ul><p><code>./deploy/pipeline/ppvehicle</code></p><ul><li>vehicle_attr.py</li><li>vehicle_plate.py</li></ul><p><code>./deploy/pipeline/pphuman</code></p><ul><li>attr_infer.py</li></ul><h2 id="执行脚本"><a href="#执行脚本" class="headerlink" title="执行脚本"></a>执行脚本</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> pphuman</span></span><br><span class="line">python deploy/pipeline/pipeline.py --config deploy/pipeline/config/infer_cfg_pphuman.yml --video_file=demo_input/act1.mp4 --device=gpu --output_dir=demo_output</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> ppvehicle</span></span><br><span class="line">python deploy/pipeline/pipeline.py --config deploy/pipeline/config/infer_cfg_ppvehicle.yml --video_file=demo_input/car_t1.mp4 --device=gpu --output_dir=demo_output</span><br></pre></td></tr></table></figure><p><img src="/2023/08/09/paddleDetection-OpenCV检测框转中文/A.png" alt></p><p><img src="/2023/08/09/paddleDetection-OpenCV检测框转中文/B.png" alt></p>]]></content>
    
    
      
      
    <summary type="html">&lt;script src=&quot;\assets\js\APlayer.min.js&quot;&gt; &lt;/script&gt;&lt;font color=&quot;gold&quot;&gt;注：OpenCV不能直接显示中文，通过PIL转换会损失一部分算力性能&lt;/font&gt;

&lt;h2 id=&quot;修改源码（可视化）&quot;&gt;&lt;a href=&quot;</summary>
      
    
    
    
    <category term="深度学习" scheme="http://yoursite.com/categories/深度学习/"/>
    
    
    <category term="paddlepaddle" scheme="http://yoursite.com/tags/paddlepaddle/"/>
    
    <category term="opencv" scheme="http://yoursite.com/tags/opencv/"/>
    
  </entry>
  
  <entry>
    <title>arcface_paddle</title>
    <link href="http://yoursite.com/2023/08/08/arcface-paddle/"/>
    <id>http://yoursite.com/2023/08/08/arcface-paddle/</id>
    <published>2023-08-08T09:12:11.000Z</published>
    <updated>2023-08-09T10:03:27.504Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><h3 id="GPU（物理）"><a href="#GPU（物理）" class="headerlink" title="GPU（物理）"></a>GPU（物理）</h3><ul><li><p>NVIDIA 3090*2</p></li><li><p>显卡驱动 515.43.04</p></li><li><p>CUDA版本 11.7</p></li><li><p>CUDAtoolkit (cuda_11.7.0_515.43.04_linux)</p></li><li><p>cuDNN (v8.4.1)</p></li><li><p><font color="gold">paddlepaddle 多卡训练需要NCLL支持</font> (ncll v2.12.12 cuda11.7)</p></li></ul><h3 id="paddlepaddle版本"><a href="#paddlepaddle版本" class="headerlink" title="paddlepaddle版本"></a>paddlepaddle版本</h3><ul><li>paddlepaddle-gpu==2.2.0rc0（虚拟环境cuda11.2）</li></ul><h3 id="python环境"><a href="#python环境" class="headerlink" title="python环境"></a>python环境</h3><ul><li><p>CentOS7.9</p></li><li><p>anaconda3</p></li><li><p>python3.8</p></li></ul><h2 id="anaconda安装insightface"><a href="#anaconda安装insightface" class="headerlink" title="anaconda安装insightface"></a>anaconda安装insightface</h2><p><font color="gold">重要：pillow版本建议选择9.5 否则过高会导致安装insightface报错</font> （错误原因：pillow10移除了getsize方法，需要修改对应位置源码为<code>getbbox</code>或 <code>getlength</code>）</p><p>告警信息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tools/test_recognition.py:<span class="number">627</span>: DeprecationWarning: getsize <span class="keyword">is</span> deprecated <span class="keyword">and</span> will be removed <span class="keyword">in</span> Pillow <span class="number">10</span> (<span class="number">2023</span><span class="number">-07</span><span class="number">-01</span>). Use getbbox <span class="keyword">or</span> getlength instead.</span><br><span class="line">  tw = font.getsize(text)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><h4 id="环境安装"><a href="#环境安装" class="headerlink" title="环境安装"></a>环境安装</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> .</span></span><br><span class="line">conda install paddlepaddle-gpu==2.2.0rc0 cudatoolkit=11.2 -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/Paddle/ -c conda-forge</span><br><span class="line"><span class="meta">#</span><span class="bash"> insightface</span></span><br><span class="line">pip install -r requirements.txt -i https://mirror.baidu.com/pypi/simple</span><br><span class="line"><span class="meta">#</span><span class="bash"> insightface/recongition/arcface_paddle/</span></span><br><span class="line">pip install -r requirement.txt -i https://mirror.baidu.com/pypi/simple</span><br><span class="line"><span class="meta">#</span><span class="bash"> insightface-paddle</span></span><br><span class="line">pip install insightface-paddle -i https://mirror.baidu.com/pypi/simple</span><br></pre></td></tr></table></figure><a id="more"></a><h4 id="获取数据集"><a href="#获取数据集" class="headerlink" title="获取数据集"></a>获取数据集</h4><p><a href="https://github.com/deepinsight/insightface/tree/master/recognition/_datasets_" target="_blank" rel="noopener">https://github.com/deepinsight/insightface/tree/master/recognition/_datasets_</a></p><ul><li>MS1M_v2: MS1M-ArcFace</li><li>MS1M_v3: MS1M-RetinaFace</li></ul><p>从 MXNet 格式数据集抽取图像</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python tools/mx_recordio_2_images.py --root_dir ms1m-retinaface-t1/ --output_dir MS1M_v3/</span><br></pre></td></tr></table></figure><p>数据抽取完成后，格式如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">MS1M_v3</span><br><span class="line">|_ images</span><br><span class="line">|  |_ 00000001.jpg</span><br><span class="line">|  |_ ...</span><br><span class="line">|  |_ 05179510.jpg</span><br><span class="line">|_ label.txt</span><br><span class="line">|_ agedb_30.bin</span><br><span class="line">|_ cfp_ff.bin</span><br><span class="line">|_ cfp_fp.bin</span><br><span class="line">|_ lfw.bin</span><br></pre></td></tr></table></figure><p>标签数据格式如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 图像路径与标签的分隔符: &quot;\t&quot;</span><br><span class="line"># 以下是 label.txt 每行的格式</span><br><span class="line">images/00000001.jpg 0</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h4 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h4><p>使用双卡</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export CUDA_VISIBLE_DEVICES=0,1</span><br></pre></td></tr></table></figure><p>训练脚本<code>scripts/train_static.sh</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 降级scipy,scipy版本过高会报错</span></span><br><span class="line">pip install scipy==1.7.1 -i https://mirror.baidu.com/pypi/simple</span><br></pre></td></tr></table></figure><p>训练静态模型</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">python -m paddle.distributed.launch --gpus=1 tools/train.py \</span><br><span class="line">    --config_file configs/ms1mv3_r50.py \</span><br><span class="line">    --is_static True \</span><br><span class="line">    --backbone FresResNet50 \</span><br><span class="line">    --classifier LargeScaleClassifier \</span><br><span class="line">    --embedding_size 512 \</span><br><span class="line">    --model_parallel True \</span><br><span class="line">    --dropout 0.0 \</span><br><span class="line">    --sample_ratio 0.1 \</span><br><span class="line">    --loss ArcFace \</span><br><span class="line">    --batch_size 64 \</span><br><span class="line">    --dataset MS1M_v3 \</span><br><span class="line">    --num_classes 93431 \</span><br><span class="line">    --data_dir MS1M_v3/ \</span><br><span class="line">    --label_file MS1M_v3/label.txt \</span><br><span class="line">    --is_bin False \</span><br><span class="line">    --log_interval_step 100 \</span><br><span class="line">    --validation_interval_step 2000 \</span><br><span class="line">    --fp16 True \</span><br><span class="line">    --use_dynamic_loss_scaling True \</span><br><span class="line">    --init_loss_scaling 27648.0 \</span><br><span class="line">    --num_workers 8 \</span><br><span class="line">    --train_unit 'epoch' \</span><br><span class="line">    --warmup_num 0 \</span><br><span class="line">    --train_num 25 \</span><br><span class="line">    --decay_boundaries "10,16,22" \</span><br><span class="line">    --output MS1M_v3_arcface_static_0.1</span><br></pre></td></tr></table></figure><p><font color="red">3090单卡容易爆显存</font> batch_size可由128调整至64，或开启多卡训练，需ncll</p><p>模型评价<code>sh scripts/validation_static.sh</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">python tools/validation.py \</span><br><span class="line">    --is_static True \</span><br><span class="line">    --backbone FresResNet50 \</span><br><span class="line">    --embedding_size 512 \</span><br><span class="line">    --checkpoint_dir MS1M_v3_arcface_static_0.1/FresResNet50/24 \</span><br><span class="line">    --data_dir MS1M_v3/ \</span><br><span class="line">    --val_targets lfw,cfp_fp,agedb_30 \</span><br><span class="line">    --batch_size 64</span><br></pre></td></tr></table></figure><p>模型导出<code>sh scripts/export_static.sh</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">python tools/export.py \</span><br><span class="line">    --is_static True \</span><br><span class="line">    --export_type paddle \</span><br><span class="line">    --backbone FresResNet50 \</span><br><span class="line">    --embedding_size 512 \</span><br><span class="line">    --checkpoint_dir MS1M_v3_arcface_static_0.1/FresResNet50/24 \</span><br><span class="line">    --output_dir MS1M_v3_arcface_static_0.1/FresResNet50/exported_model</span><br></pre></td></tr></table></figure><p>模型推理<code>sh scripts/inference.sh</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python tools/inference.py \</span><br><span class="line">    --export_type paddle \</span><br><span class="line">    --model_file MS1M_v3_arcface_static_0.1/FresResNet50/exported_model/FresResNet50.pdmodel \</span><br><span class="line">    --params_file MS1M_v3_arcface_static_0.1/FresResNet50/exported_model/FresResNet50.pdiparams \</span><br><span class="line">    --image_path MS1M_v3/images/00000001.jpg</span><br></pre></td></tr></table></figure><h4 id="构建人像索引"><a href="#构建人像索引" class="headerlink" title="构建人像索引"></a>构建人像索引</h4><ul><li>建立图像文件夹</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">fridends</span><br><span class="line">|_ gallery</span><br><span class="line">|  |_ Chandler</span><br><span class="line">|      |_ Chandler01.jpg</span><br><span class="line">|      |_ ...</span><br><span class="line">|      |_ Chandler50.jpg</span><br><span class="line">|  |_ ...</span><br><span class="line">|  |_ Ross</span><br><span class="line">|      |_ Ross01.jpg</span><br><span class="line">|      |_ ...</span><br><span class="line">|      |_ Ross50.jpg</span><br><span class="line">|_ label.txt</span><br></pre></td></tr></table></figure><ul><li>建立索引文件<code>label.txt</code></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">./Chandler/Chandler00037.jpgChandler</span><br><span class="line">./Chandler/Chandler00021.pngChandler</span><br><span class="line">./Chandler/Chandler00040.jpgChandler</span><br><span class="line">./Chandler/Chandler00041.jpgChandler</span><br><span class="line">./Chandler/Chandler00004.pngChandler</span><br><span class="line">./Chandler/Chandler00034.pngChandler</span><br><span class="line">./Chandler/Chandler00008.pngChandler</span><br><span class="line">...</span><br><span class="line">./Ross/Ross00016.jpgRoss</span><br><span class="line">./Ross/Ross00022.jpgRoss</span><br><span class="line">./Ross/Ross00019.jpgRoss</span><br><span class="line">./Ross/Ross00024.jpgRoss</span><br><span class="line">./Ross/Ross00001.jpgRoss</span><br><span class="line">./Ross/Ross00039.jpgRoss</span><br><span class="line">./Ross/Ross00038.jpgRoss</span><br><span class="line">./Ross/Ross00017.jpgRoss</span><br><span class="line">./Ross/Ross00034.jpgRoss</span><br><span class="line">./Ross/Ross00002.pngRoss</span><br></pre></td></tr></table></figure><ul><li>构建索引</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insightfacepaddle --build_index ./demo/friends/index.bin --img_dir ./demo/friends/gallery --label ./demo/friends/gallery/label.txt</span><br></pre></td></tr></table></figure><h4 id="检测图片"><a href="#检测图片" class="headerlink" title="检测图片"></a>检测图片</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python tools/test_recognition.py --det --rec --index=./demo/friends/index.bin --input=./test/测试2.jpg --output=./output</span><br></pre></td></tr></table></figure><p><img src="/2023/08/08/arcface-paddle/A.png" alt></p><h4 id="预测图片"><a href="#预测图片" class="headerlink" title="预测图片"></a>预测图片</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python tools/test_recognition.py --det --rec --index=./demo/friends/index.bin --input=./test/测试2.jpg --output=./output</span><br></pre></td></tr></table></figure><p><img src="/2023/08/08/arcface-paddle/B.png" alt></p><h4 id="预测视频"><a href="#预测视频" class="headerlink" title="预测视频"></a>预测视频</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python tools/test_recognition.py --det --rec --index=./demo/friends/index.bin --input=./test/mp4v.mp4 --output=./output</span><br></pre></td></tr></table></figure><h4 id="python脚本"><a href="#python脚本" class="headerlink" title="python脚本"></a>python脚本</h4><p>脚本封装位置：arcface_paddle/python</p><ul><li>构建索引</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> insightface_paddle <span class="keyword">as</span> face</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line">parser = face.parser()</span><br><span class="line">args = parser.parse_args()</span><br><span class="line">args.build_index = <span class="string">"./demo/friends/index.bin"</span></span><br><span class="line">args.img_dir = <span class="string">"./demo/friends/gallery"</span></span><br><span class="line">args.label = <span class="string">"./demo/friends/gallery/label.txt"</span></span><br><span class="line">predictor = face.InsightFace(args)</span><br><span class="line">predictor.build_index()</span><br></pre></td></tr></table></figure><ul><li>视频</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> insightface_paddle <span class="keyword">as</span> face</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line">PR = os.path.dirname(os.path.abspath(__file__))</span><br><span class="line"></span><br><span class="line">logging.basicConfig(level=logging.INFO)</span><br><span class="line"></span><br><span class="line">parser = face.parser()</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">args.det = <span class="literal">True</span></span><br><span class="line">args.rec = <span class="literal">True</span></span><br><span class="line">args.index = os.path.join(PR, <span class="string">"demo/friends/index.bin"</span>)</span><br><span class="line">args.output = os.path.join(PR, <span class="string">"output"</span>)</span><br><span class="line">input_path = os.path.join(PR, <span class="string">"test/MP4V.mp4"</span>)</span><br><span class="line"></span><br><span class="line">predictor = face.InsightFace(args)</span><br><span class="line">res = predictor.predict(input_path, print_info=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> res:</span><br><span class="line">    print(_.get(<span class="string">'labels'</span>))</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;环境&quot;&gt;&lt;a href=&quot;#环境&quot; class=&quot;headerlink&quot; title=&quot;环境&quot;&gt;&lt;/a&gt;环境&lt;/h2&gt;&lt;h3 id=&quot;GPU（物理）&quot;&gt;&lt;a href=&quot;#GPU（物理）&quot; class=&quot;headerlink&quot; title=&quot;GPU（物理）&quot;&gt;&lt;/a&gt;GPU（物理）&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;NVIDIA 3090*2&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;显卡驱动 515.43.04&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;CUDA版本 11.7&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;CUDAtoolkit (cuda_11.7.0_515.43.04_linux)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;cuDNN (v8.4.1)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;font color=&quot;gold&quot;&gt;paddlepaddle 多卡训练需要NCLL支持&lt;/font&gt; (ncll v2.12.12 cuda11.7)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;paddlepaddle版本&quot;&gt;&lt;a href=&quot;#paddlepaddle版本&quot; class=&quot;headerlink&quot; title=&quot;paddlepaddle版本&quot;&gt;&lt;/a&gt;paddlepaddle版本&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;paddlepaddle-gpu==2.2.0rc0（虚拟环境cuda11.2）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;python环境&quot;&gt;&lt;a href=&quot;#python环境&quot; class=&quot;headerlink&quot; title=&quot;python环境&quot;&gt;&lt;/a&gt;python环境&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;CentOS7.9&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;anaconda3&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;python3.8&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;anaconda安装insightface&quot;&gt;&lt;a href=&quot;#anaconda安装insightface&quot; class=&quot;headerlink&quot; title=&quot;anaconda安装insightface&quot;&gt;&lt;/a&gt;anaconda安装insightface&lt;/h2&gt;&lt;p&gt;&lt;font color=&quot;gold&quot;&gt;重要：pillow版本建议选择9.5 否则过高会导致安装insightface报错&lt;/font&gt; （错误原因：pillow10移除了getsize方法，需要修改对应位置源码为&lt;code&gt;getbbox&lt;/code&gt;或 &lt;code&gt;getlength&lt;/code&gt;）&lt;/p&gt;
&lt;p&gt;告警信息：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;tools/test_recognition.py:&lt;span class=&quot;number&quot;&gt;627&lt;/span&gt;: DeprecationWarning: getsize &lt;span class=&quot;keyword&quot;&gt;is&lt;/span&gt; deprecated &lt;span class=&quot;keyword&quot;&gt;and&lt;/span&gt; will be removed &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; Pillow &lt;span class=&quot;number&quot;&gt;10&lt;/span&gt; (&lt;span class=&quot;number&quot;&gt;2023&lt;/span&gt;&lt;span class=&quot;number&quot;&gt;-07&lt;/span&gt;&lt;span class=&quot;number&quot;&gt;-01&lt;/span&gt;). Use getbbox &lt;span class=&quot;keyword&quot;&gt;or&lt;/span&gt; getlength instead.&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  tw = font.getsize(text)[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h4 id=&quot;环境安装&quot;&gt;&lt;a href=&quot;#环境安装&quot; class=&quot;headerlink&quot; title=&quot;环境安装&quot;&gt;&lt;/a&gt;环境安装&lt;/h4&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; .&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;conda install paddlepaddle-gpu==2.2.0rc0 cudatoolkit=11.2 -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/Paddle/ -c conda-forge&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; insightface&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;pip install -r requirements.txt -i https://mirror.baidu.com/pypi/simple&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; insightface/recongition/arcface_paddle/&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;pip install -r requirement.txt -i https://mirror.baidu.com/pypi/simple&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; insightface-paddle&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;pip install insightface-paddle -i https://mirror.baidu.com/pypi/simple&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="深度学习" scheme="http://yoursite.com/categories/深度学习/"/>
    
    
    <category term="paddlepaddle" scheme="http://yoursite.com/tags/paddlepaddle/"/>
    
  </entry>
  
  <entry>
    <title>paddleDetection</title>
    <link href="http://yoursite.com/2023/08/07/paddleDetection/"/>
    <id>http://yoursite.com/2023/08/07/paddleDetection/</id>
    <published>2023-08-07T08:20:53.000Z</published>
    <updated>2023-08-09T10:03:02.777Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><h3 id="GPU"><a href="#GPU" class="headerlink" title="GPU"></a>GPU</h3><ul><li><p>NVIDIA 3090*2</p></li><li><p>显卡驱动 515.43.04</p></li><li><p>CUDA版本 11.7</p></li><li><p>CUDAtoolkit (cuda_11.7.0_515.43.04_linux)</p></li><li><p>cuDNN (v8.4.1)</p></li><li><p><font color="gold">paddlepaddle 多卡训练需要NCLL支持</font> (ncll v2.12.12 cuda11.7)</p></li></ul><h3 id="paddlepaddle版本"><a href="#paddlepaddle版本" class="headerlink" title="paddlepaddle版本"></a>paddlepaddle版本</h3><ul><li>v2.4.2</li></ul><h3 id="python环境"><a href="#python环境" class="headerlink" title="python环境"></a>python环境</h3><ul><li><p>CentOS7.9</p></li><li><p>anaconda3</p></li><li><p>python3.8</p></li></ul><h2 id="普通视频处理"><a href="#普通视频处理" class="headerlink" title="普通视频处理"></a>普通视频处理</h2><font color="gold">h264格式视频被opencv解析帧率超过65535报错</font><p>源码：</p><p><code>./deploy/pipeline/pipeline.py</code> predict_video</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">out_path = os.path.join(self.output_dir, video_out_name + <span class="string">".mp4"</span>)</span><br><span class="line">fourcc = cv2.VideoWriter_fourcc(* <span class="string">'mp4v'</span>)</span><br><span class="line">writer = cv2.VideoWriter(out_path, fourcc, fps, (width, height))</span><br></pre></td></tr></table></figure><p>输入：cv2.VideoCapture()</p><p>输出：cv2.VideoWriter(）</p><p>本GPU模式下对时长5分钟的行人检测视频处理时间约20分钟（高精度模型），视频体积增大13倍（100M-&gt;1.3G）</p><a id="more"></a><h2 id="视频流"><a href="#视频流" class="headerlink" title="视频流"></a>视频流</h2><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ol><li><p>h264格式存在问题（需要重新编译opencv），实时帧数过高（&gt;65535）cv2.VideoWriter报错，无法保存视频</p><p>问题原因：<code>libx264</code>基于<code>GPL</code>，<code>ffmpeg</code>编码器要使用<code>libx264</code>，必须<code>--enable-gpl</code>，而<code>opencv</code>使用的是<code>MIT</code>许可</p></li><li><p>如果视频流传输加载较慢，获取不到视频报错(修改<code>pipeline.py</code> predict_video)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FrameQueueEmptyException</span><span class="params">(Exception)</span>:</span></span><br><span class="line">    <span class="string">"""An Exception for check frame queue"""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, time)</span>:</span></span><br><span class="line">        self.time = time</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">f"<span class="subst">&#123;self.time&#125;</span>秒无法获取frame队列，请检查摄像头是否正常运转"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">11</span>):</span><br><span class="line">    <span class="keyword">if</span> framequeue.empty():</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br><span class="line">        print(<span class="string">f"Couldn't catch framequeue for <span class="subst">&#123;i&#125;</span> seconds"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">10</span>:</span><br><span class="line">        <span class="keyword">raise</span> FrameQueueEmptyException</span><br></pre></td></tr></table></figure></li></ol><h3 id="摄像头-amp-应用"><a href="#摄像头-amp-应用" class="headerlink" title="摄像头 &amp; 应用"></a>摄像头 &amp; 应用</h3><p>以<code>liveCMS</code>应用为例</p><p>摄像头推流api：<code>http://&lt;livecms_host&gt;:&lt;ip&gt;/api/v1/stream/list</code></p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"StreamCount"</span>: <span class="number">1</span>,</span><br><span class="line">  <span class="attr">"Streams"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">"AudioEnable"</span>: <span class="literal">false</span>,</span><br><span class="line">      <span class="attr">"CDN"</span>: <span class="string">""</span>,</span><br><span class="line">      <span class="attr">"CascadeSize"</span>: <span class="number">0</span>,</span><br><span class="line">      <span class="attr">"ChannelID"</span>: <span class="string">"34020000001320000131"</span>,</span><br><span class="line">      <span class="attr">"ChannelName"</span>: <span class="string">"192.168.1.131"</span>,</span><br><span class="line">      <span class="attr">"CloudRecord"</span>: <span class="literal">false</span>,</span><br><span class="line">      <span class="attr">"DeviceID"</span>: <span class="string">"34020000002000000207"</span>,</span><br><span class="line">      <span class="attr">"Duration"</span>: <span class="number">20520</span>,</span><br><span class="line">      <span class="attr">"FLV"</span>: <span class="string">"http://192.168.9.165:10005/sms/34020000002020000001/flv/hls/34020000002000000207_34020000001320000131.flv"</span>,</span><br><span class="line">      <span class="attr">"HLS"</span>: <span class="string">"http://192.168.9.165:10005/sms/34020000002020000001/hls/34020000002000000207_34020000001320000131/live.m3u8"</span>,</span><br><span class="line">      <span class="attr">"InBitRate"</span>: <span class="number">914</span>,</span><br><span class="line">      <span class="attr">"InBytes"</span>: <span class="number">2196788210</span>,</span><br><span class="line">      <span class="attr">"NumOutputs"</span>: <span class="number">1</span>,</span><br><span class="line">      <span class="attr">"Ondemand"</span>: <span class="literal">true</span>,</span><br><span class="line">      <span class="attr">"OutBytes"</span>: <span class="number">2308731853</span>,</span><br><span class="line">      <span class="attr">"RTMP"</span>: <span class="string">"rtmp://192.168.9.165:11935/hls/34020000002000000207_34020000001320000131"</span>,</span><br><span class="line">      <span class="attr">"RTPCount"</span>: <span class="number">2049812</span>,</span><br><span class="line">      <span class="attr">"RTPLostCount"</span>: <span class="number">3526</span>,</span><br><span class="line">      <span class="attr">"RTPLostRate"</span>: <span class="number">0.6256656017039404</span>,</span><br><span class="line">      <span class="attr">"RTSP"</span>: <span class="string">"rtsp://192.168.9.165:554/34020000002000000207_34020000001320000131"</span>,</span><br><span class="line">      <span class="attr">"RecordStartAt"</span>: <span class="string">""</span>,</span><br><span class="line">      <span class="attr">"RelaySize"</span>: <span class="number">0</span>,</span><br><span class="line">      <span class="attr">"SMSID"</span>: <span class="string">"34020000002020000001"</span>,</span><br><span class="line">      <span class="attr">"SnapURL"</span>: <span class="string">"/sms/34020000002020000001/snap/34020000002000000207/34020000001320000131.jpg?t=1690332332290541800"</span>,</span><br><span class="line">      <span class="attr">"SourceAudioCodecName"</span>: <span class="string">"aac"</span>,</span><br><span class="line">      <span class="attr">"SourceAudioSampleRate"</span>: <span class="number">8000</span>,</span><br><span class="line">      <span class="attr">"SourceVideoCodecName"</span>: <span class="string">"h264"</span>,</span><br><span class="line">      <span class="attr">"SourceVideoFrameRate"</span>: <span class="number">30</span>,</span><br><span class="line">      <span class="attr">"SourceVideoHeight"</span>: <span class="number">720</span>,</span><br><span class="line">      <span class="attr">"SourceVideoWidth"</span>: <span class="number">1280</span>,</span><br><span class="line">      <span class="attr">"StartAt"</span>: <span class="string">"2023-07-26 08:45:30"</span>,</span><br><span class="line">      <span class="attr">"StreamID"</span>: <span class="string">"stream:34020000002000000207:34020000001320000131"</span>,</span><br><span class="line">      <span class="attr">"Transport"</span>: <span class="string">"UDP"</span>,</span><br><span class="line">      <span class="attr">"VideoFrameCount"</span>: <span class="number">605632</span>,</span><br><span class="line">      <span class="attr">"WEBRTC"</span>: <span class="string">"webrtc://192.168.9.165:10005/sms/34020000002020000001/rtc/34020000002000000207_34020000001320000131"</span>,</span><br><span class="line">      <span class="attr">"WS_FLV"</span>: <span class="string">"ws://192.168.9.165:10005/sms/34020000002020000001/ws-flv/hls/34020000002000000207_34020000001320000131.flv"</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="执行paddleDetection-–rtst指令"><a href="#执行paddleDetection-–rtst指令" class="headerlink" title="执行paddleDetection –rtst指令"></a>执行paddleDetection –rtst指令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python deploy/pipeline/pipeline.py --config deploy/pipeline/config/examples/infer_cfg_human_attr.yml --rtsp http://192.168.9.165:10005/sms/34020000002020000001/flv/hls/34020000002000000207_34020000001320000131.flv --device=gpu</span><br></pre></td></tr></table></figure><h2 id="使用Python脚本"><a href="#使用Python脚本" class="headerlink" title="使用Python脚本"></a>使用Python脚本</h2><h3 id="pipeline-pipeline-py"><a href="#pipeline-pipeline-py" class="headerlink" title="pipeline/pipeline.py"></a>pipeline/pipeline.py</h3><p>在对应config文件中将MOT的<code>tracker_config</code>修改为绝对路径</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">MOT:</span></span><br><span class="line">  <span class="attr">model_dir:</span> <span class="string">https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip</span></span><br><span class="line">  <span class="attr">tracker_config:</span> <span class="string">/exp/work/video/PaddleDetection/deploy/pipeline/config/tracker_config.yml</span></span><br><span class="line">  <span class="attr">batch_size:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure><p>修改<code>pipeline.py</code>启动脚本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    paddle.enable_static()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># parse params from command</span></span><br><span class="line">    parser = argsparser()</span><br><span class="line">    FLAGS = parser.parse_args()</span><br><span class="line">    FLAGS.device = <span class="string">'gpu'</span></span><br><span class="line">    FLAGS.config = <span class="string">'/exp/work/video/PaddleDetection/deploy/pipeline/config/examples/infer_cfg_human_attr.yml'</span></span><br><span class="line">    FLAGS.video_file = <span class="string">'/exp/work/video/PaddleDetection/demo_input/t2.mp4'</span></span><br><span class="line">    FLAGS.output_dir = <span class="string">'/exp/work/video/PaddleDetection/demo_output/'</span></span><br><span class="line">    FLAGS.device = FLAGS.device.upper()</span><br><span class="line">    <span class="keyword">assert</span> FLAGS.device <span class="keyword">in</span> [<span class="string">'CPU'</span>, <span class="string">'GPU'</span>, <span class="string">'XPU'</span>, <span class="string">'NPU'</span></span><br><span class="line">                            ], <span class="string">"device should be CPU, GPU, XPU or NPU"</span></span><br><span class="line"></span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>修改<code>cfg_utils.py</code> argsparser；parse_args，修改和注释掉config作为必要条件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_args</span><span class="params">(self, argv=None)</span>:</span></span><br><span class="line">    args = super(ArgsParser, self).parse_args(argv)</span><br><span class="line">    <span class="comment"># assert args.config is not None, \</span></span><br><span class="line">    <span class="comment">#     "Please specify --config=configure_file_path."</span></span><br><span class="line">    args.opt = self._parse_opt(args.opt)</span><br><span class="line">    <span class="keyword">return</span> args</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">parser.add_argument(</span><br><span class="line">    <span class="string">"--config"</span>,</span><br><span class="line">    type=str,</span><br><span class="line">    default=<span class="literal">None</span>,</span><br><span class="line">    help=(<span class="string">"Path of configure"</span>),</span><br><span class="line">    required=<span class="literal">False</span>) <span class="comment"># True ——&gt; False</span></span><br></pre></td></tr></table></figure><p>启动脚本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python pipeline.py</span><br></pre></td></tr></table></figure><p>或将其封装为接口</p><h3 id="python-infer-py"><a href="#python-infer-py" class="headerlink" title="python/infer.py"></a>python/infer.py</h3><p>导出预测模型</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 导出YOLOv3检测模型</span></span><br><span class="line">python tools/export_model.py -c configs/yolov3/yolov3_darknet53_270e_coco.yml --output_dir=./inference_model \</span><br><span class="line"> -o weights=https://paddledet.bj.bcebos.com/models/yolov3_darknet53_270e_coco.pdparams</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 导出HigherHRNet(bottom-up)关键点检测模型</span></span><br><span class="line">python tools/export_model.py -c configs/keypoint/higherhrnet/higherhrnet_hrnet_w32_512.yml -o weights=https://paddledet.bj.bcebos.com/models/keypoint/higherhrnet_hrnet_w32_512.pdparams</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 导出HRNet(top-down)关键点检测模型</span></span><br><span class="line">python tools/export_model.py -c configs/keypoint/hrnet/hrnet_w32_384x288.yml -o weights=https://paddledet.bj.bcebos.com/models/keypoint/hrnet_w32_384x288.pdparams</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 导出FairMOT多目标跟踪模型</span></span><br><span class="line">python tools/export_model.py -c configs/mot/fairmot/fairmot_dla34_30e_1088x608.yml -o weights=https://paddledet.bj.bcebos.com/models/mot/fairmot_dla34_30e_1088x608.pdparams</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 导出ByteTrack多目标跟踪模型(相当于只导出检测器)</span></span><br><span class="line">python tools/export_model.py -c configs/mot/bytetrack/detector/ppyoloe_crn_l_36e_640x640_mot17half.yml -o weights=https://paddledet.bj.bcebos.com/models/mot/ppyoloe_crn_l_36e_640x640_mot17half.pdparams</span><br></pre></td></tr></table></figure><p>预测</p><p>修改<code>deploy/python/infer.py</code>启动脚本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    paddle.enable_static()</span><br><span class="line">    parser = argsparser()</span><br><span class="line">    FLAGS = parser.parse_args()</span><br><span class="line">    FLAGS.device = <span class="string">'gpu'</span></span><br><span class="line">    FLAGS.video_file = <span class="string">'/exp/work/video/PaddleDetection/demo_input/t2.mp4'</span></span><br><span class="line">    FLAGS.output_dir = <span class="string">'/exp/work/video/PaddleDetection/demo_output/'</span></span><br><span class="line">    FLAGS.model_dir = <span class="string">'/exp/work/video/PaddleDetection/inference_model/yolov3_darknet53_270e_coco'</span></span><br><span class="line">    print_arguments(FLAGS)</span><br><span class="line">    FLAGS.device = FLAGS.device.upper()</span><br><span class="line">    <span class="keyword">assert</span> FLAGS.device <span class="keyword">in</span> [<span class="string">'CPU'</span>, <span class="string">'GPU'</span>, <span class="string">'XPU'</span>, <span class="string">'NPU'</span></span><br><span class="line">                            ], <span class="string">"device should be CPU, GPU, XPU or NPU"</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="keyword">not</span> FLAGS.use_gpu, <span class="string">"use_gpu has been deprecated, please use --device"</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> <span class="keyword">not</span> (</span><br><span class="line">        FLAGS.enable_mkldnn == <span class="literal">False</span> <span class="keyword">and</span> FLAGS.enable_mkldnn_bfloat16 == <span class="literal">True</span></span><br><span class="line">    ), <span class="string">'To enable mkldnn bfloat, please turn on both enable_mkldnn and enable_mkldnn_bfloat16'</span></span><br><span class="line"></span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h2 id="pp-human"><a href="#pp-human" class="headerlink" title="pp-human"></a>pp-human</h2><p>行人检测模块，对应配置文件位置：<code>deploy/pipeline/config/examples/infer_cfg_pphuman.yml</code></p><p>通过命令启动检测脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python deploy/pipeline/pipeline.py --config deploy/pipeline/config/examples/infer_cfg_human_attr.yml --device=gpu --video_file=demo_input/t1.mp4 --output_dir=demo_output/</span><br></pre></td></tr></table></figure><p><img src="/2023/08/07/paddleDetection/PaddleDetection/A.png" alt></p><h2 id="pp-vehicle"><a href="#pp-vehicle" class="headerlink" title="pp-vehicle"></a>pp-vehicle</h2><p>车辆检测模块，对应配置文件位置：<code>deploy/pipeline/config/examples/infer_cfg_ppvehicle.yml</code></p><p>通过命令启动检测脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python deploy/pipeline/pipeline.py --config deploy/pipeline/config/infer_cfg_ppvehicle.yml --video_file=demo_input/car.mp4 --device=gpu --output_dir=demo_output</span><br></pre></td></tr></table></figure><p><img src="/2023/08/07/paddleDetection/PaddleDetection/B.png" alt></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;环境&quot;&gt;&lt;a href=&quot;#环境&quot; class=&quot;headerlink&quot; title=&quot;环境&quot;&gt;&lt;/a&gt;环境&lt;/h2&gt;&lt;h3 id=&quot;GPU&quot;&gt;&lt;a href=&quot;#GPU&quot; class=&quot;headerlink&quot; title=&quot;GPU&quot;&gt;&lt;/a&gt;GPU&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;NVIDIA 3090*2&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;显卡驱动 515.43.04&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;CUDA版本 11.7&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;CUDAtoolkit (cuda_11.7.0_515.43.04_linux)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;cuDNN (v8.4.1)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;font color=&quot;gold&quot;&gt;paddlepaddle 多卡训练需要NCLL支持&lt;/font&gt; (ncll v2.12.12 cuda11.7)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;paddlepaddle版本&quot;&gt;&lt;a href=&quot;#paddlepaddle版本&quot; class=&quot;headerlink&quot; title=&quot;paddlepaddle版本&quot;&gt;&lt;/a&gt;paddlepaddle版本&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;v2.4.2&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;python环境&quot;&gt;&lt;a href=&quot;#python环境&quot; class=&quot;headerlink&quot; title=&quot;python环境&quot;&gt;&lt;/a&gt;python环境&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;CentOS7.9&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;anaconda3&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;python3.8&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;普通视频处理&quot;&gt;&lt;a href=&quot;#普通视频处理&quot; class=&quot;headerlink&quot; title=&quot;普通视频处理&quot;&gt;&lt;/a&gt;普通视频处理&lt;/h2&gt;&lt;font color=&quot;gold&quot;&gt;h264格式视频被opencv解析帧率超过65535报错&lt;/font&gt;

&lt;p&gt;源码：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;./deploy/pipeline/pipeline.py&lt;/code&gt; predict_video&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;out_path = os.path.join(self.output_dir, video_out_name + &lt;span class=&quot;string&quot;&gt;&quot;.mp4&quot;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;fourcc = cv2.VideoWriter_fourcc(* &lt;span class=&quot;string&quot;&gt;&#39;mp4v&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;writer = cv2.VideoWriter(out_path, fourcc, fps, (width, height))&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;输入：cv2.VideoCapture()&lt;/p&gt;
&lt;p&gt;输出：cv2.VideoWriter(）&lt;/p&gt;
&lt;p&gt;本GPU模式下对时长5分钟的行人检测视频处理时间约20分钟（高精度模型），视频体积增大13倍（100M-&amp;gt;1.3G）&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="http://yoursite.com/categories/深度学习/"/>
    
    
    <category term="paddlepaddle" scheme="http://yoursite.com/tags/paddlepaddle/"/>
    
  </entry>
  
  <entry>
    <title>搭建nebula-CentOS集群（极速版）</title>
    <link href="http://yoursite.com/2023/07/10/%E6%90%AD%E5%BB%BAnebula-CentOS%E9%9B%86%E7%BE%A4%EF%BC%88%E6%9E%81%E9%80%9F%E7%89%88%EF%BC%89/"/>
    <id>http://yoursite.com/2023/07/10/%E6%90%AD%E5%BB%BAnebula-CentOS%E9%9B%86%E7%BE%A4%EF%BC%88%E6%9E%81%E9%80%9F%E7%89%88%EF%BC%89/</id>
    <published>2023-07-10T01:05:06.000Z</published>
    <updated>2023-08-10T01:38:30.425Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><ul><li>系统</li></ul><p>CentOS7.9</p><ul><li>机器</li></ul><p>三台，分别为<code>nebula01</code>、<code>nebula02</code>、<code>nebula03</code>、</p><ul><li>安装位置</li></ul><p><code>/usr/local</code></p><ul><li>nebula版本</li></ul><p><code>2.6.1</code></p><ul><li>nebula-graph-studio版本</li></ul><p><code>3.2.3</code></p><h2 id="快速开始"><a href="#快速开始" class="headerlink" title="快速开始"></a>快速开始</h2><ul><li>下载<code>tar.gz</code>文件</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local</span><br><span class="line">wget https://oss-cdn.nebula-graph.com.cn/package/2.6.1/nebula-graph-2.6.1.el7.x86_64.tar.gz</span><br><span class="line">wget https://oss-cdn.nebula-graph.com.cn/nebula-graph-studio/3.2.3/nebula-graph-studio-3.2.3.x86_64.tar.gz</span><br></pre></td></tr></table></figure><ul><li>解压缩并重命名文件</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf nebula-graph-2.6.1.el7.x86_64.tar.gz &amp;&amp; mv nebula-graph-2.6.1.el7.x86_64 nebula</span><br><span class="line">tar -zxvf nebula-graph-studio-3.2.3.x86_64.tar.gz &amp;&amp; mv nebula-graph-studio-3.2.3.x86_64 nebula-graph-studio</span><br></pre></td></tr></table></figure><ul><li>复制配置文件</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd nebula/etc</span><br><span class="line">cp nebula-graphd.conf.default nebula-graphd.conf</span><br><span class="line">cp nebula-metad.conf.default nebula-metad.conf</span><br><span class="line">cp nebula-storaged.conf.default nebula-storaged.conf</span><br></pre></td></tr></table></figure><a id="more"></a><ul><li>修改配置文件</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim nebula-graphd.conf</span><br></pre></td></tr></table></figure><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line"><span class="comment"># The directory to host logging files</span></span><br><span class="line"><span class="string">--log_dir=/data/nebula/logs</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="comment"># Comma separated Meta Server Addresses</span></span><br><span class="line"><span class="string">--meta_server_addrs=nebula01:9559,nebula02:9559,nebula03:9559</span></span><br><span class="line"><span class="comment"># Local IP used to identify the nebula-graphd process.</span></span><br><span class="line"><span class="comment"># Change it to an address other than loopback if the service is distributed or</span></span><br><span class="line"><span class="comment"># will be accessed remotely.</span></span><br><span class="line"><span class="string">--local_ip=nebula01</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim nebula-metad.conf</span><br></pre></td></tr></table></figure><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line"><span class="comment"># The directory to host logging files</span></span><br><span class="line"><span class="string">--log_dir=/data/nebula/logs</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="comment"># Comma separated Meta Server addresses</span></span><br><span class="line"><span class="string">--meta_server_addrs=nebula01:9559,nebula02:9559,nebula03:9559</span></span><br><span class="line"><span class="comment"># Local IP used to identify the nebula-metad process.</span></span><br><span class="line"><span class="comment"># Change it to an address other than loopback if the service is distributed or</span></span><br><span class="line"><span class="comment"># will be accessed remotely.</span></span><br><span class="line"><span class="string">--local_ip=nebula01</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="comment"># Root data path, here should be only single path for metad</span></span><br><span class="line"><span class="string">--data_path=/data/nebula/meta</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim nebula-storaged.conf</span><br></pre></td></tr></table></figure><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line"><span class="comment"># The directory to host logging files</span></span><br><span class="line"><span class="string">--log_dir=/data/nebula/logs</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="comment"># Comma separated Meta server addresses</span></span><br><span class="line"><span class="string">--meta_server_addrs=nebula01:9559,nebula02:9559,nebula03:9559</span></span><br><span class="line"><span class="comment"># Local IP used to identify the nebula-storaged process.</span></span><br><span class="line"><span class="comment"># Change it to an address other than loopback if the service is distributed or</span></span><br><span class="line"><span class="comment"># will be accessed remotely.</span></span><br><span class="line"><span class="string">--local_ip=nebula01</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="comment"># Root data path. Split by comma. e.g. --data_path=/disk1/path1/,/disk2/path2/</span></span><br><span class="line"><span class="comment"># One path per Rocksdb instance.</span></span><br><span class="line"><span class="string">--data_path=/data/nebula/storage</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure><ul><li>分发nebula文件，并修改对应的<code>--local_ip</code></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd ../..</span><br><span class="line">scp -r nebula nebula02:`pwd`/</span><br><span class="line">scp -r nebula nebula03:`pwd`/</span><br></pre></td></tr></table></figure><ul><li>启动集群</li></ul><p><font color="gold">注：</font>因为meta机器选择了nebula01、02、03共三台机器，启动时需要将三台同时执行命令拉起</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nebula/scripts/nebula.service start all</span><br></pre></td></tr></table></figure><ul><li>检查集群状态</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nebula/scripts/nebula.service status all</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[INFO] nebula-metad(de03025): Running as 24680, Listening on 9559 </span><br><span class="line">[INFO] nebula-graphd(de03025): Running as 25453, Listening on 9669 </span><br><span class="line">[INFO] nebula-storaged(de03025): Running as 24787, Listening on 9779</span><br></pre></td></tr></table></figure><p>此时<code>nebula</code>文件夹下生成<code>cluster.id</code>文件，集群成功启动</p><ul><li>启动nebula-graph-studio</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd nebula-graph-studio</span><br><span class="line">nohup ./server &amp;</span><br></pre></td></tr></table></figure><p><img src="/2023/07/10/搭建nebula-CentOS集群（极速版）/A.png" alt></p><p><font color="gold">注：</font>同样可用于集群数据恢复（<code>/data</code>下的数据已备份）</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;环境准备&quot;&gt;&lt;a href=&quot;#环境准备&quot; class=&quot;headerlink&quot; title=&quot;环境准备&quot;&gt;&lt;/a&gt;环境准备&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;系统&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;CentOS7.9&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;机器&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;三台，分别为&lt;code&gt;nebula01&lt;/code&gt;、&lt;code&gt;nebula02&lt;/code&gt;、&lt;code&gt;nebula03&lt;/code&gt;、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;安装位置&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;/usr/local&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;nebula版本&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;2.6.1&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;nebula-graph-studio版本&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;3.2.3&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&quot;快速开始&quot;&gt;&lt;a href=&quot;#快速开始&quot; class=&quot;headerlink&quot; title=&quot;快速开始&quot;&gt;&lt;/a&gt;快速开始&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;下载&lt;code&gt;tar.gz&lt;/code&gt;文件&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;cd /usr/local&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;wget https://oss-cdn.nebula-graph.com.cn/package/2.6.1/nebula-graph-2.6.1.el7.x86_64.tar.gz&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;wget https://oss-cdn.nebula-graph.com.cn/nebula-graph-studio/3.2.3/nebula-graph-studio-3.2.3.x86_64.tar.gz&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;解压缩并重命名文件&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;tar -zxvf nebula-graph-2.6.1.el7.x86_64.tar.gz &amp;amp;&amp;amp; mv nebula-graph-2.6.1.el7.x86_64 nebula&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;tar -zxvf nebula-graph-studio-3.2.3.x86_64.tar.gz &amp;amp;&amp;amp; mv nebula-graph-studio-3.2.3.x86_64 nebula-graph-studio&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;复制配置文件&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;cd nebula/etc&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;cp nebula-graphd.conf.default nebula-graphd.conf&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;cp nebula-metad.conf.default nebula-metad.conf&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;cp nebula-storaged.conf.default nebula-storaged.conf&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
    
    <category term="database" scheme="http://yoursite.com/tags/database/"/>
    
    <category term="nebua" scheme="http://yoursite.com/tags/nebua/"/>
    
  </entry>
  
  <entry>
    <title>Nebula-Spark和图算法</title>
    <link href="http://yoursite.com/2023/03/24/Nebula-Spark/"/>
    <id>http://yoursite.com/2023/03/24/Nebula-Spark/</id>
    <published>2023-03-24T06:34:47.000Z</published>
    <updated>2023-03-31T02:42:06.474Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><h2 id="Nebula-Spark-Connector"><a href="#Nebula-Spark-Connector" class="headerlink" title="Nebula Spark Connector"></a>Nebula Spark Connector</h2><p>下载地址&amp;官方文档：【<a href="https://github.com/vesoft-inc/nebula-spark-connector" target="_blank" rel="noopener">https://github.com/vesoft-inc/nebula-spark-connector</a>】</p><h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><p><strong>·</strong> nebula：<font color="orange">2.6.1</font><br><strong>·</strong> hadoop：<font color="orange">2.7</font><br><strong>·</strong> spark：<font color="orange">2.4.7</font><br><strong>·</strong> pyspark：<font color="orange">2.4.7</font><br><strong>·</strong> python：<font color="orange">3.7.16</font><br><strong>·</strong> nebula-spark-connector：<font color="orange">2.6.1</font></p><h3 id="编译打包nebula-spark-connector"><a href="#编译打包nebula-spark-connector" class="headerlink" title="编译打包nebula-spark-connector"></a>编译打包nebula-spark-connector</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> nebula-spark-connector-2.6.1/nebula-spark-connector</span><br><span class="line">$ mvn clean package -Dmaven.test.skip=<span class="literal">true</span> -Dgpg.skip -Dmaven.javadoc.skip=<span class="literal">true</span></span><br></pre></td></tr></table></figure><p>成功后在<code>nebula-spark-connector/target/</code> 目录下得到 <code>nebula-spark-connector-2.6.1.jar</code>文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@root target]<span class="comment"># ll</span></span><br><span class="line">total 106792</span><br><span class="line">drwxr-xr-x 3 root root        17 Mar 11 14:14 classes</span><br><span class="line">-rw-r--r-- 1 root root         1 Mar 11 14:14 classes.-497386701.timestamp</span><br><span class="line">-rw-r--r-- 1 root root         1 Mar 11 14:14 classes.timestamp</span><br><span class="line">-rw-r--r-- 1 root root     30701 Mar 11 14:15 jacoco.exec</span><br><span class="line">drwxr-xr-x 2 root root        28 Mar 11 14:15 maven-archiver</span><br><span class="line">-rw-r--r-- 1 root root 108375457 Mar 11 14:16 nebula-spark-connector-2.6.1.jar</span><br><span class="line">-rw-r--r-- 1 root root    583482 Mar 11 14:16 nebula-spark-connector-2.6.1-javadoc.jar</span><br><span class="line">-rw-r--r-- 1 root root     36358 Mar 11 14:16 nebula-spark-connector-2.6.1-sources.jar</span><br><span class="line">-rw-r--r-- 1 root root    315392 Mar 11 14:15 original-nebula-spark-connector-2.6.1.jar</span><br><span class="line">drwxr-xr-x 4 root root        37 Mar 11 14:15 site</span><br></pre></td></tr></table></figure><h3 id="PySpark-读取-NebulaGraph-数据"><a href="#PySpark-读取-NebulaGraph-数据" class="headerlink" title="PySpark 读取 NebulaGraph 数据"></a>PySpark 读取 NebulaGraph 数据</h3><p>从 <code>metaAddress</code> 为 <code>&quot;metad0:9559&quot;</code> 的 Nebula Graph 中读取整个 tag 下的数据为一个 dataframe：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">df = spark.read.format(</span><br><span class="line">  <span class="string">"com.vesoft.nebula.connector.NebulaDataSource"</span>).option(</span><br><span class="line">    <span class="string">"type"</span>, <span class="string">"vertex"</span>).option(</span><br><span class="line">    <span class="string">"spaceName"</span>, <span class="string">"basketballplayer"</span>).option(</span><br><span class="line">    <span class="string">"label"</span>, <span class="string">"player"</span>).option(</span><br><span class="line">    <span class="string">"returnCols"</span>, <span class="string">"name,age"</span>).option(</span><br><span class="line">    <span class="string">"metaAddress"</span>, <span class="string">"metad0:9559"</span>).option(</span><br><span class="line">    <span class="string">"partitionNumber"</span>, <span class="number">1</span>).load()</span><br></pre></td></tr></table></figure><p>然后可以像这样 <code>show</code> 这个 dataframe：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>df.show(n=<span class="number">2</span>)</span><br><span class="line">+---------+--------------+---+</span><br><span class="line">|_vertexId|          name|age|</span><br><span class="line">+---------+--------------+---+</span><br><span class="line">|player105|   Danny Green| <span class="number">31</span>|</span><br><span class="line">|player109|Tiago Splitter| <span class="number">34</span>|</span><br><span class="line">+---------+--------------+---+</span><br><span class="line">only showing top <span class="number">2</span> rows</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="PySpark-写-NebulaGraph-数据"><a href="#PySpark-写-NebulaGraph-数据" class="headerlink" title="PySpark 写 NebulaGraph 数据"></a>PySpark 写 NebulaGraph 数据</h3><p>默认不指定的情况下 <code>writeMode</code> 是 <code>insert</code>：</p><h4 id="写入点"><a href="#写入点" class="headerlink" title="写入点"></a>写入点</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">df.write.format(<span class="string">"com.vesoft.nebula.connector.NebulaDataSource"</span>).option(</span><br><span class="line">    <span class="string">"type"</span>, <span class="string">"vertex"</span>).option(</span><br><span class="line">    <span class="string">"spaceName"</span>, <span class="string">"basketballplayer"</span>).option(</span><br><span class="line">    <span class="string">"label"</span>, <span class="string">"player"</span>).option(</span><br><span class="line">    <span class="string">"vidPolicy"</span>, <span class="string">""</span>).option(</span><br><span class="line">    <span class="string">"vertexField"</span>, <span class="string">"_vertexId"</span>).option(</span><br><span class="line">    <span class="string">"batch"</span>, <span class="number">1</span>).option(</span><br><span class="line">    <span class="string">"metaAddress"</span>, <span class="string">"metad0:9559"</span>).option(</span><br><span class="line">    <span class="string">"graphAddress"</span>, <span class="string">"graphd1:9669"</span>).option(</span><br><span class="line">    <span class="string">"passwd"</span>, <span class="string">"nebula"</span>).option(</span><br><span class="line">    <span class="string">"user"</span>, <span class="string">"root"</span>).save()</span><br></pre></td></tr></table></figure><h4 id="删除点"><a href="#删除点" class="headerlink" title="删除点"></a>删除点</h4><p>如果想指定 <code>delete</code> 或者 <code>update</code> 的非默认写入模式，增加 <code>writeMode</code> 的配置，比如 <code>delete</code> 的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">df.write.format(<span class="string">"com.vesoft.nebula.connector.NebulaDataSource"</span>).option(</span><br><span class="line">    <span class="string">"type"</span>, <span class="string">"vertex"</span>).option(</span><br><span class="line">    <span class="string">"spaceName"</span>, <span class="string">"basketballplayer"</span>).option(</span><br><span class="line">    <span class="string">"label"</span>, <span class="string">"player"</span>).option(</span><br><span class="line">    <span class="string">"vidPolicy"</span>, <span class="string">""</span>).option(</span><br><span class="line">    <span class="string">"vertexField"</span>, <span class="string">"_vertexId"</span>).option(</span><br><span class="line">    <span class="string">"batch"</span>, <span class="number">1</span>).option(</span><br><span class="line">    <span class="string">"metaAddress"</span>, <span class="string">"metad0:9559"</span>).option(</span><br><span class="line">    <span class="string">"graphAddress"</span>, <span class="string">"graphd1:9669"</span>).option(</span><br><span class="line">    <span class="string">"passwd"</span>, <span class="string">"nebula"</span>).option(</span><br><span class="line">    <span class="string">"writeMode"</span>, <span class="string">"delete"</span>).option(</span><br><span class="line">    <span class="string">"user"</span>, <span class="string">"root"</span>).save()</span><br></pre></td></tr></table></figure><h4 id="写入边"><a href="#写入边" class="headerlink" title="写入边"></a>写入边</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">df.write.format(<span class="string">"com.vesoft.nebula.connector.NebulaDataSource"</span>)\</span><br><span class="line">    .mode(<span class="string">"overwrite"</span>)\</span><br><span class="line">    .option(<span class="string">"srcPolicy"</span>, <span class="string">""</span>)\</span><br><span class="line">    .option(<span class="string">"dstPolicy"</span>, <span class="string">""</span>)\</span><br><span class="line">    .option(<span class="string">"metaAddress"</span>, <span class="string">"metad0:9559"</span>)\</span><br><span class="line">    .option(<span class="string">"graphAddress"</span>, <span class="string">"graphd:9669"</span>)\</span><br><span class="line">    .option(<span class="string">"user"</span>, <span class="string">"root"</span>)\</span><br><span class="line">    .option(<span class="string">"passwd"</span>, <span class="string">"nebula"</span>)\</span><br><span class="line">    .option(<span class="string">"type"</span>, <span class="string">"edge"</span>)\</span><br><span class="line">    .option(<span class="string">"spaceName"</span>, <span class="string">"basketballplayer"</span>)\</span><br><span class="line">    .option(<span class="string">"label"</span>, <span class="string">"server"</span>)\</span><br><span class="line">    .option(<span class="string">"srcVertexField"</span>, <span class="string">"srcid"</span>)\</span><br><span class="line">    .option(<span class="string">"dstVertexField"</span>, <span class="string">"dstid"</span>)\</span><br><span class="line">    .option(<span class="string">"rankField"</span>, <span class="string">""</span>)\</span><br><span class="line">    .option(<span class="string">"batch"</span>, <span class="number">100</span>)\</span><br><span class="line">    .option(<span class="string">"writeMode"</span>, <span class="string">"insert"</span>).save()   <span class="comment"># delete to delete edge, update to update edge</span></span><br></pre></td></tr></table></figure><h4 id="删除边"><a href="#删除边" class="headerlink" title="删除边"></a>删除边</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">df.write.format(<span class="string">"com.vesoft.nebula.connector.NebulaDataSource"</span>)\</span><br><span class="line">    .mode(<span class="string">"overwrite"</span>)\</span><br><span class="line">    .option(<span class="string">"srcPolicy"</span>, <span class="string">""</span>)\</span><br><span class="line">    .option(<span class="string">"dstPolicy"</span>, <span class="string">""</span>)\</span><br><span class="line">    .option(<span class="string">"metaAddress"</span>, <span class="string">"metad0:9559"</span>)\</span><br><span class="line">    .option(<span class="string">"graphAddress"</span>, <span class="string">"graphd:9669"</span>)\</span><br><span class="line">    .option(<span class="string">"user"</span>, <span class="string">"root"</span>)\</span><br><span class="line">    .option(<span class="string">"passwd"</span>, <span class="string">"nebula"</span>)\</span><br><span class="line">    .option(<span class="string">"type"</span>, <span class="string">"edge"</span>)\</span><br><span class="line">    .option(<span class="string">"spaceName"</span>, <span class="string">"basketballplayer"</span>)\</span><br><span class="line">    .option(<span class="string">"label"</span>, <span class="string">"server"</span>)\</span><br><span class="line">    .option(<span class="string">"srcVertexField"</span>, <span class="string">"srcid"</span>)\</span><br><span class="line">    .option(<span class="string">"dstVertexField"</span>, <span class="string">"dstid"</span>)\</span><br><span class="line">    .option(<span class="string">"randkField"</span>, <span class="string">""</span>)\</span><br><span class="line">    .option(<span class="string">"batch"</span>, <span class="number">100</span>)\</span><br><span class="line">    .option(<span class="string">"writeMode"</span>, <span class="string">"delete"</span>).save()   <span class="comment"># delete to delete edge, update to update edge</span></span><br></pre></td></tr></table></figure><h3 id="关于-PySpark-读写的-option"><a href="#关于-PySpark-读写的-option" class="headerlink" title="关于 PySpark 读写的 option"></a>关于 PySpark 读写的 option</h3><p>对于其他的 option，比如删除点的时候的 <code>withDeleteEdge</code> 可以参考 [nebula/connector/NebulaOptions.scala</p><p>](<a href="https://github.com/vesoft-inc/nebula-spark-connector/blob/master/nebula-spark-connector/src/main/scala/com/vesoft/nebula/connector/NebulaOptions.scala" target="_blank" rel="noopener">https://github.com/vesoft-inc/nebula-spark-connector/blob/master/nebula-spark-connector/src/main/scala/com/vesoft/nebula/connector/NebulaOptions.scala</a>) 的字符串配置定义，我们可以看到它的字符串定义字段是 <code>deleteEdge</code> ：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** write config */</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">RATE_LIMIT</span>: <span class="type">String</span>   = <span class="string">"rateLimit"</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">VID_POLICY</span>: <span class="type">String</span>   = <span class="string">"vidPolicy"</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">SRC_POLICY</span>: <span class="type">String</span>   = <span class="string">"srcPolicy"</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">DST_POLICY</span>: <span class="type">String</span>   = <span class="string">"dstPolicy"</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">VERTEX_FIELD</span>         = <span class="string">"vertexField"</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">SRC_VERTEX_FIELD</span>     = <span class="string">"srcVertexField"</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">DST_VERTEX_FIELD</span>     = <span class="string">"dstVertexField"</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">RANK_FIELD</span>           = <span class="string">"randkField"</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">BATCH</span>: <span class="type">String</span>        = <span class="string">"batch"</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">VID_AS_PROP</span>: <span class="type">String</span>  = <span class="string">"vidAsProp"</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">SRC_AS_PROP</span>: <span class="type">String</span>  = <span class="string">"srcAsProp"</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">DST_AS_PROP</span>: <span class="type">String</span>  = <span class="string">"dstAsProp"</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">RANK_AS_PROP</span>: <span class="type">String</span> = <span class="string">"rankAsProp"</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">WRITE_MODE</span>: <span class="type">String</span>   = <span class="string">"writeMode"</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">DELETE_EDGE</span>: <span class="type">String</span>  = <span class="string">"deleteEdge"</span></span><br></pre></td></tr></table></figure><h2 id="PySpark-调用-Nebula-Spark-Connector"><a href="#PySpark-调用-Nebula-Spark-Connector" class="headerlink" title="PySpark 调用 Nebula Spark Connector"></a>PySpark 调用 Nebula Spark Connector</h2><h3 id="Pycharm"><a href="#Pycharm" class="headerlink" title="Pycharm"></a>Pycharm</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入SparkSession、findspark自动获取$&#123;SPARK_HOME&#125;，定义虚拟环境</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> findspark</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">"HADOOP_CONF_DIR"</span>] = <span class="string">"/exp/server/hadoop-2.7.7/etc/hadoop"</span></span><br><span class="line">os.environ[<span class="string">"YARN_CONF_DIR"</span>] = <span class="string">"/exp/server/hadoop-2.7.7/etc/hadoop"</span></span><br><span class="line">os.environ[<span class="string">"SPARK_HOME"</span>] = <span class="string">"/exp/server/spark-2.4.7-bin-hadoop2.7"</span></span><br><span class="line">os.environ[<span class="string">"PYSPARK_PYTHON"</span>] = <span class="string">"/root/anaconda3/envs/pyspark38/bin/python"</span></span><br><span class="line">os.environ[<span class="string">"PYSPARK_DRIVER_PYTHON"</span>] = <span class="string">"/root/anaconda3/envs/pyspark38/bin/python"</span></span><br><span class="line">findspark.init()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建SparkSession对象，导入target下的jar包</span></span><br><span class="line">spark = SparkSession.builder.config(</span><br><span class="line">    <span class="string">"spark.jars"</span>,</span><br><span class="line">    <span class="string">"/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar"</span>).config(</span><br><span class="line">    <span class="string">"spark.driver.extraClassPath"</span>,</span><br><span class="line">    <span class="string">"/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar"</span>).appName(</span><br><span class="line">    <span class="string">"nebula-connector"</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取nebula-graph数据</span></span><br><span class="line"><span class="comment"># read vertex</span></span><br><span class="line">df_tag = spark.read.format(</span><br><span class="line">    <span class="string">"com.vesoft.nebula.connector.NebulaDataSource"</span>).option(</span><br><span class="line">    <span class="string">"sep"</span>, <span class="string">"\t"</span>).option(</span><br><span class="line">    <span class="string">"type"</span>, <span class="string">"vertex"</span>).option(</span><br><span class="line">    <span class="string">"spaceName"</span>, <span class="string">"space"</span>).option(</span><br><span class="line">    <span class="string">"label"</span>, <span class="string">"tag"</span>).option(</span><br><span class="line">    <span class="string">"returnCols"</span>, <span class="string">""</span>).option(</span><br><span class="line">    <span class="string">"metaAddress"</span>, <span class="string">"metahost:9559"</span>).option(</span><br><span class="line">    <span class="string">"partitionNumber"</span>, <span class="number">1</span>).load()</span><br><span class="line"></span><br><span class="line"><span class="comment"># read edge</span></span><br><span class="line">df_edge = spark.read.format(</span><br><span class="line">    <span class="string">"com.vesoft.nebula.connector.NebulaDataSource"</span>).option(</span><br><span class="line">    <span class="string">"sep"</span>, <span class="string">"\t"</span>).option(</span><br><span class="line">    <span class="string">"type"</span>, <span class="string">"edge"</span>).option(</span><br><span class="line">    <span class="string">"spaceName"</span>, <span class="string">"space"</span>).option(</span><br><span class="line">    <span class="string">"label"</span>, <span class="string">"edge"</span>).option(</span><br><span class="line">    <span class="string">"returnCols"</span>, <span class="string">""</span>).option(</span><br><span class="line">    <span class="string">"metaAddress"</span>, <span class="string">"metahost:9559"</span>).option(</span><br><span class="line">    <span class="string">"partitionNumber"</span>, <span class="number">1</span>).load()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 写回nebula-graph</span></span><br><span class="line"><span class="comment"># write vertex</span></span><br><span class="line">df_tag.write.format(<span class="string">"com.vesoft.nebula.connector.NebulaDataSource"</span>).option(</span><br><span class="line">    <span class="string">"type"</span>, <span class="string">"vertex"</span>).option(</span><br><span class="line">    <span class="string">"spaceName"</span>, <span class="string">"space"</span>).option(</span><br><span class="line">    <span class="string">"label"</span>, <span class="string">"tag"</span>).option(</span><br><span class="line">    <span class="string">"vidPolicy"</span>, <span class="string">""</span>).option(</span><br><span class="line">    <span class="string">"vertexField"</span>, <span class="string">"_vertexId"</span>).option(</span><br><span class="line">    <span class="string">"batch"</span>, <span class="number">1</span>).option(</span><br><span class="line">    <span class="string">"metaAddress"</span>, <span class="string">"metahost:9559"</span>).option(</span><br><span class="line">    <span class="string">"graphAddress"</span>, <span class="string">"graphhost:9669"</span>).option(</span><br><span class="line">    <span class="string">"passwd"</span>, <span class="string">"nebula"</span>).option(</span><br><span class="line">    <span class="string">"writeMode"</span>, <span class="string">"update"</span>).option(</span><br><span class="line">    <span class="string">"user"</span>, <span class="string">"root"</span>).save()</span><br><span class="line"></span><br><span class="line">df_tag.show()</span><br><span class="line">df_edge.show()</span><br></pre></td></tr></table></figure><h3 id="Spark-submit"><a href="#Spark-submit" class="headerlink" title="Spark-submit"></a>Spark-submit</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$&#123;SPARK_HOME&#125;</span>/bin/spark-submit --master <span class="built_in">local</span>[*] \</span><br><span class="line">--deploy-mode client  \</span><br><span class="line">--jars file:/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar \</span><br><span class="line">/exp/work/pyspark/nebula/nebula_reader.py</span><br></pre></td></tr></table></figure><h2 id="NebulaAlgorithm"><a href="#NebulaAlgorithm" class="headerlink" title="NebulaAlgorithm"></a>NebulaAlgorithm</h2><p>下载地址&amp;官方文档：【&lt;<a href="https://github.com/vesoft-inc/nebula-algorithm】" target="_blank" rel="noopener">https://github.com/vesoft-inc/nebula-algorithm】</a></p><h3 id="环境-1"><a href="#环境-1" class="headerlink" title="环境"></a>环境</h3><p><strong>·</strong> nebula：<font color="orange">2.6.1</font><br><strong>·</strong> hadoop：<font color="orange">2.7</font><br><strong>·</strong> spark：<font color="orange">2.4.7</font><br><strong>·</strong> pyspark：<font color="orange">2.4.7</font><br><strong>·</strong> python：<font color="orange">3.7.16</font><br><strong>·</strong> nebula-spark-connector：<font color="orange">2.6.1</font><br><strong>·</strong> nebula-algorithm：<font color="orange">2.6.1</font></p><h3 id="编译打包nebula-spark-connector-1"><a href="#编译打包nebula-spark-connector-1" class="headerlink" title="编译打包nebula-spark-connector"></a>编译打包nebula-spark-connector</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> nebula-algorithm-2.6.1/nebula-algorithm</span><br><span class="line">$ mvn clean package -Dgpg.skip -Dmaven.javadoc.skip=<span class="literal">true</span> -Dmaven.test.skip=<span class="literal">true</span></span><br></pre></td></tr></table></figure><p>成功后在<code>nebula-algorithm/target/</code> 目录下得到 <code>nebula-algorithm-2.6.1.jar</code>文件</p><h2 id="PySpark-调用-Nebula-Algorithm"><a href="#PySpark-调用-Nebula-Algorithm" class="headerlink" title="PySpark 调用 Nebula Algorithm"></a>PySpark 调用 Nebula Algorithm</h2><h3 id="Pycharm-1"><a href="#Pycharm-1" class="headerlink" title="Pycharm"></a>Pycharm</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> findspark</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession, DataFrame</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.window <span class="keyword">import</span> Window</span><br><span class="line"><span class="keyword">from</span> py4j.java_gateway <span class="keyword">import</span> java_import</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">"HADOOP_CONF_DIR"</span>] = <span class="string">"/exp/server/hadoop-2.7.7/etc/hadoop"</span></span><br><span class="line">os.environ[<span class="string">"YARN_CONF_DIR"</span>] = <span class="string">"/exp/server/hadoop-2.7.7/etc/hadoop"</span></span><br><span class="line">os.environ[<span class="string">"SPARK_HOME"</span>] = <span class="string">"/exp/server/spark-2.4.7-bin-hadoop2.7"</span></span><br><span class="line">os.environ[<span class="string">"PYSPARK_PYTHON"</span>] = <span class="string">"/root/anaconda3/envs/pyspark37/bin/python"</span></span><br><span class="line">os.environ[<span class="string">"PYSPARK_DRIVER_PYTHON"</span>] = <span class="string">"/root/anaconda3/envs/pyspark37/bin/python"</span></span><br><span class="line">findspark.init()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.config(</span><br><span class="line">    <span class="string">"spark.jars"</span>,</span><br><span class="line">    <span class="string">"/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar"</span>).config(</span><br><span class="line">    <span class="string">"spark.driver.extraClassPath"</span>,</span><br><span class="line">    <span class="string">"/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar"</span>).config(</span><br><span class="line">    <span class="string">"spark.jars"</span>,</span><br><span class="line">    <span class="string">"/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar"</span>).config(</span><br><span class="line">    <span class="string">"spark.driver.extraClassPath"</span>,</span><br><span class="line">    <span class="string">"/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar"</span>).appName(</span><br><span class="line">    <span class="string">"nebula-connector"</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># spark = SparkSession.builder.appName("PageRankExample").getOrCreate()</span></span><br><span class="line">jspark = spark._jsparkSession</span><br><span class="line"></span><br><span class="line"><span class="comment"># import "com.vesoft.nebula.algorithm.config.SparkConfig"</span></span><br><span class="line">java_import(spark._jvm, <span class="string">"com.vesoft.nebula.algorithm.config.SparkConfig"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># import "com.vesoft.nebula.algorithm.config.PRConfig"</span></span><br><span class="line">java_import(spark._jvm, <span class="string">"com.vesoft.nebula.algorithm.config.PRConfig"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># import "com.vesoft.nebula.algorithm.lib.PageRankAlgo"</span></span><br><span class="line">java_import(spark._jvm, <span class="string">"com.vesoft.nebula.algorithm.lib.PageRankAlgo"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将string类型vid转int类型vid</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_string_id_to_long_id</span><span class="params">(df)</span>:</span></span><br><span class="line">    df = df.drop(<span class="string">"display_desc"</span>).drop(<span class="string">"creation_time"</span>)</span><br><span class="line">    src_id_df = df.select(<span class="string">"_srcId"</span>).withColumnRenamed(<span class="string">"_srcId"</span>, <span class="string">"id"</span>)</span><br><span class="line">    dst_id_df = df.select(<span class="string">"_dstId"</span>).withColumnRenamed(<span class="string">"_dstId"</span>, <span class="string">"id"</span>)</span><br><span class="line">    id_df = src_id_df.union(dst_id_df).distinct()</span><br><span class="line">    encode_id = id_df.withColumn(<span class="string">"encodedId"</span>, dense_rank().over(Window.orderBy(<span class="string">"id"</span>)))</span><br><span class="line">    <span class="comment"># encode_id.write.option("header", True).csv("file:///tmp/encodeId.csv")</span></span><br><span class="line">    src_join_df = df.join(encode_id, df._srcId == encode_id.id) \</span><br><span class="line">        .drop(<span class="string">"_srcId"</span>) \</span><br><span class="line">        .drop(<span class="string">"id"</span>) \</span><br><span class="line">        .withColumnRenamed(<span class="string">"encodedId"</span>, <span class="string">"_srcId"</span>)</span><br><span class="line"></span><br><span class="line">    df_sv = df.join(encode_id, df._srcId == encode_id.id) \</span><br><span class="line">        .drop(<span class="string">"_srcId"</span>) \</span><br><span class="line">        .drop(<span class="string">"_rank"</span>) \</span><br><span class="line">        .drop(<span class="string">"_dstId"</span>) \</span><br><span class="line">        .withColumnRenamed(<span class="string">"id"</span>, <span class="string">"src_vid"</span>) \</span><br><span class="line">        .withColumnRenamed(<span class="string">"encodedId"</span>, <span class="string">"_srcId"</span>)</span><br><span class="line"></span><br><span class="line">    df_dv = src_join_df.join(encode_id, src_join_df._dstId == encode_id.id) \</span><br><span class="line">        .drop(<span class="string">"_dstId"</span>) \</span><br><span class="line">        .drop(<span class="string">"_rank"</span>) \</span><br><span class="line">        .drop(<span class="string">"degree"</span>) \</span><br><span class="line">        .withColumnRenamed(<span class="string">"encodedId"</span>, <span class="string">"_dstId"</span>) \</span><br><span class="line">        .withColumnRenamed(<span class="string">"_srcID"</span>, <span class="string">"_src"</span>) \</span><br><span class="line">        .withColumnRenamed(<span class="string">"id"</span>, <span class="string">"dst_vid"</span>)</span><br><span class="line"></span><br><span class="line">    dst_join_df = src_join_df.join(encode_id, src_join_df._dstId == encode_id.id) \</span><br><span class="line">        .drop(<span class="string">"_dstId"</span>) \</span><br><span class="line">        .drop(<span class="string">"_rank"</span>) \</span><br><span class="line">        .drop(<span class="string">"degree"</span>) \</span><br><span class="line">        .withColumnRenamed(<span class="string">"encodedId"</span>, <span class="string">"_dstId"</span>).drop(<span class="string">"id"</span>)</span><br><span class="line"></span><br><span class="line">    df_v = df_dv.join(df_sv, df_dv._src == df_sv._srcId).drop(<span class="string">"_src"</span>)</span><br><span class="line">    <span class="keyword">return</span> dst_join_df, df_v</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">df = spark.read.format(</span><br><span class="line">    <span class="string">"com.vesoft.nebula.connector.NebulaDataSource"</span>).option(</span><br><span class="line">    <span class="comment"># "sep", "\t").option(</span></span><br><span class="line">    <span class="string">"type"</span>, <span class="string">"edge"</span>).option(</span><br><span class="line">    <span class="string">"spaceName"</span>, <span class="string">"DWD_GRAPH_LY_V3_2023"</span>).option(</span><br><span class="line">    <span class="string">"label"</span>, <span class="string">"edge_phone"</span>).option(</span><br><span class="line">    <span class="string">"returnCols"</span>, <span class="string">""</span>).option(</span><br><span class="line">    <span class="string">"metaAddress"</span>, <span class="string">"192.168.100.45:9559"</span>).option(</span><br><span class="line">    <span class="string">"partitionNumber"</span>, <span class="number">1</span>).load()</span><br><span class="line"></span><br><span class="line">df.orderBy(<span class="string">"creation_time"</span>).show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">df_int, df_v = convert_string_id_to_long_id(df)</span><br><span class="line">prConfig = spark._jvm.PRConfig(<span class="number">1</span>, <span class="number">0.8</span>)</span><br><span class="line">prResult = spark._jvm.PageRankAlgo.apply(jspark, df_int._jdf, prConfig, <span class="literal">False</span>)</span><br><span class="line">df_v.show(<span class="number">20</span>, <span class="literal">False</span>)</span><br><span class="line">prResult.show(<span class="number">20</span>, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将jdf转df</span></span><br><span class="line">prResult = prResult.toDF()</span><br><span class="line">pagerank_df = DataFrame(prResult, spark)</span><br><span class="line">pagerank_df.sort(col(<span class="string">"pagerank"</span>).desc()).show(<span class="number">60</span>, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> PYSPARK (pyspark=2.4.7, python=3.7)</span></span><br><span class="line"><span class="comment"># conda activate pyspark37</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> SPARK-SUBMIT</span></span><br><span class="line"><span class="comment"># $&#123;SPARK_HOME&#125;/bin/spark-submit --master local[*] \</span></span><br><span class="line"><span class="comment"># --deploy-mode client \</span></span><br><span class="line"><span class="comment"># --driver-class-path file:/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar \</span></span><br><span class="line"><span class="comment"># --driver-class-path file:/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar \</span></span><br><span class="line"><span class="comment"># --jars file:/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar \</span></span><br><span class="line"><span class="comment"># --jars file:/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar \</span></span><br><span class="line"><span class="comment"># /exp/work/pyspark/nebula/pagerank.py</span></span><br></pre></td></tr></table></figure><h3 id="Spark-submit-1"><a href="#Spark-submit-1" class="headerlink" title="Spark-submit"></a>Spark-submit</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$&#123;SPARK_HOME&#125;</span>/bin/spark-submit --master <span class="built_in">local</span>[*] \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">--driver-class-path file:/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar \</span><br><span class="line">--driver-class-path file:/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar \</span><br><span class="line">--jars file:/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar \</span><br><span class="line">--jars file:/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar \</span><br><span class="line">/exp/work/pyspark/nebula/pagerank.py</span><br></pre></td></tr></table></figure><h3 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br></pre></td><td class="code"><pre><span class="line">ssh://root@192.168.100.43:22/root/anaconda3/envs/pyspark37/bin/python -u /exp/work/pyspark/nebula/pagerank.py</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/exp/server/spark-2.4.7-bin-hadoop2.7/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]</span><br><span class="line">23/03/24 14:42:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Setting default log level to &quot;WARN&quot;.</span><br><span class="line">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</span><br><span class="line">+--------------------+--------------------+-----+-------------+------------+</span><br><span class="line">|              _srcId|              _dstId|_rank|creation_time|display_desc|</span><br><span class="line">+--------------------+--------------------+-----+-------------+------------+</span><br><span class="line">|dwd_accounttelegr...|dwd_phone66802018560|    0|   1678692201|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone12366827389|    0|   1678692201|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone86151162...|    0|   1678692201|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone79653470394|    0|   1678692210|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone99890918...|    0|   1678692210|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone99890535...|    0|   1678692210|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone85620578...|    0|   1678692210|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone99899666...|    0|   1678692210|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone88802607465|    0|   1678692219|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone62831556...|    0|   1678692228|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone95965067...|    0|   1678692228|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone85595212324|    0|   1678692228|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone86132873...|    0|   1678692279|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone63948774...|    0|   1678692279|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone85590645678|    0|   1678692279|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone63948229...|    0|   1678692288|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone99899913...|    0|   1678692288|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone99890302...|    0|   1678692288|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone99890986...|    0|   1678692288|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone16728042478|    0|   1678692297|  注册手机号|</span><br><span class="line">+--------------------+--------------------+-----+-------------+------------+</span><br><span class="line">only showing top 20 rows</span><br><span class="line"></span><br><span class="line">23/03/24 14:42:39 WARN window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.</span><br><span class="line">23/03/24 14:42:39 WARN window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.</span><br><span class="line">[Stage 3:&gt;                  (0 + 1) / 1][Stage 4:&gt;                  (0 + 1) / 1]23/03/24 14:42:49 WARN storage.BlockManager: Block rdd_29_0 already exists on this machine; not re-adding it</span><br><span class="line">23/03/24 14:42:50 WARN window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.</span><br><span class="line">23/03/24 14:42:50 WARN window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.</span><br><span class="line">23/03/24 14:42:50 WARN window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.</span><br><span class="line">+----------------------+------+---------------------------------+------+</span><br><span class="line">|dst_vid               |_dstId|src_vid                          |_srcId|</span><br><span class="line">+----------------------+------+---------------------------------+------+</span><br><span class="line">|dwd_phone8613951884225|18141 |dwd_accountctrip1187326021       |1     |</span><br><span class="line">|dwd_phone8618705166775|19486 |dwd_accountctrip18705166775      |2     |</span><br><span class="line">|dwd_phone8613006584273|17582 |dwd_accountctripM2295153681      |3     |</span><br><span class="line">|dwd_phone8615668271999|18526 |dwd_accountctripM2494074526      |4     |</span><br><span class="line">|dwd_phone8616653556969|18757 |dwd_accountctripM2682271769      |5     |</span><br><span class="line">|dwd_phone8613562835888|17938 |dwd_accountctripM3011519721      |6     |</span><br><span class="line">|dwd_phone8618506413343|19344 |dwd_accountctripM4761127290      |7     |</span><br><span class="line">|dwd_phone8618112957512|19178 |dwd_accountctripM4915141465      |8     |</span><br><span class="line">|dwd_phone8615119464037|18222 |dwd_accountctripM548327259       |9     |</span><br><span class="line">|dwd_phone8613123368388|17719 |dwd_accountctripM601536478       |10    |</span><br><span class="line">|dwd_phone8617605442050|18905 |dwd_accountctrip_WeChat2269232661|11    |</span><br><span class="line">|dwd_phone8613123368388|17719 |dwd_accountdidi101108284         |12    |</span><br><span class="line">|dwd_phone8616653556969|18757 |dwd_accountdidi1641492054156     |13    |</span><br><span class="line">|dwd_phone8615851895366|18609 |dwd_accountdidi1746082933402     |14    |</span><br><span class="line">|dwd_phone8617561929739|18889 |dwd_accountdidi17598416475664    |15    |</span><br><span class="line">|dwd_phone8615119464037|18222 |dwd_accountdidi25441524          |16    |</span><br><span class="line">|dwd_phone8613006584273|17582 |dwd_accountdidi2881218873540     |17    |</span><br><span class="line">|dwd_phone8613951884225|18141 |dwd_accountdidi3099925           |18    |</span><br><span class="line">|dwd_phone8618600764544|19397 |dwd_accountdidi486186360832      |19    |</span><br><span class="line">|dwd_phone8618705166775|19486 |dwd_accountdidi772722            |20    |</span><br><span class="line">+----------------------+------+---------------------------------+------+</span><br><span class="line">only showing top 20 rows</span><br><span class="line"></span><br><span class="line">+-----+------------------+</span><br><span class="line">|_id  |pagerank          |</span><br><span class="line">+-----+------------------+</span><br><span class="line">|19021|1.1105466561585526|</span><br><span class="line">|9831 |0.8884373249268421|</span><br><span class="line">|5354 |0.8884373249268421|</span><br><span class="line">|4926 |0.8884373249268421|</span><br><span class="line">|21377|1.1105466561585526|</span><br><span class="line">|14609|1.1105466561585526|</span><br><span class="line">|11852|1.1105466561585526|</span><br><span class="line">|8390 |0.8884373249268421|</span><br><span class="line">|10837|0.8884373249268421|</span><br><span class="line">|4992 |0.8884373249268421|</span><br><span class="line">|20894|1.1105466561585526|</span><br><span class="line">|21780|1.1105466561585526|</span><br><span class="line">|1813 |0.8884373249268421|</span><br><span class="line">|9025 |0.8884373249268421|</span><br><span class="line">|14554|1.1105466561585526|</span><br><span class="line">|1780 |0.8884373249268421|</span><br><span class="line">|16132|1.1105466561585526|</span><br><span class="line">|22467|1.1105466561585526|</span><br><span class="line">|2117 |0.8884373249268421|</span><br><span class="line">|16321|1.1105466561585526|</span><br><span class="line">+-----+------------------+</span><br><span class="line">only showing top 20 rows</span><br><span class="line"></span><br><span class="line">+-----+------------------+</span><br><span class="line">|_id  |pagerank          |</span><br><span class="line">+-----+------------------+</span><br><span class="line">|18222|3.331639968475657 |</span><br><span class="line">|19178|2.8874213060122367|</span><br><span class="line">|19486|2.8874213060122367|</span><br><span class="line">|12765|1.9989839810853947|</span><br><span class="line">|13332|1.9989839810853947|</span><br><span class="line">|13505|1.9989839810853947|</span><br><span class="line">|17582|1.9989839810853947|</span><br><span class="line">|18889|1.776874649853684 |</span><br><span class="line">|19176|1.776874649853684 |</span><br><span class="line">|12470|1.776874649853684 |</span><br><span class="line">|19397|1.5547653186219736|</span><br><span class="line">|18141|1.5547653186219736|</span><br><span class="line">|11750|1.5547653186219736|</span><br><span class="line">|18555|1.5547653186219736|</span><br><span class="line">|13216|1.5547653186219736|</span><br><span class="line">|18905|1.5547653186219736|</span><br><span class="line">|17688|1.5547653186219736|</span><br><span class="line">|19734|1.5547653186219736|</span><br><span class="line">|12768|1.5547653186219736|</span><br><span class="line">|18609|1.5547653186219736|</span><br><span class="line">|18347|1.332655987390263 |</span><br><span class="line">|19473|1.332655987390263 |</span><br><span class="line">|12007|1.332655987390263 |</span><br><span class="line">|12100|1.332655987390263 |</span><br><span class="line">|16268|1.332655987390263 |</span><br><span class="line">|19298|1.332655987390263 |</span><br><span class="line">|23258|1.332655987390263 |</span><br><span class="line">|17938|1.332655987390263 |</span><br><span class="line">|20042|1.332655987390263 |</span><br><span class="line">|12639|1.332655987390263 |</span><br><span class="line">|13309|1.332655987390263 |</span><br><span class="line">|12739|1.332655987390263 |</span><br><span class="line">|18037|1.332655987390263 |</span><br><span class="line">|18757|1.332655987390263 |</span><br><span class="line">|18344|1.332655987390263 |</span><br><span class="line">|12461|1.332655987390263 |</span><br><span class="line">|20037|1.332655987390263 |</span><br><span class="line">|13754|1.332655987390263 |</span><br><span class="line">|18864|1.332655987390263 |</span><br><span class="line">|21713|1.332655987390263 |</span><br><span class="line">|16872|1.332655987390263 |</span><br><span class="line">|15710|1.332655987390263 |</span><br><span class="line">|12229|1.332655987390263 |</span><br><span class="line">|19183|1.332655987390263 |</span><br><span class="line">|17928|1.332655987390263 |</span><br><span class="line">|19118|1.332655987390263 |</span><br><span class="line">|20243|1.332655987390263 |</span><br><span class="line">|12223|1.332655987390263 |</span><br><span class="line">|23219|1.332655987390263 |</span><br><span class="line">|12529|1.332655987390263 |</span><br><span class="line">|20865|1.332655987390263 |</span><br><span class="line">|16538|1.332655987390263 |</span><br><span class="line">|13253|1.332655987390263 |</span><br><span class="line">|13329|1.332655987390263 |</span><br><span class="line">|13198|1.332655987390263 |</span><br><span class="line">|17719|1.332655987390263 |</span><br><span class="line">|14661|1.1105466561585526|</span><br><span class="line">|18584|1.1105466561585526|</span><br><span class="line">|12532|1.1105466561585526|</span><br><span class="line">|20466|1.1105466561585526|</span><br><span class="line">+-----+------------------+</span><br><span class="line">only showing top 60 rows</span><br></pre></td></tr></table></figure><h3 id="封装并写回nebula-graph实例"><a href="#封装并写回nebula-graph实例" class="headerlink" title="封装并写回nebula-graph实例"></a>封装并写回nebula-graph实例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> findspark</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession, DataFrame</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.window <span class="keyword">import</span> Window</span><br><span class="line"><span class="keyword">from</span> py4j.java_gateway <span class="keyword">import</span> java_import</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span> nebula_config</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> environment variables</span></span><br><span class="line">os.environ[<span class="string">"HADOOP_CONF_DIR"</span>] = <span class="string">"/exp/server/hadoop-2.7.7/etc/hadoop"</span></span><br><span class="line">os.environ[<span class="string">"YARN_CONF_DIR"</span>] = <span class="string">"/exp/server/hadoop-2.7.7/etc/hadoop"</span></span><br><span class="line">os.environ[<span class="string">"SPARK_HOME"</span>] = <span class="string">"/exp/server/spark-2.4.7-bin-hadoop2.7"</span></span><br><span class="line">os.environ[<span class="string">"PYSPARK_PYTHON"</span>] = <span class="string">"/root/anaconda3/envs/pyspark37/bin/python"</span></span><br><span class="line">os.environ[<span class="string">"PYSPARK_DRIVER_PYTHON"</span>] = <span class="string">"/root/anaconda3/envs/pyspark37/bin/python"</span></span><br><span class="line">findspark.init()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PageRank</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    pagerank</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.df = <span class="literal">None</span></span><br><span class="line">        self.dfc = <span class="literal">None</span></span><br><span class="line">        self.tag = <span class="literal">None</span></span><br><span class="line">        self.id_df = <span class="literal">None</span></span><br><span class="line">        self.df_int = <span class="literal">None</span></span><br><span class="line">        self.encode_id = <span class="literal">None</span></span><br><span class="line">        self.src_id_df = <span class="literal">None</span></span><br><span class="line">        self.dst_id_df = <span class="literal">None</span></span><br><span class="line">        self.src_join_df = <span class="literal">None</span></span><br><span class="line">        self.dst_join_df = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> spark object with nebula-spark-connector &amp; nebula-algorithm</span></span><br><span class="line">        self.spark = SparkSession.builder.appName(</span><br><span class="line">            <span class="string">"pagerank"</span>)\</span><br><span class="line">            .master(</span><br><span class="line">            <span class="string">"local[*]"</span>)\</span><br><span class="line">            .config(</span><br><span class="line">            <span class="string">"spark.jars"</span>,</span><br><span class="line">            <span class="string">"/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar"</span>)\</span><br><span class="line">            .config(</span><br><span class="line">            <span class="string">"spark.driver.extraClassPath"</span>,</span><br><span class="line">            <span class="string">"/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar"</span>)\</span><br><span class="line">            .config(</span><br><span class="line">            <span class="string">"spark.jars"</span>,</span><br><span class="line">            <span class="string">"/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar"</span>)\</span><br><span class="line">            .config(</span><br><span class="line">            <span class="string">"spark.driver.extraClassPath"</span>,</span><br><span class="line">            <span class="string">"/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar"</span>)\</span><br><span class="line">            .getOrCreate()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> java import</span></span><br><span class="line">        self.jspark = self.spark._jsparkSession</span><br><span class="line"></span><br><span class="line">        <span class="comment"># import "com.vesoft.nebula.algorithm.config.SparkConfig"</span></span><br><span class="line">        java_import(self.spark._jvm, <span class="string">"com.vesoft.nebula.algorithm.config.SparkConfig"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># import "com.vesoft.nebula.algorithm.config.PRConfig"</span></span><br><span class="line">        java_import(self.spark._jvm, <span class="string">"com.vesoft.nebula.algorithm.config.PRConfig"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># import "com.vesoft.nebula.algorithm.lib.PageRankAlgo"</span></span><br><span class="line">        java_import(self.spark._jvm, <span class="string">"com.vesoft.nebula.algorithm.lib.PageRankAlgo"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_tag</span><span class="params">(self, tag: list)</span>:</span></span><br><span class="line">        self.tag = tag</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">exe</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.sdf_create()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> covert fixed_string vid 2 long vid</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">convert</span><span class="params">(self, df)</span>:</span></span><br><span class="line">        self.df = df.drop(<span class="string">"creation_time"</span>)</span><br><span class="line">        self.src_id_df = self.df.select(<span class="string">"_srcId"</span>).withColumnRenamed(<span class="string">"_srcId"</span>, <span class="string">"id"</span>)</span><br><span class="line">        self.dst_id_df = self.df.select(<span class="string">"_dstId"</span>).withColumnRenamed(<span class="string">"_dstId"</span>, <span class="string">"id"</span>)</span><br><span class="line">        self.id_df = self.src_id_df.union(self.dst_id_df).distinct()</span><br><span class="line">        self.encode_id = self.id_df.withColumn(<span class="string">"encodedId"</span>, dense_rank().over(Window.orderBy(<span class="string">"id"</span>)))</span><br><span class="line">        self.src_join_df = self.df.join(self.encode_id, self.df._srcId == self.encode_id.id) \</span><br><span class="line">            .drop(<span class="string">"_srcId"</span>) \</span><br><span class="line">            .drop(<span class="string">"id"</span>) \</span><br><span class="line">            .withColumnRenamed(<span class="string">"encodedId"</span>, <span class="string">"_srcId"</span>)</span><br><span class="line"></span><br><span class="line">        self.dst_join_df = self.src_join_df.join(self.encode_id, self.src_join_df._dstId == self.encode_id.id) \</span><br><span class="line">            .drop(<span class="string">"_dstId"</span>) \</span><br><span class="line">            .drop(<span class="string">"_rank"</span>) \</span><br><span class="line">            .drop(<span class="string">"degree"</span>) \</span><br><span class="line">            .withColumnRenamed(<span class="string">"encodedId"</span>, <span class="string">"_dstId"</span>).drop(<span class="string">"id"</span>)</span><br><span class="line">        <span class="comment"># self.dst_join_df.write.option("header", True).csv("file:/exp/work/pyspark/nebula/pr_file/1.csv")</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> sdf build by nebula_reader</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sdf_create</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.tag)):</span><br><span class="line">            self.dfc = self.spark.read.format(</span><br><span class="line">                <span class="string">"com.vesoft.nebula.connector.NebulaDataSource"</span>).option(</span><br><span class="line">                <span class="string">"type"</span>, <span class="string">"edge"</span>).option(</span><br><span class="line">                <span class="string">"spaceName"</span>, nebula_config.get(<span class="string">"space"</span>)).option(</span><br><span class="line">                <span class="string">"label"</span>, <span class="string">f"<span class="subst">&#123;self.tag[i]&#125;</span>"</span>).option(</span><br><span class="line">                <span class="string">"returnCols"</span>, <span class="string">"creation_time"</span>).option(</span><br><span class="line">                <span class="string">"metaAddress"</span>, nebula_config.get(<span class="string">"metaAddress"</span>)).option(</span><br><span class="line">                <span class="string">"partitionNumber"</span>, nebula_config.get(<span class="string">"partitionNumber"</span>)).load()</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">                self.df = self.dfc</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.df = self.df.unionByName(self.dfc)</span><br><span class="line"></span><br><span class="line">        self.convert(self.df)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> NEBULA-READER</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nebula_reader</span><span class="params">(sth, tag)</span>:</span></span><br><span class="line">    dataframe = sth.spark.read.format(</span><br><span class="line">        <span class="string">"com.vesoft.nebula.connector.NebulaDataSource"</span>).option(</span><br><span class="line">        <span class="string">"type"</span>, <span class="string">"vertex"</span>).option(</span><br><span class="line">        <span class="string">"spaceName"</span>, nebula_config.get(<span class="string">"space"</span>)).option(</span><br><span class="line">        <span class="string">"label"</span>, <span class="string">f"<span class="subst">&#123;tag&#125;</span>"</span>).option(</span><br><span class="line">        <span class="string">"returnCols"</span>, <span class="string">"pagerank"</span>).option(</span><br><span class="line">        <span class="string">"metaAddress"</span>, nebula_config.get(<span class="string">"metaAddress"</span>)).option(</span><br><span class="line">        <span class="string">"partitionNumber"</span>, nebula_config.get(<span class="string">"partitionNumber"</span>)).load().drop(<span class="string">"pagerank"</span>)</span><br><span class="line">    <span class="keyword">return</span> dataframe</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> UPDATE NEBULA-GRAPH</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nebula_writer</span><span class="params">(tag, dataframe)</span>:</span></span><br><span class="line">    dataframe.write.format(<span class="string">"com.vesoft.nebula.connector.NebulaDataSource"</span>).option(</span><br><span class="line">        <span class="string">"type"</span>, <span class="string">"vertex"</span>).option(</span><br><span class="line">        <span class="string">"spaceName"</span>, nebula_config.get(<span class="string">"space"</span>)).option(</span><br><span class="line">        <span class="string">"label"</span>, <span class="string">f"<span class="subst">&#123;tag&#125;</span>"</span>).option(</span><br><span class="line">        <span class="string">"vidPolicy"</span>, <span class="string">""</span>).option(</span><br><span class="line">        <span class="string">"vertexField"</span>, <span class="string">"_vertexId"</span>).option(</span><br><span class="line">        <span class="string">"writeMode"</span>, <span class="string">"update"</span>).option(</span><br><span class="line">        <span class="string">"batch"</span>, nebula_config.get(<span class="string">"batch"</span>)).option(</span><br><span class="line">        <span class="string">"metaAddress"</span>, nebula_config.get(<span class="string">"metaAddress"</span>)).option(</span><br><span class="line">        <span class="string">"graphAddress"</span>, nebula_config.get(<span class="string">"graphAddress"</span>)).option(</span><br><span class="line">        <span class="string">"passwd"</span>, nebula_config.get(<span class="string">"passwd"</span>)).option(</span><br><span class="line">        <span class="string">"user"</span>, nebula_config.get(<span class="string">"user"</span>)).save()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> process data</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(tag: list, *args)</span>:</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> algorithm object</span></span><br><span class="line">    obj = PageRank()</span><br><span class="line">    obj.set_tag(tag)</span><br><span class="line">    obj.exe()</span><br><span class="line"></span><br><span class="line">    encode_id = obj.encode_id</span><br><span class="line">    df_int = obj.dst_join_df</span><br><span class="line">    df_spark = df_int</span><br><span class="line"></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> nebula-algorithm</span></span><br><span class="line">    config = obj.spark._jvm.PRConfig(<span class="number">1</span>, <span class="number">0.8</span>)</span><br><span class="line">    result = obj.spark._jvm.PageRankAlgo.apply(obj.jspark, df_spark._jdf, config, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> jdf to sdf</span></span><br><span class="line">    result = result.toDF()</span><br><span class="line">    algo_df = DataFrame(result, obj.spark)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> long vid mapping fixed_string vid</span></span><br><span class="line">    df = encode_id.join(algo_df, encode_id.encodedId == algo_df._id) \</span><br><span class="line">        .drop(<span class="string">"encodedId"</span>) \</span><br><span class="line">        .drop(<span class="string">"_id"</span>) \</span><br><span class="line">        .withColumnRenamed(<span class="string">"id"</span>, <span class="string">"_vertexId"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> arg <span class="keyword">in</span> args:</span><br><span class="line">        df_ = nebula_reader(obj, arg)</span><br><span class="line">        df_ = df_.join(df, df_._vertexId == df._vertexId, <span class="string">"leftsemi"</span>)</span><br><span class="line">        df__ = df.join(df_, df._vertexId == df_._vertexId, <span class="string">"leftsemi"</span>)</span><br><span class="line">        nebula_writer(arg, df__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    process([<span class="string">"edge_phone"</span>], <span class="string">"dwd_phone"</span>, <span class="string">"dwd_account"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> PYSPARK (pyspark=2.4.7, python=3.7)</span></span><br><span class="line"><span class="comment"># conda activate pyspark37</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> SPARK-SUBMIT</span></span><br><span class="line"><span class="comment"># $&#123;SPARK_HOME&#125;/bin/spark-submit --master local[*] \</span></span><br><span class="line"><span class="comment"># --deploy-mode client \</span></span><br><span class="line"><span class="comment"># --driver-class-path file:/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar \</span></span><br><span class="line"><span class="comment"># --driver-class-path file:/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar \</span></span><br><span class="line"><span class="comment"># --jars file:/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar \</span></span><br><span class="line"><span class="comment"># --jars file:/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar \</span></span><br><span class="line"><span class="comment"># /exp/work/pyspark/nebula/pagerank.py</span></span><br></pre></td></tr></table></figure><h3 id="fastapi接口实例"><a href="#fastapi接口实例" class="headerlink" title="fastapi接口实例"></a>fastapi接口实例</h3><p><code>`</code>python<br>import os</p><p>import findspark</p><h1 id="from-pyspark-sql-import-SparkSession"><a href="#from-pyspark-sql-import-SparkSession" class="headerlink" title="from pyspark.sql import SparkSession"></a>from pyspark.sql import SparkSession</h1><p>from pyspark.sql import DataFrame<br>from pyspark.sql.functions import *<br>from typing import List<br>from fastapi import FastAPI, Query<br>from fastapi.middleware.cors import CORSMiddleware<br>from pydantic import BaseModel</p><p>from config import nebula_config, pagerank_dict<br>from algorithm.pagerank import PageRank<br>from algorithm.louvain import Louvain</p><p>app_router = FastAPI(docs_url=None, redoc_url=None)</p><h1 id="CORS"><a href="#CORS" class="headerlink" title="CORS"></a>CORS</h1><p>origins = [<br>    “*”<br>]</p><p>app_router.add_middleware(<br>    CORSMiddleware,<br>    allow_origins=origins,<br>    allow_credentials=True,<br>    allow_methods=[“<em>“],<br>    allow_headers=[“</em>“]<br>)</p><p>class NebulaAlgorithm(BaseModel):<br>    algo: str<br>    tag_list: List[str]</p><p>algorithm_dict = {<br>    “pagerank”: PageRank(),<br>    “louvain”: Louvain()<br>}</p><p>@app_router.post(“/nebula/algo”)<br>async def nebula_algorithm(<br>        args: NebulaAlgorithm<br>):</p><pre><code># TODO: algorithm objecttags = []for line in args.tag_list:    for item in pagerank_dict.get(line):        tags.append(item)algo = algorithm_dict.get(args.algo)obj = algoobj.set_tag(tags)obj.exe()# TODO: nebula-algorithmencode_id = obj.encode_iddf_int = obj.dst_join_dfif algo.__doc__.strip() == &apos;pagerank&apos;:    if len(tags) == 1:        df_spark = df_int    else:        df_pd = df_int.toPandas()        values = df_pd.values.tolist()        columns = df_pd.columns.tolist()        df_spark = obj.spark.createDataFrame(values, columns)    config = obj.spark._jvm.PRConfig(1, 0.8)    result = obj.spark._jvm.PageRankAlgo.apply(obj.jspark, df_spark._jdf, config, False)elif algo.__doc__.strip() == &apos;louvain&apos;:    if len(tags) == 1:        df_spark = df_int    else:        df_pd = df_int.toPandas()        values = df_pd.values.tolist()        columns = df_pd.columns.tolist()        df_spark = obj.spark.createDataFrame(values, columns)    config = obj.spark._jvm.LouvainConfig(20, 10, 0.5)    result = obj.spark._jvm.LouvainAlgo.apply(obj.jspark, df_spark._jdf, config, False)else:    return# TODO: jdf to sdfresult = result.toDF()algo_df = DataFrame(result, obj.spark)# TODO: long vid mapping fixed_string viddf = encode_id.join(algo_df, encode_id.encodedId == algo_df._id) \    .drop(&quot;encodedId&quot;) \    .drop(&quot;_id&quot;) \    .withColumnRenamed(&quot;id&quot;, &quot;_vertexId&quot;)df.show()</code></pre><p>if <strong>name</strong> == “<strong>main</strong>“:<br>    import uvicorn<br>    uvicorn.run(app=”main:app_router”, reload=True, host=’0.0.0.0’, port=7792)</p><h1 id="TODO-PYSPARK-pyspark-2-4-7-python-3-7"><a href="#TODO-PYSPARK-pyspark-2-4-7-python-3-7" class="headerlink" title="TODO: PYSPARK (pyspark=2.4.7, python=3.7)"></a>TODO: PYSPARK (pyspark=2.4.7, python=3.7)</h1><h1 id="conda-activate-pyspark37"><a href="#conda-activate-pyspark37" class="headerlink" title="conda activate pyspark37"></a>conda activate pyspark37</h1><h1 id="TODO-SPARK-SUBMIT"><a href="#TODO-SPARK-SUBMIT" class="headerlink" title="TODO: SPARK-SUBMIT"></a>TODO: SPARK-SUBMIT</h1><h1 id="SPARK-HOME-bin-spark-submit-–master-local"><a href="#SPARK-HOME-bin-spark-submit-–master-local" class="headerlink" title="${SPARK_HOME}/bin/spark-submit –master local[*] \"></a>${SPARK_HOME}/bin/spark-submit –master local[*] \</h1><h1 id="–deploy-mode-client"><a href="#–deploy-mode-client" class="headerlink" title="–deploy-mode client \"></a>–deploy-mode client \</h1><h1 id="–driver-class-path-file-exp-work-pyspark-nebula-spark-connector-nebula-spark-connector-target-nebula-spark-connector-2-6-1-jar"><a href="#–driver-class-path-file-exp-work-pyspark-nebula-spark-connector-nebula-spark-connector-target-nebula-spark-connector-2-6-1-jar" class="headerlink" title="–driver-class-path file:/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar \"></a>–driver-class-path file:/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar \</h1><h1 id="–driver-class-path-file-exp-work-pyspark-nebula-algorithm-nebula-algorithm-target-nebula-algorithm-2-6-1-jar"><a href="#–driver-class-path-file-exp-work-pyspark-nebula-algorithm-nebula-algorithm-target-nebula-algorithm-2-6-1-jar" class="headerlink" title="–driver-class-path file:/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar \"></a>–driver-class-path file:/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar \</h1><h1 id="–jars-file-exp-work-pyspark-nebula-spark-connector-nebula-spark-connector-target-nebula-spark-connector-2-6-1-jar"><a href="#–jars-file-exp-work-pyspark-nebula-spark-connector-nebula-spark-connector-target-nebula-spark-connector-2-6-1-jar" class="headerlink" title="–jars file:/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar \"></a>–jars file:/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar \</h1><h1 id="–jars-file-exp-work-pyspark-nebula-algorithm-nebula-algorithm-target-nebula-algorithm-2-6-1-jar"><a href="#–jars-file-exp-work-pyspark-nebula-algorithm-nebula-algorithm-target-nebula-algorithm-2-6-1-jar" class="headerlink" title="–jars file:/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar \"></a>–jars file:/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar \</h1><h1 id="exp-work-pyspark-nebula-main-py"><a href="#exp-work-pyspark-nebula-main-py" class="headerlink" title="/exp/work/pyspark/nebula/main.py"></a>/exp/work/pyspark/nebula/main.py</h1>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;Nebula-Spark-Connector&quot;&gt;&lt;a href=&quot;#Nebula-Spark-Connector&quot; class=&quot;headerlink&quot; title=&quot;Nebula Spark Connector&quot;&gt;&lt;/a&gt;Nebula Spark Connector&lt;/h2&gt;&lt;p&gt;下载地址&amp;amp;官方文档：【&lt;a href=&quot;https://github.com/vesoft-inc/nebula-spark-connector&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/vesoft-inc/nebula-spark-connector&lt;/a&gt;】&lt;/p&gt;
&lt;h3 id=&quot;环境&quot;&gt;&lt;a href=&quot;#环境&quot; class=&quot;headerlink&quot; title=&quot;环境&quot;&gt;&lt;/a&gt;环境&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;·&lt;/strong&gt; nebula：&lt;font color=&quot;orange&quot;&gt;2.6.1&lt;/font&gt;&lt;br&gt;&lt;strong&gt;·&lt;/strong&gt; hadoop：&lt;font color=&quot;orange&quot;&gt;2.7&lt;/font&gt;&lt;br&gt;&lt;strong&gt;·&lt;/strong&gt; spark：&lt;font color=&quot;orange&quot;&gt;2.4.7&lt;/font&gt;&lt;br&gt;&lt;strong&gt;·&lt;/strong&gt; pyspark：&lt;font color=&quot;orange&quot;&gt;2.4.7&lt;/font&gt;&lt;br&gt;&lt;strong&gt;·&lt;/strong&gt; python：&lt;font color=&quot;orange&quot;&gt;3.7.16&lt;/font&gt;&lt;br&gt;&lt;strong&gt;·&lt;/strong&gt; nebula-spark-connector：&lt;font color=&quot;orange&quot;&gt;2.6.1&lt;/font&gt;&lt;/p&gt;
&lt;h3 id=&quot;编译打包nebula-spark-connector&quot;&gt;&lt;a href=&quot;#编译打包nebula-spark-connector&quot; class=&quot;headerlink&quot; title=&quot;编译打包nebula-spark-connector&quot;&gt;&lt;/a&gt;编译打包nebula-spark-connector&lt;/h3&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ &lt;span class=&quot;built_in&quot;&gt;cd&lt;/span&gt; nebula-spark-connector-2.6.1/nebula-spark-connector&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ mvn clean package -Dmaven.test.skip=&lt;span class=&quot;literal&quot;&gt;true&lt;/span&gt; -Dgpg.skip -Dmaven.javadoc.skip=&lt;span class=&quot;literal&quot;&gt;true&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;成功后在&lt;code&gt;nebula-spark-connector/target/&lt;/code&gt; 目录下得到 &lt;code&gt;nebula-spark-connector-2.6.1.jar&lt;/code&gt;文件&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;(base) [root@root target]&lt;span class=&quot;comment&quot;&gt;# ll&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;total 106792&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;drwxr-xr-x 3 root root        17 Mar 11 14:14 classes&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;-rw-r--r-- 1 root root         1 Mar 11 14:14 classes.-497386701.timestamp&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;-rw-r--r-- 1 root root         1 Mar 11 14:14 classes.timestamp&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;-rw-r--r-- 1 root root     30701 Mar 11 14:15 jacoco.exec&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;drwxr-xr-x 2 root root        28 Mar 11 14:15 maven-archiver&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;-rw-r--r-- 1 root root 108375457 Mar 11 14:16 nebula-spark-connector-2.6.1.jar&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;-rw-r--r-- 1 root root    583482 Mar 11 14:16 nebula-spark-connector-2.6.1-javadoc.jar&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;-rw-r--r-- 1 root root     36358 Mar 11 14:16 nebula-spark-connector-2.6.1-sources.jar&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;-rw-r--r-- 1 root root    315392 Mar 11 14:15 original-nebula-spark-connector-2.6.1.jar&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;drwxr-xr-x 4 root root        37 Mar 11 14:15 site&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;PySpark-读取-NebulaGraph-数据&quot;&gt;&lt;a href=&quot;#PySpark-读取-NebulaGraph-数据&quot; class=&quot;headerlink&quot; title=&quot;PySpark 读取 NebulaGraph 数据&quot;&gt;&lt;/a&gt;PySpark 读取 NebulaGraph 数据&lt;/h3&gt;&lt;p&gt;从 &lt;code&gt;metaAddress&lt;/code&gt; 为 &lt;code&gt;&amp;quot;metad0:9559&amp;quot;&lt;/code&gt; 的 Nebula Graph 中读取整个 tag 下的数据为一个 dataframe：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;df = spark.read.format(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;string&quot;&gt;&quot;com.vesoft.nebula.connector.NebulaDataSource&quot;&lt;/span&gt;).option(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;type&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;vertex&quot;&lt;/span&gt;).option(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;spaceName&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;basketballplayer&quot;&lt;/span&gt;).option(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;label&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;player&quot;&lt;/span&gt;).option(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;returnCols&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;name,age&quot;&lt;/span&gt;).option(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;metaAddress&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;metad0:9559&quot;&lt;/span&gt;).option(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;partitionNumber&quot;&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;).load()&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;然后可以像这样 &lt;code&gt;show&lt;/code&gt; 这个 dataframe：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;&amp;gt;&amp;gt;&amp;gt; &lt;/span&gt;df.show(n=&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;+---------+--------------+---+&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|_vertexId|          name|age|&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;+---------+--------------+---+&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|player105|   Danny Green| &lt;span class=&quot;number&quot;&gt;31&lt;/span&gt;|&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|player109|Tiago Splitter| &lt;span class=&quot;number&quot;&gt;34&lt;/span&gt;|&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;+---------+--------------+---+&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;only showing top &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt; rows&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
    
    <category term="nebula" scheme="http://yoursite.com/tags/nebula/"/>
    
    <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>ElasticSearch环境搭建</title>
    <link href="http://yoursite.com/2023/02/16/ElasticSearch%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    <id>http://yoursite.com/2023/02/16/ElasticSearch%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</id>
    <published>2023-02-16T03:16:07.000Z</published>
    <updated>2023-03-24T06:52:51.416Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p>* 配合nebula全文检索测试环境而搭建的es单机环境</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><h3 id="下载地址"><a href="#下载地址" class="headerlink" title="下载地址"></a>下载地址</h3><p>【<a href="https://www.elastic.co/cn/downloads/elasticsearch#ga-release" target="_blank" rel="noopener">https://www.elastic.co/cn/downloads/elasticsearch#ga-release</a>】 (或前往elastic中文社区下载中心【<a href="https://elasticsearch.cn/download/" target="_blank" rel="noopener">https://elasticsearch.cn/download/</a>】)</p><ul><li>选择linux版本</li></ul><p><img src="/2023/02/16/ElasticSearch环境搭建/A.png" alt></p><h3 id="安装ES"><a href="#安装ES" class="headerlink" title="安装ES"></a>安装ES</h3><ul><li>解压缩</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> tar xf elasticsearch-7.14.2-linux-x86_64.tar.gz</span><br></pre></td></tr></table></figure><ul><li>创建es用户</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> useradd es &amp;&amp; passwd es</span><br></pre></td></tr></table></figure><ul><li>更名</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> mv elasticsearch-7.14.2 elasticsearch</span><br></pre></td></tr></table></figure><ul><li>赋予es用户权限</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> chown -R es:es elasticsearch</span><br></pre></td></tr></table></figure><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><pre><code>* 可使用es自带的java环境：ES_JAVA_HOME</code></pre><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> vim /etc/profile</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ES_JAVA_HOME</span></span><br><span class="line"><span class="built_in">export</span> ES_JAVA_HOME=/data/elasticsearch/jdk/</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$ES_JAVA_HOME</span>/bin:<span class="variable">$PATH</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">source</span> /etc/profile</span></span><br></pre></td></tr></table></figure><ul><li>elasticsearch config</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> elasticsearch/config</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vim elasticsearch.yml</span></span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">node.name:</span> <span class="string">node-1</span>                          <span class="comment">##节点名称</span></span><br><span class="line"><span class="attr">path.data:</span> <span class="string">/usr/local/elasticsearch/data</span>        <span class="comment">##数据存放路径</span></span><br><span class="line"><span class="attr">path.logs:</span> <span class="string">/usr/local/elasticsearch/logs</span>        <span class="comment">##日志存放路径 </span></span><br><span class="line"><span class="attr">bootstrap.memory_lock:</span> <span class="literal">true</span>                <span class="comment">##避免es使用swap交换分区</span></span><br><span class="line"><span class="attr">indices.requests.cache.size:</span> <span class="number">5</span><span class="string">%</span>            <span class="comment">##缓存配置</span></span><br><span class="line"><span class="attr">indices.queries.cache.size:</span> <span class="number">10</span><span class="string">%</span>            <span class="comment">##缓存配置</span></span><br><span class="line"><span class="attr">network.host:</span> <span class="number">192.168</span><span class="number">.80</span><span class="number">.128</span>               <span class="comment">##本机IP</span></span><br><span class="line"><span class="attr">http.port:</span> <span class="number">9200</span>                            <span class="comment">##默认端口</span></span><br><span class="line"><span class="attr">cluster.initial_master_nodes:</span> <span class="string">["node-1"]</span>   <span class="comment">##设置符合主节点条件的节点的主机名或 IP 地址来引导启动集群</span></span><br><span class="line"><span class="attr">http.cors.enabled:</span> <span class="literal">true</span>                    <span class="comment">##跨域</span></span><br><span class="line"><span class="attr">http.cors.allow-origin:</span> <span class="string">"*"</span></span><br></pre></td></tr></table></figure><ul><li>将当前用户软硬限制调大</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vim /etc/security/limits.conf</span></span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">es</span> <span class="string">soft</span> <span class="string">nofile</span> <span class="number">65535</span></span><br><span class="line"><span class="string">es</span> <span class="string">hard</span> <span class="string">nofile</span> <span class="number">65537</span></span><br><span class="line"><span class="string">es</span> <span class="string">soft</span> <span class="string">memlock</span> <span class="string">unlimited</span></span><br><span class="line"><span class="string">es</span> <span class="string">hard</span> <span class="string">memlock</span> <span class="string">unlimited</span></span><br></pre></td></tr></table></figure><ul><li>修改vm.max_map_count内存</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysctl.conf</span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">vm.max_map_count=655360</span></span><br></pre></td></tr></table></figure><h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> su es</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> ../bin</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ./elasticsearch -d</span></span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="Kibana"><a href="#Kibana" class="headerlink" title="Kibana"></a>Kibana</h2><h3 id="下载地址-1"><a href="#下载地址-1" class="headerlink" title="下载地址"></a>下载地址</h3><p>[<a href="https://elasticsearch.cn/download/]" target="_blank" rel="noopener">https://elasticsearch.cn/download/]</a></p><ul><li>选择和es相同版本</li></ul><h3 id="安装kibana"><a href="#安装kibana" class="headerlink" title="安装kibana"></a>安装kibana</h3><ul><li>解压缩</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> tar -zxvf kibana-7.14.2-linux-x86_64.tar.gz</span></span><br></pre></td></tr></table></figure><ul><li>更名</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mv kibana-7.14.2-linux-x86_64 kibana</span></span><br></pre></td></tr></table></figure><h3 id="配置-1"><a href="#配置-1" class="headerlink" title="配置"></a>配置</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd kibana</span><br><span class="line">vim config/kibana.yml</span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">server.port:</span> <span class="number">5601</span></span><br><span class="line"><span class="attr">server.host:</span> <span class="string">"0.0.0.0"</span></span><br><span class="line"><span class="attr">elasticsearch.hosts:</span> <span class="string">"http://192.168.80.128:9200"</span></span><br><span class="line"><span class="attr">kibana.index:</span> <span class="string">".kibana"</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 修改/etc/sudoers文件，进入超级用户, 给予es用户写权限</span></span><br><span class="line">(base) [root@localhost kibana]# chmod u+w /etc/sudoers</span><br><span class="line"><span class="meta">#</span><span class="bash"> 编辑/etc/sudoers文件</span></span><br><span class="line">(base) [root@localhost kibana]# vim /etc/sudoers</span><br><span class="line"><span class="meta">#</span><span class="bash"> 赋予es用户权限</span></span><br><span class="line">es    ALL=(ALL)       ALL</span><br><span class="line">(base) [root@localhost kibana]# chmod u-w /etc/sudoers</span><br><span class="line"><span class="meta">#</span><span class="bash"> 撤销es用户文件写权限</span></span><br><span class="line">(base) [root@localhost kibana]# sudo chown -R es:es /usr/local/kibana</span><br><span class="line">(base) [root@localhost kibana]# su es</span><br></pre></td></tr></table></figure><h3 id="启动-1"><a href="#启动-1" class="headerlink" title="启动"></a>启动</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[es@localhost kibana]$ cd bin</span><br><span class="line">[es@localhost bin]$ ./kibana</span><br></pre></td></tr></table></figure><ul><li>进入kibana</li></ul><p><code>http:192.168.80.128:5601</code></p><p><img src="/2023/02/16/ElasticSearch环境搭建/B.png" alt></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;* 配合nebula全文检索测试环境而搭建的es单机环境&lt;/p&gt;
&lt;h2 id=&quot;安装&quot;&gt;&lt;a href=&quot;#安装&quot; class=&quot;headerlink&quot; title=&quot;安装&quot;&gt;&lt;/a&gt;安装&lt;/h2&gt;&lt;h3 id=&quot;下载地址&quot;&gt;&lt;a href=&quot;#下载地址&quot; class=&quot;headerlink&quot; title=&quot;下载地址&quot;&gt;&lt;/a&gt;下载地址&lt;/h3&gt;&lt;p&gt;【&lt;a href=&quot;https://www.elastic.co/cn/downloads/elasticsearch#ga-release&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.elastic.co/cn/downloads/elasticsearch#ga-release&lt;/a&gt;】 (或前往elastic中文社区下载中心【&lt;a href=&quot;https://elasticsearch.cn/download/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://elasticsearch.cn/download/&lt;/a&gt;】)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;选择linux版本&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/2023/02/16/ElasticSearch环境搭建/A.png&quot; alt&gt;&lt;/p&gt;
&lt;h3 id=&quot;安装ES&quot;&gt;&lt;a href=&quot;#安装ES&quot; class=&quot;headerlink&quot; title=&quot;安装ES&quot;&gt;&lt;/a&gt;安装ES&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;解压缩&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; tar xf elasticsearch-7.14.2-linux-x86_64.tar.gz&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;创建es用户&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; useradd es &amp;amp;&amp;amp; passwd es&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;更名&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; mv elasticsearch-7.14.2 elasticsearch&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;赋予es用户权限&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; chown -R es:es elasticsearch&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;配置&quot;&gt;&lt;a href=&quot;#配置&quot; class=&quot;headerlink&quot; title=&quot;配置&quot;&gt;&lt;/a&gt;配置&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;* 可使用es自带的java环境：ES_JAVA_HOME
&lt;/code&gt;&lt;/pre&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; vim /etc/profile&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# ES_JAVA_HOME&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;export&lt;/span&gt; ES_JAVA_HOME=/data/elasticsearch/jdk/&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;export&lt;/span&gt; PATH=&lt;span class=&quot;variable&quot;&gt;$ES_JAVA_HOME&lt;/span&gt;/bin:&lt;span class=&quot;variable&quot;&gt;$PATH&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; &lt;span class=&quot;built_in&quot;&gt;source&lt;/span&gt; /etc/profile&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;elasticsearch config&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; &lt;span class=&quot;built_in&quot;&gt;cd&lt;/span&gt; elasticsearch/config&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; vim elasticsearch.yml&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight yaml&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;node.name:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;node-1&lt;/span&gt;                          &lt;span class=&quot;comment&quot;&gt;##节点名称&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;path.data:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;/usr/local/elasticsearch/data&lt;/span&gt;        &lt;span class=&quot;comment&quot;&gt;##数据存放路径&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;path.logs:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;/usr/local/elasticsearch/logs&lt;/span&gt;        &lt;span class=&quot;comment&quot;&gt;##日志存放路径 &lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;bootstrap.memory_lock:&lt;/span&gt; &lt;span class=&quot;literal&quot;&gt;true&lt;/span&gt;                &lt;span class=&quot;comment&quot;&gt;##避免es使用swap交换分区&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;indices.requests.cache.size:&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;string&quot;&gt;%&lt;/span&gt;            &lt;span class=&quot;comment&quot;&gt;##缓存配置&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;indices.queries.cache.size:&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;string&quot;&gt;%&lt;/span&gt;            &lt;span class=&quot;comment&quot;&gt;##缓存配置&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;network.host:&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;192.168&lt;/span&gt;&lt;span class=&quot;number&quot;&gt;.80&lt;/span&gt;&lt;span class=&quot;number&quot;&gt;.128&lt;/span&gt;               &lt;span class=&quot;comment&quot;&gt;##本机IP&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;http.port:&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;9200&lt;/span&gt;                            &lt;span class=&quot;comment&quot;&gt;##默认端口&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;cluster.initial_master_nodes:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;[&quot;node-1&quot;]&lt;/span&gt;   &lt;span class=&quot;comment&quot;&gt;##设置符合主节点条件的节点的主机名或 IP 地址来引导启动集群&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;http.cors.enabled:&lt;/span&gt; &lt;span class=&quot;literal&quot;&gt;true&lt;/span&gt;                    &lt;span class=&quot;comment&quot;&gt;##跨域&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;http.cors.allow-origin:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;*&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;将当前用户软硬限制调大&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; vim /etc/security/limits.conf&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight yaml&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;es&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;soft&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;nofile&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;65535&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;es&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;hard&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;nofile&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;65537&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;es&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;soft&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;memlock&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;unlimited&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;es&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;hard&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;memlock&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;unlimited&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;修改vm.max_map_count内存&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;vim /etc/sysctl.conf&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight yaml&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;vm.max_map_count=655360&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;启动&quot;&gt;&lt;a href=&quot;#启动&quot; class=&quot;headerlink&quot; title=&quot;启动&quot;&gt;&lt;/a&gt;启动&lt;/h3&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; su es&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; &lt;span class=&quot;built_in&quot;&gt;cd&lt;/span&gt; ../bin&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; ./elasticsearch -d&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
    
    <category term="elastic search" scheme="http://yoursite.com/tags/elastic-search/"/>
    
  </entry>
  
  <entry>
    <title>Spark-SQL</title>
    <link href="http://yoursite.com/2022/12/08/Spark-SQL/"/>
    <id>http://yoursite.com/2022/12/08/Spark-SQL/</id>
    <published>2022-12-08T08:50:52.000Z</published>
    <updated>2022-12-09T08:58:34.011Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><h2 id="SparkSQL"><a href="#SparkSQL" class="headerlink" title="SparkSQL"></a>SparkSQL</h2><p><strong>SparkSQL</strong>是spark的一个用于处理海量<strong>结构化数据</strong>的模块</p><ul><li>支持SQL语言</li><li>自动优化</li><li>性能强</li><li>兼容HIVE</li><li>API流程简单</li><li>支持标准化JDBC和ODBC连接</li><li>…</li></ul><h3 id="SparkSQL数据抽象"><a href="#SparkSQL数据抽象" class="headerlink" title="SparkSQL数据抽象"></a>SparkSQL数据抽象</h3><ul><li><p><strong><font color="orange">Pandas · DataFrame</font></strong></p><p><strong><font color="orange">·</font></strong> 二维表数据结构</p><p><strong><font color="orange">·</font></strong> 单机（本地）集合</p></li><li><p><strong><font color="orange">SparkCore · RDD</font></strong></p><p><strong><font color="orange">·</font></strong> 无标准数据结构</p><p><strong><font color="orange">·</font></strong> 分布式（分区）集合</p></li><li><p><strong><font color="orange">SparkSQL · DataFrame</font></strong></p><p><strong><font color="orange">·</font></strong> 二维表数据结构</p><p><strong><font color="orange">·</font></strong> 分布式（分区）集合</p></li></ul><h3 id="SparkSession对象"><a href="#SparkSession对象" class="headerlink" title="SparkSession对象"></a>SparkSession对象</h3><p>RDD程序的执行入口对象：<strong>SparkContext</strong></p><p>在Spark2.0以后，推出了<strong>SparkSession</strong>对象，来作为Spark编码的统一入口对象。<strong>SparkSession</strong>：</p><ul><li>用于SparkSQL编程，作为入口对象</li><li>用于SparkCore编程，通过SparkSession对象获取SparkContext</li></ul><p>构建SparkSession对象：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过builder方法来构建SparkSession对象</span></span><br><span class="line"><span class="comment"># appName：设置程序名称</span></span><br><span class="line"><span class="comment"># config：配置常用属性</span></span><br><span class="line"><span class="comment"># getOrCreate：完成创建SparkSession对象</span></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).config(<span class="string">"spark.sql.shuffle.partitions"</span>, <span class="string">"4"</span>).getOrCreate()</span><br></pre></td></tr></table></figure><p>通过SparkSesion对象获取SparkContext对象：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).config(<span class="string">"spark.sql.shuffle.partitions"</span>, <span class="string">"4"</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line">sc = spark.sparkContext</span><br></pre></td></tr></table></figure><a id="more"></a><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).config(<span class="string">"spark.sql.shuffle.partitions"</span>, <span class="string">"4"</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">df1 = spark.read.csv(<span class="string">"hdfs://master:8020/input/pip_file.txt"</span>, sep=<span class="string">','</span>, header=<span class="literal">False</span>)</span><br><span class="line">df2 = df1.toDF(<span class="string">"id"</span>, <span class="string">"package"</span>, <span class="string">"version"</span>)</span><br><span class="line">df2.printSchema()         <span class="comment"># 表结构</span></span><br><span class="line">df2.show()                <span class="comment"># 展示表</span></span><br><span class="line">df2.createTempView(<span class="string">"pip"</span>) <span class="comment"># 创建"pip"表,保存在内存中</span></span><br><span class="line"><span class="comment"># SQL风格</span></span><br><span class="line">spark.sql(<span class="string">"""</span></span><br><span class="line"><span class="string">SELECT * FROM pip WHERE package='pyspark'</span></span><br><span class="line"><span class="string">"""</span>).show</span><br><span class="line"><span class="comment"># DSL风格</span></span><br><span class="line">df2.where(<span class="string">"package='pyspark'"</span>).limit(<span class="number">5</span>).show()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- id: string (nullable = true)</span><br><span class="line"> |-- package: string (nullable = true)</span><br><span class="line"> |-- version: string (nullable = true)</span><br><span class="line"></span><br><span class="line">+---+-------------+--------+</span><br><span class="line">| id|      package| version|</span><br><span class="line">+---+-------------+--------+</span><br><span class="line">|  1|        flask|   1.1.4|</span><br><span class="line">|  2|      fastapi|  0.78.0|</span><br><span class="line">|  3|         h5py|  2.10.0|</span><br><span class="line">|  4|        keras|   2.6.0|</span><br><span class="line">|  5|        jieba|  0.42.1|</span><br><span class="line">|  6|        numpy|  1.23.3|</span><br><span class="line">|  7|opencv-python|4.6.0.66|</span><br><span class="line">|  8|       pandas|   1.4.3|</span><br><span class="line">|  9|       pillow|   9.2.0|</span><br><span class="line">| 10|         py4j|0.10.9.5|</span><br><span class="line">| 11|      pyspark|   3.3.1|</span><br><span class="line">| 12|      sklearn|     0.0|</span><br><span class="line">| 13|   tensorfolw|   2.6.2|</span><br><span class="line">| 14|     requests|  2.28.1|</span><br><span class="line">| 15|        redis|   3.5.3|</span><br><span class="line">+---+-------------+--------+</span><br><span class="line"></span><br><span class="line">+---+-------+-------+</span><br><span class="line">| id|package|version|</span><br><span class="line">+---+-------+-------+</span><br><span class="line">| 11|pyspark|  3.3.1|</span><br><span class="line">+---+-------+-------+</span><br><span class="line"></span><br><span class="line">+---+-------+-------+</span><br><span class="line">| id|package|version|</span><br><span class="line">+---+-------+-------+</span><br><span class="line">| 11|pyspark|  3.3.1|</span><br><span class="line">+---+-------+-------+</span><br></pre></td></tr></table></figure><h2 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h2><h3 id="DataFrame的组成"><a href="#DataFrame的组成" class="headerlink" title="DataFrame的组成"></a>DataFrame的组成</h3><p>DataFrame是一个二维表结构，在结构层面：</p><ul><li>StructureType对象描述整个DataFrame的表结构</li><li>StructField对象描述一个列的信息</li></ul><p>在数据层面：</p><ul><li>Row对象记录一行数据</li><li>Column对象记录一列数据并包含列的信息</li></ul><h3 id="DataFrame的构建"><a href="#DataFrame的构建" class="headerlink" title="DataFrame的构建"></a>DataFrame的构建</h3><h4 id="RDD转换"><a href="#RDD转换" class="headerlink" title="RDD转换"></a>RDD转换</h4><p>DataFrame对象可以从RDD转换而来</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = spark.createDataFrame(rdd, schema=[<span class="string">'id'</span>, <span class="string">'package'</span>])</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">rdd = sc.textFile(<span class="string">"hdfs://master:8020/input/pip_file.txt"</span>).map(<span class="keyword">lambda</span> x: x.split(<span class="string">","</span>)).map(<span class="keyword">lambda</span> x:(int(x[<span class="number">0</span>]), x[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">df = spark.createDataFrame(rdd, schema=[<span class="string">'id'</span>, <span class="string">'package'</span>])</span><br><span class="line">df.printSchema()</span><br><span class="line">df.show(<span class="number">10</span>, <span class="literal">False</span>) <span class="comment"># 输出前十行数据，要全部显示默认设置为True</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">"pip"</span>)</span><br><span class="line">spark.sql(<span class="string">"SELECT * FROM pip WHERE id &lt; 5"</span>).show()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- id: long (nullable = true)</span><br><span class="line"> |-- package: string (nullable = true)</span><br><span class="line"></span><br><span class="line">+---+-------------+</span><br><span class="line">|id |package      |</span><br><span class="line">+---+-------------+</span><br><span class="line">|1  |flask        |</span><br><span class="line">|2  |fastapi      |</span><br><span class="line">|3  |h5py         |</span><br><span class="line">|4  |keras        |</span><br><span class="line">|5  |jieba        |</span><br><span class="line">|6  |numpy        |</span><br><span class="line">|7  |opencv-python|</span><br><span class="line">|8  |pandas       |</span><br><span class="line">|9  |pillow       |</span><br><span class="line">|10 |py4j         |</span><br><span class="line">+---+-------------+</span><br><span class="line">only showing top 10 rows</span><br><span class="line"></span><br><span class="line">+---+-------+</span><br><span class="line">| id|package|</span><br><span class="line">+---+-------+</span><br><span class="line">|  1|  flask|</span><br><span class="line">|  2|fastapi|</span><br><span class="line">|  3|   h5py|</span><br><span class="line">|  4|  keras|</span><br><span class="line">+---+-------+</span><br></pre></td></tr></table></figure><h4 id="StructType"><a href="#StructType" class="headerlink" title="StructType"></a>StructType</h4><p>通过StructType对象来定义DataFrame的表结构，转换RDD</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入StructType对象和类型</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType, IntegerType</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义表结构</span></span><br><span class="line">schema = StructType().add(<span class="string">"id"</span>, IntegerType(), nullable=<span class="literal">True</span>).add(<span class="string">"package"</span>, StringType(), nullable=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType, IntegerType</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">rdd = sc.textFile(<span class="string">"hdfs://master:8020/input/pip_file.txt"</span>).map(<span class="keyword">lambda</span> x: x.split(<span class="string">","</span>)).map(<span class="keyword">lambda</span> x:(int(x[<span class="number">0</span>]), x[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">schema = StructType().add(<span class="string">"id"</span>, IntegerType(), nullable=<span class="literal">True</span>).add(<span class="string">"package"</span>, StringType(), nullable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">df = spark.createDataFrame(rdd, schema=schema)</span><br><span class="line">df.printSchema()</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- id: integer (nullable = true)</span><br><span class="line"> |-- package: string (nullable = true)</span><br><span class="line"></span><br><span class="line">+---+-------------+</span><br><span class="line">| id|      package|</span><br><span class="line">+---+-------------+</span><br><span class="line">|  1|        flask|</span><br><span class="line">|  2|      fastapi|</span><br><span class="line">|  3|         h5py|</span><br><span class="line">|  4|        keras|</span><br><span class="line">|  5|        jieba|</span><br><span class="line">|  6|        numpy|</span><br><span class="line">|  7|opencv-python|</span><br><span class="line">|  8|       pandas|</span><br><span class="line">|  9|       pillow|</span><br><span class="line">| 10|         py4j|</span><br><span class="line">| 11|      pyspark|</span><br><span class="line">| 12|      sklearn|</span><br><span class="line">| 13|   tensorfolw|</span><br><span class="line">| 14|     requests|</span><br><span class="line">| 15|        redis|</span><br><span class="line">+---+-------------+</span><br></pre></td></tr></table></figure><h4 id="toDF"><a href="#toDF" class="headerlink" title="toDF"></a>toDF</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType, IntegerType</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">rdd = sc.textFile(<span class="string">"hdfs://master:8020/input/pip_file.txt"</span>).map(<span class="keyword">lambda</span> x: x.split(<span class="string">","</span>)).map(<span class="keyword">lambda</span> x:(int(x[<span class="number">0</span>]), x[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># toDF快速构建DataFrame</span></span><br><span class="line"><span class="comment"># 对列类型不敏感，默认string类型</span></span><br><span class="line">df1 = rdd.toDF([<span class="string">"id"</span>, <span class="string">"package"</span>])</span><br><span class="line">df1.printSchema()</span><br><span class="line">df1.show(<span class="number">5</span>, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置schema通过toDF构建DataFrame</span></span><br><span class="line">schema = StructType().add(<span class="string">"id"</span>, IntegerType(), nullable=<span class="literal">True</span>).add(<span class="string">"package"</span>, StringType(), nullable=<span class="literal">True</span>)</span><br><span class="line">df2 = rdd.toDF(schema=schema)</span><br><span class="line">df2.printSchema()</span><br><span class="line">df2.show(<span class="number">5</span>, <span class="literal">False</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- id: long (nullable = true)</span><br><span class="line"> |-- package: string (nullable = true)</span><br><span class="line"></span><br><span class="line">+---+-------+</span><br><span class="line">|id |package|</span><br><span class="line">+---+-------+</span><br><span class="line">|1  |flask  |</span><br><span class="line">|2  |fastapi|</span><br><span class="line">|3  |h5py   |</span><br><span class="line">|4  |keras  |</span><br><span class="line">|5  |jieba  |</span><br><span class="line">+---+-------+</span><br><span class="line">only showing top 5 rows</span><br><span class="line"></span><br><span class="line">root</span><br><span class="line"> |-- id: integer (nullable = true)</span><br><span class="line"> |-- package: string (nullable = true)</span><br><span class="line"></span><br><span class="line">+---+-------+</span><br><span class="line">|id |package|</span><br><span class="line">+---+-------+</span><br><span class="line">|1  |flask  |</span><br><span class="line">|2  |fastapi|</span><br><span class="line">|3  |h5py   |</span><br><span class="line">|4  |keras  |</span><br><span class="line">|5  |jieba  |</span><br><span class="line">+---+-------+</span><br><span class="line">only showing top 5 rows</span><br></pre></td></tr></table></figure><h4 id="Pandas"><a href="#Pandas" class="headerlink" title="Pandas"></a>Pandas</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">pddf = pd.DataFrame(</span><br><span class="line">&#123;</span><br><span class="line">        <span class="string">"id"</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        <span class="string">"package"</span>: [<span class="string">"flask"</span>, <span class="string">"fastapi"</span>, <span class="string">"h5py"</span>],</span><br><span class="line">        <span class="string">"version"</span>: [<span class="string">"1.1.4"</span>, <span class="string">"0.78.0"</span>, <span class="string">"2.10.0"</span>]</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">df = spark.createDataFrame(pddf)</span><br><span class="line">df.printSchema()</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- id: long (nullable = true)</span><br><span class="line"> |-- package: string (nullable = true)</span><br><span class="line"> |-- version: string (nullable = true)</span><br><span class="line"></span><br><span class="line">+---+-------+-------+</span><br><span class="line">| id|package|version|</span><br><span class="line">+---+-------+-------+</span><br><span class="line">|  1|  flask|  1.1.4|</span><br><span class="line">|  2|fastapi| 0.78.0|</span><br><span class="line">|  3|   h5py| 2.10.0|</span><br><span class="line">+---+-------+-------+</span><br></pre></td></tr></table></figure><h3 id="通过文件构建DataFrame"><a href="#通过文件构建DataFrame" class="headerlink" title="通过文件构建DataFrame"></a>通过文件构建DataFrame</h3><h4 id="text"><a href="#text" class="headerlink" title="text"></a>text</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sparksession.read.format(<span class="string">"text|csv|json|jdbc|..."</span>).option(<span class="string">"K"</span>, <span class="string">"V"</span>).schema(StructType|String).load(<span class="string">"文件路径，支持本地文件系统和HDFS"</span>)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建StructType，text数据源，读取数据的特点是将一整行当作一列来读取，默认列名为value，类型为string</span></span><br><span class="line">schema = StructType().add(<span class="string">"data"</span>, StringType(), nullable=<span class="literal">True</span>)</span><br><span class="line">df = spark.read.format(<span class="string">"text"</span>).schema(schema=schema).load(<span class="string">"hdfs://master:8020/input/pip_file.txt"</span>)</span><br><span class="line">df.printSchema()</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- data: string (nullable = true)</span><br><span class="line"></span><br><span class="line">+--------------------+</span><br><span class="line">|                data|</span><br><span class="line">+--------------------+</span><br><span class="line">|       1,flask,1.1.4|</span><br><span class="line">|    2,fastapi,0.78.0|</span><br><span class="line">|       3,h5py,2.10.0|</span><br><span class="line">|       4,keras,2.6.0|</span><br><span class="line">|      5,jieba,0.42.1|</span><br><span class="line">|      6,numpy,1.23.3|</span><br><span class="line">|7,opencv-python,4...|</span><br><span class="line">|      8,pandas,1.4.3|</span><br><span class="line">|      9,pillow,9.2.0|</span><br><span class="line">|    10,py4j,0.10.9.5|</span><br><span class="line">|    11,pyspark,3.3.1|</span><br><span class="line">|      12,sklearn,0.0|</span><br><span class="line">| 13,tensorfolw,2.6.2|</span><br><span class="line">|  14,requests,2.28.1|</span><br><span class="line">|      15,redis,3.5.3|</span><br><span class="line">+--------------------+</span><br></pre></td></tr></table></figure><h4 id="json"><a href="#json" class="headerlink" title="json"></a>json</h4><p>json文件自带一定数据结构</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">df = spark.read.format(<span class="string">"json"</span>).load(<span class="string">"hdfs://master:8020/input/pip_file.json"</span>)</span><br><span class="line">df.printSchema()</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- id: long (nullable = true)</span><br><span class="line"> |-- package: string (nullable = true)</span><br><span class="line"> |-- version: string (nullable = true)</span><br><span class="line"></span><br><span class="line">+---+-------+-------+</span><br><span class="line">| id|package|version|</span><br><span class="line">+---+-------+-------+</span><br><span class="line">|  1|  flask|  1.1.4|</span><br><span class="line">|  2|fastapi| 0.78.0|</span><br><span class="line">|  3|   h5py| 2.10.0|</span><br><span class="line">|  4|  keras|  2.6.0|</span><br><span class="line">|  5|  jieba| 0.42.1|</span><br><span class="line">+---+-------+-------+</span><br></pre></td></tr></table></figure><h4 id="csv"><a href="#csv" class="headerlink" title="csv"></a>csv</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过.option指定属性</span></span><br><span class="line">df = spark.read.format(<span class="string">"csv"</span>).option(<span class="string">"sep"</span>, <span class="string">";"</span>).option(<span class="string">"header"</span>, <span class="literal">True</span>).option(<span class="string">"encoding"</span>, <span class="string">"utf-8"</span>).schema(<span class="string">"id INT, package STRING, version STRING"</span>).load(<span class="string">"hdfs://master:8020/input/pip_file.csv"</span>)</span><br><span class="line">df.printSchema()</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- id: integer (nullable = true)</span><br><span class="line"> |-- package: string (nullable = true)</span><br><span class="line"> |-- version: string (nullable = true)</span><br><span class="line"></span><br><span class="line">+---+-------------+--------+</span><br><span class="line">| id|      package| version|</span><br><span class="line">+---+-------------+--------+</span><br><span class="line">|  1|        flask|   1.1.4|</span><br><span class="line">|  2|      fastapi|  0.78.0|</span><br><span class="line">|  3|         h5py|  2.10.0|</span><br><span class="line">|  4|        keras|   2.6.0|</span><br><span class="line">|  5|        jieba|  0.42.1|</span><br><span class="line">|  6|        numpy|  1.23.3|</span><br><span class="line">|  7|opencv-python|4.6.0.66|</span><br><span class="line">|  8|       pandas|   1.4.3|</span><br><span class="line">|  9|       pillow|   9.2.0|</span><br><span class="line">| 10|         py4j|0.10.9.5|</span><br><span class="line">| 11|      pyspark|   3.3.1|</span><br><span class="line">| 12|      sklearn|     0.0|</span><br><span class="line">| 13|   tensorfolw|   2.6.2|</span><br><span class="line">| 14|     requests|  2.28.1|</span><br><span class="line">| 15|        redis|   3.5.3|</span><br><span class="line">+---+-------------+--------+</span><br></pre></td></tr></table></figure><h4 id="parquet"><a href="#parquet" class="headerlink" title="parquet"></a>parquet</h4><p>parquet是spark中常用的一种列示存储文件格式</p><ul><li>内置schema（列名，列类型，是否为空）</li><li>存储是以列作为存储格式</li><li>存储是序列化存储在文件中（有压缩属性体积小）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过.option指定属性</span></span><br><span class="line">df = spark.read.format(<span class="string">"parquet"</span>).load(<span class="string">"hdfs://master:8020/input/suers.parquet"</span>)</span><br><span class="line">df.printSchema()</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- name: string (nullable = true)</span><br><span class="line"> |-- favorite_color: string (nullable = true)</span><br><span class="line"> |-- favorite_numbers: array (nullable = true)</span><br><span class="line"> |    |-- element: integer (containsNull = true)</span><br><span class="line"></span><br><span class="line">+------+--------------+----------------+</span><br><span class="line">|  name|favorite_color|favorite_numbers|</span><br><span class="line">+------+--------------+----------------+</span><br><span class="line">|Alyssa|          null|  [3, 9, 15, 20]|</span><br><span class="line">|   Ben|           red|              []|</span><br><span class="line">+------+--------------+----------------+</span><br></pre></td></tr></table></figure><h3 id="DataFrame编程"><a href="#DataFrame编程" class="headerlink" title="DataFrame编程"></a>DataFrame编程</h3><p>DataFrame支持两种编程风格</p><ul><li><p><strong>DSL风格</strong></p><p>被称为领域特定语言，是DataFrame的特有API</p></li><li><p><strong>SQL风格</strong></p><p>使用SQL语句来直接处理DataFrame</p></li></ul><h4 id="DSL风格"><a href="#DSL风格" class="headerlink" title="DSL风格"></a>DSL风格</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">df = spark.read.format(<span class="string">"csv"</span>).schema(<span class="string">"id INT, package STRING, version STRING"</span>).load(<span class="string">"hdfs://master:8020/input/pip_file.txt"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取Colum对象</span></span><br><span class="line">id_colum = df[<span class="string">'id'</span>]</span><br><span class="line">package_colum = df[<span class="string">'package'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># DSL风格</span></span><br><span class="line"><span class="comment"># select API</span></span><br><span class="line">df.select([<span class="string">'id'</span>, <span class="string">'package'</span>]).show()       <span class="comment"># list</span></span><br><span class="line">df.select(<span class="string">'id'</span>, <span class="string">'package'</span>).show()         <span class="comment"># 可变参数</span></span><br><span class="line">df.select(id_colum, package_colum).show() <span class="comment"># Colum对象</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># filter API</span></span><br><span class="line">df.filter(<span class="string">'id &lt; 5'</span>).show()</span><br><span class="line">df.filter(df[<span class="string">'id'</span>] &lt; <span class="number">5</span>).show()            <span class="comment"># Colum对象</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># where API</span></span><br><span class="line">df.where(<span class="string">'id &lt; 5'</span>).show()</span><br><span class="line">df.where(df[<span class="string">'id'</span>] &lt; <span class="number">5</span>).show()             <span class="comment"># Colum对象</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># groupBy API</span></span><br><span class="line">df.groupBy(<span class="string">'package'</span>).count().show()</span><br><span class="line">df.groupBy(df[<span class="string">'package'</span>]).count().show()  <span class="comment"># Colum对象</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回值不是DataFrame，而是GroupedData对象</span></span><br><span class="line"><span class="comment"># 是一个有分组关系的数据结构，提供API做分组聚合</span></span><br><span class="line"><span class="comment"># SQL：group by后接聚合：sum、avg、count、min、max</span></span><br><span class="line"><span class="comment"># GroupedData类似于SQL分组后的数据结构，同样拥有上述5种聚合方法</span></span><br><span class="line"><span class="comment"># GroupedData调用聚合方法后，返回值依旧是DataFrame对象</span></span><br><span class="line"><span class="comment"># GroupedData只是一个中转对象，最终还是要获得DataFrame对象</span></span><br><span class="line">r = df.groupBy(<span class="string">'package'</span>)</span><br><span class="line">print(r.sum().show(), r.avg().show(), r.count().show(), r.min().show(), r.max().show())</span><br></pre></td></tr></table></figure><h4 id="SQL风格"><a href="#SQL风格" class="headerlink" title="SQL风格"></a>SQL风格</h4><p>使用SQL风格，需要将DataFrame提前注册成表</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df.createTempView(<span class="string">"pip"</span>)            <span class="comment"># 注册一个临时视图（表）</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">"pip"</span>)   <span class="comment"># 注册一个临时表，如果已存在则进行替换</span></span><br><span class="line">df.createGlobalTempView(<span class="string">"pip"</span>)      <span class="comment"># 注册一个全局表</span></span><br></pre></td></tr></table></figure><ul><li><p><strong>全局表</strong>：可跨SparkSession对象使用，在一个程序内的多个SparkSession中均可调用，查询时带上前缀<code>global_temp</code></p></li><li><p><strong>临时表</strong>：仅在当前SparkSession中可用</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">df = spark.read.format(<span class="string">"csv"</span>).schema(<span class="string">"id INT, package STRING, version STRING"</span>).load(<span class="string">"hdfs://master:8020/input/pip_file.txt"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注册临时表</span></span><br><span class="line">df.createTempView(<span class="string">"pip"</span>)</span><br><span class="line">df.createOrReplaceTempView(<span class="string">"pip"</span>)</span><br><span class="line">df.createGlobalTempView(<span class="string">"pip_2"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过SparkSession SQL API执行sql语句</span></span><br><span class="line">spark.sql(<span class="string">"SELECT package, COUNT(*) AS cnt FROM pip GROUP BY package"</span>).show()</span><br><span class="line">spark.sql(<span class="string">"SELECT package, COUNT(*) AS cnt FROM global_temp.pip_2 GROUP BY package"</span>).show()</span><br></pre></td></tr></table></figure><h3 id="pyspark-sql-functions"><a href="#pyspark-sql-functions" class="headerlink" title="pyspark.sql.functions"></a>pyspark.sql.functions</h3><p>PySpark提供的pyspark.sql.functions包包含一系列可供SparkSQL使用的计算函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br></pre></td></tr></table></figure><p>即可调用F对象调用函数进行计算。这些功能函数的返回值大多是Colum对象</p><h3 id="WordCount示例"><a href="#WordCount示例" class="headerlink" title="WordCount示例"></a>WordCount示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line"><span class="comment"># SQL风格处理</span></span><br><span class="line">rdd = sc.textFile(<span class="string">"hdfs://master:8020/input/words.txt"</span>).flatMap(<span class="keyword">lambda</span> x: x.split(<span class="string">" "</span>)).map(<span class="keyword">lambda</span> x: [x])</span><br><span class="line">df = rdd.toDF([<span class="string">"word"</span>])</span><br><span class="line">df.createTempView(<span class="string">"words"</span>)</span><br><span class="line">spark.sql(<span class="string">"SELECT word, COUNT(*) AS cnt FROM words GROUP BY word ORDER BY cnt DESC"</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># DSL风格处理</span></span><br><span class="line">df = spark.read.format(<span class="string">"text"</span>).load(<span class="string">"hdfs://master:8020/input/words.txt"</span>)</span><br><span class="line"><span class="comment"># withColumn方法</span></span><br><span class="line"><span class="comment"># ：对已存在的列进行操作，返回与一个新的列，如果名字和之前相同则替换，否则新建列</span></span><br><span class="line">df.withColumn(<span class="string">"value"</span>, F.explode(F.split(df[<span class="string">'value'</span>], <span class="string">" "</span>))).show()</span><br><span class="line">df2 = df.withColumn(<span class="string">"value"</span>, F.explode(F.split(df[<span class="string">'value'</span>], <span class="string">" "</span>))).show()</span><br><span class="line">df2.groupBy(<span class="string">"value"</span>).count().show()</span><br><span class="line">df2.groupBy(<span class="string">"value"</span>).count().withColumnRenamed(<span class="string">"value"</span>, <span class="string">"name"</span>).withColumnRenamed(<span class="string">"count"</span>, <span class="string">"cnt"</span>).orderBy(<span class="string">"cnt"</span>, ascending=<span class="literal">False</span>).show()</span><br></pre></td></tr></table></figure><h2 id="SparkSQL-Shuffle"><a href="#SparkSQL-Shuffle" class="headerlink" title="SparkSQL Shuffle"></a>SparkSQL Shuffle</h2><p>spark.sql.shuffle.partitions参数是在spark sql计算过程中，shuffle算子阶段默认的分区数是200个。对于集群模式，200较为合适，如果在local模式下运行，200较多，会在调度上带来额外的损耗，所以在local模式下建议修改较低，例如2/4/10。这个参数和RDD中设置并行度的参数相互独立</p><p>可以按优先级在三处设置：</p><ul><li><p>代码设置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).config(<span class="string">"spark.sql.shuffle.partitions"</span>, <span class="string">"2"</span>).getOrCreate()</span><br></pre></td></tr></table></figure></li><li><p>客户端参数设置</p><p><code>bin/spark-submit --conf &quot;spark.sql.shuffle.partitions=100&quot;</code></p></li><li><p>配置文件设置</p><p><font color="#008080">conf/spark-defaults.conf</font> <code>spark.sql.shuffle.partitions 100</code></p></li></ul><h2 id="SparkSQL数据清洗"><a href="#SparkSQL数据清洗" class="headerlink" title="SparkSQL数据清洗"></a>SparkSQL数据清洗</h2><h3 id="数据去重API"><a href="#数据去重API" class="headerlink" title="数据去重API"></a>数据去重API</h3><p><strong>dropDuplicates</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DataFrame API</span></span><br><span class="line"><span class="comment"># 不设置参数，对全局的列联合起来进行比较，去除重复值，只保留一条</span></span><br><span class="line">df.dropDuplicates().show()</span><br><span class="line">df.dropDuplicates([<span class="string">'id'</span>, <span class="string">'version'</span>]).show()</span><br></pre></td></tr></table></figure><h3 id="缺失值处理API"><a href="#缺失值处理API" class="headerlink" title="缺失值处理API"></a>缺失值处理API</h3><p><strong>dropna</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DataFrame API</span></span><br><span class="line"><span class="comment"># 对缺失值的数据进行删除</span></span><br><span class="line"><span class="comment"># 不设置参数，只要列中存在null即删除整行</span></span><br><span class="line">df.dropna().show()</span><br><span class="line"><span class="comment"># tresh=3表示，最少满足3个有效列，不满足即删除当前数据</span></span><br><span class="line">df.dropna(thresh=<span class="number">3</span>).show()</span><br><span class="line">df.dropna(tresh=<span class="number">2</span>,subset[<span class="string">'id'</span>, <span class="string">'version'</span>]).show()</span><br></pre></td></tr></table></figure><p><strong>fillna</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DataFrame API</span></span><br><span class="line"><span class="comment"># 对缺失值的数据进行填充</span></span><br><span class="line">df.fillna(<span class="string">"loss"</span>).show()</span><br><span class="line"><span class="comment"># 对指定列进行填充</span></span><br><span class="line">df.fillna(<span class="string">"M/A"</span>, subset[<span class="string">'version'</span>]).show()</span><br><span class="line"><span class="comment"># 指定一个字典，对所有的列提供填充依据</span></span><br><span class="line">df.fillna(&#123;<span class="string">"id"</span>: <span class="string">"unknown"</span>, <span class="string">"package"</span>: <span class="string">"unknown"</span>, <span class="string">"version"</span>: <span class="string">"none"</span>&#125;).show()</span><br></pre></td></tr></table></figure><h2 id="DataFrame数据写出"><a href="#DataFrame数据写出" class="headerlink" title="DataFrame数据写出"></a>DataFrame数据写出</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mode：传入模式，append-追加，overwrite-覆盖，ignore-忽略，error-重复异常（默认）</span></span><br><span class="line"><span class="comment"># format：传入格式：text/csv/json/parquet/orc/avro/jdbc</span></span><br><span class="line"><span class="comment"># （text源仅支持单列df写出）</span></span><br><span class="line"><span class="comment"># option：设置属性，如.option("sep", ",")</span></span><br><span class="line"><span class="comment"># save：保存路径，支持本地文件系统和HDFS</span></span><br><span class="line">df.write.mode().format().option(K, V).save(PATH)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType, IntegerType</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 0. 构建执行环境入口对象SparkSession</span></span><br><span class="line">spark = SparkSession.builder.\</span><br><span class="line">    appName(<span class="string">"test"</span>).\</span><br><span class="line">    master(<span class="string">"local[*]"</span>).\</span><br><span class="line">    config(<span class="string">"spark.sql.shuffle.partitions"</span>, <span class="number">2</span>).\</span><br><span class="line">    getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 读取数据集</span></span><br><span class="line">schema = StructType().add(<span class="string">"user_id"</span>, StringType(), nullable=<span class="literal">True</span>). \</span><br><span class="line">    add(<span class="string">"movie_id"</span>, IntegerType(), nullable=<span class="literal">True</span>). \</span><br><span class="line">    add(<span class="string">"rank"</span>, IntegerType(), nullable=<span class="literal">True</span>). \</span><br><span class="line">    add(<span class="string">"ts"</span>, StringType(), nullable=<span class="literal">True</span>)</span><br><span class="line">df = spark.read.format(<span class="string">"csv"</span>). \</span><br><span class="line">    option(<span class="string">"sep"</span>, <span class="string">"\t"</span>). \</span><br><span class="line">    option(<span class="string">"header"</span>, <span class="literal">False</span>). \</span><br><span class="line">    option(<span class="string">"encoding"</span>, <span class="string">"utf-8"</span>). \</span><br><span class="line">    schema(schema=schema). \</span><br><span class="line">    load(<span class="string">"hdfs://master:8020/input/u.data"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Write text 写出, 只能写出一个列的数据, 需要将df转换为单列df</span></span><br><span class="line">df.select(F.concat_ws(<span class="string">"---"</span>, <span class="string">"user_id"</span>, <span class="string">"movie_id"</span>, <span class="string">"rank"</span>, <span class="string">"ts"</span>)).\</span><br><span class="line">    write.\</span><br><span class="line">    mode(<span class="string">"overwrite"</span>).\</span><br><span class="line">    format(<span class="string">"text"</span>).\</span><br><span class="line">    save(<span class="string">"hdfs://master:8020/output/sql/text"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Write csv</span></span><br><span class="line">df.write.mode(<span class="string">"overwrite"</span>).\</span><br><span class="line">    format(<span class="string">"csv"</span>).\</span><br><span class="line">    option(<span class="string">"sep"</span>, <span class="string">";"</span>).\</span><br><span class="line">    option(<span class="string">"header"</span>, <span class="literal">True</span>).\</span><br><span class="line">    save(<span class="string">"hdfs://master:8020/output/sql/csv"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Write json</span></span><br><span class="line">df.write.mode(<span class="string">"overwrite"</span>).\</span><br><span class="line">    format(<span class="string">"json"</span>).\</span><br><span class="line">    save(<span class="string">"hdfs://master:8020/output/sql/json"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Write parquet</span></span><br><span class="line">df.write.mode(<span class="string">"overwrite"</span>).\</span><br><span class="line">    format(<span class="string">"parquet"</span>).\</span><br><span class="line">    save(<span class="string">"hdfs://master:8020/output/sql/parquet"</span>)</span><br></pre></td></tr></table></figure><h2 id="DataFrame-JDBC"><a href="#DataFrame-JDBC" class="headerlink" title="DataFrame JDBC"></a>DataFrame JDBC</h2><p>将mysql包添加进pyspark</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/anaconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/jars/</span><br><span class="line">rz</span><br></pre></td></tr></table></figure><h3 id="DataFrame读写数据库"><a href="#DataFrame读写数据库" class="headerlink" title="DataFrame读写数据库"></a>DataFrame读写数据库</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将DataFrame通过JDBC写入mysql</span></span><br><span class="line">df.write.mode(<span class="string">"overwrite"</span>).\</span><br><span class="line">format(<span class="string">"jdbc"</span>).\</span><br><span class="line">option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://master:3306/..."</span>).\</span><br><span class="line">option(<span class="string">"dbtable"</span>, <span class="string">"test"</span>).\</span><br><span class="line">option(<span class="string">"user"</span>, <span class="string">"root"</span>).\</span><br><span class="line">option(<span class="string">"password"</span>, <span class="string">"123456"</span>).\</span><br><span class="line">save()</span><br><span class="line"></span><br><span class="line"><span class="comment"># JDBC会自动建表，因为DataFrame中含有表结构的信息</span></span><br><span class="line"><span class="comment"># 读mysql</span></span><br><span class="line">df = spark.read.format(<span class="string">"jdbc"</span>).\</span><br><span class="line">option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://master:3306/..."</span>).\</span><br><span class="line">option(<span class="string">"dbtable"</span>, <span class="string">"test"</span>).\</span><br><span class="line">option(<span class="string">"user"</span>, <span class="string">"root"</span>).\</span><br><span class="line">option(<span class="string">"password"</span>, <span class="string">"123456"</span>).\</span><br><span class="line">load()</span><br><span class="line"></span><br><span class="line">df.printSchema()</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure><h2 id="SparkSQL函数定义"><a href="#SparkSQL函数定义" class="headerlink" title="SparkSQL函数定义"></a>SparkSQL函数定义</h2><h3 id="SparkSQL定义UDF函数"><a href="#SparkSQL定义UDF函数" class="headerlink" title="SparkSQL定义UDF函数"></a>SparkSQL定义UDF函数</h3><p>SparkSQL模块自带实现公共方法的位置在pyspark.sql.functions中，同时SparkSQL和Hive一样支持自定义函数：UDF和UDAF</p><p>目前python仅支持SparkSQL UDF自定函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注册的UDF可用于DSL和SQL风格</span></span><br><span class="line"><span class="comment"># 返回值用于DSL风格，传参内的名称用于SQL风格</span></span><br><span class="line"><span class="comment"># arg1：注册的UDF名称，仅可用于SQL风格</span></span><br><span class="line"><span class="comment"># arg2：UDF处理逻辑，是一个单独的方法</span></span><br><span class="line"><span class="comment"># arg3：声明UDF的返回值类型，UDF注册时，必须声明返回值类型，并且UDF的真实返回值一定要和声明的返回值一致</span></span><br><span class="line"><span class="comment"># 返回值对象：是一个UDF对象，仅可用于DSL语法</span></span><br><span class="line"><span class="comment"># 这种方式定义的UDF，可以通过arg1的名称用于SQL风格，通过返回值对象用于DSL风格</span></span><br><span class="line">sparksession.udf.register(arg1, arg2, arg3)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 仅能用于DSL风格</span></span><br><span class="line">pyspark.sql.functions.udf</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> IntegerType</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).config(<span class="string">"spark.sql.shuffle.partitions"</span>, <span class="number">2</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]).map(<span class="keyword">lambda</span> x: [x])</span><br><span class="line">df = rdd.toDF([<span class="string">'num'</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># sparksession.udf.register()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">num_ride_10</span><span class="params">(num)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> num * <span class="number">10</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">udf1 = spark.udf.register(<span class="string">"udf1"</span>, num_ride_10, IntegerType())</span><br><span class="line"></span><br><span class="line"><span class="comment"># SQL风格使用</span></span><br><span class="line"><span class="comment"># selectExpr：以SELECT的表达式执行（SQL字符串）</span></span><br><span class="line"><span class="comment"># 普通select方法接受普通字符串字段名，或者返回值是Column对象的计算</span></span><br><span class="line">df.selectExpr(<span class="string">"udf1(num)"</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># DSL风格</span></span><br><span class="line"><span class="comment"># 返回值UDF对象如果作为方法使用，传入的参数一定是Column对象</span></span><br><span class="line">df.select(udf1(df[<span class="string">'num'</span>])).show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># pyspark.sql.functions.udf</span></span><br><span class="line">udf2 = F.udf(num_ride_10, IntegerType())</span><br><span class="line">df.select(udf2(df[<span class="string">'num'</span>])).show()</span><br><span class="line">df.selectExpr(<span class="string">"udf2(num)"</span>).show()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">+---------+</span><br><span class="line">|udf1(num)|</span><br><span class="line">+---------+</span><br><span class="line">|       10|</span><br><span class="line">|       20|</span><br><span class="line">|       30|</span><br><span class="line">|       40|</span><br><span class="line">|       50|</span><br><span class="line">|       60|</span><br><span class="line">|       70|</span><br><span class="line">|       80|</span><br><span class="line">|       90|</span><br><span class="line">|      100|</span><br><span class="line">+---------+</span><br><span class="line"></span><br><span class="line">+---------+</span><br><span class="line">|udf1(num)|</span><br><span class="line">+---------+</span><br><span class="line">|       10|</span><br><span class="line">|       20|</span><br><span class="line">|       30|</span><br><span class="line">|       40|</span><br><span class="line">|       50|</span><br><span class="line">|       60|</span><br><span class="line">|       70|</span><br><span class="line">|       80|</span><br><span class="line">|       90|</span><br><span class="line">|      100|</span><br><span class="line">+---------+</span><br><span class="line"></span><br><span class="line">+----------------+</span><br><span class="line">|num_ride_10(num)|</span><br><span class="line">+----------------+</span><br><span class="line">|              10|</span><br><span class="line">|              20|</span><br><span class="line">|              30|</span><br><span class="line">|              40|</span><br><span class="line">|              50|</span><br><span class="line">|              60|</span><br><span class="line">|              70|</span><br><span class="line">+----------------+</span><br></pre></td></tr></table></figure><h3 id="注册返回值为数组类型的UDF"><a href="#注册返回值为数组类型的UDF" class="headerlink" title="注册返回值为数组类型的UDF"></a>注册返回值为数组类型的UDF</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StringType, ArrayType</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 0. 构建执行环境入口对象SparkSession</span></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).config(<span class="string">"spark.sql.shuffle.partitions"</span>, <span class="number">2</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建一个RDD</span></span><br><span class="line">rdd = sc.parallelize([[<span class="string">"hadoop spark flink"</span>], [<span class="string">"hadoop flink java"</span>]])</span><br><span class="line">df = rdd.toDF([<span class="string">"line"</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注册UDF, UDF的执行函数定义</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_line</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> data.split(<span class="string">" "</span>)  <span class="comment"># 返回值是一个Array对象</span></span><br><span class="line"><span class="comment"># TODO1 方式1 构建UDF</span></span><br><span class="line">udf2 = spark.udf.register(<span class="string">"udf1"</span>, split_line, ArrayType(StringType()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># DLS风格</span></span><br><span class="line">df.select(udf2(df[<span class="string">'line'</span>])).show()</span><br><span class="line"><span class="comment"># SQL风格</span></span><br><span class="line">df.createTempView(<span class="string">"lines"</span>)</span><br><span class="line">spark.sql(<span class="string">"SELECT udf1(line) FROM lines"</span>).show(truncate=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TODO 2 方式2的形式构建UDF</span></span><br><span class="line">udf3 = F.udf(split_line, ArrayType(StringType()))</span><br><span class="line">df.select(udf3(df[<span class="string">'line'</span>])).show(truncate=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">+--------------------+</span><br><span class="line">|          udf1(line)|</span><br><span class="line">+--------------------+</span><br><span class="line">|[hadoop, spark, f...|</span><br><span class="line">|[hadoop, flink, j...|</span><br><span class="line">+--------------------+</span><br><span class="line"></span><br><span class="line">+----------------------+</span><br><span class="line">|udf1(line)            |</span><br><span class="line">+----------------------+</span><br><span class="line">|[hadoop, spark, flink]|</span><br><span class="line">|[hadoop, flink, java] |</span><br><span class="line">+----------------------+</span><br><span class="line"></span><br><span class="line">+----------------------+</span><br><span class="line">|split_line(line)      |</span><br><span class="line">+----------------------+</span><br><span class="line">|[hadoop, spark, flink]|</span><br><span class="line">|[hadoop, flink, java] |</span><br><span class="line">+----------------------+</span><br></pre></td></tr></table></figure><h3 id="注册返回值为字典类型的UDF"><a href="#注册返回值为字典类型的UDF" class="headerlink" title="注册返回值为字典类型的UDF"></a>注册返回值为字典类型的UDF</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType, IntegerType, ArrayType</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 0. 构建执行环境入口对象SparkSession</span></span><br><span class="line">spark = SparkSession.builder.master(<span class="string">"local[*]"</span>).config(<span class="string">"spark.sql.shuffle.partitions"</span>, <span class="number">2</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设 有三个数字  1 2 3  我们传入数字 ,返回数字所在序号对应的 字母 然后和数字结合形成dict返回</span></span><br><span class="line"><span class="comment"># 比如传入1 我们返回 &#123;"num":1, "letters": "a"&#125;</span></span><br><span class="line">rdd = sc.parallelize([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]])</span><br><span class="line">df = rdd.toDF([<span class="string">"num"</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注册UDF</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">"num"</span>: data, <span class="string">"letters"</span>: string.ascii_letters[data]&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">UDF的返回值是字典的话, 需要用StructType来接收</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">udf1 = spark.udf.register(<span class="string">"udf1"</span>, process, StructType().add(<span class="string">"num"</span>, IntegerType(), nullable=<span class="literal">True</span>).\</span><br><span class="line">                          add(<span class="string">"letters"</span>, StringType(), nullable=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">df.selectExpr(<span class="string">"udf1(num)"</span>).show(truncate=<span class="literal">False</span>)</span><br><span class="line">df.select(udf1(df[<span class="string">'num'</span>])).show(truncate=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">+---------+</span><br><span class="line">|udf1(num)|</span><br><span class="line">+---------+</span><br><span class="line">|&#123;1, b&#125;   |</span><br><span class="line">|&#123;2, c&#125;   |</span><br><span class="line">|&#123;3, d&#125;   |</span><br><span class="line">+---------+</span><br><span class="line"></span><br><span class="line">+---------+</span><br><span class="line">|udf1(num)|</span><br><span class="line">+---------+</span><br><span class="line">|&#123;1, b&#125;   |</span><br><span class="line">|&#123;2, c&#125;   |</span><br><span class="line">|&#123;3, d&#125;   |</span><br><span class="line">+---------+</span><br></pre></td></tr></table></figure><h3 id="通过RDD模拟UDAF"><a href="#通过RDD模拟UDAF" class="headerlink" title="通过RDD模拟UDAF"></a>通过RDD模拟UDAF</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="comment"># 0. 构建执行环境入口对象SparkSession</span></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).config(<span class="string">"spark.sql.shuffle.partitions"</span>, <span class="number">2</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], <span class="number">3</span>)</span><br><span class="line">df = rdd.map(<span class="keyword">lambda</span> x: [x]).toDF([<span class="string">'num'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 折中的方式 就是使用RDD的mapPartitions 算子来完成聚合操作</span></span><br><span class="line"><span class="comment"># 如果用mapPartitions API 完成UDAF聚合, 一定要单分区</span></span><br><span class="line">single_partition_rdd = df.rdd.repartition(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(iter)</span>:</span></span><br><span class="line">    sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> iter:</span><br><span class="line">        sum += row[<span class="string">'num'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> [sum]    <span class="comment"># 一定要嵌套list, 因为mapPartitions方法要求的返回值是list对象</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(single_partition_rdd.mapPartitions(process).collect())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[15]</span><br></pre></td></tr></table></figure><h2 id="SparkSQL窗口函数"><a href="#SparkSQL窗口函数" class="headerlink" title="SparkSQL窗口函数"></a>SparkSQL窗口函数</h2><h3 id="开窗函数"><a href="#开窗函数" class="headerlink" title="开窗函数"></a>开窗函数</h3><p>开窗函数的引入是为了既显示聚集前的数据又显示聚集后的数据，即在每一行的最后一列添加聚合函数的结果</p><p>开窗用于为行为定义一个窗口（运算将要操作的行为集合），对一组值进行操作，不需要使用<font color="orange">GROUP BY</font>字句对数据进行分组，能够在同一行中同时返回基础行的列和聚合列</p><h3 id="聚合函数和开窗函数"><a href="#聚合函数和开窗函数" class="headerlink" title="聚合函数和开窗函数"></a>聚合函数和开窗函数</h3><ul><li><p>聚合函数是将多行变为一行，count、avg…；如果要显示其他的列必须将列加入到<font color="orange">GROUP BY</font>中</p></li><li><p>开窗函数是将一行变成多行，可以不使用<font color="orange">GROUP BY</font>直接显示所有数据</p></li></ul><h3 id="开窗函数分类"><a href="#开窗函数分类" class="headerlink" title="开窗函数分类"></a>开窗函数分类</h3><ul><li>聚合开窗函数</li><li>排序开窗函数</li><li>分区类型NTILE的窗口函数</li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;SparkSQL&quot;&gt;&lt;a href=&quot;#SparkSQL&quot; class=&quot;headerlink&quot; title=&quot;SparkSQL&quot;&gt;&lt;/a&gt;SparkSQL&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;SparkSQL&lt;/strong&gt;是spark的一个用于处理海量&lt;strong&gt;结构化数据&lt;/strong&gt;的模块&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;支持SQL语言&lt;/li&gt;
&lt;li&gt;自动优化&lt;/li&gt;
&lt;li&gt;性能强&lt;/li&gt;
&lt;li&gt;兼容HIVE&lt;/li&gt;
&lt;li&gt;API流程简单&lt;/li&gt;
&lt;li&gt;支持标准化JDBC和ODBC连接&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;SparkSQL数据抽象&quot;&gt;&lt;a href=&quot;#SparkSQL数据抽象&quot; class=&quot;headerlink&quot; title=&quot;SparkSQL数据抽象&quot;&gt;&lt;/a&gt;SparkSQL数据抽象&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;font color=&quot;orange&quot;&gt;Pandas · DataFrame&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;font color=&quot;orange&quot;&gt;·&lt;/font&gt;&lt;/strong&gt; 二维表数据结构&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;font color=&quot;orange&quot;&gt;·&lt;/font&gt;&lt;/strong&gt; 单机（本地）集合&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;font color=&quot;orange&quot;&gt;SparkCore · RDD&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;font color=&quot;orange&quot;&gt;·&lt;/font&gt;&lt;/strong&gt; 无标准数据结构&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;font color=&quot;orange&quot;&gt;·&lt;/font&gt;&lt;/strong&gt; 分布式（分区）集合&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;font color=&quot;orange&quot;&gt;SparkSQL · DataFrame&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;font color=&quot;orange&quot;&gt;·&lt;/font&gt;&lt;/strong&gt; 二维表数据结构&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;font color=&quot;orange&quot;&gt;·&lt;/font&gt;&lt;/strong&gt; 分布式（分区）集合&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;SparkSession对象&quot;&gt;&lt;a href=&quot;#SparkSession对象&quot; class=&quot;headerlink&quot; title=&quot;SparkSession对象&quot;&gt;&lt;/a&gt;SparkSession对象&lt;/h3&gt;&lt;p&gt;RDD程序的执行入口对象：&lt;strong&gt;SparkContext&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在Spark2.0以后，推出了&lt;strong&gt;SparkSession&lt;/strong&gt;对象，来作为Spark编码的统一入口对象。&lt;strong&gt;SparkSession&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;用于SparkSQL编程，作为入口对象&lt;/li&gt;
&lt;li&gt;用于SparkCore编程，通过SparkSession对象获取SparkContext&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;构建SparkSession对象：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; pyspark.sql &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; SparkSession&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 通过builder方法来构建SparkSession对象&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# appName：设置程序名称&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# config：配置常用属性&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# getOrCreate：完成创建SparkSession对象&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;spark = SparkSession.builder.appName(&lt;span class=&quot;string&quot;&gt;&quot;test&quot;&lt;/span&gt;).master(&lt;span class=&quot;string&quot;&gt;&quot;local[*]&quot;&lt;/span&gt;).config(&lt;span class=&quot;string&quot;&gt;&quot;spark.sql.shuffle.partitions&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;4&quot;&lt;/span&gt;).getOrCreate()&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;通过SparkSesion对象获取SparkContext对象：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; pyspark.sql &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; SparkSession&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;spark = SparkSession.builder.appName(&lt;span class=&quot;string&quot;&gt;&quot;test&quot;&lt;/span&gt;).master(&lt;span class=&quot;string&quot;&gt;&quot;local[*]&quot;&lt;/span&gt;).config(&lt;span class=&quot;string&quot;&gt;&quot;spark.sql.shuffle.partitions&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;4&quot;&lt;/span&gt;).getOrCreate()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sc = spark.sparkContext&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
    
    <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark-Core</title>
    <link href="http://yoursite.com/2022/12/07/Spark-Core/"/>
    <id>http://yoursite.com/2022/12/07/Spark-Core/</id>
    <published>2022-12-07T08:58:49.000Z</published>
    <updated>2022-12-08T09:05:29.171Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><h2 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h2><p>本地对象被发送到同个Executor内每个分区的处理线程上使用，这样每个分区实际上存放了重复的数据。而Executor本质上是进程，进程内资源共享，没必要将本地对象分发给所有分区，造成内存浪费</p><p><strong>解决方案</strong>：将本地对象设置为广播变量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.将本地对象标记为广播变量</span></span><br><span class="line">broadcast = sc.broadcast(var)</span><br><span class="line"><span class="comment"># 2.使用广播变量，从broadcast对象中取出本地对象</span></span><br><span class="line">value = broadcast.value</span><br><span class="line"><span class="comment"># 当传输的是广播对象时，spark会只给每个Executor分发一份数据</span></span><br></pre></td></tr></table></figure><p>当本地集合对象和分布式集合对象（RDD）进行关联时，需要将本地集合对象封装为广播变量</p><ul><li>节省网络IO次数</li><li>降低Executor内存占用</li></ul><h2 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h2><p>当执行累加操作时，各个分区累加自身的内容</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># spark提供累加器变量，参数是初始值</span></span><br><span class="line">acmlt = sc.accumulator(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">acmlt = sc.accumulator(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">counts</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> acmlt</span><br><span class="line">    acmlt += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">rdd1 = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], <span class="number">3</span>)</span><br><span class="line">rdd2 = rdd1.map(counts)</span><br><span class="line">print(rdd2.collect())</span><br><span class="line">print(acmlt)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[1, 2, 3]</span><br><span class="line">3</span><br></pre></td></tr></table></figure><p><font color="gold">注</font>：累加器可能因血缘关系导致重复的累加，例如一个RDD被释放后累加已经完成，此时再使用该RDD将会导致重复累加。可通过cache缓存机制来解决</p><a id="more"></a> <h2 id="DAG"><a href="#DAG" class="headerlink" title="DAG"></a>DAG</h2><p>Spark的核心是根据RDD来实现的，Spark Scheduler（spark任务调度）是spark核心实现的重要一环，其功能是组织处理RDD中每个分区的数据，根据RDD的依赖关系构建DAG有向无环图，再基于DAG划分Stage，将每个Stage中的任务发送到指定节点运行，合理规划资源的利用</p><h3 id="DAG标准定义"><a href="#DAG标准定义" class="headerlink" title="DAG标准定义"></a>DAG标准定义</h3><p>有向无环图：有方向而没有形成闭环的执行流程图</p><ul><li>有向：具有执行方向</li><li>无环：没有闭环</li></ul><h3 id="Action算子和Job"><a href="#Action算子和Job" class="headerlink" title="Action算子和Job"></a>Action算子和Job</h3><p>一个Action会产生一个DAG，如果代码中存在3个Action则会产生3个DAG；</p><p>每个DAG在应用程序运行时产生一个Job（应用程序内的子任务）</p><p><strong><font color="orange">1个Action = 1个DAG = 1个Job</font></strong></p><p>这样的代码运行起来在spark中被称为<strong><font color="orange">Application</font></strong></p><h3 id="DAG和分区"><a href="#DAG和分区" class="headerlink" title="DAG和分区"></a>DAG和分区</h3><p>DAG的最终作用是为了构建spark详细执行的物理计划，由于spark是分布式多分区的，所以DAG和分区间也具有关联</p><h3 id="DAG的宽窄依赖和阶段划分"><a href="#DAG的宽窄依赖和阶段划分" class="headerlink" title="DAG的宽窄依赖和阶段划分"></a>DAG的宽窄依赖和阶段划分</h3><p>在Spark RDD前后之间的血缘关系，分为：</p><ul><li>窄依赖：父RDD的一个分区，将全部数据发送给子RDD的一个分区；</li><li>宽依赖：父RDD的一个分区，将数据发送给子RDD的多个分区，别名：shuffle</li></ul><p>对于spark，会根据DAG，按照宽依赖划分不同的DAG阶段。划分依据：从后向前，每遇到宽依赖就划分出一个阶段，称之为stage。在stage内部，一定是窄依赖</p><p><img src="/2022/12/07/Spark-Core/A.png" alt></p><h2 id="Spark的内存迭代计算"><a href="#Spark的内存迭代计算" class="headerlink" title="Spark的内存迭代计算"></a>Spark的内存迭代计算</h2><p>窄依赖同一线程内走管道交互，进入宽依赖走网络IO交互</p><p>Spark默认收到全局并行度的限制，除了个别算子有特殊分区的情况，大部分算子都会遵循全局并行度的要求来划分自己的分区数。例如全局并行度是3，大部分算子的默认分区都是3-&gt;不建议再独立通过arg来指定分区数</p><h2 id="Spark并行度"><a href="#Spark并行度" class="headerlink" title="Spark并行度"></a>Spark并行度</h2><p>Spark的并行：在同一时间内，有多少task在同时运行</p><p>Spark的并行度：并行能力，当设置为6，即共有6个task在并行运行，RDD的分区被规划为6个分区</p><p>Spark并行度设置（优先级由高到低）：</p><ul><li>代码</li><li>客户端参数</li><li>配置文件</li><li>默认值（1），并不会全部以1来运行，多数情况下基于读取文件的分片数量来作为默认并行度</li></ul><p>全局并行度配置参数：</p><p><code>spark.default.parallelism</code></p><ul><li>代码中设置：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conf = SparkConf()</span><br><span class="line">conf.set(<span class="string">"spark.default.parallelism"</span>, <span class="string">"100"</span>)</span><br></pre></td></tr></table></figure><ul><li><p>客户端提交参数中设置<code>bin/spark-submit --conf &quot;spark.default.parallelism=100&quot;</code></p></li><li><p>配置文件<font color="#008080">conf/spark-defaults.conf</font>中设置<code>spark.default.parallelism 100</code></p></li></ul><p><font color="gold">注</font>：全局并行度是推荐设置，不要针对RDD更改分区，可能会影响内存迭代管道的构建，或者产生额外的shuffle</p><p>针对RDD并行度的设置（不推荐）：</p><p><strong><font color="orange">·</font></strong> repartition算子</p><p><strong><font color="orange">·</font></strong> coalesce算子</p><p><strong><font color="orange">·</font></strong> partitionBy算子</p><h3 id="规划Spark集群并行度"><a href="#规划Spark集群并行度" class="headerlink" title="规划Spark集群并行度"></a>规划Spark集群并行度</h3><p>设置为CPU总核心的<strong><font color="orange">2~10倍</font></strong>（或更高）* </p><p>比如集群可用的CPU核心数量为100个，建议并行度200~1000（确保是CPU核心的整数倍）</p><ul><li><p>设置为最少2倍：</p><p>CPU的一个核心同一时间只能做一件事，当拥有100个核心的情况下，设置100并行度，能利用全部的CPU，但task的压力不均衡，一旦某个task先执行完毕，会导致某个CPU核心的空闲。所以将task并行分配的数量增多，例如设置1000并行度，同一时间内有100个task在运行，900个在等待，但可以确保某个task运行完毕后会不断有task补上，不让CPU处于空闲状态，最大程度利用集群的资源</p></li></ul><h2 id="Spark任务调度"><a href="#Spark任务调度" class="headerlink" title="Spark任务调度"></a>Spark任务调度</h2><h3 id="Spark任务由Driver进行调度"><a href="#Spark任务由Driver进行调度" class="headerlink" title="Spark任务由Driver进行调度"></a>Spark任务由Driver进行调度</h3><p>包括：</p><ul><li>逻辑DAG产生</li><li>分区DAG产生</li><li>基于分区DAG构建线程task并划分</li><li>将task分配给Executor并监控其工作</li></ul><h3 id="Spark程序调度流程"><a href="#Spark程序调度流程" class="headerlink" title="Spark程序调度流程"></a>Spark程序调度流程</h3><ul><li>构建Driver（<font color="orange">Driver</font>）</li><li>构建SparkContext执行环境入口对象（<font color="orange">Driver</font>）</li><li>基于DAG scheduler调度器构建逻辑task分配（<font color="orange">Driver</font>）</li><li>基于task scheduler调度器将逻辑task分配到各个Executor上执行并监控（<font color="orange">Driver</font>）</li><li>Worker（Executor）被task scheduler管理监控，遵从指令干活并汇报进度（<font color="orange">Worker</font>）</li></ul><h3 id="Driver内部组件"><a href="#Driver内部组件" class="headerlink" title="Driver内部组件"></a>Driver内部组件</h3><ul><li><p><strong>DAG调度器</strong></p><p>将逻辑DAG进行处理，最终得到逻辑上的task划分</p></li><li><p><strong>Task调度器</strong></p><p>基于DAG调度器的产出，来规划这些逻辑的task应该在哪些物理的Executor上运行，以及监控它们</p></li></ul><h3 id="层级关系梳理"><a href="#层级关系梳理" class="headerlink" title="层级关系梳理"></a>层级关系梳理</h3><ul><li>1个spark环境可运行多个Application；</li><li>1个代码成功运行生成一个Application；</li><li>1个Application内部有多个Job；</li><li>1个Action算子产生1个Job，每个Job有自己的DAG执行图；</li><li>1个Job的DAG基于宽窄依赖划分不同的阶段；</li><li>1个阶段里基于分区数量形成多个并行的内存迭代管道；</li><li>1个内存迭代管道形成1个task（DAG调度器划分将Job内划分出具体的task任务，1个Job被划分出的task在逻辑上被称为这个job的taskset）</li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;广播变量&quot;&gt;&lt;a href=&quot;#广播变量&quot; class=&quot;headerlink&quot; title=&quot;广播变量&quot;&gt;&lt;/a&gt;广播变量&lt;/h2&gt;&lt;p&gt;本地对象被发送到同个Executor内每个分区的处理线程上使用，这样每个分区实际上存放了重复的数据。而Executor本质上是进程，进程内资源共享，没必要将本地对象分发给所有分区，造成内存浪费&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;解决方案&lt;/strong&gt;：将本地对象设置为广播变量&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 1.将本地对象标记为广播变量&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;broadcast = sc.broadcast(var)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 2.使用广播变量，从broadcast对象中取出本地对象&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;value = broadcast.value&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 当传输的是广播对象时，spark会只给每个Executor分发一份数据&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;当本地集合对象和分布式集合对象（RDD）进行关联时，需要将本地集合对象封装为广播变量&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;节省网络IO次数&lt;/li&gt;
&lt;li&gt;降低Executor内存占用&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;累加器&quot;&gt;&lt;a href=&quot;#累加器&quot; class=&quot;headerlink&quot; title=&quot;累加器&quot;&gt;&lt;/a&gt;累加器&lt;/h2&gt;&lt;p&gt;当执行累加操作时，各个分区累加自身的内容&lt;/p&gt;
 &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# spark提供累加器变量，参数是初始值&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;acmlt = sc.accumulator(&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;e.g.&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; pyspark &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; SparkConf, SparkContext&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;conf = SparkConf().setAppName(&lt;span class=&quot;string&quot;&gt;&quot;test&quot;&lt;/span&gt;).setMaster(&lt;span class=&quot;string&quot;&gt;&quot;local[*]&quot;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sc = SparkContext(conf=conf)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;acmlt = sc.accumulator(&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;counts&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(data)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;global&lt;/span&gt; acmlt&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    acmlt += &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; data&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;rdd1 = sc.parallelize([&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;], &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;rdd2 = rdd1.map(counts)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(rdd2.collect())&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(acmlt)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[1, 2, 3]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;font color=&quot;gold&quot;&gt;注&lt;/font&gt;：累加器可能因血缘关系导致重复的累加，例如一个RDD被释放后累加已经完成，此时再使用该RDD将会导致重复累加。可通过cache缓存机制来解决&lt;/p&gt;</summary>
    
    
    
    <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
    
    <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Pyspark-RDD</title>
    <link href="http://yoursite.com/2022/12/07/Pyspark-RDD/"/>
    <id>http://yoursite.com/2022/12/07/Pyspark-RDD/</id>
    <published>2022-12-07T05:45:49.000Z</published>
    <updated>2022-12-08T06:09:52.881Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><p>RDD（Resilient Distributed Dataset）弹性分布式数据集，是spark中最基本的数据抽象，代表一个不可变、可分区、其中元素可并行计算的集合</p><ul><li>Resilient：RDD中的数据可存储再内存或磁盘中</li><li>Distributed：分布式存储数据（跨机器/跨进程），用于分布式计算</li><li>Dataset：一个用于存放数据的数据集合</li></ul><h3 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h3><h4 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h4><p>RDD分区是RDD数据存储的最小单位，一份RDD数据本质上分隔成了多个分区</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 存储9个数字，设立三个分区</span></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], <span class="number">3</span>)</span><br><span class="line">rdd.glom().collect()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[1,2,3],[4,5,6],[7,8,9]]</span><br></pre></td></tr></table></figure><h4 id="RDD方法会作用在其所有方法上"><a href="#RDD方法会作用在其所有方法上" class="headerlink" title="RDD方法会作用在其所有方法上"></a>RDD方法会作用在其所有方法上</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.map(<span class="keyword">lambda</span> x: x * <span class="number">10</span>).collect()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[10,20,30,40,50,60,70,80,90]</span><br></pre></td></tr></table></figure><h4 id="RDD之间具有依赖关系"><a href="#RDD之间具有依赖关系" class="headerlink" title="RDD之间具有依赖关系"></a>RDD之间具有依赖关系</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line">rdd1 = sc.textFile(<span class="string">"../test.text"</span>)</span><br><span class="line">rdd2 = rdd1.flatMap(<span class="keyword">lambda</span> x: x.split(<span class="string">' '</span>))</span><br><span class="line">rdd3 = rdd2.map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))</span><br><span class="line">rdd4 = rdd3.reduceByKey(<span class="keyword">lambda</span> a, b: a+b)</span><br><span class="line">print(rdd4.collect())</span><br></pre></td></tr></table></figure><h4 id="Key-Value型RDD可以有分区器"><a href="#Key-Value型RDD可以有分区器" class="headerlink" title="Key-Value型RDD可以有分区器"></a>Key-Value型RDD可以有分区器</h4><p>KV型RDD：RDD内存储的数据是只有两个元素的二元元组</p><p>默认分区器：Hash分区规则，也可手动设置分区器：rdd.partitionBy()方法</p><p><font color="gold">注</font>：不是所有RDD都是KV型</p><h4 id="RDD的分区规划：会尽量靠近数据所在的服务器"><a href="#RDD的分区规划：会尽量靠近数据所在的服务器" class="headerlink" title="RDD的分区规划：会尽量靠近数据所在的服务器"></a>RDD的分区规划：会尽量靠近数据所在的服务器</h4><p>在初始RDD读取数据规划阶段，分区会尽量规划到存储数据所在服务器，直接读取本地数据，避免从网络读取数据</p><p>Spark会在确保并行计算能力的前提下，尽量确保本地读取</p><h3 id="RDD创建"><a href="#RDD创建" class="headerlink" title="RDD创建"></a>RDD创建</h3><ul><li>通过并行化集合创建（本地对象转化为分布式RDD）</li><li>读取外部数据源（读文件）</li></ul><h4 id="并行化创建"><a href="#并行化创建" class="headerlink" title="并行化创建"></a>并行化创建</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># arg1: 集合对象，如：list</span></span><br><span class="line"><span class="comment"># arg2：可选，指定分区数量</span></span><br><span class="line">rdd = SparkContext.parallelize(arg1, arg2)</span><br></pre></td></tr></table></figure><h4 id="读取文件"><a href="#读取文件" class="headerlink" title="读取文件"></a>读取文件</h4><p>通过textFile API来读取本地或者hdfs的数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># arg1: 文件路径</span></span><br><span class="line"><span class="comment"># arg2：可选，最小分区数量</span></span><br><span class="line"><span class="comment"># 当arg2超出spark允许范围，参数失效</span></span><br><span class="line">SparkContext.textFile(arg1, arg2)</span><br></pre></td></tr></table></figure><p>通过wholeTextFile API来读取小文件，这个api偏向于少量分区读取数据，是pyspark基于小文件的优化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># arg1：文件路径</span></span><br><span class="line"><span class="comment"># arg2：可选，最小分区数量</span></span><br><span class="line"><span class="comment"># 当arg2超出spark允许范围，参数失效</span></span><br><span class="line">SparkContext.wholeTextFiles(arg1, arg2)</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="RDD算子"><a href="#RDD算子" class="headerlink" title="RDD算子"></a>RDD算子</h3><p>方法、函数：本地对象的API</p><p>算子：分布式集合对象的API</p><p>RDD的算子分为两类：</p><ul><li><p>Transformation：转换算子</p><p>返回值仍旧是RDD的算子，构建执行计划</p></li><li><p>Action：行动算子</p><p>返回值不再是RDD，使执行计划开始工作</p></li></ul><h3 id="Transformation算子"><a href="#Transformation算子" class="headerlink" title="Transformation算子"></a>Transformation算子</h3><h4 id="map"><a href="#map" class="headerlink" title="map"></a>map</h4><p>将RDD的数据一条条处理（处理逻辑基于map算子接收的处理函数），返回新的RDD</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.map(func)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"TEST"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> data * <span class="number">10</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(rdd.map(add).collect())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[10, 20, 30, 40, 50, 60]</span><br></pre></td></tr></table></figure><h4 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h4><p>对RDD执行map操作，接着进行解除嵌套：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 嵌套</span></span><br><span class="line">lst = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]</span><br><span class="line"><span class="comment"># 解除嵌套</span></span><br><span class="line">lst = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([<span class="string">"hadoop spark hadoop"</span>, <span class="string">"spark hadoop hadoop"</span>, <span class="string">"hadoop flink spark"</span>])</span><br><span class="line"><span class="comment"># 得到所有的单词, 组成RDD, flatMap的传入参数 和map一致, 就是给map逻辑用的, 解除嵌套无需逻辑(传参)</span></span><br><span class="line">rdd2 = rdd.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">" "</span>))</span><br><span class="line">print(rdd2.collect())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&apos;hadoop&apos;, &apos;spark&apos;, &apos;hadoop&apos;, &apos;spark&apos;, &apos;hadoop&apos;, &apos;hadoop&apos;, &apos;hadoop&apos;, &apos;flink&apos;, &apos;spark&apos;]</span><br></pre></td></tr></table></figure><h4 id="reduceByKey"><a href="#reduceByKey" class="headerlink" title="reduceByKey"></a>reduceByKey</h4><p>针对KV型RDD自动按照key进行分组，然后根据提供的聚合逻辑完成组内数据（value）聚合操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 接受两个类型一致的传入参数，返回聚合值</span></span><br><span class="line">rdd.reduceByKey(func)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([(<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'b'</span>, <span class="number">1</span>), (<span class="string">'b'</span>, <span class="number">1</span>), (<span class="string">'a'</span>, <span class="number">1</span>)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># reduceByKey 对相同key 的数据执行聚合相加</span></span><br><span class="line">print(rdd.reduceByKey(<span class="keyword">lambda</span> a, b: a + b).collect())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(&apos;b&apos;, 2), (&apos;a&apos;, 3)]</span><br></pre></td></tr></table></figure><h4 id="mapValues"><a href="#mapValues" class="headerlink" title="mapValues"></a>mapValues</h4><p>针对二元元组RDD，对其内部的二元元组value值进行map</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 传入二元元组的value值，func只对value进行处理</span></span><br><span class="line">rdd.mapValues(func)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([(<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'b'</span>, <span class="number">1</span>), (<span class="string">'b'</span>, <span class="number">1</span>), (<span class="string">'a'</span>, <span class="number">1</span>)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># reduceByKey 对相同key 的数据执行聚合相加</span></span><br><span class="line">print(rdd.mapValues(<span class="keyword">lambda</span> x: x * <span class="number">10</span>).collect())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(&apos;a&apos;, 10), (&apos;a&apos;, 10), (&apos;b&apos;, 10), (&apos;b&apos;, 10), (&apos;a&apos;, 10)]</span><br></pre></td></tr></table></figure><h4 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h4><p>将RDD的数据进行分组</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># func要求传入一个参数，返回一个值，类型不做要求。相同的返回值将被放入同一个组中。</span></span><br><span class="line"><span class="comment"># 分组完成后，每一个组是一个二元元组，key就是返回值，所有同组数据放入一个迭代器对象中作为value</span></span><br><span class="line">rdd.groupBy(func)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([(<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'b'</span>, <span class="number">1</span>), (<span class="string">'b'</span>, <span class="number">2</span>), (<span class="string">'b'</span>, <span class="number">3</span>)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过groupBy对数据进行分组</span></span><br><span class="line"><span class="comment"># groupBy传入的函数的 意思是: 通过这个函数, 确定按照谁来分组(返回谁即可)</span></span><br><span class="line"><span class="comment"># 分组规则 和SQL是一致的, 也就是相同的在一个组(Hash分组)</span></span><br><span class="line">result = rdd.groupBy(<span class="keyword">lambda</span> t: t[<span class="number">0</span>])</span><br><span class="line">print(result.map(<span class="keyword">lambda</span> t:(t[<span class="number">0</span>], list(t[<span class="number">1</span>]))).collect())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(&apos;b&apos;, [(&apos;b&apos;, 1), (&apos;b&apos;, 2), (&apos;b&apos;, 3)]), (&apos;a&apos;, [(&apos;a&apos;, 1), (&apos;a&apos;, 1)])]</span><br></pre></td></tr></table></figure><h4 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h4><p>过滤数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># func返回值为True的参数保留，False丢弃</span></span><br><span class="line">rdd.filter(func)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过Filter算子, 过滤奇数</span></span><br><span class="line">result = rdd.filter(<span class="keyword">lambda</span> x: x % <span class="number">2</span> == <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(result.collect())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1, 3, 5]</span><br></pre></td></tr></table></figure><h4 id="distinct"><a href="#distinct" class="headerlink" title="distinct"></a>distinct</h4><p>对RDD数据进行去重</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># arg：去重分区数量，一般省略</span></span><br><span class="line">rdd.distinct(arg)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># distinct 进行RDD数据去重操作</span></span><br><span class="line">print(rdd.distinct().collect())</span><br><span class="line"></span><br><span class="line">rdd2 = sc.parallelize([(<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'a'</span>, <span class="number">3</span>)])</span><br><span class="line">print(rdd2.distinct().collect())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[2, 1, 3]</span><br><span class="line">[(&apos;a&apos;, 1), (&apos;a&apos;, 3)]</span><br></pre></td></tr></table></figure><h4 id="union"><a href="#union" class="headerlink" title="union"></a>union</h4><p>将两个RDD合并成一个RDD返回</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 仅合并，不会去重</span></span><br><span class="line"><span class="comment"># 可以合并不同类型的RDD</span></span><br><span class="line">rdd.union(other_rdd)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd1 = sc.parallelize([<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">rdd2 = sc.parallelize([<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"a"</span>])</span><br><span class="line"></span><br><span class="line">rdd3 = rdd1.union(rdd2)</span><br><span class="line">print(rdd3.collect())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1, 1, 3, 3, &apos;a&apos;, &apos;b&apos;, &apos;a&apos;]</span><br></pre></td></tr></table></figure><h4 id="join"><a href="#join" class="headerlink" title="join"></a>join</h4><p>对两个RDD执行JOIN操作（可实现SQL的内/外连接）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># jion算子只能用于二元元组</span></span><br><span class="line">rdd.join(other_rdd)            <span class="comment"># 内连接</span></span><br><span class="line">rdd.leftOuterJoin(other_rdd)   <span class="comment"># 左外连接</span></span><br><span class="line">rdd.rightOuterJoin(other_rdd)  <span class="comment"># 右外连接</span></span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd1 = sc.parallelize([ (<span class="number">1001</span>, <span class="string">"zhangsan"</span>), (<span class="number">1002</span>, <span class="string">"lisi"</span>), (<span class="number">1003</span>, <span class="string">"wangwu"</span>), (<span class="number">1004</span>, <span class="string">"zhaoliu"</span>) ])</span><br><span class="line">rdd2 = sc.parallelize([ (<span class="number">1001</span>, <span class="string">"销售部"</span>), (<span class="number">1002</span>, <span class="string">"科技部"</span>)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过join算子来进行rdd之间的关联</span></span><br><span class="line"><span class="comment"># 对于join算子来说 关联条件 按照二元元组的key来进行关联</span></span><br><span class="line">print(rdd1.join(rdd2).collect())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 左外连接, 右外连接 可以更换一下rdd的顺序 或者调用rightOuterJoin即可</span></span><br><span class="line">print(rdd1.leftOuterJoin(rdd2).collect())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[(1001, (&apos;zhangsan&apos;, &apos;销售部&apos;)), (1002, (&apos;lisi&apos;, &apos;科技部&apos;))]</span><br><span class="line">[(1004, (&apos;zhaoliu&apos;, None)), (1001, (&apos;zhangsan&apos;, &apos;销售部&apos;)), (1002, (&apos;lisi&apos;, &apos;科技部&apos;)), (1003, (&apos;wangwu&apos;, None))]</span><br></pre></td></tr></table></figure><h4 id="intersection"><a href="#intersection" class="headerlink" title="intersection"></a>intersection</h4><p>求两个RDD的交集，返回一个新RDD</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.intersection(other_rdd)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd1 = sc.parallelize([(<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'a'</span>, <span class="number">3</span>)])</span><br><span class="line">rdd2 = sc.parallelize([(<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'b'</span>, <span class="number">3</span>)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过intersection算子求RDD之间的交集, 将交集取出 返回新RDD</span></span><br><span class="line">rdd3 = rdd1.intersection(rdd2)</span><br><span class="line"></span><br><span class="line">print(rdd3.collect())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(&apos;a&apos;, 1)]</span><br></pre></td></tr></table></figure><h4 id="glom"><a href="#glom" class="headerlink" title="glom"></a>glom</h4><p>将RDD的数据按照分区加上嵌套</p><p>例如RDD数据[1,2,3,4,5]有两个分区，经过glom处理后变成：[[1,2,3]],[4,5]]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.glom()</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">print(rdd.glom().collect())</span><br><span class="line">print(rdd.glom().flatMap(<span class="keyword">lambda</span> x: x).collect()) <span class="comment"># tips：解嵌套</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[1, 2, 3, 4], [5, 6, 7, 8, 9]]</span><br><span class="line">[1, 2, 3, 4, 5, 6, 7, 8, 9]</span><br></pre></td></tr></table></figure><h4 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey"></a>groupByKey</h4><p>针对KV型RDD自动按key分组</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.groupByKey()</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([(<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'b'</span>, <span class="number">1</span>), (<span class="string">'b'</span>, <span class="number">1</span>), (<span class="string">'b'</span>, <span class="number">1</span>)])</span><br><span class="line"></span><br><span class="line">rdd2 = rdd.groupByKey()</span><br><span class="line"></span><br><span class="line">print(rdd2.map(<span class="keyword">lambda</span> x: (x[<span class="number">0</span>], list(x[<span class="number">1</span>]))).collect())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(&apos;b&apos;, [1, 1, 1]), (&apos;a&apos;, [1, 1])]</span><br></pre></td></tr></table></figure><h4 id="sortBy"><a href="#sortBy" class="headerlink" title="sortBy"></a>sortBy</h4><p>基于指定的排序函数对RDD数据进行排序</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ascending：True-升序，False-降序</span></span><br><span class="line"><span class="comment"># numPartitions：用于排序的分区数量，要进行全局排序，设置为1</span></span><br><span class="line">rdd.sortBy(func, ascending=<span class="literal">False</span>, numPartitions=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([(<span class="string">'c'</span>, <span class="number">3</span>), (<span class="string">'f'</span>, <span class="number">1</span>), (<span class="string">'b'</span>, <span class="number">11</span>), (<span class="string">'c'</span>, <span class="number">3</span>), (<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'c'</span>, <span class="number">5</span>), (<span class="string">'e'</span>, <span class="number">1</span>), (<span class="string">'n'</span>, <span class="number">9</span>), (<span class="string">'a'</span>, <span class="number">1</span>)], <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用sortBy对rdd执行排序</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 按照value 数字进行排序</span></span><br><span class="line"><span class="comment"># 参数1函数，告知Spark 按照数据的哪个列进行排序</span></span><br><span class="line"><span class="comment"># 参数2: True表示升序 False表示降序</span></span><br><span class="line"><span class="comment"># 参数3: 排序的分区数</span></span><br><span class="line"><span class="string">"""注意: 如果要全局有序, 排序分区数需设置为1"""</span></span><br><span class="line">print(rdd.sortBy(<span class="keyword">lambda</span> x: x[<span class="number">1</span>], ascending=<span class="literal">True</span>, numPartitions=<span class="number">1</span>).collect())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按照key来进行排序</span></span><br><span class="line">print(rdd.sortBy(<span class="keyword">lambda</span> x: x[<span class="number">0</span>], ascending=<span class="literal">False</span>, numPartitions=<span class="number">1</span>).collect())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[(&apos;f&apos;, 1), (&apos;a&apos;, 1), (&apos;e&apos;, 1), (&apos;a&apos;, 1), (&apos;c&apos;, 3), (&apos;c&apos;, 3), (&apos;c&apos;, 5), (&apos;n&apos;, 9), (&apos;b&apos;, 11)]</span><br><span class="line">[(&apos;n&apos;, 9), (&apos;f&apos;, 1), (&apos;e&apos;, 1), (&apos;c&apos;, 3), (&apos;c&apos;, 3), (&apos;c&apos;, 5), (&apos;b&apos;, 11), (&apos;a&apos;, 1), (&apos;a&apos;, 1)]</span><br></pre></td></tr></table></figure><h4 id="sortByKey"><a href="#sortByKey" class="headerlink" title="sortByKey"></a>sortByKey</h4><p>针对KV型RDD按照key进行排序</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ascending：True-升序，False-降序</span></span><br><span class="line"><span class="comment"># numPartitions：用于排序的分区数量，要进行全局排序，设置为1</span></span><br><span class="line"><span class="comment"># keyfunc：在排序前对key进行的处理</span></span><br><span class="line">rdd.sortByKey(ascending=<span class="literal">True</span>, numPartitions=<span class="literal">None</span>, keyfunc=&lt;function RDD.&lt;<span class="keyword">lambda</span>&gt;&gt;)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([(<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'E'</span>, <span class="number">1</span>), (<span class="string">'C'</span>, <span class="number">1</span>), (<span class="string">'D'</span>, <span class="number">1</span>), (<span class="string">'b'</span>, <span class="number">1</span>), (<span class="string">'g'</span>, <span class="number">1</span>), (<span class="string">'f'</span>, <span class="number">1</span>),</span><br><span class="line">                      (<span class="string">'y'</span>, <span class="number">1</span>), (<span class="string">'u'</span>, <span class="number">1</span>), (<span class="string">'i'</span>, <span class="number">1</span>), (<span class="string">'o'</span>, <span class="number">1</span>), (<span class="string">'p'</span>, <span class="number">1</span>),</span><br><span class="line">                      (<span class="string">'m'</span>, <span class="number">1</span>), (<span class="string">'n'</span>, <span class="number">1</span>), (<span class="string">'j'</span>, <span class="number">1</span>), (<span class="string">'k'</span>, <span class="number">1</span>), (<span class="string">'l'</span>, <span class="number">1</span>)], <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">print(rdd.sortByKey(ascending=<span class="literal">True</span>, numPartitions=<span class="number">1</span>, keyfunc=<span class="keyword">lambda</span> key: str(key).lower()).collect())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(&apos;a&apos;, 1), (&apos;b&apos;, 1), (&apos;C&apos;, 1), (&apos;D&apos;, 1), (&apos;E&apos;, 1), (&apos;f&apos;, 1), (&apos;g&apos;, 1), (&apos;i&apos;, 1), (&apos;j&apos;, 1), (&apos;k&apos;, 1), (&apos;l&apos;, 1), (&apos;m&apos;, 1), (&apos;n&apos;, 1), (&apos;o&apos;, 1), (&apos;p&apos;, 1), (&apos;u&apos;, 1), (&apos;y&apos;, 1)]</span><br></pre></td></tr></table></figure><h4 id="repartition-amp-coalesce"><a href="#repartition-amp-coalesce" class="headerlink" title="repartition &amp; coalesce"></a>repartition &amp; coalesce</h4><p>对RDD的分区执行重新分区（仅数量）</p><p><font color="gold">注</font>：尽量避免使用，影响并行计算性能。在合并到1个分区进行全局排序等场景下使用，尽可能避免增加分区，可能破坏内存迭代的计算管道</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># n：决定新的分区数量</span></span><br><span class="line"><span class="comment"># coalesce中增加分区必须指定shuffle=True</span></span><br><span class="line">rdd.repartition(n)</span><br><span class="line">rdd.coalesce(n, shuffle)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># repartition 修改分区</span></span><br><span class="line">print(rdd.repartition(<span class="number">1</span>).getNumPartitions())</span><br><span class="line"></span><br><span class="line">print(rdd.repartition(<span class="number">5</span>).getNumPartitions())</span><br><span class="line"></span><br><span class="line"><span class="comment"># coalesce 修改分区</span></span><br><span class="line">print(rdd.coalesce(<span class="number">1</span>).getNumPartitions())</span><br><span class="line"></span><br><span class="line">print(rdd.coalesce(<span class="number">5</span>, shuffle=<span class="literal">True</span>).getNumPartitions())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">5</span><br><span class="line">1</span><br><span class="line">5</span><br></pre></td></tr></table></figure><h3 id="Action算子"><a href="#Action算子" class="headerlink" title="Action算子"></a>Action算子</h3><h4 id="countByKey"><a href="#countByKey" class="headerlink" title="countByKey"></a>countByKey</h4><p>用于统计key出现的次数（一般适用于KV型RDD）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.countByKey()</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.textFile(<span class="string">"hdfs://master:8020/input/words.txt"</span>)</span><br><span class="line">rdd2 = rdd.flatMap(<span class="keyword">lambda</span> x: x.split(<span class="string">" "</span>)).map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过countByKey来对key进行计数, 这是一个Action算子</span></span><br><span class="line">result = rdd2.countByKey()</span><br><span class="line"></span><br><span class="line">print(result)</span><br><span class="line">print(type(result))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">defaultdict(&lt;class &apos;int&apos;&gt;, &#123;&apos;hadoop&apos;: 7, &apos;spark&apos;: 5, &apos;flink&apos;: 3&#125;)</span><br><span class="line">&lt;class &apos;collections.defaultdict&apos;&gt;</span><br></pre></td></tr></table></figure><h4 id="collect"><a href="#collect" class="headerlink" title="collect"></a>collect</h4><p>将RDD各个分区内的数据统一收集到Driver中，形成一个list对象</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.collect()</span><br></pre></td></tr></table></figure><p><font color="gold">注</font>：数据集大小不能超过Driver内存</p><h4 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h4><p>对RDD数据集按照func逻辑进行聚合</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对func：传入2个参数得到1个返回值，要求返回值和参数的类型保持一致</span></span><br><span class="line">rdd.reduce(func)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line">print(rdd.reduce(<span class="keyword">lambda</span> a, b: a + b))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">15</span><br></pre></td></tr></table></figure><h4 id="fold"><a href="#fold" class="headerlink" title="fold"></a>fold</h4><p>同reduce，接受传入逻辑进行聚合，但是聚合是带有初始值的。这个初始值的聚合作用在：</p><ul><li>分区内聚合</li><li>分区间聚合</li></ul><p>例如：[[1, 2, 3], [4, 5, 6], [7, 8, 9]]</p><p>数据分布在三个<strong>分区</strong>上</p><p><strong><font color="orange">分区1</font></strong>：123聚合时带上10作为初始值得到16</p><p><strong><font color="orange">分区2</font></strong>：456聚合时带上10作为初始值得到25</p><p><strong><font color="orange">分区3</font></strong>：789聚合时带上10作为初始值得到34</p><p>最后再做3个分区间的聚合：16+25+34得到85</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.fold(src, func)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">print(rdd.fold(<span class="number">10</span>, <span class="keyword">lambda</span> a, b: a + b))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">85</span><br></pre></td></tr></table></figure><h4 id="first·take·count·top"><a href="#first·take·count·top" class="headerlink" title="first·take·count·top"></a>first·take·count·top</h4><p><strong><font color="orange">first</font></strong></p><p>取出RDD的第一个元素</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;sc.parallelize([<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]).first()</span><br><span class="line"><span class="number">3</span></span><br></pre></td></tr></table></figure><p><strong><font color="orange">take</font></strong></p><p>取出RDD的前n个元素，组合成list返回</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]).take(<span class="number">5</span>)</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br></pre></td></tr></table></figure><p> <strong><font color="orange">count</font></strong></p><p>计算RDD有多少条数据，返回值是一个数字</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;sc.parallelize([<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]).count()</span><br><span class="line"><span class="number">6</span></span><br></pre></td></tr></table></figure><p><strong><font color="orange">top</font></strong></p><p>对RDD数据集进行降序排序，取结果前n个</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;sc.parallelize([<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]).top(<span class="number">3</span>)</span><br><span class="line">[<span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>]</span><br></pre></td></tr></table></figure><h4 id="takeSample"><a href="#takeSample" class="headerlink" title="takeSample"></a>takeSample</h4><p>随机抽样RDD数据，可用于数据检查</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># arg1：True表示运行取同一个数据，False表示不允许取同一个数据（和数据内容无关，是否重复表示的是同一个位置的数据）</span></span><br><span class="line"><span class="comment"># arg2：抽样数目</span></span><br><span class="line"><span class="comment"># arg3：可选，随机数种子，随意传进一个数字</span></span><br><span class="line">takeSample(arg1：<span class="literal">True</span>/<span class="literal">False</span>, arg2:采样数, arg3:随机数种子)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">6</span>], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(rdd.takeSample(<span class="literal">False</span>, <span class="number">5</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[2, 7, 6, 6, 3]</span><br></pre></td></tr></table></figure><h4 id="takeOrdered"><a href="#takeOrdered" class="headerlink" title="takeOrdered"></a>takeOrdered</h4><p>对RDD进行排序，取结果前n个</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># arg1：需要几个数据</span></span><br><span class="line"><span class="comment"># arg2：对排序的数据进行更改（不会更改数据本身，仅在排序时使用）</span></span><br><span class="line">rdd.takeOrdered(arg1, arg2)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">6</span>], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(rdd.takeOrdered(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">print(rdd.takeOrdered(<span class="number">3</span>, <span class="keyword">lambda</span> x: -x))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[1, 2, 3]</span><br><span class="line">[9, 7, 6]</span><br></pre></td></tr></table></figure><h4 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h4><p>对RDD的每一个元素执行提供的逻辑操作（同map），无返回值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.foreach(func)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">6</span>], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">result = rdd.foreach(<span class="keyword">lambda</span> x: print(x * <span class="number">10</span>))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">10</span><br><span class="line">30</span><br><span class="line">20</span><br><span class="line">40</span><br><span class="line">70</span><br><span class="line">90</span><br><span class="line">60</span><br></pre></td></tr></table></figure><p>特性：由Executor直接输出</p><h4 id="saveAsTextFile"><a href="#saveAsTextFile" class="headerlink" title="saveAsTextFile"></a>saveAsTextFile</h4><p>将RDD数据写入文本文件</p><p>支持：本地写出或hdfs等文件系统</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.saveAsTextFile()</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">6</span>], <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">rdd.saveAsTextFile(<span class="string">"hdfs://master:8020/output/out_test1"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -ls /output/out_test1</span><br><span class="line">hadoop fs -cat /output/out_test1/*</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Found 4 items</span><br><span class="line">-rw-r--r--   3 root supergroup          0 2022-12-06 17:44 /output/out_test1/_SUCCESS</span><br><span class="line">-rw-r--r--   3 root supergroup          4 2022-12-06 17:44 /output/out_test1/part-00000</span><br><span class="line">-rw-r--r--   3 root supergroup          4 2022-12-06 17:44 /output/out_test1/part-00001</span><br><span class="line">-rw-r--r--   3 root supergroup          6 2022-12-06 17:44 /output/out_test1/part-00002</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">3</span><br><span class="line">2</span><br><span class="line">4</span><br><span class="line">7</span><br><span class="line">9</span><br><span class="line">6</span><br></pre></td></tr></table></figure><p>特性：由Executor直接写入文件</p><h4 id="mapPartitions"><a href="#mapPartitions" class="headerlink" title="mapPartitions"></a>mapPartitions</h4><p>不同于map每次操作一个分区的单一对象，mapPartitions一次操作一整个分区的数据，作为一个迭代器对象传入进来</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.mapPartitions(func)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">6</span>], <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(iter)</span>:</span></span><br><span class="line">    result = list()</span><br><span class="line">    <span class="keyword">for</span> it <span class="keyword">in</span> iter:</span><br><span class="line">        result.append(it * <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">print(rdd.mapPartitions(process).collect())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[10, 20, 70]</span><br></pre></td></tr></table></figure><h4 id="foreachPartition"><a href="#foreachPartition" class="headerlink" title="foreachPartition"></a>foreachPartition</h4><p>和foreach一致，foreach一条条处理，而foreachPartition一次处理一整个分区的数据，类似于没有返回值的mapPartitions</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.foreachPartition(func)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">6</span>], <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(iter)</span>:</span></span><br><span class="line">    result = list()</span><br><span class="line">    <span class="keyword">for</span> it <span class="keyword">in</span> iter:</span><br><span class="line">        result.append(it * <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">        print(result)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">rdd.foreachPartition(process)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[20]</span><br><span class="line">[20, 40]</span><br><span class="line">[10]</span><br><span class="line">[10, 30]</span><br><span class="line">[70]</span><br><span class="line">[70, 90]</span><br><span class="line">[70, 90, 60]</span><br></pre></td></tr></table></figure><h4 id="partitionBy"><a href="#partitionBy" class="headerlink" title="partitionBy"></a>partitionBy</h4><p>对RDD进行自定义分区操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># arg1：重新分区后的分区数量</span></span><br><span class="line"><span class="comment"># arg2：自定义分区规则，通过函数传入，函数返回值必须位int型（分区编号从0开始，不得超过分区数-1）</span></span><br><span class="line">rdd.partitionBy(arg1, arg2)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([(<span class="string">'hadoop'</span>, <span class="number">1</span>), (<span class="string">'spark'</span>, <span class="number">1</span>), (<span class="string">'hello'</span>, <span class="number">1</span>), (<span class="string">'flink'</span>, <span class="number">1</span>), (<span class="string">'hadoop'</span>, <span class="number">1</span>), (<span class="string">'spark'</span>, <span class="number">1</span>)])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用partitionBy 自定义 分区</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(k)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">'hadoop'</span> == k <span class="keyword">or</span> <span class="string">'hello'</span> == k: <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"><span class="keyword">if</span> <span class="string">'spark'</span> == k: <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(rdd.partitionBy(<span class="number">3</span>, process).glom().collect())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[(&apos;hadoop&apos;, 1), (&apos;hello&apos;, 1), (&apos;hadoop&apos;, 1)], [(&apos;spark&apos;, 1), (&apos;spark&apos;, 1)], [(&apos;flink&apos;, 1)]]</span><br></pre></td></tr></table></figure><h2 id="RDD的数据是过程数据"><a href="#RDD的数据是过程数据" class="headerlink" title="RDD的数据是过程数据"></a>RDD的数据是过程数据</h2><p>RDD之间进行相互迭代计算（Transformation的转换），当执行开启以后，新RDD生成，旧RDD消失。所以RDD的数据是过程数据，仅在处理的过程中存在，一旦处理完成便会被释放，旨在最大化的合理利用系统资源</p><p>当RDD被释放后需要被重新使用，会从头开始执行</p><h3 id="RDD缓存"><a href="#RDD缓存" class="headerlink" title="RDD缓存"></a>RDD缓存</h3><p>防止当RDD被释放而又要被重新调用的情况下，避免从头执行代码，使用RDD缓存API</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">rdd.cache()                                  <span class="comment"># 缓存到内存中</span></span><br><span class="line">rdd.persist(StorageLevel.MEMORY_ONLY)        <span class="comment"># 仅在内存缓存</span></span><br><span class="line">rdd.persist(StorageLevel.MEMORY_ONLY_2)      <span class="comment"># 仅在内存缓存，生成2个副本</span></span><br><span class="line">rdd.persist(StorageLevel.DISK_ONLY)          <span class="comment"># 仅缓存到硬盘</span></span><br><span class="line">rdd.persist(StorageLevel.DISK_ONLY_2)        <span class="comment"># 仅缓存到硬盘，生成2个副本</span></span><br><span class="line">rdd.persist(StorageLevel.DISK_ONLY_3)        <span class="comment"># 仅缓存到硬盘，生成3个副本</span></span><br><span class="line">rdd.persist(StorageLevel.MEMORY_AND_DISK)    <span class="comment"># 先在内存缓存，内存不够缓存到硬盘</span></span><br><span class="line">rdd.persist(StorageLevel.MEMORY_AND_DISK_2)  <span class="comment"># 先在内存缓存，内存不够缓存到硬盘，生成2个副本</span></span><br><span class="line">rdd.persist(StorageLevel.OFF_HEAP)           <span class="comment"># 堆外内存</span></span><br><span class="line">rdd.unpersist()                              <span class="comment"># 主动清理缓存</span></span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.storagelevel <span class="keyword">import</span> StorageLevel</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd1 = sc.textFile(<span class="string">"hdfs://master:8020/input/words.txt"</span>)</span><br><span class="line">rdd2 = rdd1.flatMap(<span class="keyword">lambda</span> x: x.split(<span class="string">" "</span>))</span><br><span class="line">rdd3 = rdd2.map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">rdd3.cache()</span><br><span class="line">rdd3.persist(StorageLevel.MEMORY_AND_DISK_2)</span><br><span class="line"></span><br><span class="line">rdd4 = rdd3.reduceByKey(<span class="keyword">lambda</span> a, b: a + b)</span><br><span class="line">print(rdd4.collect())</span><br><span class="line"></span><br><span class="line">rdd5 = rdd3.groupByKey()</span><br><span class="line">rdd6 = rdd5.mapValues(<span class="keyword">lambda</span> x: sum(x))</span><br><span class="line">print(rdd6.collect())</span><br><span class="line"></span><br><span class="line">rdd3.unpersist()</span><br><span class="line">time.sleep(<span class="number">100000</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[(&apos;hadoop&apos;, 1), (&apos;hello&apos;, 3), (&apos;spark&apos;, 1), (&apos;flink&apos;, 1)]</span><br><span class="line">[(&apos;hadoop&apos;, 1), (&apos;hello&apos;, 3), (&apos;spark&apos;, 1), (&apos;flink&apos;, 1)]</span><br></pre></td></tr></table></figure><p><font color="gold">注</font>：缓存分散存储在各Executor所在服务器中。缓存从设计上来说是不安全的，缓存一旦丢失，需要重新计算缓存，必须保留被缓存RDD的前置血缘关系</p><h3 id="RDD-CheckPoint"><a href="#RDD-CheckPoint" class="headerlink" title="RDD CheckPoint"></a>RDD CheckPoint</h3><p>用于保存RDD数据，仅支持硬盘存储，且可以写入HDFS（cache不行），从设计上来说是安全的，不保留RDD的前置血缘关系</p><p>ChickPoint集中收集各个分区的数据进行存储，而非cache的分散存储</p><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.storagelevel <span class="keyword">import</span> StorageLevel</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 告知spark, 开启CheckPoint功能</span></span><br><span class="line">sc.setCheckpointDir(<span class="string">"hdfs://master:8020/output/ckp"</span>)</span><br><span class="line">rdd1 = sc.textFile(<span class="string">"hdfs://master:8020/input/words.txt"</span>)</span><br><span class="line">rdd2 = rdd1.flatMap(<span class="keyword">lambda</span> x: x.split(<span class="string">" "</span>))</span><br><span class="line">rdd3 = rdd2.map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用checkpoint API 保存数据即可</span></span><br><span class="line">rdd3.checkpoint()</span><br><span class="line"></span><br><span class="line">rdd4 = rdd3.reduceByKey(<span class="keyword">lambda</span> a, b: a + b)</span><br><span class="line">print(rdd4.collect())</span><br><span class="line"></span><br><span class="line">rdd5 = rdd3.groupByKey()</span><br><span class="line">rdd6 = rdd5.mapValues(<span class="keyword">lambda</span> x: sum(x))</span><br><span class="line">print(rdd6.collect())</span><br><span class="line"></span><br><span class="line">rdd3.unpersist()</span><br><span class="line">time.sleep(<span class="number">100000</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[(&apos;hadoop&apos;, 1), (&apos;hello&apos;, 3), (&apos;spark&apos;, 1), (&apos;flink&apos;, 1)]</span><br><span class="line">[(&apos;hadoop&apos;, 1), (&apos;hello&apos;, 3), (&apos;spark&apos;, 1), (&apos;flink&apos;, 1)]</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;RDD&quot;&gt;&lt;a href=&quot;#RDD&quot; class=&quot;headerlink&quot; title=&quot;RDD&quot;&gt;&lt;/a&gt;RDD&lt;/h2&gt;&lt;p&gt;RDD（Resilient Distributed Dataset）弹性分布式数据集，是spark中最基本的数据抽象，代表一个不可变、可分区、其中元素可并行计算的集合&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Resilient：RDD中的数据可存储再内存或磁盘中&lt;/li&gt;
&lt;li&gt;Distributed：分布式存储数据（跨机器/跨进程），用于分布式计算&lt;/li&gt;
&lt;li&gt;Dataset：一个用于存放数据的数据集合&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;特性&quot;&gt;&lt;a href=&quot;#特性&quot; class=&quot;headerlink&quot; title=&quot;特性&quot;&gt;&lt;/a&gt;特性&lt;/h3&gt;&lt;h4 id=&quot;分区&quot;&gt;&lt;a href=&quot;#分区&quot; class=&quot;headerlink&quot; title=&quot;分区&quot;&gt;&lt;/a&gt;分区&lt;/h4&gt;&lt;p&gt;RDD分区是RDD数据存储的最小单位，一份RDD数据本质上分隔成了多个分区&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 存储9个数字，设立三个分区&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;rdd = sc.parallelize([&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;6&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;7&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;8&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;9&lt;/span&gt;], &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;rdd.glom().collect()&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[[1,2,3],[4,5,6],[7,8,9]]&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h4 id=&quot;RDD方法会作用在其所有方法上&quot;&gt;&lt;a href=&quot;#RDD方法会作用在其所有方法上&quot; class=&quot;headerlink&quot; title=&quot;RDD方法会作用在其所有方法上&quot;&gt;&lt;/a&gt;RDD方法会作用在其所有方法上&lt;/h4&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;rdd.map(&lt;span class=&quot;keyword&quot;&gt;lambda&lt;/span&gt; x: x * &lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;).collect()&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[10,20,30,40,50,60,70,80,90]&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h4 id=&quot;RDD之间具有依赖关系&quot;&gt;&lt;a href=&quot;#RDD之间具有依赖关系&quot; class=&quot;headerlink&quot; title=&quot;RDD之间具有依赖关系&quot;&gt;&lt;/a&gt;RDD之间具有依赖关系&lt;/h4&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;sc = SparkContext(conf=conf)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;rdd1 = sc.textFile(&lt;span class=&quot;string&quot;&gt;&quot;../test.text&quot;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;rdd2 = rdd1.flatMap(&lt;span class=&quot;keyword&quot;&gt;lambda&lt;/span&gt; x: x.split(&lt;span class=&quot;string&quot;&gt;&#39; &#39;&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;rdd3 = rdd2.map(&lt;span class=&quot;keyword&quot;&gt;lambda&lt;/span&gt; x: (x, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;rdd4 = rdd3.reduceByKey(&lt;span class=&quot;keyword&quot;&gt;lambda&lt;/span&gt; a, b: a+b)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(rdd4.collect())&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h4 id=&quot;Key-Value型RDD可以有分区器&quot;&gt;&lt;a href=&quot;#Key-Value型RDD可以有分区器&quot; class=&quot;headerlink&quot; title=&quot;Key-Value型RDD可以有分区器&quot;&gt;&lt;/a&gt;Key-Value型RDD可以有分区器&lt;/h4&gt;&lt;p&gt;KV型RDD：RDD内存储的数据是只有两个元素的二元元组&lt;/p&gt;
&lt;p&gt;默认分区器：Hash分区规则，也可手动设置分区器：rdd.partitionBy()方法&lt;/p&gt;
&lt;p&gt;&lt;font color=&quot;gold&quot;&gt;注&lt;/font&gt;：不是所有RDD都是KV型&lt;/p&gt;
&lt;h4 id=&quot;RDD的分区规划：会尽量靠近数据所在的服务器&quot;&gt;&lt;a href=&quot;#RDD的分区规划：会尽量靠近数据所在的服务器&quot; class=&quot;headerlink&quot; title=&quot;RDD的分区规划：会尽量靠近数据所在的服务器&quot;&gt;&lt;/a&gt;RDD的分区规划：会尽量靠近数据所在的服务器&lt;/h4&gt;&lt;p&gt;在初始RDD读取数据规划阶段，分区会尽量规划到存储数据所在服务器，直接读取本地数据，避免从网络读取数据&lt;/p&gt;
&lt;p&gt;Spark会在确保并行计算能力的前提下，尽量确保本地读取&lt;/p&gt;
&lt;h3 id=&quot;RDD创建&quot;&gt;&lt;a href=&quot;#RDD创建&quot; class=&quot;headerlink&quot; title=&quot;RDD创建&quot;&gt;&lt;/a&gt;RDD创建&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;通过并行化集合创建（本地对象转化为分布式RDD）&lt;/li&gt;
&lt;li&gt;读取外部数据源（读文件）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;并行化创建&quot;&gt;&lt;a href=&quot;#并行化创建&quot; class=&quot;headerlink&quot; title=&quot;并行化创建&quot;&gt;&lt;/a&gt;并行化创建&lt;/h4&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# arg1: 集合对象，如：list&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# arg2：可选，指定分区数量&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;rdd = SparkContext.parallelize(arg1, arg2)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h4 id=&quot;读取文件&quot;&gt;&lt;a href=&quot;#读取文件&quot; class=&quot;headerlink&quot; title=&quot;读取文件&quot;&gt;&lt;/a&gt;读取文件&lt;/h4&gt;&lt;p&gt;通过textFile API来读取本地或者hdfs的数据&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# arg1: 文件路径&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# arg2：可选，最小分区数量&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 当arg2超出spark允许范围，参数失效&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;SparkContext.textFile(arg1, arg2)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;通过wholeTextFile API来读取小文件，这个api偏向于少量分区读取数据，是pyspark基于小文件的优化&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# arg1：文件路径&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# arg2：可选，最小分区数量&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 当arg2超出spark允许范围，参数失效&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;SparkContext.wholeTextFiles(arg1, arg2)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
    
    <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
    <category term="pyspark" scheme="http://yoursite.com/tags/pyspark/"/>
    
  </entry>
  
  <entry>
    <title>Pyspark</title>
    <link href="http://yoursite.com/2022/12/05/Pyspark/"/>
    <id>http://yoursite.com/2022/12/05/Pyspark/</id>
    <published>2022-12-05T08:45:40.000Z</published>
    <updated>2022-12-07T05:47:29.586Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><h2 id="Pyspark库安装"><a href="#Pyspark库安装" class="headerlink" title="Pyspark库安装"></a>Pyspark库安装</h2><p>（本文基于上文spark基础）</p><h3 id="Python库安装"><a href="#Python库安装" class="headerlink" title="Python库安装"></a>Python库安装</h3><p>在三台机器分别安装pyspark</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda activate pyspark</span><br><span class="line">pip install pyspark -i https://mirror.baidu.com/pypi/simple</span><br></pre></td></tr></table></figure><h3 id="Windows补丁"><a href="#Windows补丁" class="headerlink" title="Windows补丁"></a>Windows补丁</h3><p>将hadoop.dll置于C:/windows/system32/目录下，然后配置hadoop工具包的环境变量</p><p><img src="/2022/12/05/Pyspark/A.png" alt></p><p>安装相关python库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pyspark pyhive pymysql jieba -i https://mirror.baidu.com/pypi/simple</span><br></pre></td></tr></table></figure><h3 id="为pycharm添加ssh解释器环境"><a href="#为pycharm添加ssh解释器环境" class="headerlink" title="为pycharm添加ssh解释器环境"></a>为pycharm添加ssh解释器环境</h3><p><img src="/2022/12/05/Pyspark/B.png" alt></p><p><img src="/2022/12/05/Pyspark/C.png" alt></p><p><img src="/2022/12/05/Pyspark/D.png" alt></p><a id="more"></a><h2 id="SparkContext对象"><a href="#SparkContext对象" class="headerlink" title="SparkContext对象"></a>SparkContext对象</h2><p>Spark Application程序的入口为SparkContext。任何一个spark应用都要先构建SparkContext对象：</p><ul><li>创建SparkConf对象</li><li>基于SparkConf创建SparkContext</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conf = SparkConf().setAppName(appName).setMaster(master)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br></pre></td></tr></table></figure><h2 id="WorldCount测试程序"><a href="#WorldCount测试程序" class="headerlink" title="WorldCount测试程序"></a>WorldCount测试程序</h2><h3 id="Windows"><a href="#Windows" class="headerlink" title="Windows"></a>Windows</h3><p>在pycharm中新建python脚本，通过解释器执行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"><span class="comment"># 提交到yarn集群执行时，需配置环境变量</span></span><br><span class="line"><span class="comment"># import os</span></span><br><span class="line"><span class="comment"># os.environ["HADOOP_CONF_DIR"] = "/usr/local/hadoop/etc/hadoop"</span></span><br><span class="line"><span class="comment"># os.environ["YARN_CONF_DIR"] = "/usr/local/hadoop/etc/hadoop"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># 通过脚本执行时无需在代码中指定Master</span></span><br><span class="line">    <span class="comment"># conf = SparkConf().setAppName("WordCountHelloWorld")</span></span><br><span class="line">    <span class="comment"># 直接在pycharm执行</span></span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">"WordCountHelloWorld"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">    <span class="comment"># 通过SparkConf对象构建SparkContext对象</span></span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 需求 : wordcount单词计数, 读取HDFS上的words.txt文件, 对其内部的单词统计出现 的数量</span></span><br><span class="line">    <span class="comment"># 读取hdfs文件</span></span><br><span class="line">    file_rdd = sc.textFile(<span class="string">"hdfs://master:8020/input/words.txt"</span>)</span><br><span class="line">    <span class="comment"># 读取本地文件</span></span><br><span class="line">    <span class="comment"># file_rdd = sc.textFile("/usr/local/words.txt")</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将单词进行切割, 得到一个存储全部单词的集合对象</span></span><br><span class="line">    words_rdd = file_rdd.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将单词转换为元组对象, key是单词, value是数字1</span></span><br><span class="line">    words_with_one_rdd = words_rdd.map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将元组的value 按照key来分组, 对所有的value执行聚合操作(相加)</span></span><br><span class="line">    result_rdd = words_with_one_rdd.reduceByKey(<span class="keyword">lambda</span> a, b: a + b)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 通过collect方法收集RDD的数据打印输出结果</span></span><br><span class="line">    print(result_rdd.collect())</span><br></pre></td></tr></table></figure><h3 id="CentOS"><a href="#CentOS" class="headerlink" title="CentOS"></a>CentOS</h3><p>在根目录创建一份py脚本，通过spark客户端执行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/spark/bin/spark-submit --master local[*] /root/helloworld.py</span><br><span class="line">/usr/local/spark/bin/spark-submit --master yarn /root/helloworld.py</span><br></pre></td></tr></table></figure><h2 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h2><h3 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h3><p>构建SparkContext对象等非任务处理由Driver执行，RDD数据任务处理由Executor执行，再由Driver处理分布式计算结果</p><h3 id="Master-Node"><a href="#Master-Node" class="headerlink" title="Master Node"></a>Master Node</h3><p>spark自身的JVM框架JVM Driver和JVM Executor之间可以相互通讯，Python通过构建SparkContext对象与JVM Driver进行连接（Python的Driver代码翻译成JVM代码-py4j库，变成JVM Driver）</p><h3 id="Worker-Node"><a href="#Worker-Node" class="headerlink" title="Worker Node"></a>Worker Node</h3><p>Driver的操作指令发送给JVM Executor（RPC），JVM Executor再通过pyspark守护进程将指令发送给pyspark守护进程，pyspark守护进程将指令调度到运行的python进程中去。Executor端本质上是由python进程再工作</p><p>Driver段是直接由py4j直接翻译过去，Executor端则是转发</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;Pyspark库安装&quot;&gt;&lt;a href=&quot;#Pyspark库安装&quot; class=&quot;headerlink&quot; title=&quot;Pyspark库安装&quot;&gt;&lt;/a&gt;Pyspark库安装&lt;/h2&gt;&lt;p&gt;（本文基于上文spark基础）&lt;/p&gt;
&lt;h3 id=&quot;Python库安装&quot;&gt;&lt;a href=&quot;#Python库安装&quot; class=&quot;headerlink&quot; title=&quot;Python库安装&quot;&gt;&lt;/a&gt;Python库安装&lt;/h3&gt;&lt;p&gt;在三台机器分别安装pyspark&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;conda activate pyspark&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;pip install pyspark -i https://mirror.baidu.com/pypi/simple&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;Windows补丁&quot;&gt;&lt;a href=&quot;#Windows补丁&quot; class=&quot;headerlink&quot; title=&quot;Windows补丁&quot;&gt;&lt;/a&gt;Windows补丁&lt;/h3&gt;&lt;p&gt;将hadoop.dll置于C:/windows/system32/目录下，然后配置hadoop工具包的环境变量&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2022/12/05/Pyspark/A.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;安装相关python库&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;pip install pyspark pyhive pymysql jieba -i https://mirror.baidu.com/pypi/simple&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;为pycharm添加ssh解释器环境&quot;&gt;&lt;a href=&quot;#为pycharm添加ssh解释器环境&quot; class=&quot;headerlink&quot; title=&quot;为pycharm添加ssh解释器环境&quot;&gt;&lt;/a&gt;为pycharm添加ssh解释器环境&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;/2022/12/05/Pyspark/B.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2022/12/05/Pyspark/C.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2022/12/05/Pyspark/D.png&quot; alt&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
    
    <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
    <category term="pyspark" scheme="http://yoursite.com/tags/pyspark/"/>
    
  </entry>
  
  <entry>
    <title>Spark基础</title>
    <link href="http://yoursite.com/2022/12/01/Spark%E5%9F%BA%E7%A1%80/"/>
    <id>http://yoursite.com/2022/12/01/Spark%E5%9F%BA%E7%A1%80/</id>
    <published>2022-12-01T07:40:17.000Z</published>
    <updated>2023-04-13T01:43:42.844Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><font color="gold">定义</font>：<font color="gold">Apache Spark是用于大规模数据处理的统一分析引擎。</font>其核心数据结构：弹性分布式数据集（RDD）能够在大规模集群中做内存运算，且具有一定容错方式。</p><h2 id="Spark框架"><a href="#Spark框架" class="headerlink" title="Spark框架"></a>Spark框架</h2><h3 id="组成"><a href="#组成" class="headerlink" title="组成"></a>组成</h3><p><font color="orange"><strong>· Spark Core</strong></font>：以RDD为数据抽象，提供Python、Java、Scala、R语言的API和Spark的核心功能，是Spark运行的基础；</p><p><font color="orange"><strong>· SparkSQL</strong></font>：基于SparkCore，提供机构化数据处理模块，支持以SQL语言对数据进行处理；同时可以作为StructuredStreaming模块的基础，进行数据流式计算；</p><p><font color="orange"><strong>· SparkStreaming</strong></font>：基于SparkCore，提供数据流式计算；</p><p><font color="orange"><strong>· MLlib</strong></font>：基于SparkCore，内置大量机器学习库和算法API，进行机器学习计算；</p><p><font color="orange"><strong>· GraphX</strong></font>：基于SparkCore，提供了大量图计算API，用于分布式图计算</p><h3 id="运行模式"><a href="#运行模式" class="headerlink" title="运行模式"></a>运行模式</h3><p><strong>· 本地模式（单机/local）</strong>：以一个独立进程，通过内部的多线程模拟Spark运行环境</p><p><strong>· Standalone模式（集群）</strong>：Spark的各个角色以独立进程形式存在，组成集群环境</p><p><strong>· Hadoop YARN模式（集群）</strong>：Spark的各个角色运行在YARN容器内部，组成集群环境</p><p><strong>· Kubernetes模式（容器集群）</strong>：Spark的各个角色运行在Kubernetes容器内部，组成集群环境</p><p><strong>· 云服务模式</strong></p><h3 id="角色"><a href="#角色" class="headerlink" title="角色"></a>角色</h3><ul><li><strong>资源</strong></li></ul><p><strong>· Master角色</strong>：集群资源管理</p><p><strong>· Worker角色</strong>：单机资源管理(所在服务器资源管理)</p><ul><li><strong>任务</strong></li></ul><p><strong>· Driver角色</strong>：单个任务管理</p><p><strong>· Executor角色</strong>：单个任务计算</p><a id="more"></a><h2 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h2><h3 id="Hadoop伪分布式搭建"><a href="#Hadoop伪分布式搭建" class="headerlink" title="Hadoop伪分布式搭建"></a>Hadoop伪分布式搭建</h3><h4 id="准备master虚拟机"><a href="#准备master虚拟机" class="headerlink" title="准备master虚拟机"></a>准备master虚拟机</h4><ul><li>配置静态ip</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysconfig/network-scripts/ifcfg-ens33</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#修改</span><br><span class="line">BOOTPROTO=&quot;static&quot;</span><br><span class="line">ONBOOT=&quot;yes&quot;</span><br><span class="line"></span><br><span class="line"># 新增</span><br><span class="line">IPADDR=&quot;192.168.80.129&quot;</span><br><span class="line">GATEWAY=&quot;192.168.80.2&quot;</span><br><span class="line">NETMASK=&quot;255.255.255.0&quot;</span><br><span class="line">DNS1=&quot;8.8.8.8&quot;</span><br><span class="line">DNS2=&quot;114.114.114.114&quot;</span><br><span class="line">IPV6_PRIVACY=&quot;no&quot;</span><br></pre></td></tr></table></figure><ul><li>重启网卡服务</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service network restart</span><br></pre></td></tr></table></figure><ul><li>关闭防火墙，并禁止防火墙开机自启</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br></pre></td></tr></table></figure><ul><li>关闭selinux</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/selinux/config</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># This file controls the state of SELinux on the system.</span><br><span class="line"># SELINUX= can take one of these three values:</span><br><span class="line">#     enforcing - SELinux security policy is enforced.</span><br><span class="line">#     permissive - SELinux prints warnings instead of enforcing.</span><br><span class="line">#     disabled - No SELinux policy is loaded.</span><br><span class="line">SELINUX=disabled</span><br><span class="line"># SELINUXTYPE= can take one of three values:</span><br><span class="line">#     targeted - Targeted processes are protected,</span><br><span class="line">#     minimum - Modification of targeted policy. Only selected processes are protected. </span><br><span class="line">#     mls - Multi Level Security protection.</span><br><span class="line">SELINUXTYPE=targeted</span><br></pre></td></tr></table></figure><ul><li>重启master</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reboot</span><br></pre></td></tr></table></figure><p>重启完成后将相关软件包（jdk、hadoop、spark等）导入至/usr/local目录下并解压</p><ul><li>解压jdk</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf jdk-8u211-linux-x64.tar.gz -C /usr/local</span><br></pre></td></tr></table></figure><p>配置jdk环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#set java environment </span><br><span class="line">JAVA_HOME=/usr/local/jdk1.8.0_211</span><br><span class="line">CLASSPATH=.:$JAVA_HOME/lib </span><br><span class="line">PATH=$JAVA_HOME/bin:$PATH </span><br><span class="line">export JAVA_HOME CLASSPATH PATH</span><br></pre></td></tr></table></figure><p>重启环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><p>检查安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">java version &quot;1.8.0_211&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_211-b12)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.211-b12, mixed mode)</span><br></pre></td></tr></table></figure><h4 id="配置linux集群"><a href="#配置linux集群" class="headerlink" title="配置linux集群"></a>配置linux集群</h4><p>将master完整克隆两台node1、node2机器，配置静态ip</p><p>配置主机名，分别执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hostname</span><br></pre></td></tr></table></figure><p>将三台机器的主机名设置为master、node1、node2</p><p>配置三台虚拟机的域名映射</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hosts</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"></span><br><span class="line">192.168.80.129 master</span><br><span class="line">192.168.80.130 node1</span><br><span class="line">192.168.80.131 node2</span><br></pre></td></tr></table></figure><p>此时三台机器已经可以互相ping通</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ping master</span><br><span class="line">ping node1</span><br><span class="line">ping node2</span><br></pre></td></tr></table></figure><p>分别重启三台机器</p><p>生成三台机器的公钥和私钥，分别执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure><p>反复回车，在/root/.ssh隐藏目录下生成私钥id_rsa和公钥id_rsa.pub</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Generating public/private rsa key pair.</span><br><span class="line">Enter file in which to save the key (/root/.ssh/id_rsa): </span><br><span class="line">Created directory &apos;/root/.ssh&apos;.</span><br><span class="line">Enter passphrase (empty for no passphrase): </span><br><span class="line">Enter same passphrase again: </span><br><span class="line">Your identification has been saved in /root/.ssh/id_rsa.</span><br><span class="line">Your public key has been saved in /root/.ssh/id_rsa.pub.</span><br><span class="line">The key fingerprint is:</span><br><span class="line">SHA256:/Vd3sEsHhz3CcxGwbRp5lj6+srZ7OpwAvyKS+qoTi9Q root@master</span><br><span class="line">The key&apos;s randomart image is:</span><br><span class="line">+---[RSA 2048]----+</span><br><span class="line">|             ..o.|</span><br><span class="line">|            . +oo|</span><br><span class="line">|             B+Oo|</span><br><span class="line">|         o    @=.|</span><br><span class="line">|  .     S +  .oo=|</span><br><span class="line">| o E       + ..++|</span><br><span class="line">|o o   .     = +. |</span><br><span class="line">|.o   o . . . B ..|</span><br><span class="line">| .oo+.. . . .=O. |</span><br><span class="line">+----[SHA256]-----+</span><br></pre></td></tr></table></figure><p>在三台虚拟机执行命令将公钥拷贝到master</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id master</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">The authenticity of host &apos;master (192.168.80.129)&apos; can&apos;t be established.</span><br><span class="line">ECDSA key fingerprint is SHA256:9UGNQgh5SWXh/1Z9iWTOzBSbqXf8kjbxc5SC73j9ct4.</span><br><span class="line">ECDSA key fingerprint is MD5:ee:b1:5d:3c:a5:2b:2e:08:cd:85:44:68:fe:c7:29:d9.</span><br><span class="line">Are you sure you want to continue connecting (yes/no)? yes</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys</span><br><span class="line">root@master&apos;s password: </span><br><span class="line"></span><br><span class="line">Number of key(s) added: 1</span><br><span class="line"></span><br><span class="line">Now try logging into the machine, with:   &quot;ssh &apos;master&apos;&quot;</span><br><span class="line">and check to make sure that only the key(s) you wanted were added.</span><br></pre></td></tr></table></figure><p>将master的公钥拷贝到node上</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp /root/.ssh/authorized_keys node1:/root/.ssh</span><br><span class="line">scp /root/.ssh/authorized_keys node2:/root/.ssh</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">The authenticity of host &apos;node1 (192.168.80.130)&apos; can&apos;t be established.</span><br><span class="line">ECDSA key fingerprint is SHA256:9UGNQgh5SWXh/1Z9iWTOzBSbqXf8kjbxc5SC73j9ct4.</span><br><span class="line">ECDSA key fingerprint is MD5:ee:b1:5d:3c:a5:2b:2e:08:cd:85:44:68:fe:c7:29:d9.</span><br><span class="line">Are you sure you want to continue connecting (yes/no)? yes</span><br><span class="line">Warning: Permanently added &apos;node1,192.168.80.130&apos; (ECDSA) to the list of known hosts.</span><br><span class="line">root@node1&apos;s password: </span><br><span class="line">authorized_keys                                      100% 1177     1.2MB/s   00:00</span><br></pre></td></tr></table></figure><p>测试ssh免密登录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh node1</span><br><span class="line">ssh node2</span><br><span class="line">exit</span><br></pre></td></tr></table></figure><h4 id="安装hadoop"><a href="#安装hadoop" class="headerlink" title="安装hadoop"></a>安装hadoop</h4><p>解压安装包并重命名</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hadoop-3.3.1.tar.gz -C /usr/local</span><br><span class="line">mv hadoop-3.3.1 hadoop</span><br></pre></td></tr></table></figure><p>修改配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hadoop/etc/hadoop</span><br></pre></td></tr></table></figure><p>hadoop-env.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim hadoop-env.sh</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_211</span><br><span class="line">export HDFS_NAMENODE_USER=root</span><br><span class="line">export HDFS_DATANODE_USER=root</span><br><span class="line">export HDFS_SECONDARYNAMENODE_USER=root</span><br><span class="line">export YARN_RESOURCEMANAGER_USER=root</span><br><span class="line">export YARN_NODEMANAGER_USER=root</span><br></pre></td></tr></table></figure><p>core-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim core-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://master:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 设置Hadoop本地保存数据路径 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/data/hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 设置HDFS web UI用户身份 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 整合hive 用户代理设置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>hdfs-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim hdfs-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置SNN进程运行机器位置信息 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>node1:9868<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>mapred-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim mapred-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置MR程序默认运行模式： yarn集群模式 local本地模式 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- MR程序历史服务器端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 历史服务器web端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.app.mapreduce.am.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>yarn-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim yarn-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置YARN集群主角色运行机器位置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 是否将对容器实施物理内存限制 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 是否将对容器实施虚拟内存限制。 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 开启日志聚集 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 设置yarn历史服务器地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log.server.url<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>http://master:19888/jobhistory/logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 保存的时间7天 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>workers</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim workers</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">master</span><br><span class="line">node1</span><br><span class="line">node2</span><br></pre></td></tr></table></figure><p>slaves</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim workers</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">master</span><br><span class="line">node1</span><br><span class="line">node2</span><br></pre></td></tr></table></figure><p>将安装包分发至其他机器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local</span><br><span class="line">scp -r hadoop root@node1:/usr/local</span><br><span class="line">scp -r hadoop root@node2:/usr/local</span><br></pre></td></tr></table></figure><p>配置hadoop环境变量，分发至其他机器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># set hadoop env</span><br><span class="line">export HADOOP_HOME=/usr/local/hadoop</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp /etc/profile node1:/etc/</span><br><span class="line">scp /etc/profile node2:/etc/</span><br></pre></td></tr></table></figure><p>在三台机器分别重启环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><p>首次启动，先格式化namenode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin hdfs namenode -format</span><br></pre></td></tr></table></figure><p>启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br><span class="line">start-yarn.sh</span><br><span class="line">mapred --daemon start historyserver</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">start-all.sh</span><br><span class="line">mapred --daemon start historyserver</span><br></pre></td></tr></table></figure><p>查看任务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">61488 Jps</span><br><span class="line">60417 NameNode</span><br><span class="line">60597 DataNode</span><br><span class="line">60981 ResourceManager</span><br><span class="line">61133 NodeManager</span><br><span class="line">61567 JobHistoryServer</span><br></pre></td></tr></table></figure><p>查看web页面（hadoop3.0版本以后web端口跟改为9870）</p><p><code>master:9870</code></p><p><img src="/2022/12/01/Spark基础/A.png" alt></p><p>关闭任务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stop-dfs.sh</span><br><span class="line">stop-yarn.sh</span><br><span class="line">mapred --daemon stop historyserver</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">stop-all.sh</span><br><span class="line">mapred --daemon stop historyserver</span><br></pre></td></tr></table></figure><h3 id="Anaconda3安装"><a href="#Anaconda3安装" class="headerlink" title="Anaconda3安装"></a>Anaconda3安装</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh ./Anaconda3-2021.05-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><p>配置清华源</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.condarc</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">channels:</span><br><span class="line">  - defaults</span><br><span class="line">show_channel_urls: true</span><br><span class="line">default_channels:</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2</span><br><span class="line">custom_channels:</span><br><span class="line">  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br></pre></td></tr></table></figure><p>创建pyspark虚拟环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n pyspark python=3.8</span><br></pre></td></tr></table></figure><h3 id="Spark-local模式搭建"><a href="#Spark-local模式搭建" class="headerlink" title="Spark local模式搭建"></a>Spark local模式搭建</h3><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>解压spark安装包并重命名</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /usr/local</span><br><span class="line">mv spark-3.2.0-bin-hadoop3.2 spark</span><br></pre></td></tr></table></figure><p>配置环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_HOME=/usr/local/spark</span><br><span class="line">export PYSPARK_PYTHON=/usr/local/anaconda3/envs/pyspark/bin/python3.8</span><br><span class="line">export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><p><font color="gold">注</font>：查找pyspark虚拟环境位置：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/anaconda3/envs/pyspark/bin</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bashrc</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_211</span><br><span class="line">export PYSPARK_PYTHON=/usr/local/anaconda3/envs/pyspark/bin/python3.8</span><br></pre></td></tr></table></figure><h4 id="启动pyspark交互式解释器"><a href="#启动pyspark交互式解释器" class="headerlink" title="启动pyspark交互式解释器"></a>启动pyspark交互式解释器</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/pyspark</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Python 3.8.8 (default, Apr 13 2021, 19:58:26) </span><br><span class="line">[GCC 7.3.0] :: Anaconda, Inc. on linux</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line">Using Spark&apos;s default log4j profile: org/apache/spark/log4j-defaults.properties</span><br><span class="line">Setting default log level to &quot;WARN&quot;.</span><br><span class="line">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</span><br><span class="line">22/12/02 17:38:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  &apos;_/</span><br><span class="line">   /__ / .__/\_,_/_/ /_/\_\   version 3.2.0</span><br><span class="line">      /_/</span><br><span class="line"></span><br><span class="line">Using Python version 3.8.8 (default, Apr 13 2021 19:58:26)</span><br><span class="line">Spark context Web UI available at http://master:4040</span><br><span class="line">Spark context available as &apos;sc&apos; (master = local[*], app id = local-1669973937905).</span><br><span class="line">SparkSession available as &apos;spark&apos;.</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure><p>进入浏览器任务页面<code>master:4040</code>，可以查看信息</p><p><img src="/2022/12/01/Spark基础/B.png" alt></p><p>执行一条pyspark指令后：</p><p><img src="/2022/12/01/Spark基础/C.png" alt></p><h3 id="Spark-StandAlone模式搭建"><a href="#Spark-StandAlone模式搭建" class="headerlink" title="Spark StandAlone模式搭建"></a>Spark StandAlone模式搭建</h3><h4 id="StandAlone"><a href="#StandAlone" class="headerlink" title="StandAlone"></a>StandAlone</h4><p>StandAlone模式是Spark自带的集群模式，Master角色以Master进程形式存在，Worker角色以Worker进程形式存在。其中Driver角色运行在Master进程内，Executor角色运行在Worker进程内。此外，还可以开启第三个进程：历史服务器（HistoryServer），用于保存Spark app运行后的事件日志。</p><h4 id="环境分发"><a href="#环境分发" class="headerlink" title="环境分发"></a>环境分发</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp Anaconda3-2021.05-Linux-x86_64.sh node1:`pwd`/</span><br><span class="line">scp Anaconda3-2021.05-Linux-x86_64.sh node2:`pwd`/</span><br></pre></td></tr></table></figure><p>进入node1、node2安装anaconda，同master：配置conda源，创建虚拟环境</p><p>将master:/etc/profile和bashrc中的环境变量复制到node1、2中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scp /etc/profile node1:/etc/profile</span><br><span class="line">scp /etc/profile node2:/etc/profile</span><br><span class="line">scp ~/.bashrc node1:~/</span><br><span class="line">scp ~/.bashrc node2:~/</span><br></pre></td></tr></table></figure><h4 id="创建hadoop用户（仅有root用户可跳过）"><a href="#创建hadoop用户（仅有root用户可跳过）" class="headerlink" title="创建hadoop用户（仅有root用户可跳过）"></a>创建hadoop用户（仅有root用户可跳过）</h4><p>hadoop用户拥有yarn的最高权限</p><p>新建用户</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo adduser hadoop</span><br><span class="line">passwd hadoop</span><br></pre></td></tr></table></figure><p>添加用户组</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo usermod -a -G hadoop hadoop</span><br></pre></td></tr></table></figure><p>赋予root权限</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sudoers</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">## Allow root to run any commands anywhere </span><br><span class="line">root    ALL=(ALL)       ALL</span><br><span class="line">hadoop    ALL=(ALL)       ALL</span><br></pre></td></tr></table></figure><p>添加权限</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/</span><br><span class="line">chown -R hadoop:hadoop hadoop*</span><br><span class="line">chown -R hadoop:hadoop spark*</span><br></pre></td></tr></table></figure><h4 id="配置spark配置文件"><a href="#配置spark配置文件" class="headerlink" title="配置spark配置文件"></a>配置spark配置文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">su - hadoop</span><br><span class="line">cd /usr/local/spark/conf</span><br><span class="line">mv workers.template workers</span><br><span class="line">vim workers</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">master</span><br><span class="line">node1</span><br><span class="line">node2</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mv spark-env.sh.template spark-env.sh</span><br><span class="line">vim spark-env.sh</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 设置JAVA安装目录</span></span></span><br><span class="line">JAVA_HOME=/usr/local/jdk1.8.0_211</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群</span></span></span><br><span class="line">HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop</span><br><span class="line">YARN_CONF_DIR=/usr/local/hadoop/etc/hadoop</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 指定spark master的IP和提交任务的通信端口</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 告知spark的master运行在哪个机器上</span></span><br><span class="line">export SPARK_MASTER_HOST=master</span><br><span class="line"><span class="meta">#</span><span class="bash"> 告知sparkmaster的通讯端口</span></span><br><span class="line">export SPARK_MASTER_PORT=7077</span><br><span class="line"><span class="meta">#</span><span class="bash"> 告知spark master的 webui端口</span></span><br><span class="line">SPARK_MASTER_WEBUI_PORT=8080</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> worker cpu可用核数</span></span><br><span class="line">SPARK_WORKER_CORES=1</span><br><span class="line"><span class="meta">#</span><span class="bash"> worker可用内存</span></span><br><span class="line">SPARK_WORKER_MEMORY=1g</span><br><span class="line"><span class="meta">#</span><span class="bash"> worker的工作通讯地址</span></span><br><span class="line">SPARK_WORKER_PORT=7078</span><br><span class="line"><span class="meta">#</span><span class="bash"> worker的 webui地址</span></span><br><span class="line">SPARK_WORKER_WEBUI_PORT=8081</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 设置历史服务器</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置的意思是  将spark程序运行的历史日志 存到hdfs的/sparklog文件夹中</span></span><br><span class="line">SPARK_HISTORY_OPTS="-Dspark.history.fs.logDirectory=hdfs://master:8020/sparklog/ -Dspark.history.fs.cleaner.enabled=true"</span><br></pre></td></tr></table></figure><p>启动hadoop</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -ls /</span><br></pre></td></tr></table></figure><p>此时没有sparklog文件。创建sparklog文件,赋予权限</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir /sparklog</span><br><span class="line">hadoop fs -chmod 777 /sparklog</span><br></pre></td></tr></table></figure><p>继续配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mv spark-defaults.conf.template spark-defaults.conf</span><br><span class="line">vim spark-defaults.conf</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 开启spark的日期记录功能</span><br><span class="line">spark.eventLog.enabled true</span><br><span class="line"># 设置spark日志记录的路径</span><br><span class="line">spark.eventLog.dir hdfs://master:8020/sparklog/ </span><br><span class="line"># 设置spark日志是否启动压缩</span><br><span class="line">spark.eventLog.compress true</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mv log4j.properties.template log4j.properties</span><br><span class="line">vim log4j.properties</span><br></pre></td></tr></table></figure><p>将INFO改为WARN，减少冗余日志</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Set everything to be logged to the console</span><br><span class="line">log4j.rootCategory=WARN, console</span><br></pre></td></tr></table></figure><h4 id="分发spark配置文件"><a href="#分发spark配置文件" class="headerlink" title="分发spark配置文件"></a>分发spark配置文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local</span><br><span class="line">scp -r spark node1:`pwd`/</span><br><span class="line">scp -r spark node2:`pwd`/</span><br></pre></td></tr></table></figure><h3 id="启动spark集群"><a href="#启动spark集群" class="headerlink" title="启动spark集群"></a>启动spark集群</h3><p>启动历史服务器进程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd spark</span><br><span class="line">sbin/start-history-server.sh</span><br><span class="line">jps</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">49297 JobHistoryServer</span><br><span class="line">49778 HistoryServer</span><br><span class="line">48296 DataNode</span><br><span class="line">49835 Jps</span><br><span class="line">48734 ResourceManager</span><br><span class="line">48910 NodeManager</span><br><span class="line">48127 NameNode</span><br></pre></td></tr></table></figure><p>启动集群</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-all.sh</span><br><span class="line">jps</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">49297 JobHistoryServer</span><br><span class="line">50162 Jps</span><br><span class="line">50020 Master</span><br><span class="line">48296 DataNode</span><br><span class="line">50107 Worker</span><br><span class="line">48734 ResourceManager</span><br><span class="line">48910 NodeManager</span><br><span class="line">48127 NameNode</span><br><span class="line">49903 HistoryServer</span><br></pre></td></tr></table></figure><p>进入<code>master:8080</code>web端口可以看到spark集群界面</p><p><img src="/2022/12/01/Spark基础/D.png" alt></p><h4 id="StandAlone集群测试"><a href="#StandAlone集群测试" class="headerlink" title="StandAlone集群测试"></a>StandAlone集群测试</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/spark/bin</span><br><span class="line">./pyspark --master spark://master:7077</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Python 3.8.15 (default, Nov 24 2022, 15:19:38) </span><br><span class="line">[GCC 11.2.0] :: Anaconda, Inc. on linux</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line">22/12/05 11:54:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  &apos;_/</span><br><span class="line">   /__ / .__/\_,_/_/ /_/\_\   version 3.2.0</span><br><span class="line">      /_/</span><br><span class="line"></span><br><span class="line">Using Python version 3.8.15 (default, Nov 24 2022 15:19:38)</span><br><span class="line">Spark context Web UI available at http://master:4040</span><br><span class="line">Spark context available as &apos;sc&apos; (master = spark://master:7077, app id = app-20221205115438-0000).</span><br><span class="line">SparkSession available as &apos;spark&apos;.</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure><p><img src="/2022/12/01/Spark基础/E.png" alt></p><p>在/usr/local下创建一个words.txt文件，写入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop spark flink</span><br><span class="line">hadoop spark flink hadoop hadoop</span><br><span class="line">hadoop spark flink hadoop hadoop spark spark</span><br></pre></td></tr></table></figure><p>创建input文件夹，将文件上传至hdfs</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir /input/</span><br><span class="line">hdfs dfs -put /usr/local/words.txt /input/</span><br><span class="line">hadoop fs -ls /input</span><br><span class="line">hadoop fs -cat /input/words.txt</span><br></pre></td></tr></table></figure><p>执行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sc.textFile(<span class="string">"hdfs://master:8020/input/words.txt"</span>).flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">" "</span>)).map(<span class="keyword">lambda</span> x:(x,<span class="number">1</span>)).reduceByKey(<span class="keyword">lambda</span> a,b:a+b).collect()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(&apos;hadoop&apos;, 7), (&apos;spark&apos;, 5), (&apos;flink&apos;, 3)]</span><br></pre></td></tr></table></figure><p><img src="/2022/12/01/Spark基础/F.png" alt></p><p><img src="/2022/12/01/Spark基础/G.png" alt></p><p><img src="/2022/12/01/Spark基础/H.png" alt></p><h2 id="Spark-on-YARN"><a href="#Spark-on-YARN" class="headerlink" title="Spark on YARN"></a>Spark on YARN</h2><ul><li><p>将spark部署到yarn集群中可以提高对资源的利用率，无需部署spark集群，只需要一台充当spark客户端的服务器即可提交任务到yarn集群运行</p></li><li><p>Master角色由yarn的ResourceManager担任</p></li><li>Worker角色由yarn的NodeManager担任</li><li>Driver角色运行在yarn容器内或提交任务的客户端进程中</li><li>Executor运行在yarn提供的容器内</li></ul><p>让spark计算任务运行在yarn容器内部，资源管理交友yarn的ResourceManager和NodeManager代替</p><h3 id="启动Spark-on-YARN"><a href="#启动Spark-on-YARN" class="headerlink" title="启动Spark on YARN"></a>启动Spark on YARN</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/spark</span><br><span class="line">./sbin/stop-all.sh #关闭standalone集群</span><br><span class="line">bin/pyspark --master yarn</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Python 3.8.15 (default, Nov 24 2022, 15:19:38) </span><br><span class="line">[GCC 11.2.0] :: Anaconda, Inc. on linux</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line">22/12/05 15:27:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">22/12/05 15:27:49 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.</span><br><span class="line"> Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  &apos;_/</span><br><span class="line">   /__ / .__/\_,_/_/ /_/\_\   version 3.2.0</span><br><span class="line">      /_/</span><br><span class="line"></span><br><span class="line">Using Python version 3.8.15 (default, Nov 24 2022 15:19:38)</span><br><span class="line">Spark context Web UI available at http://master:4040</span><br><span class="line">Spark context available as &apos;sc&apos; (master = yarn, app id = application_1670210110128_0001).</span><br><span class="line">SparkSession available as &apos;spark&apos;.</span><br><span class="line">&gt;&gt;&gt; sc.parallelize([1,2,3,4,5]).map(lambda x:x*10).collect()</span><br><span class="line">[10, 20, 30, 40, 50]                                                            </span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure><p>执行程序测试，或通过spark客户端spark-submit提交代码，spark‘算法会运行在容器中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit --master yarn /usr/local/spark/examples/src/main/python/pi.py 100</span><br></pre></td></tr></table></figure><p><img src="/2022/12/01/Spark基础/I.png" alt></p><p><img src="/2022/12/01/Spark基础/J.png" alt></p><h3 id="Spark-on-YARN部署模式"><a href="#Spark-on-YARN部署模式" class="headerlink" title="Spark on YARN部署模式"></a>Spark on YARN部署模式</h3><ul><li><p>Cluster（集群模式）</p><p>Driver运行在yarn容器内部，和ApplicationMaster在同一个容器内</p></li><li><p>Client（客户端模式）</p><p>Driver运行在客户端进程中，例如Driver运行在spark-submit客户端的进程中</p></li></ul><p>其中集群模式在容器内进行通讯，效率高，但是日志同样存放于容器内部</p><h4 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --master yarn --deploy-mode client /usr/local/spark/examples/src/main/python/pi.py 100</span><br></pre></td></tr></table></figure><h4 id="Cluster"><a href="#Cluster" class="headerlink" title="Cluster"></a>Cluster</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --master yarn --deploy-mode cluster /usr/local/spark/examples/src/main/python/pi.py 100</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;font color=&quot;gold&quot;&gt;定义&lt;/font&gt;：&lt;font color=&quot;gold&quot;&gt;Apache Spark是用于大规模数据处理的统一分析引擎。&lt;/font&gt;其核心数据结构：弹性分布式数据集（RDD）能够在大规模集群中做内存运算，且具有一定容错方式。&lt;/p&gt;
&lt;h2 id=&quot;Spark框架&quot;&gt;&lt;a href=&quot;#Spark框架&quot; class=&quot;headerlink&quot; title=&quot;Spark框架&quot;&gt;&lt;/a&gt;Spark框架&lt;/h2&gt;&lt;h3 id=&quot;组成&quot;&gt;&lt;a href=&quot;#组成&quot; class=&quot;headerlink&quot; title=&quot;组成&quot;&gt;&lt;/a&gt;组成&lt;/h3&gt;&lt;p&gt;&lt;font color=&quot;orange&quot;&gt;&lt;strong&gt;· Spark Core&lt;/strong&gt;&lt;/font&gt;：以RDD为数据抽象，提供Python、Java、Scala、R语言的API和Spark的核心功能，是Spark运行的基础；&lt;/p&gt;
&lt;p&gt;&lt;font color=&quot;orange&quot;&gt;&lt;strong&gt;· SparkSQL&lt;/strong&gt;&lt;/font&gt;：基于SparkCore，提供机构化数据处理模块，支持以SQL语言对数据进行处理；同时可以作为StructuredStreaming模块的基础，进行数据流式计算；&lt;/p&gt;
&lt;p&gt;&lt;font color=&quot;orange&quot;&gt;&lt;strong&gt;· SparkStreaming&lt;/strong&gt;&lt;/font&gt;：基于SparkCore，提供数据流式计算；&lt;/p&gt;
&lt;p&gt;&lt;font color=&quot;orange&quot;&gt;&lt;strong&gt;· MLlib&lt;/strong&gt;&lt;/font&gt;：基于SparkCore，内置大量机器学习库和算法API，进行机器学习计算；&lt;/p&gt;
&lt;p&gt;&lt;font color=&quot;orange&quot;&gt;&lt;strong&gt;· GraphX&lt;/strong&gt;&lt;/font&gt;：基于SparkCore，提供了大量图计算API，用于分布式图计算&lt;/p&gt;
&lt;h3 id=&quot;运行模式&quot;&gt;&lt;a href=&quot;#运行模式&quot; class=&quot;headerlink&quot; title=&quot;运行模式&quot;&gt;&lt;/a&gt;运行模式&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;· 本地模式（单机/local）&lt;/strong&gt;：以一个独立进程，通过内部的多线程模拟Spark运行环境&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;· Standalone模式（集群）&lt;/strong&gt;：Spark的各个角色以独立进程形式存在，组成集群环境&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;· Hadoop YARN模式（集群）&lt;/strong&gt;：Spark的各个角色运行在YARN容器内部，组成集群环境&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;· Kubernetes模式（容器集群）&lt;/strong&gt;：Spark的各个角色运行在Kubernetes容器内部，组成集群环境&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;· 云服务模式&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;角色&quot;&gt;&lt;a href=&quot;#角色&quot; class=&quot;headerlink&quot; title=&quot;角色&quot;&gt;&lt;/a&gt;角色&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;资源&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;· Master角色&lt;/strong&gt;：集群资源管理&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;· Worker角色&lt;/strong&gt;：单机资源管理(所在服务器资源管理)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;任务&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;· Driver角色&lt;/strong&gt;：单个任务管理&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;· Executor角色&lt;/strong&gt;：单个任务计算&lt;/p&gt;</summary>
    
    
    
    <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
    
    <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Nebula-全文检索</title>
    <link href="http://yoursite.com/2022/11/25/Nebula-%E5%85%A8%E6%96%87%E6%A3%80%E7%B4%A2/"/>
    <id>http://yoursite.com/2022/11/25/Nebula-%E5%85%A8%E6%96%87%E6%A3%80%E7%B4%A2/</id>
    <published>2022-11-25T06:17:41.000Z</published>
    <updated>2022-11-25T08:35:41.947Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><h2 id="连接ES"><a href="#连接ES" class="headerlink" title="连接ES"></a>连接ES</h2><h3 id="添加Nebula全文索引模板文件"><a href="#添加Nebula全文索引模板文件" class="headerlink" title="添加Nebula全文索引模板文件"></a>添加Nebula全文索引模板文件</h3><p><code>http://localhost:5601/app/dev_tools#/console</code></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">curl -H "Content-Type: application/json; charset=utf-8" -XPUT http://localhost:9200/_template/nebula_index_template -d '</span><br><span class="line">&#123;</span><br><span class="line"> <span class="attr">"template"</span>: <span class="string">"nebula*"</span>,</span><br><span class="line">  <span class="attr">"settings"</span>: &#123;</span><br><span class="line">    <span class="attr">"index"</span>: &#123;</span><br><span class="line">      <span class="attr">"number_of_shards"</span>: <span class="number">3</span>,</span><br><span class="line">      <span class="attr">"number_of_replicas"</span>: <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"mappings"</span>: &#123;</span><br><span class="line">    <span class="attr">"properties"</span> : &#123;</span><br><span class="line">            <span class="attr">"tag_id"</span> : &#123; <span class="attr">"type"</span> : <span class="string">"long"</span> &#125;,</span><br><span class="line">            <span class="attr">"column_id"</span> : &#123; <span class="attr">"type"</span> : <span class="string">"text"</span> &#125;,</span><br><span class="line">            <span class="attr">"value"</span> :&#123; <span class="attr">"type"</span> : <span class="string">"keyword"</span>&#125;</span><br><span class="line">        &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;'</span><br></pre></td></tr></table></figure><h3 id="登录ES客户端"><a href="#登录ES客户端" class="headerlink" title="登录ES客户端"></a>登录ES客户端</h3><p><code>SIGN IN TEXT SERVICE (localhost:9200,&quot;admin&quot;,&quot;123456&quot;)</code></p><h3 id="查看ES客户端详情"><a href="#查看ES客户端详情" class="headerlink" title="查看ES客户端详情"></a>查看ES客户端详情</h3><p><code>SHOW TEXT SEARCH CLIENTS;</code></p><h3 id="退出ES客户端"><a href="#退出ES客户端" class="headerlink" title="退出ES客户端"></a>退出ES客户端</h3><p><code>SIGN OUT TEXT SERVICE</code></p><h2 id="配置Nebula"><a href="#配置Nebula" class="headerlink" title="配置Nebula"></a>配置Nebula</h2><h3 id="安装storage服务"><a href="#安装storage服务" class="headerlink" title="安装storage服务"></a>安装storage服务</h3><p>进入/usr/local/nebula根目录下，进入/etc子目录，找到nebula-storaged-listener.conf.production文件，复制一份并去除.production后缀。将文件中的listenr地址改为真实地址。</p><p>随后启动listener：<code>./bin/nebula-storaged --flagfile etc/nebula-storaged-listener.conf</code></p><h3 id="添加listener"><a href="#添加listener" class="headerlink" title="添加listener"></a>添加listener</h3><p><code>ADD LISTENER ELASTICSEARCH 192.168.100.1:9789,192.168.100.2:9789;</code>(<font color="gold">注：如果有多台图库集群，都要配置</font>)</p><p>进入nebula-console，执行<code>SHOW LISTENER</code>可以查看图空间的所有listener。</p><p>要删除所有listener，执行：<code>REMOVE LISTENER ELASTICSEARCH</code>(<font color="gold">注：一个图空间仅可执行一次</font>)</p><a id="more"></a><h2 id="刷表"><a href="#刷表" class="headerlink" title="刷表"></a>刷表</h2><h3 id="创建图空间"><a href="#创建图空间" class="headerlink" title="创建图空间"></a>创建图空间</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">SPACE</span> nebula_graph (partition_num = <span class="number">20</span>, replica_factor = <span class="number">2</span>, vid_type = FIXED_STRING(<span class="number">128</span>))</span><br></pre></td></tr></table></figure><h3 id="Tag"><a href="#Tag" class="headerlink" title="Tag"></a>Tag</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> TAG <span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> player(<span class="keyword">name</span> <span class="keyword">string</span>, age <span class="keyword">string</span>);</span><br><span class="line"><span class="keyword">CREATE</span> TAG <span class="keyword">INDEX</span> <span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> <span class="keyword">name</span> <span class="keyword">ON</span> player(<span class="keyword">name</span>(<span class="number">20</span>),age(<span class="number">10</span>));</span><br><span class="line"><span class="keyword">CREATE</span> FULLTEXT TAG <span class="keyword">INDEX</span> nebula_index_1 <span class="keyword">ON</span> player(<span class="keyword">name</span>,age);</span><br></pre></td></tr></table></figure><h3 id="Edge-Type"><a href="#Edge-Type" class="headerlink" title="Edge Type"></a>Edge Type</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> EDGE <span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> edge_player(created_time <span class="built_in">TIMESTAMP</span> <span class="literal">NULL</span> <span class="keyword">DEFAULT</span> <span class="keyword">now</span>());</span><br></pre></td></tr></table></figure><h3 id="全文索引"><a href="#全文索引" class="headerlink" title="全文索引"></a>全文索引</h3><p><font color="gold">注：</font>1.要将图库索引同步到es中去，需要在创建索引时将索引以nebula为开头命名。</p><p>2.执行<code>REBUILD FULLTEXT INDEX</code>将索引同步到es，此时es中还未完成索引的创建，需要在图库录入数据后自动同步到es（可能存在延迟，时间不定）</p><p>3.仅可使用<code>LOOKUP</code>语句进行模糊检索</p><p>4.删除图空间，全文索引不会自动删除，需要手动执行：<code>DROP FULLTEXT INDEX &lt;nebula_index*&gt;</code></p><h3 id="查看全文索引"><a href="#查看全文索引" class="headerlink" title="查看全文索引"></a>查看全文索引</h3><p><code>SHOW FULLTEXT INDEXES</code></p><h2 id="数据测试"><a href="#数据测试" class="headerlink" title="数据测试"></a>数据测试</h2><h3 id="插入数据"><a href="#插入数据" class="headerlink" title="插入数据"></a>插入数据</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> VERTEX player(<span class="keyword">name</span>, age) <span class="keyword">VALUES</span> \</span><br><span class="line">  <span class="string">"Russell Westbrook"</span>: (<span class="string">"Russell Westbrook"</span>, <span class="number">30</span>), \</span><br><span class="line">  <span class="string">"Chris Paul"</span>: (<span class="string">"Chris Paul"</span>, <span class="number">33</span>),\</span><br><span class="line">  <span class="string">"Boris Diaw"</span>: (<span class="string">"Boris Diaw"</span>, <span class="number">36</span>),\</span><br><span class="line">  <span class="string">"David West"</span>: (<span class="string">"David West"</span>, <span class="number">38</span>),\</span><br><span class="line">  <span class="string">"Danny Green"</span>: (<span class="string">"Danny Green"</span>, <span class="number">31</span>),\</span><br><span class="line">  <span class="string">"Tim Duncan"</span>: (<span class="string">"Tim Duncan"</span>, <span class="number">42</span>),\</span><br><span class="line">  <span class="string">"James Harden"</span>: (<span class="string">"James Harden"</span>, <span class="number">29</span>),\</span><br><span class="line">  <span class="string">"Tony Parker"</span>: (<span class="string">"Tony Parker"</span>, <span class="number">36</span>),\</span><br><span class="line">  <span class="string">"Aron Baynes"</span>: (<span class="string">"Aron Baynes"</span>, <span class="number">32</span>),\</span><br><span class="line">  <span class="string">"Ben Simmons"</span>: (<span class="string">"Ben Simmons"</span>, <span class="number">22</span>),\</span><br><span class="line">  <span class="string">"Blake Griffin"</span>: (<span class="string">"Blake Griffin"</span>, <span class="number">30</span>);</span><br></pre></td></tr></table></figure><h3 id="模糊检索"><a href="#模糊检索" class="headerlink" title="模糊检索"></a>模糊检索</h3><p><code>LOOKUP ON player WHERE PREFIX(player.name, &quot;B&quot;);</code></p><p><code>LOOKUP ON player WHERE WILDCARD(player.name, &quot;*ri*&quot;) YIELD player.name, player.age;</code></p><p><code>LOOKUP ON player WHERE WILDCARD(player.name, &quot;*ri*&quot;) | YIELD count(*);</code></p><p><code>LOOKUP ON player WHERE REGEXP(player.name, &quot;R.*&quot;) YIELD player.name, player.age;</code></p><p><code>LOOKUP ON player WHERE REGEXP(player.name, &quot;.*&quot;);</code></p><p><code>LOOKUP ON player WHERE FUZZY(player.name, &quot;Tim Dunncan&quot;, AUTO, OR, 100) YIELD player.name;</code></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;连接ES&quot;&gt;&lt;a href=&quot;#连接ES&quot; class=&quot;headerlink&quot; title=&quot;连接ES&quot;&gt;&lt;/a&gt;连接ES&lt;/h2&gt;&lt;h3 id=&quot;添加Nebula全文索引模板文件&quot;&gt;&lt;a href=&quot;#添加Nebula全文索引模板文件&quot; class=&quot;headerlink&quot; title=&quot;添加Nebula全文索引模板文件&quot;&gt;&lt;/a&gt;添加Nebula全文索引模板文件&lt;/h3&gt;&lt;p&gt;&lt;code&gt;http://localhost:5601/app/dev_tools#/console&lt;/code&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;curl -H &quot;Content-Type: application/json; charset=utf-8&quot; -XPUT http://localhost:9200/_template/nebula_index_template -d &#39;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt; &lt;span class=&quot;attr&quot;&gt;&quot;template&quot;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&quot;nebula*&quot;&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;&quot;settings&quot;&lt;/span&gt;: &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;attr&quot;&gt;&quot;index&quot;&lt;/span&gt;: &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;attr&quot;&gt;&quot;number_of_shards&quot;&lt;/span&gt;: &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;attr&quot;&gt;&quot;number_of_replicas&quot;&lt;/span&gt;: &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &amp;#125;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;&quot;mappings&quot;&lt;/span&gt;: &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;attr&quot;&gt;&quot;properties&quot;&lt;/span&gt; : &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;attr&quot;&gt;&quot;tag_id&quot;&lt;/span&gt; : &amp;#123; &lt;span class=&quot;attr&quot;&gt;&quot;type&quot;&lt;/span&gt; : &lt;span class=&quot;string&quot;&gt;&quot;long&quot;&lt;/span&gt; &amp;#125;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;attr&quot;&gt;&quot;column_id&quot;&lt;/span&gt; : &amp;#123; &lt;span class=&quot;attr&quot;&gt;&quot;type&quot;&lt;/span&gt; : &lt;span class=&quot;string&quot;&gt;&quot;text&quot;&lt;/span&gt; &amp;#125;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;attr&quot;&gt;&quot;value&quot;&lt;/span&gt; :&amp;#123; &lt;span class=&quot;attr&quot;&gt;&quot;type&quot;&lt;/span&gt; : &lt;span class=&quot;string&quot;&gt;&quot;keyword&quot;&lt;/span&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&#39;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;登录ES客户端&quot;&gt;&lt;a href=&quot;#登录ES客户端&quot; class=&quot;headerlink&quot; title=&quot;登录ES客户端&quot;&gt;&lt;/a&gt;登录ES客户端&lt;/h3&gt;&lt;p&gt;&lt;code&gt;SIGN IN TEXT SERVICE (localhost:9200,&amp;quot;admin&amp;quot;,&amp;quot;123456&amp;quot;)&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&quot;查看ES客户端详情&quot;&gt;&lt;a href=&quot;#查看ES客户端详情&quot; class=&quot;headerlink&quot; title=&quot;查看ES客户端详情&quot;&gt;&lt;/a&gt;查看ES客户端详情&lt;/h3&gt;&lt;p&gt;&lt;code&gt;SHOW TEXT SEARCH CLIENTS;&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&quot;退出ES客户端&quot;&gt;&lt;a href=&quot;#退出ES客户端&quot; class=&quot;headerlink&quot; title=&quot;退出ES客户端&quot;&gt;&lt;/a&gt;退出ES客户端&lt;/h3&gt;&lt;p&gt;&lt;code&gt;SIGN OUT TEXT SERVICE&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&quot;配置Nebula&quot;&gt;&lt;a href=&quot;#配置Nebula&quot; class=&quot;headerlink&quot; title=&quot;配置Nebula&quot;&gt;&lt;/a&gt;配置Nebula&lt;/h2&gt;&lt;h3 id=&quot;安装storage服务&quot;&gt;&lt;a href=&quot;#安装storage服务&quot; class=&quot;headerlink&quot; title=&quot;安装storage服务&quot;&gt;&lt;/a&gt;安装storage服务&lt;/h3&gt;&lt;p&gt;进入/usr/local/nebula根目录下，进入/etc子目录，找到nebula-storaged-listener.conf.production文件，复制一份并去除.production后缀。将文件中的listenr地址改为真实地址。&lt;/p&gt;
&lt;p&gt;随后启动listener：&lt;code&gt;./bin/nebula-storaged --flagfile etc/nebula-storaged-listener.conf&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&quot;添加listener&quot;&gt;&lt;a href=&quot;#添加listener&quot; class=&quot;headerlink&quot; title=&quot;添加listener&quot;&gt;&lt;/a&gt;添加listener&lt;/h3&gt;&lt;p&gt;&lt;code&gt;ADD LISTENER ELASTICSEARCH 192.168.100.1:9789,192.168.100.2:9789;&lt;/code&gt;(&lt;font color=&quot;gold&quot;&gt;注：如果有多台图库集群，都要配置&lt;/font&gt;)&lt;/p&gt;
&lt;p&gt;进入nebula-console，执行&lt;code&gt;SHOW LISTENER&lt;/code&gt;可以查看图空间的所有listener。&lt;/p&gt;
&lt;p&gt;要删除所有listener，执行：&lt;code&gt;REMOVE LISTENER ELASTICSEARCH&lt;/code&gt;(&lt;font color=&quot;gold&quot;&gt;注：一个图空间仅可执行一次&lt;/font&gt;)&lt;/p&gt;</summary>
    
    
    
    <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
    
    <category term="-database -nebula" scheme="http://yoursite.com/tags/database-nebula/"/>
    
  </entry>
  
  <entry>
    <title>Docker</title>
    <link href="http://yoursite.com/2022/09/16/Docker/"/>
    <id>http://yoursite.com/2022/09/16/Docker/</id>
    <published>2022-09-16T07:15:39.000Z</published>
    <updated>2023-02-14T03:51:35.127Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><h2 id="CentOS-Docker安装"><a href="#CentOS-Docker安装" class="headerlink" title="CentOS Docker安装"></a>CentOS Docker安装</h2><p>CentOS7要求64位，且内核版本高于3.10</p><p>查看CentOS内核版本：<code>uname -r</code></p><p><strong><font size="4" color="orange">·</font></strong> 安装Docker：卸载旧版本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> sudo yum remove docker \</span><br><span class="line">                  docker-client \</span><br><span class="line">                  docker-client-latest \</span><br><span class="line">                  docker-common \</span><br><span class="line">                  docker-latest \</span><br><span class="line">                  docker-latest-logrotate \</span><br><span class="line">                  docker-logrotate \</span><br><span class="line">                  docker-engine</span><br></pre></td></tr></table></figure><p><strong><font size="4" color="orange">·</font></strong> 安装所需软件包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> sudo yum install -y yum-utils \</span><br><span class="line">  device-mapper-persistent-data \</span><br><span class="line">  lvm2</span><br></pre></td></tr></table></figure><p><strong><font size="4" color="orange">·</font></strong> 设置Docker仓库（清华源）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> sudo yum-config-manager \</span><br><span class="line">    --add-repo \</span><br><span class="line">    https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/centos/docker-ce.repo</span><br></pre></td></tr></table></figure><p><strong><font size="4" color="orange">·</font></strong> 安装Docker Engine-Community</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> sudo yum install docker-ce docker-ce-cli containerd.io</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="启动Docker"><a href="#启动Docker" class="headerlink" title="启动Docker"></a>启动Docker</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> sudo systemctl start docker</span><br></pre></td></tr></table></figure><p>通过运行自带的hello-world镜像来验证是否正确安装了Docker</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> sudo docker run hello-world</span><br></pre></td></tr></table></figure><h3 id="运行应用程序"><a href="#运行应用程序" class="headerlink" title="运行应用程序"></a>运行应用程序</h3><p>使用<code>docker run</code>命令在容器内运行一个脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker run centos /bin/echo "hello docker"</span><br></pre></td></tr></table></figure><p><img src="/2022/09/16/Docker/A.png" alt></p><p>其中centos为指定要运行的镜像。Docker 会先从本地主机上查找镜像是否存在，如果不存在，则到Docker Hub镜像仓库下载公共镜像</p><h3 id="运行交互式容器"><a href="#运行交互式容器" class="headerlink" title="运行交互式容器"></a>运行交互式容器</h3><p>使用docker命令的两个参数 <font color="orange">-i</font> <font color="orange">-t</font> 实现对运行容器的交互</p><p><strong><font size="4" color="orange">·</font></strong>  <font color="orange">-t</font>：在新的容器内指定一个伪终端或终端</p><p><strong><font size="4" color="orange">·</font></strong>  <font color="orange">-i</font>：允许对容器内的标准输入（STDIN）进行交互</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker run -i -t centos /bin/bash</span><br></pre></td></tr></table></figure><p><img src="/2022/09/16/Docker/B.png" alt></p><p>运行 <code>exit</code> 命令或者使用键盘CTRL+D退出该容器</p><h3 id="后台模式运行容器"><a href="#后台模式运行容器" class="headerlink" title="后台模式运行容器"></a>后台模式运行容器</h3><p>使用下列命令创建一个以进程方式运行的容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker run -d centos /bin/sh -c "while true; do echo hello world; sleep 1; done"</span><br></pre></td></tr></table></figure><p><img src="/2022/09/16/Docker/C.png" alt></p><p>执行后并没有输出”hello world”，而是输出了一串字符（容器ID），可以通过该容器ID来查看对应的容器里执行了什么</p><p>执行<code>docker ps</code>命令查看运行的容器，其中：</p><p><strong>·</strong> <font color="orange">CONTAINER ID</font> ：容器ID</p><p><strong>·</strong> <font color="orange">IMAGE</font>：使用的镜像</p><p><strong>·</strong> <font color="orange">COMMAND</font>：启动容器时执行的命令</p><p><strong>·</strong> <font color="orange">CREATED</font>：容器创建时间</p><p><strong>·</strong> <font color="orange">STATUS</font>：容器状态（<font color="#008080">created</font>：已创建；<font color="#008080">restarting</font>：重启中；<font color="#008080">running/Up</font>：运行中；<font color="#008080">removing</font>：迁移中；<font color="#008080">paused</font>：暂停中；<font color="#008080">exited</font>：已停止；<font color="#008080">dead</font>：死亡）</p><p><strong>·</strong> <font color="orange">PORTS</font>：端口信息和使用的连接类型（tcp\udp）</p><p><strong>·</strong> <font color="orange">NAMES</font>：自动分配的容器名称</p><p><img src="/2022/09/16/Docker/D.png" alt></p><p>要查看容器内部的标准输出，执行<code>docker logs &lt;CONTAINER ID&gt;</code></p><p><img src="/2022/09/16/Docker/E.png" alt></p><h3 id="停止容器"><a href="#停止容器" class="headerlink" title="停止容器"></a>停止容器</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker stop &lt;CONTAINER ID&gt;</span><br></pre></td></tr></table></figure><h2 id="Docker-容器"><a href="#Docker-容器" class="headerlink" title="Docker 容器"></a>Docker 容器</h2><h3 id="获取镜像"><a href="#获取镜像" class="headerlink" title="获取镜像"></a>获取镜像</h3><p>假设本地没有ubuntu镜像，使用<code>docker pull</code>命令来载入一个ubuntu镜像</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker pull ubuntu</span><br></pre></td></tr></table></figure><h3 id="启动容器"><a href="#启动容器" class="headerlink" title="启动容器"></a>启动容器</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker run -it ubuntu /bin/bash</span><br></pre></td></tr></table></figure><h3 id="查看全部容器"><a href="#查看全部容器" class="headerlink" title="查看全部容器"></a>查看全部容器</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker ps -a</span><br></pre></td></tr></table></figure><h3 id="启动停止运行的容器"><a href="#启动停止运行的容器" class="headerlink" title="启动停止运行的容器"></a>启动停止运行的容器</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker ps -a # 获取容器ID</span><br><span class="line"><span class="meta">$</span> docker start &lt;CONTAINER ID&gt;</span><br></pre></td></tr></table></figure><h3 id="后台运行容器"><a href="#后台运行容器" class="headerlink" title="后台运行容器"></a>后台运行容器</h3><p><code>-d</code>可以指定容器的运行模式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker run -itd --name ubuntu-test ubuntu /bin/bash</span><br></pre></td></tr></table></figure><h3 id="停止容器-1"><a href="#停止容器-1" class="headerlink" title="停止容器"></a>停止容器</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker stop &lt;CONTAINER ID&gt;</span><br></pre></td></tr></table></figure><h3 id="重启容器"><a href="#重启容器" class="headerlink" title="重启容器"></a>重启容器</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker restart &lt;CONTAINER ID&gt;</span><br></pre></td></tr></table></figure><h3 id="进入容器"><a href="#进入容器" class="headerlink" title="进入容器"></a>进入容器</h3><p>使用了<code>-d</code>参数后，容器进入后台运行，此时需要进入容器，需要指令<code>docker attach</code>或<code>docker exec</code></p><p>其中<code>docker attach</code>命令下如果容器退出，则容器会停止，而<code>docker exec</code>命令会退出容器终端，但不会导致容器的停止</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker attach &lt;CONTAINER ID&gt;</span><br><span class="line"><span class="meta">$</span> docker exec -it &lt;CONTAINER ID&gt; /bin/bash</span><br></pre></td></tr></table></figure><h3 id="导出与导入容器"><a href="#导出与导入容器" class="headerlink" title="导出与导入容器"></a>导出与导入容器</h3><p>导出容器：导出本地某个容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker export &lt;CONTAINER ID&gt; &gt; centos.tar</span><br></pre></td></tr></table></figure><p>导入容器：将容器快照文件导入到镜像</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> cat docker/centos.tar | docker import - test/centos:v1</span><br></pre></td></tr></table></figure><p>或通过指定URL/目录来导入</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker import http://example.com/exampleimage.tgz example/imagerepo</span><br></pre></td></tr></table></figure><h3 id="删除容器"><a href="#删除容器" class="headerlink" title="删除容器"></a>删除容器</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker rm -f &lt;CONTAINER ID&gt;</span><br></pre></td></tr></table></figure><p>清理所有处于终止状态的容器：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker container prune</span><br></pre></td></tr></table></figure><h2 id="Docker镜像"><a href="#Docker镜像" class="headerlink" title="Docker镜像"></a>Docker镜像</h2><p>运行容器时，使用的镜像如果在本地中不存在，docker会自动从docker镜像仓库中下载。默认Docker Hub公共镜像源</p><h3 id="获取Docker镜像"><a href="#获取Docker镜像" class="headerlink" title="获取Docker镜像"></a>获取Docker镜像</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker images</span><br></pre></td></tr></table></figure><p>其中：</p><p><strong>·</strong> <font color="orange">REPOSITORY</font> ：镜像仓库源</p><p><strong>·</strong> <font color="orange">TAG</font>：镜像标签</p><p><strong>·</strong> <font color="orange">IMAGE ID</font>：镜像ID</p><p><strong>·</strong> <font color="orange">CREATED</font>：镜像创建时间</p><p><strong>·</strong> <font color="orange">SIZE</font>：镜像大小</p><p><img src="/2022/09/16/Docker/K.png" alt></p><p>同一仓库源可以有多个 TAG，代表这个仓库源的不同个版本，如python仓库源里，有3.6.7、3.9.0两个不同的版本了。使用 &lt;REPOSITORY> : &lt;TAG> 来定义不同的镜像</p><p>例：使用3.9.0版本的pythob来运行容器，执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker run -it python:3.9.0 /usr/bin/env python</span><br></pre></td></tr></table></figure><p>镜像后的/usr/bin/env python是具体的命令，这里的作用是启动交互式python IDLE。如果不指定镜像的版本标签，docker将默认使用python:latest镜像</p><p><img src="/2022/09/16/Docker/L.png" alt></p><h3 id="查找镜像"><a href="#查找镜像" class="headerlink" title="查找镜像"></a>查找镜像</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker search centos</span><br></pre></td></tr></table></figure><p>其中：</p><p><strong>·</strong> <font color="orange">NAME</font> ：镜像仓库源名称</p><p><strong>·</strong> <font color="orange">DESCRIPTION</font>：镜像描述</p><p><strong>·</strong> <font color="orange">STARS</font>：点赞数</p><p><strong>·</strong> <font color="orange">OFFICIAL</font>：是否为Docker官方发布镜像</p><p><strong>·</strong> <font color="orange">AUTOMATED</font>：自动构建</p><h3 id="预加载镜像"><a href="#预加载镜像" class="headerlink" title="预加载镜像"></a>预加载镜像</h3><p>当在本地主机当中使用不存在的镜像时Docker将会自动下载镜像。如果要预加载镜像，可以使用<code>docker pull</code>命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker pull centos:7</span><br></pre></td></tr></table></figure><p>下载完成后，就可以直接使用这个镜像运行容器</p><h3 id="删除镜像"><a href="#删除镜像" class="headerlink" title="删除镜像"></a>删除镜像</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker rmi &lt;IMAGE ID&gt;</span><br></pre></td></tr></table></figure><h3 id="创建自己的镜像"><a href="#创建自己的镜像" class="headerlink" title="创建自己的镜像"></a>创建自己的镜像</h3><h4 id="·-从已创建容器当中更新镜像"><a href="#·-从已创建容器当中更新镜像" class="headerlink" title="· 从已创建容器当中更新镜像"></a><strong>·</strong> <font color="orange">从已创建容器当中更新镜像</font></h4><p>通过镜像进入一个容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker run -it centos:7 bin/bash</span><br></pre></td></tr></table></figure><p>例：安装vim</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> yum install vim</span><br></pre></td></tr></table></figure><p><img src="/2022/09/16/Docker/M.png" alt></p><p>执行<code>exit</code>命令退出容器，接着使用<code>docker commit</code>命令提交容器副本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker commit -m="add vim" -a="9.9" &lt;CONTAINER ID&gt; centos7:vim</span><br></pre></td></tr></table></figure><p>其中：</p><p><strong>·</strong> <font color="orange">-m</font> ：镜像描述信息</p><p><strong>·</strong> <font color="orange">-a</font>：镜像作者</p><p><strong>·</strong> <font color="orange">CONTAINER ID</font>：容器ID</p><p><strong>·</strong> <font color="orange">centos7</font>：镜像名称</p><p><strong>·</strong> <font color="orange">vim</font>：镜像标签</p><p>启动安装vim的镜像容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker run -it centos:vim /bin/bash</span><br></pre></td></tr></table></figure><h4 id="·-使用Dockerfile构建镜像"><a href="#·-使用Dockerfile构建镜像" class="headerlink" title="· 使用Dockerfile构建镜像"></a><strong>·</strong> <font color="orange">使用Dockerfile构建镜像</font></h4><p>使用<code>docker build</code>命令，构建一个全新的镜像。需要在自定目录下创建Dockerfile文件，并添加构建指令</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> python:<span class="number">3.9</span>.<span class="number">0</span></span><br><span class="line"><span class="keyword">ADD</span> fastapi /</span><br><span class="line"><span class="keyword">WORKDIR</span> /</span><br><span class="line"><span class="keyword">RUN</span> /usr/local/bin/python -m pip install --upgrade pip -i https://pypi.tuna.tsinghua.edu.cn/simple --trusted-host pypi.tuna.tsinghua.edu.cn</span><br><span class="line"><span class="keyword">RUN</span> pip install fastapi[all] -i https://pypi.tuna.tsinghua.edu.cn/simple --trusted-host pypi.tuna.tsinghua.edu.cn</span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="number">8089</span></span><br><span class="line"><span class="keyword">CMD</span> ["python","main.py"]</span><br></pre></td></tr></table></figure><p><img src="/2022/09/16/Docker/N.png" alt></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker images</span><br></pre></td></tr></table></figure><p><img src="/2022/09/16/Docker/O.png" alt></p><p>使用构建的镜像创建容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker run -it -p 8089:8089 fastapi:v1.0</span><br></pre></td></tr></table></figure><h3 id="设置镜像标签"><a href="#设置镜像标签" class="headerlink" title="设置镜像标签"></a>设置镜像标签</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> dcoker tag &lt;IMAGE ID&gt; &lt;REPOSITORY&gt;:&lt;TAG&gt;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> dcoker tag 8720FB78CF34 fastapi:v1.1</span><br></pre></td></tr></table></figure><p>为已有标签的镜像添加标签，该镜像将共用两个标签而不会被替换</p><h2 id="运行应用示例"><a href="#运行应用示例" class="headerlink" title="运行应用示例"></a>运行应用示例</h2><h3 id="启动应用"><a href="#启动应用" class="headerlink" title="启动应用"></a>启动应用</h3><p>载入并运行镜像，其中<code>-P</code>命令使容器内部使用的网络端口随机映射到主机上</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker pull training/webapp</span><br><span class="line"><span class="meta">$</span> docker run -d -P training/webapp python app.py</span><br></pre></td></tr></table></figure><p><img src="/2022/09/16/Docker/F.png" alt></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker ps</span><br></pre></td></tr></table></figure><p>可以通过-p参数来设置端口</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker run -d -p 5000:5000 training/webapp python app.py</span><br></pre></td></tr></table></figure><p><img src="/2022/09/16/Docker/G.png" alt></p><p><img src="/2022/09/16/Docker/I.png" alt></p><p><img src="/2022/09/16/Docker/H.png" alt></p><p>查看容器内端口的指令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker port &lt;CONTAINER ID&gt;</span><br></pre></td></tr></table></figure><p>查看应用程序日志/进程：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker logs -f &lt;CONTAINER ID&gt;</span><br><span class="line"><span class="meta">$</span> docker top &lt;CONTAINER ID&gt;</span><br></pre></td></tr></table></figure><p>查看Docker应用底层信息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker inspect</span><br></pre></td></tr></table></figure><h3 id="停止应用容器"><a href="#停止应用容器" class="headerlink" title="停止应用容器"></a>停止应用容器</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker stop &lt;CONTAINER ID&gt;</span><br></pre></td></tr></table></figure><h3 id="重启应用容器"><a href="#重启应用容器" class="headerlink" title="重启应用容器"></a>重启应用容器</h3><p>对停止的应用：<code>docker start</code></p><p>对运行中的应用：<code>docker restart</code></p><p>查询最后一次创建的容器：<code>docker ps -l</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker start 707a9f61dd9a</span><br><span class="line">docker ps -l</span><br></pre></td></tr></table></figure><h3 id="移除应用容器"><a href="#移除应用容器" class="headerlink" title="移除应用容器"></a>移除应用容器</h3><font color="gold">注</font>：容器必须是停止状态<br><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span>docker rm &lt;CONTAINER ID&gt;</span><br></pre></td></tr></table></figure><br><br><img src="/2022/09/16/Docker/J.png" alt><br><br>### Docker网络端口映射<br><br>以fastapi镜像为例<br><br><strong>·</strong> <font color="orange">-P</font>：随机映射<br><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker run -it -P &lt;REPOSITORY&gt;:&lt;TAG&gt; ...</span><br></pre></td></tr></table></figure><br><br><strong>·</strong> <font color="orange">-p</font>：指定端口映射、指定容器绑定的ip地址<br><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker run -it -p 8089:8089 &lt;REPOSITORY&gt;:&lt;TAG&gt; ...</span><br></pre></td></tr></table></figure><br><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker run -it -p 127.0.0.1:8089:8089 &lt;REPOSITORY&gt;:&lt;TAG&gt; ...</span><br></pre></td></tr></table></figure><br><br>默认都是绑定tcp端口，如要绑定UPD协议端口，需要在端口后加上<font color="#008080">\udp</font><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker run -it -p 127.0.0.1:8089:8089/udp &lt;REPOSITORY&gt;:&lt;TAG&gt; ...</span><br></pre></td></tr></table></figure><p><strong>·</strong> <font color="orange">port</font>：快速查看端口绑定情况</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker port &lt;CONTAINER ID&gt; &lt;PORT&gt;</span><br></pre></td></tr></table></figure><p>指定容器名称、用户：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker run -it -p 127.0.0.1:8089:8089 --name fastapi --user=root &lt;REPOSITORY&gt;:&lt;TAG&gt; ...</span><br></pre></td></tr></table></figure><h2 id="Dockerfile"><a href="#Dockerfile" class="headerlink" title="Dockerfile"></a>Dockerfile</h2><p>Dockerfile是用于构建镜像的文本文件，其中包含了构建镜像时需要用到的各个指令和说明</p><p>Dockerfile的指令每执行一次都会在docker上新建一层镜像，如果执行过多会脏成镜像膨胀过大，例如之前的fastapi镜像</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> python:<span class="number">3.9</span>.<span class="number">0</span></span><br><span class="line"><span class="keyword">ADD</span> fastapi /</span><br><span class="line"><span class="keyword">WORKDIR</span> /</span><br><span class="line"><span class="keyword">RUN</span> /usr/local/bin/python -m pip install --upgrade pip -i https://pypi.tuna.tsinghua.edu.cn/simple --trusted-host pypi.tuna.tsinghua.edu.cn</span><br><span class="line"><span class="keyword">RUN</span> pip install fastapi[all] -i https://pypi.tuna.tsinghua.edu.cn/simple --trusted-host pypi.tuna.tsinghua.edu.cn</span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="number">8089</span></span><br><span class="line"><span class="keyword">CMD</span> ["python","main.py"]</span><br></pre></td></tr></table></figure><p>可以将其修改为：</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> python:<span class="number">3.9</span>.<span class="number">0</span></span><br><span class="line"><span class="keyword">ADD</span> fastapi /</span><br><span class="line"><span class="keyword">WORKDIR</span> /</span><br><span class="line"><span class="keyword">RUN</span> /usr/local/bin/python -m pip install --upgrade pip -i https://pypi.tuna.tsinghua.edu.cn/simple --trusted-host pypi.tuna.tsinghua.edu.cn \</span><br><span class="line">&amp;&amp; pip install fastapi[all] -i https://pypi.tuna.tsinghua.edu.cn/simple --trusted-host pypi.tuna.tsinghua.edu.cn</span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="number">8089</span></span><br><span class="line"><span class="keyword">CMD</span> ["python","main.py"]</span><br></pre></td></tr></table></figure><p>以<code>&amp;&amp;</code>符号连接的命令，在执行后只会构建一层镜像</p><h3 id="构建命令"><a href="#构建命令" class="headerlink" title="构建命令"></a>构建命令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker build -t &lt;REPOSITORY&gt;:&lt;TAG&gt; .</span><br></pre></td></tr></table></figure><p>最后<code>.</code>代表本次执行的上下文路径（打包本机目录下文件的全部内容）</p><h3 id="Dockerfile常用构建指令"><a href="#Dockerfile常用构建指令" class="headerlink" title="Dockerfile常用构建指令"></a>Dockerfile常用构建指令</h3><p><strong><font size="4">·</font> <font size="4" color="orange">COPY</font></strong></p><p>复制指令，从上下文目录中复制文件或者目录到容器里指定路径</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">COPY</span> [--chown=&lt;user&gt;:&lt;group&gt;][&lt;source&gt;,... &lt;target&gt;]</span><br></pre></td></tr></table></figure><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">COPY</span> fastapi /</span><br></pre></td></tr></table></figure><p>其中：</p><p><strong>·</strong> <font color="orange">[–chown=&lt;user&gt;:&lt;group&gt;]</font> ：可选参数，用户改变复制到容器内文件的拥有者和属组</p><p><strong>·</strong> <font color="orange">&lt;source&gt;</font>：源文件或源目录</p><p><strong>·</strong> <font color="orange">&lt;targer&gt;</font>：目标路径，容器内的指定路径，如果不存在则会自动创建</p><p><strong><font size="4">·</font> <font size="4" color="orange">ADD</font></strong></p><p>与COPY使用方法类似，主要用于执行的源文件为tar压缩文件情况下，会自动复制并解压到目标路径中</p><p><strong><font size="4">·</font> <font size="4" color="orange">CMD</font></strong></p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CMD</span> &lt;shell 命令&gt; </span><br><span class="line"><span class="keyword">CMD</span> ["&lt;可执行文件或命令&gt;","&lt;param1&gt;","&lt;param2&gt;",...] </span><br><span class="line"><span class="keyword">CMD</span> ["&lt;param1&gt;","&lt;param2&gt;",...]  # 该写法是为 ENTRYPOINT 指令指定的程序提供默认参数</span><br></pre></td></tr></table></figure><p>运行程序，在执行<code>docker run</code>时执行</p><p><strong><font size="4">·</font> <font size="4" color="orange">ENTRYPOINT</font></strong></p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ENTRYPOINT</span> ["&lt;executeable&gt;","&lt;param1&gt;","&lt;param2&gt;",...]</span><br></pre></td></tr></table></figure><p>执行<code>docker run</code>时接受命令行参数，但在命令行指定了 <font color="orange">–entrypoint</font> 时，将覆盖<font color="#008080">ENTRYPOINT</font>指令指定的程序，且如果存在多个，仅有最后一个生效</p><p><strong><font size="4">·</font> <font size="4" color="orange">WORKDIR</font></strong></p><p>指定工作目录。用<code>WORKDIR</code>指定的工作目录，会在构建镜像的每一层中都存在（<code>WORKDIR</code>指定的工作目录，必须是提前创建好的）</p><p><code>docker build</code>构建镜像过程中的，每一个<code>RUN</code>命令都是新建的一层。只有通过<code>WORKDIR</code>创建的目录才会一直存在</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">WORKDIR</span> &lt;工作目录路径&gt;</span><br></pre></td></tr></table></figure><p><strong><font size="4">·</font> <font size="4" color="orange">EXPOSE</font></strong></p><p>声明端口。在运行时使用随机端口映射时，执行<code>docker run -P</code>，会自动随机映射<font color="#008080">EXPOSE</font>的端口</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">EXPOSE</span> &lt;端口<span class="number">1</span>&gt; [&lt;端口<span class="number">2</span>&gt;...]</span><br></pre></td></tr></table></figure><p><strong><font size="4">·</font> <font size="4" color="orange">ENV</font></strong></p><p>设置环境变量</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ENV</span> &lt;key&gt; &lt;value&gt;</span><br><span class="line"><span class="keyword">ENV</span> &lt;key1&gt;=&lt;value1&gt; &lt;key2&gt;=&lt;value2&gt;...</span><br></pre></td></tr></table></figure><p><strong><font size="4">·</font> <font size="4" color="orange">ARG</font></strong></p><p>构建参数，与 ENV 作用一致。不过作用域不一样。ARG 设置的环境变量仅对 Dockerfile 内有效，也就是说只有 docker build 的过程中有效，构建好的镜像内不存在此环境变量</p><p>构建命令 docker build 中可以用 –build-arg &lt;参数名&gt;=&lt;值&gt; 来覆盖</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ARG</span> &lt;参数名&gt;[=&lt;默认值&gt;]</span><br></pre></td></tr></table></figure><p><strong><font size="4">·</font> <font size="4" color="orange">VOLUME</font></strong></p><p>定义匿名数据卷。在启动容器时忘记挂载数据卷，会自动挂载到匿名卷，用于避免重要的数据因容器重启而丢失、避免容器体积不断变大。在执行<code>docker run</code>时，可以通过<code>-v</code>参数修改挂载点</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">VOLUME</span> ["&lt;路径1&gt;", "&lt;路径2&gt;"...]</span><br><span class="line"><span class="keyword">VOLUME</span> &lt;路径&gt;</span><br></pre></td></tr></table></figure><p><strong><font size="4">·</font> <font size="4" color="orange">USER</font></strong></p><p>指定执行后续命令的用户和用户组，用于切换后续命令执行的用户（用户和用户组必须提前已经存在）</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">USER</span> &lt;用户名&gt;[:&lt;用户组&gt;]</span><br></pre></td></tr></table></figure><p><strong><font size="4">·</font> <font size="4" color="orange">HEALTHCHECK</font></strong></p><p>指定某个程序或者指令来监控 docker 容器服务的运行状态</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">HEALTHCHECK</span> [选项] CMD &lt;命令&gt;：设置检查容器健康状况的命令</span><br><span class="line"><span class="keyword">HEALTHCHECK</span> NONE：如果基础镜像有健康检查指令，使用这行可以屏蔽掉其健康检查指令</span><br><span class="line"><span class="keyword">HEALTHCHECK</span> [选项] CMD &lt;命令&gt; : 这边 CMD 后面跟随的命令使用，可以参考 CMD 的用法。</span><br></pre></td></tr></table></figure><p><strong><font size="4">·</font> <font size="4" color="orange">ONBUILD</font></strong></p><p>用于延迟构建命令的执行。就是 Dockerfile 里使用<code>ONBUILD</code>指定的命令，在本次构建镜像的过程中不会执行（假设镜像为 test-build）。当有新的Dockerfile使用了之前构建的镜像FROM test-build ，这时执行新镜像的Dockerfile构建时候，会执行test-build的Dockerfile里<code>ONBUILD</code>指定的命令</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ONBUILD</span> &lt;其它指令&gt;</span><br></pre></td></tr></table></figure><p><strong><font size="4">·</font> <font size="4" color="orange">LABEL</font></strong></p><p>LABEL 指令以键值对的形式给镜像添加一些元数据（metadata）</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">LABEL</span> &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; ...</span><br></pre></td></tr></table></figure><h2 id="Docker打包"><a href="#Docker打包" class="headerlink" title="Docker打包"></a>Docker打包</h2><h3 id="打包tar"><a href="#打包tar" class="headerlink" title="打包tar"></a>打包tar</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker save -o &lt;filename&gt; &lt;REPOSITORY&gt;:&lt;TAG&gt;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker save -o fastapi.tar fastapi:v1.0</span><br></pre></td></tr></table></figure><h3 id="加载tar"><a href="#加载tar" class="headerlink" title="加载tar"></a>加载tar</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker load &lt; fastapi.tar</span><br></pre></td></tr></table></figure><h2 id="进入容器内部"><a href="#进入容器内部" class="headerlink" title="进入容器内部"></a>进入容器内部</h2><h3 id="编辑容器"><a href="#编辑容器" class="headerlink" title="编辑容器"></a>编辑容器</h3><p>运行一个容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker run -d -P &lt;REPOSITORY&gt;:&lt;TAG&gt;</span><br></pre></td></tr></table></figure><p>进入容器内部</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker exec -it &lt;CONTAINER ID&gt; [bash]/[/bin/sh]/[/bin/bash]</span><br></pre></td></tr></table></figure><p>退出容器内部</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> quit</span><br></pre></td></tr></table></figure><h3 id="复制容器内代码文件"><a href="#复制容器内代码文件" class="headerlink" title="复制容器内代码文件"></a>复制容器内代码文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker cp &lt;CONTAINER ID&gt;:/file/path/within/container /root/path/target</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;CentOS-Docker安装&quot;&gt;&lt;a href=&quot;#CentOS-Docker安装&quot; class=&quot;headerlink&quot; title=&quot;CentOS Docker安装&quot;&gt;&lt;/a&gt;CentOS Docker安装&lt;/h2&gt;&lt;p&gt;CentOS7要求64位，且内核版本高于3.10&lt;/p&gt;
&lt;p&gt;查看CentOS内核版本：&lt;code&gt;uname -r&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;font size=&quot;4&quot; color=&quot;orange&quot;&gt;·&lt;/font&gt;&lt;/strong&gt; 安装Docker：卸载旧版本&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; sudo yum remove docker \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                  docker-client \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                  docker-client-latest \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                  docker-common \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                  docker-latest \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                  docker-latest-logrotate \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                  docker-logrotate \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                  docker-engine&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;&lt;font size=&quot;4&quot; color=&quot;orange&quot;&gt;·&lt;/font&gt;&lt;/strong&gt; 安装所需软件包&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; sudo yum install -y yum-utils \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  device-mapper-persistent-data \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  lvm2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;&lt;font size=&quot;4&quot; color=&quot;orange&quot;&gt;·&lt;/font&gt;&lt;/strong&gt; 设置Docker仓库（清华源）&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; sudo yum-config-manager \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    --add-repo \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/centos/docker-ce.repo&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;&lt;font size=&quot;4&quot; color=&quot;orange&quot;&gt;·&lt;/font&gt;&lt;/strong&gt; 安装Docker Engine-Community&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; sudo yum install docker-ce docker-ce-cli containerd.io&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="Docker" scheme="http://yoursite.com/categories/Docker/"/>
    
    
    <category term="Docker" scheme="http://yoursite.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Nebula-fastapi接口维护文档</title>
    <link href="http://yoursite.com/2022/08/19/Nebula-fastapi%E6%8E%A5%E5%8F%A3%E7%BB%B4%E6%8A%A4%E6%96%87%E6%A1%A3/"/>
    <id>http://yoursite.com/2022/08/19/Nebula-fastapi%E6%8E%A5%E5%8F%A3%E7%BB%B4%E6%8A%A4%E6%96%87%E6%A1%A3/</id>
    <published>2022-08-19T09:40:40.000Z</published>
    <updated>2022-11-25T09:10:52.210Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><h1 id="普通接口"><a href="#普通接口" class="headerlink" title="普通接口"></a>普通接口</h1><h2 id="1-导入fastapi、连接nebula连接池"><a href="#1-导入fastapi、连接nebula连接池" class="headerlink" title="1.导入fastapi、连接nebula连接池"></a>1.导入fastapi、连接nebula连接池</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastapi <span class="keyword">import</span> FastAPI</span><br><span class="line"><span class="keyword">from</span> nebula2.gclient.net <span class="keyword">import</span> ConnectionPool</span><br><span class="line"><span class="keyword">from</span> nebula2.Config <span class="keyword">import</span> Config</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭在线文档，防止攻击</span></span><br><span class="line">app_router = FastAPI(docs_url=<span class="literal">None</span>, redoc_url=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">config = Config()</span><br><span class="line">config.max_connection_pool_size = <span class="number">10</span></span><br><span class="line"><span class="comment"># 连接超时时间</span></span><br><span class="line">config.timeout = <span class="number">60000</span></span><br><span class="line"><span class="comment"># 关闭空闲连接时间</span></span><br><span class="line">config.idle_time = <span class="number">0</span></span><br><span class="line"><span class="comment"># 检查空闲连接时间间隔</span></span><br><span class="line">config.interval_check = <span class="number">-1</span></span><br><span class="line"><span class="comment"># 初始化连接池</span></span><br><span class="line">connection_pool = ConnectionPool()</span><br><span class="line"><span class="comment"># 如果给定的服务器正常，则返回true，否则返回false</span></span><br><span class="line">ok = connection_pool.init([(<span class="string">'host'</span>, <span class="number">9669</span>)], config)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="keyword">import</span> uvicorn</span><br><span class="line">    uvicorn.run(app=<span class="string">"nebula_api:app_router"</span>, reload=<span class="literal">True</span>, debug=<span class="literal">True</span>, host=host, port=port)</span><br></pre></td></tr></table></figure><h2 id="2-CORS跨域访问设置"><a href="#2-CORS跨域访问设置" class="headerlink" title="2.CORS跨域访问设置"></a>2.CORS跨域访问设置</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastapi.middleware.cors <span class="keyword">import</span> CORSMiddleware</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># CORS</span></span><br><span class="line">origins = [</span><br><span class="line">    <span class="string">"*"</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">app_router.add_middleware(</span><br><span class="line">    CORSMiddleware,</span><br><span class="line">    allow_origins=origins,</span><br><span class="line">    allow_credentials=<span class="literal">True</span>,</span><br><span class="line">    allow_methods=[<span class="string">"*"</span>],</span><br><span class="line">    allow_headers=[<span class="string">"*"</span>]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="3-一般查询接口"><a href="#3-一般查询接口" class="headerlink" title="3.一般查询接口"></a>3.一般查询接口</h2><h3 id="1-单点查询"><a href="#1-单点查询" class="headerlink" title="1.单点查询"></a>1.单点查询</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从连接池中获取会话</span></span><br><span class="line">session = connection_pool.get_session(<span class="string">'root'</span>, <span class="string">'nebula'</span>)</span><br><span class="line">session.execute(<span class="string">'USE ai_project'</span>)</span><br><span class="line"></span><br><span class="line">data_final = []</span><br><span class="line">errorcode = <span class="number">0</span></span><br><span class="line">result = &#123;</span><br><span class="line">    <span class="string">"data"</span>: data_final,</span><br><span class="line">    <span class="string">"errorcode"</span>: errorcode</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 举例，查询标签example</span></span><br><span class="line">data_select = session.execute(</span><br><span class="line">    str(<span class="string">f'match (v:example) return id(v),v.name;'</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询出错，错误代码1</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> data_select.is_succeeded():</span><br><span class="line">    session.release()</span><br><span class="line">    result[<span class="string">"errorcode"</span>] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询结果不为空，显示数据</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> data_select.is_empty():</span><br><span class="line">    size = data_select.row_size()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> range(size):</span><br><span class="line">        data1 = data_select.row_values(index)[<span class="number">0</span>].as_string()</span><br><span class="line">        data2 = data_select.row_values(index)[<span class="number">1</span>].as_string()</span><br><span class="line">        data_dict = &#123;</span><br><span class="line">            <span class="string">"entity_id"</span>: data1,</span><br><span class="line">            <span class="string">"name"</span>: data2,</span><br><span class="line">        &#125;</span><br><span class="line">        data_final.append(data_dict)</span><br><span class="line"></span><br><span class="line">    session.release()</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询类型下实体为空，错误代码2</span></span><br><span class="line">session.release()</span><br><span class="line">result[<span class="string">"errorcode"</span>] = <span class="number">2</span></span><br><span class="line"><span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><h3 id="2-树型查询"><a href="#2-树型查询" class="headerlink" title="2.树型查询"></a>2.树型查询</h3><p>主函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自定义类和函数</span></span><br><span class="line"><span class="keyword">from</span> api_class <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">session = connection_pool.get_session(<span class="string">'root'</span>, <span class="string">'nebula'</span>)</span><br><span class="line">session.execute(<span class="string">'USE ai_project'</span>)</span><br><span class="line"></span><br><span class="line">data_final = []</span><br><span class="line">errorcode = <span class="number">0</span></span><br><span class="line">result = &#123;</span><br><span class="line">    <span class="string">"data"</span>: data_final,</span><br><span class="line">    <span class="string">"errorcode"</span>: errorcode</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以example_id为根节点</span></span><br><span class="line">data_select = session.execute(</span><br><span class="line">    str(<span class="string">f'match Ret=(v)-[e:edge_example*0..15]-&gt;(p) where id(v)=="<span class="subst">&#123;example_id&#125;</span>" return nodes(Ret);'</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询出错，错误代码1</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> data_select.is_succeeded():</span><br><span class="line">    session.release()</span><br><span class="line">    result[<span class="string">"errorcode"</span>] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询不为空，显示数据</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> data_select.is_empty():</span><br><span class="line"></span><br><span class="line">    size = data_select.row_size()</span><br><span class="line">    data_list = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> range(size):</span><br><span class="line">        data1 = data_select.row_values(index)[<span class="number">0</span>].as_list()</span><br><span class="line">        item = re.finditer(<span class="string">r'\(\"(.*?)\" :(.*?)\&#123;(.*?)\&#125;\)'</span>, str(data1))</span><br><span class="line"></span><br><span class="line">        parent_id = <span class="string">'0'</span></span><br><span class="line">        <span class="keyword">for</span> match <span class="keyword">in</span> item:</span><br><span class="line">            kv_dict = &#123;</span><br><span class="line">                <span class="string">'parent_id'</span>: parent_id,</span><br><span class="line">                <span class="string">'entity_id'</span>: match.group(<span class="number">1</span>).strip(),</span><br><span class="line">                <span class="string">'entity_type'</span>: match.group(<span class="number">2</span>).strip()</span><br><span class="line">            &#125;</span><br><span class="line">            kvs = match.group(<span class="number">3</span>).replace(<span class="string">" "</span>, <span class="string">""</span>).split(<span class="string">","</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> kv <span class="keyword">in</span> kvs:</span><br><span class="line">                kv_dict[kv.strip().split(<span class="string">':'</span>)[<span class="number">0</span>]] = kv.split(<span class="string">':'</span>)[<span class="number">1</span>][<span class="number">1</span>:<span class="number">-1</span>]</span><br><span class="line">            <span class="keyword">if</span> kv_dict <span class="keyword">not</span> <span class="keyword">in</span> data_list:</span><br><span class="line">                data_list.append(kv_dict)</span><br><span class="line"></span><br><span class="line">            parent_id = match.group(<span class="number">1</span>).strip()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># json对象列表转换为树形json对象</span></span><br><span class="line">    data_final = ApiFuncs.list_to_tree(data_list)</span><br><span class="line">    <span class="comment"># 树形json对象按参数排序</span></span><br><span class="line">    ApiFuncs.data_sort(data_final)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> data_final:</span><br><span class="line">        <span class="keyword">del</span> item[<span class="string">"parent_id"</span>]</span><br><span class="line"></span><br><span class="line">    session.release()</span><br><span class="line">    result[<span class="string">"data"</span>] = data_final</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">session.release()</span><br><span class="line">result[<span class="string">"errorcode"</span>] = <span class="number">2</span></span><br><span class="line"><span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><p>将json对象列表转换为树形json对象的函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ApiFuncs</span>:</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">list_to_tree</span><span class="params">(data)</span>:</span></span><br><span class="line">        root = []</span><br><span class="line">        node = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> d <span class="keyword">in</span> data:</span><br><span class="line">            <span class="keyword">if</span> d.get(<span class="string">"parent_id"</span>) == <span class="string">'0'</span>:</span><br><span class="line">                root.append(d)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                node.append(d)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> root:</span><br><span class="line">            ApiFuncs.add_node(p, node)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> len(root) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> node</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> root</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_node</span><span class="params">(p, node)</span>:</span></span><br><span class="line">        p[<span class="string">"children"</span>] = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> node:</span><br><span class="line">            <span class="keyword">if</span> n.get(<span class="string">"parent_id"</span>) == p.get(<span class="string">"entity_id"</span>):</span><br><span class="line">                p[<span class="string">"children"</span>].append(n)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> p[<span class="string">"children"</span>]:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> t.get(<span class="string">"children"</span>):</span><br><span class="line">                t[<span class="string">"children"</span>] = []</span><br><span class="line"></span><br><span class="line">            t[<span class="string">"children"</span>].append(ApiFuncs.add_node(t, node))</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> t[<span class="string">"children"</span>]:</span><br><span class="line">                <span class="keyword">del</span> t[<span class="string">"children"</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> len(p[<span class="string">"children"</span>]) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br></pre></td></tr></table></figure><p>将树形json对象按参数排序的函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SerialNameError</span><span class="params">(Exception)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, message)</span>:</span></span><br><span class="line">        self.message = message</span><br><span class="line">...</span><br><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_sort</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> elem <span class="keyword">in</span> data:</span><br><span class="line">        <span class="keyword">if</span> elem[<span class="string">"serial"</span>] == <span class="string">"_NULL_"</span>:</span><br><span class="line">            elem[<span class="string">"serial"</span>] = <span class="string">"0"</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        data.sort(key=<span class="keyword">lambda</span> x: int(x[<span class="string">"serial"</span>]))</span><br><span class="line">    <span class="keyword">except</span> ValueError:</span><br><span class="line">        <span class="keyword">raise</span> SerialNameError(<span class="string">"序号似乎无效"</span>)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> data:</span><br><span class="line">        <span class="keyword">if</span> <span class="string">"children"</span> <span class="keyword">in</span> item.keys():</span><br><span class="line">            ApiFuncs.data_sort(item[<span class="string">"children"</span>])</span><br><span class="line">    <span class="keyword">return</span></span><br></pre></td></tr></table></figure><h2 id="4-一般编辑接口"><a href="#4-一般编辑接口" class="headerlink" title="4.一般编辑接口"></a>4.一般编辑接口</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 继承BaseModel类的参数</span></span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"><span class="keyword">from</span> pydantic <span class="keyword">import</span> BaseModel</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SampleEdit</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    example_kv: List[dict]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">session = connection_pool.get_session(<span class="string">'root'</span>, <span class="string">'nebula'</span>)</span><br><span class="line">session.execute(<span class="string">'USE ai_project'</span>)</span><br><span class="line"></span><br><span class="line">result = &#123;</span><br><span class="line">    <span class="string">"msg"</span>: <span class="literal">True</span>,</span><br><span class="line">    <span class="string">"errorcode"</span>: <span class="number">0</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> eg.example_kv:</span><br><span class="line">    <span class="comment"># 新增</span></span><br><span class="line">    <span class="keyword">if</span> item[<span class="string">"type_operation"</span>] == <span class="string">'add'</span>:</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        ==========增==========</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        temp_id = item[<span class="string">"name"</span>] + <span class="string">"_"</span> + ApiFuncs.create_time()</span><br><span class="line">        temp_md5 = ApiFuncs.create_md5(temp_id)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 检索当前编号，创建实体时对编号自增</span></span><br><span class="line">        serial_list = session.execute(</span><br><span class="line">            str(<span class="string">f'match (v:example) return v.seq;'</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        data = []</span><br><span class="line">        size = serial_list.row_size()</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(size):</span><br><span class="line">            <span class="keyword">if</span> serial_list.row_values(index)[<span class="number">0</span>].is_string():</span><br><span class="line">                data1 = serial_list.row_values(index)[<span class="number">0</span>].as_string()</span><br><span class="line">                data.append(int(data1))</span><br><span class="line">        <span class="keyword">if</span> data:</span><br><span class="line">            count = max(data)</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            count = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        session.execute(</span><br><span class="line">            str(<span class="string">f'INSERT VERTEX example ( name,seq ) VALUES "<span class="subst">&#123;temp_md5&#125;</span>":("<span class="subst">&#123;item[<span class="string">"name"</span>]&#125;</span>", "<span class="subst">&#123;count&#125;</span>");'</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 删除</span></span><br><span class="line">    <span class="keyword">elif</span> item[<span class="string">"type_operation"</span>] == <span class="string">'delete'</span>:</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        ==========删==========</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        session.execute(str(<span class="string">f'DELETE VERTEX "<span class="subst">&#123;item[<span class="string">"id"</span>]&#125;</span>";'</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 修改</span></span><br><span class="line">    <span class="keyword">elif</span> item[<span class="string">"type_operation"</span>] == <span class="string">'update'</span>:</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        ==========改==========</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        temp_md5 = item[<span class="string">"id"</span>]</span><br><span class="line"></span><br><span class="line">        session.execute(</span><br><span class="line">            str(<span class="string">f'UPDATE VERTEX ON example "<span class="subst">&#123;temp_md5&#125;</span>" SET name = "<span class="subst">&#123;item[<span class="string">"name"</span>]&#125;</span>";'</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭连接池</span></span><br><span class="line">session.release()</span><br><span class="line"><span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><p>测试接口：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">data = &#123;</span><br><span class="line">        <span class="string">'example_kv'</span>: [</span><br><span class="line">                &#123;<span class="string">"type_operation"</span>: <span class="string">"add"</span>, <span class="string">"name"</span>: <span class="string">"test"</span>&#125;,</span><br><span class="line">            &#123;...&#125;,</span><br><span class="line">            ...</span><br><span class="line">        ],</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">body = json.dumps(data)</span><br><span class="line">response = requests.post(<span class="string">'http://127.0.0.1:8888/example'</span>, data=body)</span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure><h2 id="5-功能函数"><a href="#5-功能函数" class="headerlink" title="5.功能函数"></a>5.功能函数</h2><p>查找树型json结构中关键字</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_name</span><span class="params">(json_data, e)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> element <span class="keyword">in</span> json_data:</span><br><span class="line">        match = re.search(e, element[<span class="string">"name"</span>])</span><br><span class="line">        <span class="keyword">if</span> match:</span><br><span class="line">            id_collect.append(element[<span class="string">"entity_id"</span>])</span><br><span class="line">        <span class="keyword">if</span> <span class="string">"children"</span> <span class="keyword">in</span> element.keys():</span><br><span class="line">            find_name(element[<span class="string">"children"</span>], e)</span><br><span class="line">    <span class="keyword">return</span> id_collect</span><br></pre></td></tr></table></figure><p>分页</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> annotations</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> fastapi <span class="keyword">import</span> FastAPI, Depends</span><br><span class="line"><span class="keyword">from</span> fastapi_pagination <span class="keyword">import</span> paginate, add_pagination</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> TypeVar, Generic, Sequence</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> fastapi <span class="keyword">import</span> Query</span><br><span class="line"><span class="keyword">from</span> fastapi_pagination.bases <span class="keyword">import</span> AbstractPage, AbstractParams, RawParams</span><br><span class="line"><span class="keyword">from</span> pydantic <span class="keyword">import</span> BaseModel</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">T = TypeVar(<span class="string">"T"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Params</span><span class="params">(BaseModel, AbstractParams)</span>:</span></span><br><span class="line">    page: int = Query(<span class="number">1</span>, ge=<span class="number">1</span>, description=<span class="string">"Page number"</span>)</span><br><span class="line">    size: int = Query(<span class="number">17</span>, gt=<span class="number">0</span>, le=<span class="number">100</span>, description=<span class="string">"Page size"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">to_raw_params</span><span class="params">(self)</span> -&gt; RawParams:</span></span><br><span class="line">        <span class="keyword">return</span> RawParams(</span><br><span class="line">            limit=self.size,</span><br><span class="line">            offset=self.size * (self.page - <span class="number">1</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Page</span><span class="params">(AbstractPage[T], Generic[T])</span>:</span></span><br><span class="line">    results: Sequence[T]</span><br><span class="line">    total: int</span><br><span class="line">    page: int</span><br><span class="line">    size: int</span><br><span class="line">    next: str</span><br><span class="line">    previous: str</span><br><span class="line">    total_pages: int</span><br><span class="line"></span><br><span class="line">    __params_type__ = Params</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">            cls,</span></span></span><br><span class="line"><span class="function"><span class="params">            results: results,</span></span></span><br><span class="line"><span class="function"><span class="params">            total: int,</span></span></span><br><span class="line"><span class="function"><span class="params">            params: Params,</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span> -&gt; Page[T]:</span></span><br><span class="line">        page = params.page</span><br><span class="line">        size = params.size</span><br><span class="line">        total_pages = math.ceil(total / params.size)</span><br><span class="line">        next = <span class="string">f"?page=<span class="subst">&#123;page + <span class="number">1</span>&#125;</span>&amp;size=<span class="subst">&#123;size&#125;</span>"</span> <span class="keyword">if</span> (page + <span class="number">1</span>) &lt;= total_pages <span class="keyword">else</span> <span class="string">"null"</span></span><br><span class="line">        previous = <span class="string">f"?page=<span class="subst">&#123;page - <span class="number">1</span>&#125;</span>&amp;size=<span class="subst">&#123;size&#125;</span>"</span> <span class="keyword">if</span> (page - <span class="number">1</span>) &gt;= <span class="number">1</span> <span class="keyword">else</span> <span class="string">"null"</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> cls(results=results, total=total, page=params.page,</span><br><span class="line">                   size=params.size,</span><br><span class="line">                   next=next,</span><br><span class="line">                   previous=previous,</span><br><span class="line">                   total_pages=total_pages)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    sample_id: str</span><br><span class="line">    sample_name: str</span><br><span class="line">    time: str</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sample_data = [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"sample_id"</span>: <span class="string">"000001"</span>,</span><br><span class="line">      <span class="string">"sample_name"</span>: <span class="string">"test1"</span>,</span><br><span class="line">      <span class="string">"time"</span>: <span class="string">"2022-08-11 21:15:33"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"sample_id"</span>: <span class="string">"000002"</span>,</span><br><span class="line">      <span class="string">"sample_name"</span>: <span class="string">"test2"</span>,</span><br><span class="line">      <span class="string">"time"</span>: <span class="string">"2022-08-12 13:45:56"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"sample_id"</span>: <span class="string">"000003"</span>,</span><br><span class="line">      <span class="string">"sample_name"</span>: <span class="string">"test3"</span>,</span><br><span class="line">      <span class="string">"time"</span>: <span class="string">"2022-08-12 13:45:59"</span></span><br><span class="line">    &#125;,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.get('/sample', response_model=Page[User])</span></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">get_users</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> paginate(sample_data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">add_pagination(app)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="keyword">import</span> uvicorn</span><br><span class="line">    uvicorn.run(app=<span class="string">"1:app"</span>, reload=<span class="literal">True</span>, host=<span class="string">'127.0.0.1'</span>, port=<span class="number">9999</span>)</span><br></pre></td></tr></table></figure><h1 id="项目级"><a href="#项目级" class="headerlink" title="项目级"></a>项目级</h1><h3 id="文件目录结构"><a href="#文件目录结构" class="headerlink" title="文件目录结构"></a>文件目录结构</h3><font color="#008080">│  api_class.py</font><br><font color="#008080">│  main.py</font><br><font color="#008080">│</font><br><font color="#008080">├─api</font><br><font color="#008080">│  │  nebuladb.py</font><br><font color="#008080">│  │  proj_1.py</font><br><font color="#008080">│  │  proj_2.py</font><br><font color="#008080">│  │  __init__.py</font><br><font color="#008080">│  │</font><br><font color="#008080">│  └─__pycache__</font><br><font color="#008080">│          nebuladb.cpython-39.pyc</font><br><font color="#008080">│          proj_1cpython-39.pyc</font><br><font color="#008080">│          proj_2.cpython-39.pyc</font><br><font color="#008080">│          __init__.cpython-39.pyc</font><br><font color="#008080">│</font><br><font color="#008080">├─data</font><br><font color="#008080">│      nebula_data.txt</font><br><font color="#008080">│</font><br><font color="#008080">├─logs</font><br><font color="#008080">│      2022-xx-01.txt</font><br><font color="#008080">│      2022-xx-02.txt</font><br><font color="#008080">│      2022-xx-03.txt</font><br><font color="#008080">│      2022-xx-04.txt</font><br><font color="#008080">│</font><br><font color="#008080">├─scripts</font><br><font color="#008080">│      post_1.py</font><br><font color="#008080">│      test_1.py</font><br><font color="#008080">│</font><br><font color="#008080">├─utils</font><br><font color="#008080">│  │  client.py</font><br><font color="#008080">│  │  creat_data.py</font><br><font color="#008080">│  │  log.py</font><br><font color="#008080">│  │  snapshot_day_by_day.py</font><br><font color="#008080">│  │  __init__.py</font><br><font color="#008080">│  │</font><br><font color="#008080">│  └─__pycache__</font><br><font color="#008080">│          log.cpython-39.pyc</font><br><font color="#008080">│          __init__.cpython-39.pyc</font><br><font color="#008080">│</font><br><font color="#008080">└─__pycache__</font><br><font color="#008080">        api_class.cpython-39.pyc</font><br><font color="#008080">        main.cpython-39.pyc</font><h3 id="主函数main-py"><a href="#主函数main-py" class="headerlink" title="主函数main.py"></a>主函数main.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> api <span class="keyword">import</span> main</span><br><span class="line"><span class="keyword">from</span> fastapi_pagination <span class="keyword">import</span> add_pagination</span><br><span class="line"></span><br><span class="line">app_router = main()</span><br><span class="line">add_pagination(app_router)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="keyword">import</span> uvicorn</span><br><span class="line">    uvicorn.run(app=<span class="string">"main:app_router"</span>, reload=<span class="literal">True</span>, debug=<span class="literal">True</span>, host=<span class="string">'0.0.0.0'</span>, port=<span class="number">8888</span>)</span><br></pre></td></tr></table></figure><h3 id="日志utils-log-py"><a href="#日志utils-log-py" class="headerlink" title="日志utils/log.py"></a>日志utils/log.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> logging.handlers</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogInit</span><span class="params">(object)</span>:</span></span><br><span class="line">    __instance = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        _log_dir = os.path.join(os.path.dirname(__file__), <span class="string">'../logs'</span>)</span><br><span class="line">        _log_name = time.strftime(<span class="string">'%Y-%m-%d'</span>, time.localtime(time.time())) + <span class="string">'.txt'</span></span><br><span class="line"></span><br><span class="line">        self.logger = logging.getLogger(_log_name)</span><br><span class="line">        self.logger.setLevel(logging.DEBUG)</span><br><span class="line">        formatter = logging.Formatter(</span><br><span class="line">            <span class="string">'%(asctime)s - %(levelname)s - %(filename)s - %(funcName)s - %(lineno)s - %(message)s'</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.logger.handlers:</span><br><span class="line">            file_log_handler = logging.handlers.RotatingFileHandler(os.path.join(_log_dir, _log_name),</span><br><span class="line">                                                                    maxBytes=<span class="number">10</span> * <span class="number">1024</span> * <span class="number">1024</span>, backupCount=<span class="number">3</span>,</span><br><span class="line">                                                                    encoding=<span class="string">"utf-8"</span>)</span><br><span class="line">            file_log_handler.setLevel(logging.INFO)</span><br><span class="line">            file_log_handler.setFormatter(formatter)</span><br><span class="line">            self.logger.addHandler(file_log_handler)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_logger</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> LogInit.__instance:</span><br><span class="line">            LogInit.__instance = LogInit()</span><br><span class="line">        <span class="keyword">return</span> LogInit.__instance</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">logger = LogInit.set_logger().logger</span><br></pre></td></tr></table></figure><p>使用方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> utils.log <span class="keyword">import</span> logger</span><br><span class="line"></span><br><span class="line">logger.info(<span class="string">"信息"</span>)</span><br><span class="line">logger.warning(<span class="string">"警告"</span>)</span><br><span class="line">logger.error(<span class="string">"错误"</span>)</span><br></pre></td></tr></table></figure><h3 id="数据库配置api-nebuladb-py"><a href="#数据库配置api-nebuladb-py" class="headerlink" title="数据库配置api/nebuladb.py"></a>数据库配置api/nebuladb.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nebula2.Config <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">from</span> nebula2.gclient.net <span class="keyword">import</span> ConnectionPool</span><br><span class="line"></span><br><span class="line"><span class="comment"># NEBULA</span></span><br><span class="line">config = Config()</span><br><span class="line">config.max_connection_pool_size = <span class="number">10</span></span><br><span class="line">config.timeout = <span class="number">60000</span></span><br><span class="line">config.idle_time = <span class="number">0</span></span><br><span class="line">config.interval_check = <span class="number">-1</span></span><br><span class="line">connection_pool = ConnectionPool()</span><br><span class="line">ok = connection_pool.init([(<span class="string">'192.168.80.128'</span>, <span class="number">9669</span>)], config)</span><br></pre></td></tr></table></figure><h3 id="图谱快照保存脚本utils-snapshot-day-by-day-py"><a href="#图谱快照保存脚本utils-snapshot-day-by-day-py" class="headerlink" title="图谱快照保存脚本utils/snapshot_day_by_day.py"></a>图谱快照保存脚本utils/snapshot_day_by_day.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> schedule</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> nebula2.gclient.net <span class="keyword">import</span> ConnectionPool</span><br><span class="line"><span class="keyword">from</span> nebula2.Config <span class="keyword">import</span> Config</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">config = Config()</span><br><span class="line">config.max_connection_pool_size = <span class="number">10</span></span><br><span class="line">config.timeout = <span class="number">60000</span></span><br><span class="line">config.idle_time = <span class="number">0</span></span><br><span class="line">config.interval_check = <span class="number">-1</span></span><br><span class="line">connection_pool = ConnectionPool()</span><br><span class="line">ok = connection_pool.init([(<span class="string">'192.168.80.128'</span>, <span class="number">9669</span>)], config)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_snapshot</span><span class="params">()</span>:</span></span><br><span class="line">    session = connection_pool.get_session(<span class="string">'root'</span>, <span class="string">'nebula'</span>)</span><br><span class="line">    session.execute(<span class="string">'USE ai_project'</span>)</span><br><span class="line">    session.execute(str(<span class="string">'CREATE SNAPSHOT'</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">del_snapshot</span><span class="params">()</span>:</span></span><br><span class="line">    session = connection_pool.get_session(<span class="string">'root'</span>, <span class="string">'nebula'</span>)</span><br><span class="line">    session.execute(<span class="string">'USE ai_project'</span>)</span><br><span class="line">    data = session.execute(str(<span class="string">'SHOW SNAPSHOTS'</span>))</span><br><span class="line">    datas = []</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> data.is_empty():</span><br><span class="line">        size = data.row_size()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(size):</span><br><span class="line">            data1 = data.row_values(index)[<span class="number">0</span>].as_string()</span><br><span class="line">            datas.append(data1)</span><br><span class="line">    <span class="keyword">if</span> len(datas) &gt; <span class="number">3</span>:</span><br><span class="line">        data_final = datas[:<span class="number">-3</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> data_final:</span><br><span class="line">            session.execute(str(<span class="string">f'DROP SNAPSHOT <span class="subst">&#123;i&#125;</span>'</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">schedule.every().day.at(<span class="string">"22:00"</span>).do(add_snapshot)</span><br><span class="line">schedule.every().day.at(<span class="string">"22:00"</span>).do(del_snapshot)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    schedule.run_pending()</span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="api包api-init-py配置"><a href="#api包api-init-py配置" class="headerlink" title="api包api/__init__.py配置"></a>api包api/__init__.py配置</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastapi <span class="keyword">import</span> FastAPI</span><br><span class="line"><span class="keyword">from</span> fastapi.middleware.cors <span class="keyword">import</span> CORSMiddleware</span><br><span class="line"><span class="keyword">from</span> api.stdlib <span class="keyword">import</span> stdlib_router</span><br><span class="line"><span class="keyword">from</span> api.quota <span class="keyword">import</span> quota_router</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    app = FastAPI(docs_url=<span class="literal">None</span>, redoc_url=<span class="literal">None</span>)</span><br><span class="line">    app_cors(app)</span><br><span class="line">    app_stdlib(app)</span><br><span class="line">    app_quota(app)</span><br><span class="line">    app_ocr(app)</span><br><span class="line">    <span class="keyword">return</span> app</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">app_cors</span><span class="params">(app: FastAPI)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    CORS</span></span><br><span class="line"><span class="string">    :param app:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    origins = [</span><br><span class="line">        <span class="string">"*"</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    app.add_middleware(</span><br><span class="line">        CORSMiddleware,</span><br><span class="line">        allow_origins=origins,</span><br><span class="line">        allow_credentials=<span class="literal">True</span>,</span><br><span class="line">        allow_methods=[<span class="string">"*"</span>],</span><br><span class="line">        allow_headers=[<span class="string">"*"</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">app_proj_1</span><span class="params">(app: FastAPI)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    项目1</span></span><br><span class="line"><span class="string">    :param app:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    app.include_router(proj_1_router)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">app_proj_2</span><span class="params">(app: FastAPI)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    项目2</span></span><br><span class="line"><span class="string">    :param app:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    app.include_router(proj_2_router)</span><br></pre></td></tr></table></figure><h3 id="项目接口api-proj-1-2-py"><a href="#项目接口api-proj-1-2-py" class="headerlink" title="项目接口api/proj_1(2).py"></a>项目接口api/proj_1(2).py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastapi <span class="keyword">import</span> APIRouter</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.log <span class="keyword">import</span> logger</span><br><span class="line"><span class="keyword">from</span> api_class <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> .nebuladb <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">proj_1_router = APIRouter(</span><br><span class="line">    prefix=<span class="string">"/api"</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="meta">@stdlib_router.api_route("/project/standard", methods=['GET'])</span></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">get_project_standard</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    获取工程规范</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 从连接池中获取会话</span></span><br><span class="line">    session = connection_pool.get_session(<span class="string">'root'</span>, <span class="string">'nebula'</span>)</span><br><span class="line">    session.execute(<span class="string">'USE ai_project'</span>)</span><br><span class="line"></span><br><span class="line">    data_final = []</span><br><span class="line">    errorcode = <span class="number">0</span></span><br><span class="line">    result = &#123;</span><br><span class="line">        <span class="string">"data"</span>: data_final,</span><br><span class="line">        <span class="string">"errorcode"</span>: errorcode</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># 举例，查询标签example</span></span><br><span class="line">    data_select = session.execute(</span><br><span class="line">        str(<span class="string">f'match (v:example) return id(v),v.name;'</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 查询出错，错误代码1</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> data_select.is_succeeded():</span><br><span class="line">        session.release()</span><br><span class="line">        result[<span class="string">"errorcode"</span>] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 查询结果不为空，显示数据</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> data_select.is_empty():</span><br><span class="line">        size = data_select.row_size()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(size):</span><br><span class="line">            data1 = data_select.row_values(index)[<span class="number">0</span>].as_string()</span><br><span class="line">            data2 = data_select.row_values(index)[<span class="number">1</span>].as_string()</span><br><span class="line">            data_dict = &#123;</span><br><span class="line">                <span class="string">"entity_id"</span>: data1,</span><br><span class="line">                <span class="string">"name"</span>: data2,</span><br><span class="line">            &#125;</span><br><span class="line">            data_final.append(data_dict)</span><br><span class="line"></span><br><span class="line">        session.release()</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 查询类型下实体为空，错误代码2</span></span><br><span class="line">    session.release()</span><br><span class="line">    result[<span class="string">"errorcode"</span>] = <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;普通接口&quot;&gt;&lt;a href=&quot;#普通接口&quot; class=&quot;headerlink&quot; title=&quot;普通接口&quot;&gt;&lt;/a&gt;普通接口&lt;/h1&gt;&lt;h2 id=&quot;1-导入fastapi、连接nebula连接池&quot;&gt;&lt;a href=&quot;#1-导入fastapi、连接nebula连接池&quot; class=&quot;headerlink&quot; title=&quot;1.导入fastapi、连接nebula连接池&quot;&gt;&lt;/a&gt;1.导入fastapi、连接nebula连接池&lt;/h2&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; fastapi &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; FastAPI&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; nebula2.gclient.net &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; ConnectionPool&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; nebula2.Config &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; Config&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 关闭在线文档，防止攻击&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;app_router = FastAPI(docs_url=&lt;span class=&quot;literal&quot;&gt;None&lt;/span&gt;, redoc_url=&lt;span class=&quot;literal&quot;&gt;None&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;config = Config()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;config.max_connection_pool_size = &lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 连接超时时间&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;config.timeout = &lt;span class=&quot;number&quot;&gt;60000&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 关闭空闲连接时间&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;config.idle_time = &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 检查空闲连接时间间隔&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;config.interval_check = &lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 初始化连接池&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;connection_pool = ConnectionPool()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 如果给定的服务器正常，则返回true，否则返回false&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;ok = connection_pool.init([(&lt;span class=&quot;string&quot;&gt;&#39;host&#39;&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;9669&lt;/span&gt;)], config)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; __name__ == &lt;span class=&quot;string&quot;&gt;&quot;__main__&quot;&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; uvicorn&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    uvicorn.run(app=&lt;span class=&quot;string&quot;&gt;&quot;nebula_api:app_router&quot;&lt;/span&gt;, reload=&lt;span class=&quot;literal&quot;&gt;True&lt;/span&gt;, debug=&lt;span class=&quot;literal&quot;&gt;True&lt;/span&gt;, host=host, port=port)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;2-CORS跨域访问设置&quot;&gt;&lt;a href=&quot;#2-CORS跨域访问设置&quot; class=&quot;headerlink&quot; title=&quot;2.CORS跨域访问设置&quot;&gt;&lt;/a&gt;2.CORS跨域访问设置&lt;/h2&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; fastapi.middleware.cors &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; CORSMiddleware&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# CORS&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;origins = [&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;*&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;app_router.add_middleware(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    CORSMiddleware,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    allow_origins=origins,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    allow_credentials=&lt;span class=&quot;literal&quot;&gt;True&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    allow_methods=[&lt;span class=&quot;string&quot;&gt;&quot;*&quot;&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    allow_headers=[&lt;span class=&quot;string&quot;&gt;&quot;*&quot;&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="Web" scheme="http://yoursite.com/categories/Web/"/>
    
    
    <category term="nebula" scheme="http://yoursite.com/tags/nebula/"/>
    
    <category term="fastapi" scheme="http://yoursite.com/tags/fastapi/"/>
    
  </entry>
  
  <entry>
    <title>PaddleOCR表格识别</title>
    <link href="http://yoursite.com/2022/07/21/PaddleOCR%E8%A1%A8%E6%A0%BC%E8%AF%86%E5%88%AB/"/>
    <id>http://yoursite.com/2022/07/21/PaddleOCR%E8%A1%A8%E6%A0%BC%E8%AF%86%E5%88%AB/</id>
    <published>2022-07-21T06:15:22.000Z</published>
    <updated>2022-09-22T03:43:56.275Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p>PaddleOCR2.5根目录下的ppstructure文件模块是PaddleOCR提供的一个可用于复杂文档结构分析处理的OCR工具包<br>github文档页面：<a href="https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.5/ppstructure/README_ch.md" target="_blank" rel="noopener">https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.5/ppstructure/README_ch.md</a></p><h2 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h2><p><strong><font size="4" color="orange">·</font></strong> 安装paddleocr version&gt;=2.5<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install &quot;paddleocr&gt;=2.5&quot;</span><br></pre></td></tr></table></figure></p><p><strong><font size="4" color="orange">·</font></strong> 安装版面分析依赖包layoutparser<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U https://paddleocr.bj.bcebos.com/whl/layoutparser-0.0.0-py3-none-any.whl</span><br></pre></td></tr></table></figure></p><p><strong><font size="4" color="orange">·</font></strong> 安装DocVQA依赖包paddlenlp（DocVQA功能，选装）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install paddlenlp</span><br></pre></td></tr></table></figure></p><h2 id="快速开始"><a href="#快速开始" class="headerlink" title="快速开始"></a>快速开始</h2><p>在PaddleOCR/ppstructure目录下进入CMD命令行，或者创建python脚本启动<br><strong><font size="4" color="orange">·</font></strong> 表格识别</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">paddleocr --image_dir=docs/table/table.jpg --type=structure --layout=false</span><br></pre></td></tr></table></figure><p>python脚本：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">from</span> paddleocr <span class="keyword">import</span> PPStructure,save_structure_res</span><br><span class="line"></span><br><span class="line">table_engine = PPStructure(layout=<span class="literal">False</span>, show_log=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">save_folder = <span class="string">'./output'</span></span><br><span class="line">img_path = <span class="string">'PaddleOCR/ppstructure/docs/table/table.jpg'</span></span><br><span class="line">img = cv2.imread(img_path)</span><br><span class="line">result = table_engine(img)</span><br><span class="line">save_structure_res(result, save_folder, os.path.basename(img_path).split(<span class="string">'.'</span>)[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> result:</span><br><span class="line">    line.pop(<span class="string">'img'</span>)</span><br><span class="line">    print(line)</span><br></pre></td></tr></table></figure><p><strong><font size="4" color="orange">·</font></strong> 版面分析</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">paddleocr --image_dir=docs/table/1.png --type=structure --table=false --ocr=false</span><br></pre></td></tr></table></figure><p>python脚本：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">from</span> paddleocr <span class="keyword">import</span> PPStructure,save_structure_res</span><br><span class="line"></span><br><span class="line">table_engine = PPStructure(table=<span class="literal">False</span>, ocr=<span class="literal">False</span>, show_log=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">save_folder = <span class="string">'./output'</span></span><br><span class="line">img_path = <span class="string">'PaddleOCR/ppstructure/docs/table/1.png'</span></span><br><span class="line">img = cv2.imread(img_path)</span><br><span class="line">result = table_engine(img)</span><br><span class="line">save_structure_res(result, save_folder, os.path.basename(img_path).split(<span class="string">'.'</span>)[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> result:</span><br><span class="line">    line.pop(<span class="string">'img'</span>)</span><br><span class="line">    print(line)</span><br></pre></td></tr></table></figure><p><strong><font size="4" color="orange">·</font></strong> 版面分析+表格识别</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">paddleocr --image_dir=docs/table/1.png --type=structure</span><br></pre></td></tr></table></figure><p>python脚本：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">from</span> paddleocr <span class="keyword">import</span> PPStructure, draw_structure_result, save_structure_res</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">table_engine = PPStructure(show_log=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">save_folder = <span class="string">'./output'</span></span><br><span class="line">img_path = <span class="string">'./ppstructure/docs/table/table.jpg'</span></span><br><span class="line">img = cv2.imread(img_path)</span><br><span class="line">result = table_engine(img)</span><br><span class="line">save_structure_res(result, save_folder, os.path.basename(img_path).split(<span class="string">'.'</span>)[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> result:</span><br><span class="line">    line.pop(<span class="string">'img'</span>)</span><br><span class="line">    print(line)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">font_path = <span class="string">'./doc/fonts/simfang.ttf'</span></span><br><span class="line">image = Image.open(img_path).convert(<span class="string">'RGB'</span>)</span><br><span class="line">im_show = draw_structure_result(image, result, font_path=font_path)</span><br><span class="line">im_show = Image.fromarray(im_show)</span><br><span class="line">im_show.save(<span class="string">'result.jpg'</span>)</span><br></pre></td></tr></table></figure></p><a id="more"></a><p>参数说明</p><table><thead><tr><th>字段</th><th>说明</th><th>默认值</th></tr></thead><tbody><tr><td>output</td><td>excel和识别结果保存的地址</td><td>./output/table</td></tr><tr><td>table_max_len</td><td>表格结构模型预测时，图像的长边resize尺度</td><td>488</td></tr><tr><td>table_model_dir</td><td>表格结构模型 inference 模型地址</td><td>None</td></tr><tr><td>table_char_dict_path</td><td>表格结构模型所用字典地址</td><td>../ppocr/utils/dict/table_structure_dict.txt</td></tr><tr><td>layout_path_model</td><td>版面分析模型模型地址，可以为在线地址或者本地地址，当为本地地址时，需要指定 layout_label_map, 命令行模式下可通过–layout_label_map=’{0: “Text”, 1: “Title”, 2: “List”, 3:”Table”, 4:”Figure”}’ 指定</td><td>lp://PubLayNet/ppyolov2_r50vd_dcn_365e_publaynet/config</td></tr><tr><td>layout_label_map</td><td>版面分析模型模型label映射字典</td><td>None</td></tr><tr><td>model_name_or_path</td><td>VQA SER模型地址</td><td>None</td></tr><tr><td>max_seq_length</td><td>VQA SER模型最大支持token长度</td><td>512</td></tr><tr><td>label_map_path</td><td>VQA SER 标签文件地址</td><td>./vqa/labels/labels_ser.txt</td></tr><tr><td>layout</td><td>前向中是否执行版面分析</td><td>True</td></tr><tr><td>table</td><td>前向中是否执行表格识别</td><td>True</td></tr><tr><td>ocr</td><td>对于版面分析中的非表格区域，是否执行ocr。当layout为False时会被自动设置为False</td><td>True</td></tr><tr><td>structure_version</td><td>表格结构化模型版本，可选 PP-STRUCTURE。PP-STRUCTURE支持表格结构化模型</td><td>pp-structure</td></tr></tbody></table><h2 id="模型下载"><a href="#模型下载" class="headerlink" title="模型下载"></a>模型下载</h2><table><thead><tr><th>模型类型</th><th>模型名称</th><th>模型简介</th><th>下载地址</th></tr></thead><tbody><tr><td>版面分析模型</td><td>ppyolov2_r50vd_dcn_365e_publaynet</td><td>PubLayNet 数据集训练的版面分析模型，可以划分文字、标题、表格、图片以及列表5类区域</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/model/layout-parser/ppyolov2_r50vd_dcn_365e_publaynet.tar" target="_blank" rel="noopener">推理模型</a>/<a href="https://paddle-model-ecology.bj.bcebos.com/model/layout-parser/ppyolov2_r50vd_dcn_365e_publaynet_pretrained.pdparams" target="_blank" rel="noopener">训练模型</a></td></tr><tr><td>OCR模型</td><td>ch_PP-OCRv3_det_infer</td><td>PubLayNet数据集训练的中英文超轻量PP-OCRv3模型</td><td><a href="https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_infer.tar" target="_blank" rel="noopener">推理模型</a>/<a href="https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_distill_train.tar" target="_blank" rel="noopener">训练模型</a></td></tr><tr><td>OCR模型</td><td>en_ppocr_mobile_v2.0_table_rec</td><td>PubLayNet数据集训练的中英文超轻量PP-OCRv3模型</td><td><a href="https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_infer.tar" target="_blank" rel="noopener">推理模型</a>/<a href="https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_train.tar" target="_blank" rel="noopener">训练模型</a></td></tr><tr><td>表格识别模型</td><td>en_ppocr_mobile_v2.0_table_structure</td><td>PubLayNet数据集训练的英文表格场景的表格结构预测</td><td><a href="https://paddleocr.bj.bcebos.com/dygraph_v2.0/table/en_ppocr_mobile_v2.0_table_structure_infer.tar" target="_blank" rel="noopener">推理模型</a>/<a href="https://paddleocr.bj.bcebos.com/dygraph_v2.1/table/en_ppocr_mobile_v2.0_table_structure_train.tar" target="_blank" rel="noopener">训练模型</a></td></tr></tbody></table><h2 id="预测示例（以版面分析-表格为例）"><a href="#预测示例（以版面分析-表格为例）" class="headerlink" title="预测示例（以版面分析+表格为例）"></a>预测示例（以版面分析+表格为例）</h2><h3 id="命令行"><a href="#命令行" class="headerlink" title="命令行"></a>命令行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">paddleocr --image_dir=docs/table/table.jpg --type=structure --layout=false</span><br></pre></td></tr></table></figure><p><img src="/2022/07/21/PaddleOCR表格识别/A.png" alt></p><h3 id="Python脚本"><a href="#Python脚本" class="headerlink" title="Python脚本"></a>Python脚本</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">from</span> paddleocr <span class="keyword">import</span> PPStructure, draw_structure_result, save_structure_res</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">table_engine = PPStructure(show_log=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">save_folder = <span class="string">'./output'</span></span><br><span class="line">img_path = <span class="string">'./ppstructure/docs/table/table.jpg'</span></span><br><span class="line">img = cv2.imread(img_path)</span><br><span class="line">result = table_engine(img)</span><br><span class="line">save_structure_res(result, save_folder, os.path.basename(img_path).split(<span class="string">'.'</span>)[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> result:</span><br><span class="line">    line.pop(<span class="string">'img'</span>)</span><br><span class="line">    print(line)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">font_path = <span class="string">'./doc/fonts/simfang.ttf'</span></span><br><span class="line">image = Image.open(img_path).convert(<span class="string">'RGB'</span>)</span><br><span class="line">im_show = draw_structure_result(image, result, font_path=font_path)</span><br><span class="line">im_show = Image.fromarray(im_show)</span><br><span class="line">im_show.save(<span class="string">'result.jpg'</span>)</span><br></pre></td></tr></table></figure><p><img src="/2022/07/21/PaddleOCR表格识别/B.png" alt></p><h3 id="服务部署"><a href="#服务部署" class="headerlink" title="服务部署"></a>服务部署</h3><p>安装structure_system</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hub install deploy\hubserving\structure_system\</span><br></pre></td></tr></table></figure><p>修改<font color="#00BFFF">config.json</font>，不使用GPU</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"modules_info"</span>: &#123;</span><br><span class="line">        <span class="attr">"structure_system"</span>: &#123;</span><br><span class="line">            <span class="attr">"init_args"</span>: &#123;</span><br><span class="line">                <span class="attr">"version"</span>: <span class="string">"1.0.0"</span>,</span><br><span class="line">                <span class="attr">"use_gpu"</span>: <span class="literal">false</span></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">"predict_args"</span>: &#123;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"port"</span>: <span class="number">8870</span>,</span><br><span class="line">    <span class="attr">"use_multiprocess"</span>: <span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"workers"</span>: <span class="number">2</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>启动服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hub serving start -c ./deploy/hubserving/structure_system/config.json</span><br></pre></td></tr></table></figure><p>版面分析+表格识别</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python tools/test_hubserving.py --server_url http://127.0.0.1:8870/predict/structure_system --image_dir ppstructure/docs/table/table.jpg</span><br></pre></td></tr></table></figure><p><img src="/2022/07/21/PaddleOCR表格识别/C.png" alt></p><p>将识别得到的html标签内容复制另存为.html文件进行对比得到如下结果：</p><p><img src="/2022/07/21/PaddleOCR表格识别/D.png" alt></p><h3 id="在Pycharm中部署服务并识别"><a href="#在Pycharm中部署服务并识别" class="headerlink" title="在Pycharm中部署服务并识别"></a>在Pycharm中部署服务并识别</h3><p>进入<font color="#008080">deploy/hubserving/structure_system/</font><font color="orange">params.py</font>，修改默认模型位置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">from</span> deploy.hubserving.structure_table.params <span class="keyword">import</span> read_params <span class="keyword">as</span> table_read_params</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_params</span><span class="params">()</span>:</span></span><br><span class="line">    cfg = table_read_params()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># params for layout parser model</span></span><br><span class="line">    cfg.layout_path_model = <span class="string">'lp://PubLayNet/ppyolov2_r50vd_dcn_365e_publaynet/config'</span></span><br><span class="line">    cfg.layout_label_map = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    cfg.mode = <span class="string">'structure'</span></span><br><span class="line">    cfg.output = <span class="string">'./output'</span></span><br><span class="line">    <span class="keyword">return</span> cfg</span><br></pre></td></tr></table></figure><p>可以看到，<font color="#008080">structure_system</font>/<font color="orange">params.py</font>引用了<font color="#008080">structure_table</font>/<font color="orange">params.py</font>下的<font color="orange">read_params</font>参数，再进入到<font color="#008080">structure_table</font>/<font color="orange">params.py</font>文件中，该文件则是引用了<font color="#008080">ocr_system</font>/<font color="orange">params.py</font>下的<font color="orange">read_params</font>参数，这些参数主要用作OCR识别，所以，如果在不同场景下要使用不同模型时，最好将各个<font color="orange">params.py</font>重写。修改后的文件如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># structure_system/params.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Config</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_params</span><span class="params">()</span>:</span></span><br><span class="line">    cfg = Config()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#params for text detector</span></span><br><span class="line">    cfg.det_algorithm = <span class="string">"DB"</span></span><br><span class="line">    cfg.det_model_dir = <span class="string">"C:/Users/9.9/.paddleocr/whl/det/ch/ch_PP-OCRv3_det_infer"</span></span><br><span class="line">    cfg.det_limit_side_len = <span class="number">960</span></span><br><span class="line">    cfg.det_limit_type = <span class="string">'max'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#DB parmas</span></span><br><span class="line">    cfg.det_db_thresh = <span class="number">0.3</span></span><br><span class="line">    cfg.det_db_box_thresh = <span class="number">0.5</span></span><br><span class="line">    cfg.det_db_unclip_ratio = <span class="number">1.6</span></span><br><span class="line">    cfg.use_dilation = <span class="literal">False</span></span><br><span class="line">    cfg.det_db_score_mode = <span class="string">"fast"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#EAST parmas</span></span><br><span class="line">    cfg.det_east_score_thresh = <span class="number">0.8</span></span><br><span class="line">    cfg.det_east_cover_thresh = <span class="number">0.1</span></span><br><span class="line">    cfg.det_east_nms_thresh = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#params for text recognizer</span></span><br><span class="line">    cfg.rec_algorithm = <span class="string">"CRNN"</span></span><br><span class="line">    cfg.rec_model_dir = <span class="string">r"C:/Users/9.9/.paddleocr/whl/rec/ch/ch_PP-OCRv3_rec_infer/"</span></span><br><span class="line"></span><br><span class="line">    cfg.rec_image_shape = <span class="string">"3, 48, 320"</span></span><br><span class="line">    cfg.rec_batch_num = <span class="number">6</span></span><br><span class="line">    cfg.max_text_length = <span class="number">25</span></span><br><span class="line"></span><br><span class="line">    cfg.rec_char_dict_path = <span class="string">r"C:/Users/9.9/software/PaddleOCR-release-2.5/ppocr/utils/ppocr_keys_v1.txt"</span></span><br><span class="line">    cfg.use_space_char = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#params for text classifier</span></span><br><span class="line">    cfg.use_angle_cls = <span class="literal">True</span></span><br><span class="line">    cfg.cls_model_dir = <span class="string">r"C:/Users/9.9/software/PaddleOCR-release-2.5/inference/ch_ppocr_mobile_v2.0_cls_infer/"</span></span><br><span class="line">    cfg.cls_image_shape = <span class="string">"3, 48, 192"</span></span><br><span class="line">    cfg.label_list = [<span class="string">'0'</span>, <span class="string">'180'</span>]</span><br><span class="line">    cfg.cls_batch_num = <span class="number">30</span></span><br><span class="line">    cfg.cls_thresh = <span class="number">0.9</span></span><br><span class="line"></span><br><span class="line">    cfg.use_pdserving = <span class="literal">False</span></span><br><span class="line">    cfg.use_tensorrt = <span class="literal">False</span></span><br><span class="line">    cfg.drop_score = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># params for table structure model</span></span><br><span class="line">    cfg.table_max_len = <span class="number">488</span></span><br><span class="line">    cfg.table_model_dir = <span class="string">r'C:\Users\9.9\.paddleocr\whl\table\en_ppocr_mobile_v2.0_table_structure_infer/'</span></span><br><span class="line">    cfg.table_char_dict_path = <span class="string">'C:/Users/9.9/software/PaddleOCR-release-2.5/ppocr/utils/dict/table_structure_dict.txt'</span></span><br><span class="line">    cfg.show_log = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># params for layout parser model</span></span><br><span class="line">    cfg.layout_path_model = <span class="string">'lp://PubLayNet/ppyolov2_r50vd_dcn_365e_publaynet/config'</span></span><br><span class="line">    <span class="comment"># cfg.layout_path_model = './inference/ppyolov2_r50vd_dcn_365e_publaynet'</span></span><br><span class="line">    cfg.layout_label_map = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    cfg.mode = <span class="string">'structure'</span></span><br><span class="line">    cfg.output = <span class="string">'./output'</span></span><br><span class="line">    <span class="keyword">return</span> cfg</span><br></pre></td></tr></table></figure><p>再新建python文件用于提取具体内容，通过pandas分析得到结构</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">exec_ocr</span><span class="params">(cmd: str)</span>:</span></span><br><span class="line">    pip = os.popen(cmd)</span><br><span class="line">    <span class="keyword">return</span> pip.buffer.read().decode(encoding=<span class="string">'utf8'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">img_dir = <span class="string">r"C:\Users\9.9\software\PaddleOCR-release-2.5\ppstructure\docs\table\table.jpg"</span></span><br><span class="line">ocr_dir = <span class="string">r"python C:\Users\9.9\software\PaddleOCR-release-2.5\tools\test_hubserving.py"</span></span><br><span class="line">res = exec_ocr(<span class="string">fr"<span class="subst">&#123;ocr_dir&#125;</span> --server_url http://127.0.0.1:8870/predict/structure_system --image_dir <span class="subst">&#123;img_dir&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"system_res.txt"</span>, <span class="string">"w"</span>, encoding=<span class="string">"utf-8"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(res)</span><br><span class="line"></span><br><span class="line">ocr_file = open(<span class="string">"system_res.txt"</span>, <span class="string">"r"</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">alldata_res = ocr_file.read()</span><br><span class="line">system_res = re.search(<span class="string">r"'html': '(.*)'&#125;, 'type': 'Table'&#125;"</span>, alldata_res)</span><br><span class="line">print(system_res.group(<span class="number">1</span>))</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"system_res.html"</span>, <span class="string">"w"</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(system_res.group(<span class="number">1</span>))</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"system_res.html"</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    df = pd.read_html(f.read(), encoding=<span class="string">'utf-8'</span>, index_col=<span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">    df = df.loc[:, ~df.columns.str.contains(<span class="string">"^Unnamed"</span>)]</span><br><span class="line"></span><br><span class="line">print(df)</span><br><span class="line">df.to_csv(<span class="string">'system_res.csv'</span>)</span><br></pre></td></tr></table></figure><p><img src="/2022/07/21/PaddleOCR表格识别/E.png" alt></p><p><img src="/2022/07/21/PaddleOCR表格识别/F.png" alt></p><p><img src="/2022/07/21/PaddleOCR表格识别/G.png" alt></p><h2 id="PP-Structure表格模型训练"><a href="#PP-Structure表格模型训练" class="headerlink" title="PP-Structure表格模型训练"></a>PP-Structure表格模型训练</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>前往Paddle的github主页下载安装PaddleDetection：<a href="https://github.com/PaddlePaddle/PaddleDetection" target="_blank" rel="noopener">https://github.com/PaddlePaddle/PaddleDetection</a>，并执行<code>pip install -r requirements.txt</code>安装其他依赖</p><h3 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h3><p>下载<a href="https://github.com/ibm-aur-nlp/PubLayNet" target="_blank" rel="noopener">PubLayNet</a>数据集，可通过链接（<a href="https://dax-cdn.cdn.appdomain.cloud/dax-publaynet/1.0.0/publaynet.tar.gz?_ga=2.104193024.1076900768.1622560733-649911202.1622560733" target="_blank" rel="noopener">https://dax-cdn.cdn.appdomain.cloud/dax-publaynet/1.0.0/publaynet.tar.gz?_ga=2.104193024.1076900768.1622560733-649911202.1622560733</a>）直接下载（约<font color="red">95GB</font>）</p><h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h3><p>修改<font color="#008080">configs/ppyolo/</font><font color="#CCCCFF">ppyolov2_r50vd_dcn_365e_coco.yml</font>文件的配置进行训练：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">_BASE_:</span> <span class="string">[</span></span><br><span class="line">  <span class="string">'../datasets/coco_detection.yml'</span><span class="string">,</span>   <span class="comment"># 主要说明了训练数据和验证数据的路径</span></span><br><span class="line">  <span class="string">'../runtime.yml'</span><span class="string">,</span>                   <span class="comment"># 主要说明了公共的运行参数，比如是否使用GPU、每多少个epoch存储checkpoint等</span></span><br><span class="line">  <span class="string">'./_base_/ppyolov2_r50vd_dcn.yml'</span><span class="string">,</span>  <span class="comment"># 主要说明了学习率和优化器的配置</span></span><br><span class="line">  <span class="string">'./_base_/optimizer_365e.yml'</span><span class="string">,</span>      <span class="comment"># 主要说明模型和主干网络的情况</span></span><br><span class="line">  <span class="string">'./_base_/ppyolov2_reader.yml'</span><span class="string">,</span>     <span class="comment"># 主要说明数据读取器配置，如batch size，并发加载子进程数等，同时包含读取后预处理操作，如resize、数据增强等等</span></span><br><span class="line"><span class="string">]</span></span><br><span class="line"></span><br><span class="line"><span class="attr">snapshot_epoch:</span> <span class="number">8</span></span><br><span class="line"><span class="attr">weights:</span> <span class="string">output/ppyolov2_r50vd_dcn_365e_coco/model_final</span></span><br></pre></td></tr></table></figure><p>来到<font color="#008080">datasets/</font><font color="#CCCCFF">coco_detection.yml</font>文件中，修改下载好的训练集的位置</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">metric:</span> <span class="string">COCO</span></span><br><span class="line"><span class="attr">num_classes:</span> <span class="number">80</span></span><br><span class="line"></span><br><span class="line"><span class="attr">TrainDataset:</span></span><br><span class="line">  <span class="type">!COCODataSet</span></span><br><span class="line">    <span class="attr">image_dir:</span> <span class="string">C:/Users/9.9/software/publaynet/train</span></span><br><span class="line">    <span class="attr">anno_path:</span> <span class="string">C:/Users/9.9/software/publaynet/train.json</span></span><br><span class="line">    <span class="attr">dataset_dir:</span> <span class="string">dataset/coco</span></span><br><span class="line">    <span class="attr">data_fields:</span> <span class="string">['image',</span> <span class="string">'gt_bbox'</span><span class="string">,</span> <span class="string">'gt_class'</span><span class="string">,</span> <span class="string">'is_crowd'</span><span class="string">]</span></span><br><span class="line"></span><br><span class="line"><span class="attr">EvalDataset:</span></span><br><span class="line">  <span class="type">!COCODataSet</span></span><br><span class="line">    <span class="attr">image_dir:</span> <span class="string">C:/Users/9.9/software/publaynet/val</span></span><br><span class="line">    <span class="attr">anno_path:</span> <span class="string">C:/Users/9.9/software/publaynet/val.json</span></span><br><span class="line">    <span class="attr">dataset_dir:</span> <span class="string">dataset/coco</span></span><br><span class="line"></span><br><span class="line"><span class="attr">TestDataset:</span></span><br><span class="line">  <span class="type">!ImageFolder</span></span><br><span class="line">    <span class="attr">anno_path:</span> <span class="string">C:/Users/9.9/software/publaynet/val.json</span> <span class="comment"># also support txt (like VOC's label_list.txt)</span></span><br><span class="line">    <span class="attr">dataset_dir:</span> <span class="string">dataset/coco</span> <span class="comment"># if set, anno_path will be 'dataset_dir/anno_path'</span></span><br></pre></td></tr></table></figure><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python tools/train.py -c configs/ppyolo/ppyolov2_r50vd_dcn_365e_coco.yml</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;PaddleOCR2.5根目录下的ppstructure文件模块是PaddleOCR提供的一个可用于复杂文档结构分析处理的OCR工具包&lt;br&gt;github文档页面：&lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.5/ppstructure/README_ch.md&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.5/ppstructure/README_ch.md&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;安装依赖&quot;&gt;&lt;a href=&quot;#安装依赖&quot; class=&quot;headerlink&quot; title=&quot;安装依赖&quot;&gt;&lt;/a&gt;安装依赖&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;&lt;font size=&quot;4&quot; color=&quot;orange&quot;&gt;·&lt;/font&gt;&lt;/strong&gt; 安装paddleocr version&amp;gt;=2.5&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;pip install &amp;quot;paddleocr&amp;gt;=2.5&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;font size=&quot;4&quot; color=&quot;orange&quot;&gt;·&lt;/font&gt;&lt;/strong&gt; 安装版面分析依赖包layoutparser&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;pip install -U https://paddleocr.bj.bcebos.com/whl/layoutparser-0.0.0-py3-none-any.whl&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;font size=&quot;4&quot; color=&quot;orange&quot;&gt;·&lt;/font&gt;&lt;/strong&gt; 安装DocVQA依赖包paddlenlp（DocVQA功能，选装）&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;pip install paddlenlp&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;h2 id=&quot;快速开始&quot;&gt;&lt;a href=&quot;#快速开始&quot; class=&quot;headerlink&quot; title=&quot;快速开始&quot;&gt;&lt;/a&gt;快速开始&lt;/h2&gt;&lt;p&gt;在PaddleOCR/ppstructure目录下进入CMD命令行，或者创建python脚本启动&lt;br&gt;&lt;strong&gt;&lt;font size=&quot;4&quot; color=&quot;orange&quot;&gt;·&lt;/font&gt;&lt;/strong&gt; 表格识别&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;paddleocr --image_dir=docs/table/table.jpg --type=structure --layout=false&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;python脚本：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; os&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; cv2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; paddleocr &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; PPStructure,save_structure_res&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;table_engine = PPStructure(layout=&lt;span class=&quot;literal&quot;&gt;False&lt;/span&gt;, show_log=&lt;span class=&quot;literal&quot;&gt;True&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;save_folder = &lt;span class=&quot;string&quot;&gt;&#39;./output&#39;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;img_path = &lt;span class=&quot;string&quot;&gt;&#39;PaddleOCR/ppstructure/docs/table/table.jpg&#39;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;img = cv2.imread(img_path)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;result = table_engine(img)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;save_structure_res(result, save_folder, os.path.basename(img_path).split(&lt;span class=&quot;string&quot;&gt;&#39;.&#39;&lt;/span&gt;)[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; line &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; result:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    line.pop(&lt;span class=&quot;string&quot;&gt;&#39;img&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    print(line)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;&lt;font size=&quot;4&quot; color=&quot;orange&quot;&gt;·&lt;/font&gt;&lt;/strong&gt; 版面分析&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;paddleocr --image_dir=docs/table/1.png --type=structure --table=false --ocr=false&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;python脚本：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; os&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; cv2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; paddleocr &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; PPStructure,save_structure_res&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;table_engine = PPStructure(table=&lt;span class=&quot;literal&quot;&gt;False&lt;/span&gt;, ocr=&lt;span class=&quot;literal&quot;&gt;False&lt;/span&gt;, show_log=&lt;span class=&quot;literal&quot;&gt;True&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;save_folder = &lt;span class=&quot;string&quot;&gt;&#39;./output&#39;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;img_path = &lt;span class=&quot;string&quot;&gt;&#39;PaddleOCR/ppstructure/docs/table/1.png&#39;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;img = cv2.imread(img_path)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;result = table_engine(img)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;save_structure_res(result, save_folder, os.path.basename(img_path).split(&lt;span class=&quot;string&quot;&gt;&#39;.&#39;&lt;/span&gt;)[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; line &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; result:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    line.pop(&lt;span class=&quot;string&quot;&gt;&#39;img&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    print(line)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;&lt;font size=&quot;4&quot; color=&quot;orange&quot;&gt;·&lt;/font&gt;&lt;/strong&gt; 版面分析+表格识别&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;paddleocr --image_dir=docs/table/1.png --type=structure&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;python脚本：&lt;br&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; os&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; cv2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; paddleocr &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; PPStructure, draw_structure_result, save_structure_res&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; PIL &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; Image&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;table_engine = PPStructure(show_log=&lt;span class=&quot;literal&quot;&gt;True&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;save_folder = &lt;span class=&quot;string&quot;&gt;&#39;./output&#39;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;img_path = &lt;span class=&quot;string&quot;&gt;&#39;./ppstructure/docs/table/table.jpg&#39;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;img = cv2.imread(img_path)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;result = table_engine(img)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;save_structure_res(result, save_folder, os.path.basename(img_path).split(&lt;span class=&quot;string&quot;&gt;&#39;.&#39;&lt;/span&gt;)[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; line &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; result:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    line.pop(&lt;span class=&quot;string&quot;&gt;&#39;img&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    print(line)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;font_path = &lt;span class=&quot;string&quot;&gt;&#39;./doc/fonts/simfang.ttf&#39;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;image = Image.open(img_path).convert(&lt;span class=&quot;string&quot;&gt;&#39;RGB&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;im_show = draw_structure_result(image, result, font_path=font_path)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;im_show = Image.fromarray(im_show)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;im_show.save(&lt;span class=&quot;string&quot;&gt;&#39;result.jpg&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://yoursite.com/categories/机器学习/"/>
    
    
    <category term="OCR" scheme="http://yoursite.com/tags/OCR/"/>
    
  </entry>
  
  <entry>
    <title>PaddleOCR</title>
    <link href="http://yoursite.com/2022/07/12/PaddleOCR/"/>
    <id>http://yoursite.com/2022/07/12/PaddleOCR/</id>
    <published>2022-07-12T06:20:14.000Z</published>
    <updated>2022-09-22T03:43:08.898Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>进入PaddleOCR的github页面（<a href="https://github.com/PaddlePaddle/PaddleOCR" target="_blank" rel="noopener">https://github.com/PaddlePaddle/PaddleOCR</a>），进行下载和解压</p><p>使用pip进行安装，这里因为速度很慢推荐使用百度源<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install paddlepaddle -i https://mirror.baidu.com/pypi/simple</span><br></pre></td></tr></table></figure></p><p>安装shapely（<a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/" target="_blank" rel="noopener">https://www.lfd.uci.edu/~gohlke/pythonlibs/</a>），下载shapely对应python和系统版本的安装包（我使用的是py39，windows_64），python版本目前不能超过3.9</p><p>将下载好的Shapely-1.8.2-cp39-cp39-win_amd64.whl放进python根目录下的libs文件夹内，通过cmd或pycharm终端使用pip执行安装<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install Shapely-1.8.2-cp39-cp39-win_amd64.whl</span><br></pre></td></tr></table></figure></p><p>完成以后接着来到PaddleOCR目录下，通过终端安装依赖，这里同样推荐使用百度源<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt -i https://mirror.baidu.com/pypi/simple</span><br></pre></td></tr></table></figure></p><p>到这里，PaddleOCR的安装完成了</p><p>如果再执行<code>pip install -r requirements.txt -i https://mirror.baidu.com/pypi/simple</code>到安装opencv4.4.0.46包时报错：</p><font size="4" color="red">error: subprocess-exited-with-error</font><p>说明python的版本可能存在问题，需要切换虚拟环境或回退python版本，因为opencv-python目前仅支持py3.6-3.9版本</p><p>opencv-python镜像：<a href="https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple/opencv-python/" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple/opencv-python/</a><br><a id="more"></a></p><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><h3 id="官方模型"><a href="#官方模型" class="headerlink" title="官方模型"></a>官方模型</h3><p>先使用官方模型对数据进行测试（v2.0）：<a href="https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.0/README_ch.md" target="_blank" rel="noopener">https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.0/README_ch.md</a></p><p><img src="/2022/07/12/PaddleOCR/1.png" alt><br>其中推理模型（inference model）相当于已训练完成的模型，可以直接拿来预测，而预训练模型（trained model)属于半成品，在使用本地的数据训练模型时需要用到</p><p>将推理模型下载后，来到PaddleOCR目录下新建inference文件夹，用来存放模型</p><p><img src="/2022/07/12/PaddleOCR/2.png" alt><br><img src="/2022/07/12/PaddleOCR/3.png" alt></p><p>检查每个文件夹下是否存在<font color="orange">inference.pdiparams</font>、<font color="orange">inference.pdiparams.info</font>、<font color="orange">inference.pdmodel</font>三个文件，如果出现不和谐的文件夹是官方打包时出错，将文件夹内的内容提取出来即可</p><p>如果直接使用可能会报错：</p><font color="red">raise Exception(“not found any img file in {}”.format(img_file))<br>Exception: not found any img file in ./doc/imgs/test.jpg</font><h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><p>官方的快速开始教程：<a href="https://github.com/PaddlePaddle/PaddleOCR/blob/develop/doc/doc_ch/quickstart.md" target="_blank" rel="noopener">https://github.com/PaddlePaddle/PaddleOCR/blob/develop/doc/doc_ch/quickstart.md</a></p><p>首先将测试目标test.jpg存放进PaddleOCR/doc/imgs/目录下，到cmd或pycharm终端里执行：<br><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">python tools/infer/predict_system.py --image_dir="./doc/imgs/test.jpg" \</span><br><span class="line">         --det_model_dir="./inference/ch_ppocr_server_v2.<span class="number">0</span>_det_infer/" \</span><br><span class="line">         --rec_model_dir="./inference/ch_ppocr_server_v2.<span class="number">0</span>_rec_infer/" \</span><br><span class="line">         --cls_model_dir="./inference/ch_ppocr_mobile_v2.<span class="number">0</span>_cls_infer/" \</span><br><span class="line">                 --use_angle_cls=True \</span><br><span class="line">         --use_space_char=True</span><br></pre></td></tr></table></figure></p><p>（windows下使用官方文档里中的”python3 tools/…”会出不来结果，linux下没有问题）</p><p><img src="/2022/07/12/PaddleOCR/4.png" alt></p><h3 id="使用pycharm预测"><a href="#使用pycharm预测" class="headerlink" title="使用pycharm预测"></a>使用pycharm预测</h3><p>需要修改的代码位于根目录下/tools/infer文件夹下，其中<font color="orange">predict_det.py</font>用于检测文本，<font color="orange">predict_rec.py</font>用于识别文本，<font color="orange">predict_system.py</font>可用于检测和识别，这3个.py文件共用同一个配置文件<font color="orange">utility.py</font>，需要到其中定位并修改：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 是否使用gpu</span></span><br><span class="line">parser.add_argument(<span class="string">"--use_gpu"</span>, type=str2bool, default=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 图片文件位置</span></span><br><span class="line">parser.add_argument(<span class="string">"--image_dir"</span>, type=str, default=<span class="string">"../../demo/test.jpg"</span>)</span><br><span class="line"><span class="comment"># 检测模型路径</span></span><br><span class="line">parser.add_argument(<span class="string">"--det_model_dir"</span>, type=str, default=<span class="string">"../../inference/ch_ppocr_server_v2.0_det_infer/"</span>)</span><br><span class="line"><span class="comment"># 识别模型路径</span></span><br><span class="line">parser.add_argument(<span class="string">"--rec_model_dir"</span>, type=str, default=<span class="string">"../../inference/ch_ppocr_server_v2.0_rec_infer"</span>)</span><br><span class="line"><span class="comment"># 分类模型路径</span></span><br><span class="line">parser.add_argument(<span class="string">"--cls_model_dir"</span>, type=str, default=<span class="string">"../../inference/ch_ppocr_mobile_v2.0_cls_infer"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 字典路径（ic15_dict.txt是英文字典，ppocr_keys_v1.txt是中文字典，检测一般不区分中英文，但是识别需要区分中英文）</span></span><br><span class="line">parser.add_argument(</span><br><span class="line">    <span class="string">"--rec_char_dict_path"</span>,</span><br><span class="line">    type=str,</span><br><span class="line">    default=<span class="string">"../../ppocr/utils/ic15_dict.txt"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 字体路径，2处</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_ocr_box_txt</span><span class="params">(image,</span></span></span><br><span class="line"><span class="function"><span class="params">                     boxes,</span></span></span><br><span class="line"><span class="function"><span class="params">                     txts,</span></span></span><br><span class="line"><span class="function"><span class="params">                     scores=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                     drop_score=<span class="number">0.5</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                     font_path=<span class="string">"./doc/simfang.ttf"</span>)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">text_visual</span><span class="params">(texts,</span></span></span><br><span class="line"><span class="function"><span class="params">                scores,</span></span></span><br><span class="line"><span class="function"><span class="params">                img_h=<span class="number">400</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                img_w=<span class="number">600</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                threshold=<span class="number">0.</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                font_path=<span class="string">"./doc/simfang.ttf"</span>)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 程序输出路径</span></span><br><span class="line">parser.add_argument(</span><br><span class="line">    <span class="string">"--draw_img_save_dir"</span>, type=str, default=<span class="string">"./inference_results"</span>)</span><br></pre></td></tr></table></figure></p><p>接着运行<font color="orange">predict_system.py</font></p><h2 id="服务部署（基于PaddleHub-Serving）"><a href="#服务部署（基于PaddleHub-Serving）" class="headerlink" title="服务部署（基于PaddleHub Serving）"></a>服务部署（基于PaddleHub Serving）</h2><p>deploy/hubserving服务部署目录下包括检测（<font color="orange">ocr_det</font>）、识别（<font color="orange">ocr_rec</font>）、2阶段串联（<font color="orange">ocr_system</font>）三种服务包和分类模块服务包（<font color="orange">ocr_cls</font>），可以根据需求选择相应的服务包进行安装和启动。每个服务包下包括3个.py文件和一个config.json配置文件：<br><strong>-</strong> <font color="orange">__init__.py</font><br><strong>-</strong> <font color="#00BFFF">config.json</font>：配置文件<br><strong>-</strong> <font color="orange">module.py</font>：主模块，必选，包含服务的完整逻辑<br><strong>-</strong> <font color="orange">params.py</font>：参数文件，必选，包含模型路径、前后处理参数等参数</p><h3 id="准备环境"><a href="#准备环境" class="headerlink" title="准备环境"></a>准备环境</h3><p><font color="orange"><strong>1.</strong></font>使用pip安装paddlehub，python版本需要高于3.6.2<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install paddlehub==2.1.0 --upgrade -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure></p><p><font color="orange"><strong>2.</strong></font>配置模型文件，将之前下载的模型路径修改到<font color="orange">params.py</font>当中<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cfg.det_model_dir = <span class="string">"./inference/ch_ppocr_server_v2.0_det_infer/"</span></span><br><span class="line">cfg.rec_model_dir = <span class="string">"./inference/ch_ppocr_server_v2.0_rec_infer/"</span></span><br><span class="line">cfg.cls_model_dir = <span class="string">"./inference/ch_ppocr_mobile_v2.0_cls_infer/"</span></span><br></pre></td></tr></table></figure></p><p><font color="orange"><strong>3.</strong></font>安装服务模块，进入paddleOCR根目录，执行：<br><strong>Linux</strong>环境<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 安装检测服务模块：  </span><br><span class="line">$ hub install deploy/hubserving/ocr_det/</span><br><span class="line"></span><br><span class="line"># 或，安装分类服务模块：  </span><br><span class="line">$ hub install deploy/hubserving/ocr_cls/</span><br><span class="line"></span><br><span class="line"># 或，安装识别服务模块：  </span><br><span class="line">$ hub install deploy/hubserving/ocr_rec/</span><br><span class="line"></span><br><span class="line"># 或，安装检测+识别串联服务模块：  </span><br><span class="line">$ hub install deploy/hubserving/ocr_system/</span><br></pre></td></tr></table></figure></p><p><strong>windows</strong>环境<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 安装检测服务模块：  </span><br><span class="line">hub install deploy\hubserving\ocr_det\</span><br><span class="line"></span><br><span class="line"># 或，安装分类服务模块：  </span><br><span class="line">hub install deploy\hubserving\ocr_cls\</span><br><span class="line"></span><br><span class="line"># 或，安装识别服务模块：  </span><br><span class="line">hub install deploy\hubserving\ocr_rec\</span><br><span class="line"></span><br><span class="line"># 或，安装检测+识别串联服务模块：</span><br><span class="line">hub install deploy\hubserving\ocr_system\</span><br></pre></td></tr></table></figure></p><p><img src="/2022/07/12/PaddleOCR/5.png" alt><br>执行<code>hub list</code>可以查看已安装的模块：<br><img src="/2022/07/12/PaddleOCR/6.png" alt></p><h3 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h3><h4 id="方式1-命令行启动（仅支持CPU）"><a href="#方式1-命令行启动（仅支持CPU）" class="headerlink" title="方式1.命令行启动（仅支持CPU）"></a>方式1.命令行启动（仅支持CPU）</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ hub serving start --modules [Module1==Version1, Module2==Version2, ...] \</span><br><span class="line">                    --port XXXX \</span><br><span class="line">                    --use_multiprocess \</span><br><span class="line">                    --workers \</span><br></pre></td></tr></table></figure><table><thead><tr><th>参数</th><th style="text-align:left">作用</th></tr></thead><tbody><tr><td>–modules/-m</td><td style="text-align:left">PaddleHub Serving预安装模型，以多个Module==Version键值对的形式列出<code>当不指定Version时，默认选择最新版本</code></td></tr><tr><td>–port/-p</td><td style="text-align:left">服务端口，默认为8866</td></tr><tr><td>–use_multiprocess</td><td style="text-align:left">是否启用并发方式，默认为单进程方式，推荐多核CPU机器使用此方式<code>Windows操作系统只支持单进程方式</code></td></tr><tr><td>–workers</td><td style="text-align:left">在并发方式下指定的并发任务数，默认为<code>2*cpu_count-1</code>，其中<code>cpu_count</code>为CPU核数</td></tr></tbody></table><p>例：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hub serving start -m ocr_system</span><br></pre></td></tr></table></figure></p><p><img src="/2022/07/12/PaddleOCR/7.png" alt></p><h4 id="方式2-通过配置文件config-json启动（支持CPU、GPU）"><a href="#方式2-通过配置文件config-json启动（支持CPU、GPU）" class="headerlink" title="方式2.通过配置文件config.json启动（支持CPU、GPU）"></a>方式2.通过配置文件config.json启动（支持CPU、GPU）</h4><p><font color="#00BFFF">config.json</font>内容格式如下：<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"modules_info"</span>: &#123;</span><br><span class="line">        <span class="attr">"ocr_system"</span>: &#123;</span><br><span class="line">            <span class="attr">"init_args"</span>: &#123;</span><br><span class="line">                <span class="attr">"version"</span>: <span class="string">"1.0.0"</span>,</span><br><span class="line">                <span class="attr">"use_gpu"</span>: <span class="literal">false</span></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">"predict_args"</span>: &#123;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"port"</span>: <span class="number">8868</span>,</span><br><span class="line">    <span class="attr">"use_multiprocess"</span>: <span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"workers"</span>: <span class="number">2</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>·</strong> <code>init_args</code>中的可配参数与<font color="orange">module.py</font>中的<font color="#008080">_initialize</font>函数接口保持一致，当<code>use_gpu</code>的值为<font color="orange">true</font>时，表示使用GPU启动服务</p><p><strong>·</strong> <code>predict_args</code>中的可配参数与<font color="orange">module.py</font>中的<font color="#008080">predict</font>函数接口保持一致</p><p><strong>·</strong> 使用配置文件启动服务时，其他参数会被忽略</p><p><strong>·</strong> 如果使用GPU预测(即，<code>use_gpu</code>置为<font color="orange">true</font>)，则需要在启动服务之前，设置<font color="#008080">CUDA_VISIBLE_DEVICES</font>环境变量，如：<font color="#008080">export CUDA_VISIBLE_DEVICES=0</font>，否则不用设置</p><p><strong>·</strong> <code>use_gpu</code>不可与<code>use_multiprocess</code>同时为true</p><p>例：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hub serving start -c ./deploy/hubserving/ocr_system/config.json</span><br></pre></td></tr></table></figure></p><p><img src="/2022/07/12/PaddleOCR/8.png" alt></p><h3 id="发送预测请求"><a href="#发送预测请求" class="headerlink" title="发送预测请求"></a>发送预测请求</h3><p>需要通过POST方法传递2个参数：server_url 和 image_path<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python tools/test_hubserving.py --server_url http://127.0.0.1:8868/predict/ocr_system --image_dir ./demo/</span><br></pre></td></tr></table></figure></p><p><img src="/2022/07/12/PaddleOCR/9.png" alt><br>如果在启动时遇到警告：</p><p><font color="gold">C:\Python39\lib\site-packages\attrdict\mapping.py:4: DeprecationWarning: Using or importing the ABCs from ‘collections’ instead of from ‘collections.abc’ is deprecated since Python 3.3, and in 3.10 it will stop working<br>    from collections import Mapping</font><br>需要到对应的Python\lib\site-packages\attrdict\目录下修改<font color="orange">default.py</font>、<font color="orange">mapping.py</font>、<font color="orange">merge.py</font> 和 <font color="orange">mixins.py</font>文件中<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> ...</span><br></pre></td></tr></table></figure></p><p>修改为<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections.abc <span class="keyword">import</span> ...</span><br></pre></td></tr></table></figure></p><h3 id="返回结构"><a href="#返回结构" class="headerlink" title="返回结构"></a>返回结构</h3><p>返回的结果为列表，其中每一项为一个字典，字典一共可能包含下列三种字段：</p><table><thead><tr><th>字段名</th><th>数据类型</th><th>含义</th></tr></thead><tbody><tr><td>text</td><td>str</td><td>文本内容</td></tr><tr><td>confidence</td><td>float</td><td>文本识别置信度</td></tr><tr><td>text_region</td><td>list</td><td>文本位置坐标</td></tr></tbody></table><p>不同模块返回的字段不同，如，文本识别服务模块返回结果不含text_region字段</p><table><thead><tr><th>字段名</th><th>ocr_det</th><th>ocr_cls</th><th>ocr_rec</th><th>ocr_system</th></tr></thead><tbody><tr><td>text</td><td></td><td></td><td><font color="orange">√</font></td><td><font color="orange">√</font></td></tr><tr><td>confidence</td><td></td><td><font color="orange">√</font></td><td><font color="orange">√</font></td><td><font color="orange">√</font></td></tr><tr><td>text_region</td><td><font color="orange">√</font></td><td></td><td></td><td><font color="orange">√</font></td></tr></tbody></table><h3 id="自定义修改服务模块"><a href="#自定义修改服务模块" class="headerlink" title="自定义修改服务模块"></a>自定义修改服务模块</h3><p><strong>·</strong> 停止服务：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hub serving stop --port/-p XXXX</span><br></pre></td></tr></table></figure></p><p><strong>·</strong> 到相应的<font color="orange">module.py</font> 和 <font color="orange">params.py</font>等文件中根据实际需求修改代码<br>例如，如果需要替换部署服务所用模型，则需要到<font color="orange">params.py</font>中修改模型路径参数<font color="#008080">det_model_dir</font> 和 <font color="#008080">rec_model_dir</font>，如果需要关闭文本方向分类器，则将参数<font color="#008080">use_angle_cls</font>置为<font color="orange">False</font>，当然，同时可能还需要修改其他相关参数，强烈建议修改后先直接运行<font color="orange">module.py</font>调试，能正确运行预测后再启动服务测试</p><p><strong>·</strong> 卸载旧服务包<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hub uninstall ocr_system</span><br></pre></td></tr></table></figure></p><p><strong>·</strong> 安装修改后的新服务包<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hub install deploy/hubserving/ocr_system/</span><br></pre></td></tr></table></figure></p><p><strong>·</strong> 重新启动服务<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hub serving start -m ocr_system</span><br></pre></td></tr></table></figure></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;安装&quot;&gt;&lt;a href=&quot;#安装&quot; class=&quot;headerlink&quot; title=&quot;安装&quot;&gt;&lt;/a&gt;安装&lt;/h2&gt;&lt;p&gt;进入PaddleOCR的github页面（&lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/PaddlePaddle/PaddleOCR&lt;/a&gt;），进行下载和解压&lt;/p&gt;
&lt;p&gt;使用pip进行安装，这里因为速度很慢推荐使用百度源&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;pip install paddlepaddle -i https://mirror.baidu.com/pypi/simple&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;安装shapely（&lt;a href=&quot;https://www.lfd.uci.edu/~gohlke/pythonlibs/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.lfd.uci.edu/~gohlke/pythonlibs/&lt;/a&gt;），下载shapely对应python和系统版本的安装包（我使用的是py39，windows_64），python版本目前不能超过3.9&lt;/p&gt;
&lt;p&gt;将下载好的Shapely-1.8.2-cp39-cp39-win_amd64.whl放进python根目录下的libs文件夹内，通过cmd或pycharm终端使用pip执行安装&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;pip install Shapely-1.8.2-cp39-cp39-win_amd64.whl&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;完成以后接着来到PaddleOCR目录下，通过终端安装依赖，这里同样推荐使用百度源&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;pip install -r requirements.txt -i https://mirror.baidu.com/pypi/simple&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;到这里，PaddleOCR的安装完成了&lt;/p&gt;
&lt;p&gt;如果再执行&lt;code&gt;pip install -r requirements.txt -i https://mirror.baidu.com/pypi/simple&lt;/code&gt;到安装opencv4.4.0.46包时报错：&lt;/p&gt;
&lt;font size=&quot;4&quot; color=&quot;red&quot;&gt;error: subprocess-exited-with-error&lt;/font&gt;

&lt;p&gt;说明python的版本可能存在问题，需要切换虚拟环境或回退python版本，因为opencv-python目前仅支持py3.6-3.9版本&lt;/p&gt;
&lt;p&gt;opencv-python镜像：&lt;a href=&quot;https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple/opencv-python/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple/opencv-python/&lt;/a&gt;&lt;br&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://yoursite.com/categories/机器学习/"/>
    
    
    <category term="OCR" scheme="http://yoursite.com/tags/OCR/"/>
    
  </entry>
  
</feed>
