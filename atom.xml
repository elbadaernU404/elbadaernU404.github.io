<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>青域</title>
  
  
  <link href="http://yoursite.com/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2024-01-25T11:21:45.306Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>tianL.R</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>yolov8-火灾检测</title>
    <link href="http://yoursite.com/2024/01/25/yolov8%EF%BC%9A%E7%81%AB%E7%81%BE%E6%A3%80%E6%B5%8B/"/>
    <id>http://yoursite.com/2024/01/25/yolov8%EF%BC%9A%E7%81%AB%E7%81%BE%E6%A3%80%E6%B5%8B/</id>
    <published>2024-01-25T09:23:20.000Z</published>
    <updated>2024-01-25T11:21:45.306Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><h3 id="GPU"><a href="#GPU" class="headerlink" title="GPU"></a>GPU</h3><ul><li><font color="gold">NVIDIA 3090*2</font></li><li><font color="gold">显卡驱动 535.104.05</font></li><li><font color="gold">CUDA版本 12.2</font></li><li><font color="gold">CUDAtoolkit</font> (cuda_12.2.2_535.104.05_linux)</li><li><font color="gold">cuDNN (v8.9.7)</font></li></ul><h3 id="yolo版本"><a href="#yolo版本" class="headerlink" title="yolo版本"></a>yolo版本</h3><ul><li><font color="gold">v8.1.5</font> (ultralytics yolov8)</li></ul><h3 id="pytorch版本"><a href="#pytorch版本" class="headerlink" title="pytorch版本"></a>pytorch版本</h3><ul><li><font color="gold">v2.1.2</font></li></ul><h3 id="python环境"><a href="#python环境" class="headerlink" title="python环境"></a>python环境</h3><ul><li><font color="gold">CentOS7.9</font></li><li><font color="gold">anaconda3</font></li><li><font color="gold">python3.9</font></li></ul><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>源码主页：<a href="https://github.com/ultralytics/ultralytics" target="_blank" rel="noopener">https://github.com/ultralytics/ultralytics</a></p><p>官方文档：<a href="https://docs.ultralytics.com/zh" target="_blank" rel="noopener">https://docs.ultralytics.com/zh</a></p><p><img src="/2024/01/25/yolov8：火灾检测/1.png" alt></p><h3 id="克隆源码"><a href="#克隆源码" class="headerlink" title="克隆源码"></a>克隆源码</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://hub.nuaa.cf/ultralytics/ultralytics.git</span><br></pre></td></tr></table></figure><h3 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pip install ultralytics -i https://mirror.baidu.com/pypi/simple</span><br></pre></td></tr></table></figure><h3 id="环境验证"><a href="#环境验证" class="headerlink" title="环境验证"></a>环境验证</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yolo predict model=yolov8n.pt source=ultralytics/assets/zidane.jpg</span><br></pre></td></tr></table></figure><p>执行完毕后得到输出的结果如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">(py39_yolov8) [root@jdz yolov8]# yolo predict model=yolov8n.pt source=ultralytics/assets/zidane.jpg </span><br><span class="line">Ultralytics YOLOv8.1.5 🚀 Python-3.9.18 torch-2.1.2+cu121 CUDA:0 (NVIDIA GeForce RTX 3090, 24260MiB)</span><br><span class="line">YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs</span><br><span class="line"></span><br><span class="line">image 1/1 /exp/work/video/yolov8/ultralytics/assets/zidane.jpg: 384x640 2 persons, 1 tie, 216.9ms</span><br><span class="line">Speed: 7.3ms preprocess, 216.9ms inference, 762.4ms postprocess per image at shape (1, 3, 384, 640)</span><br><span class="line">Results saved to runs/detect/predict</span><br><span class="line">💡 Learn more at https://docs.ultralytics.com/modes/predict</span><br></pre></td></tr></table></figure><p>将在<code>Results saved to runs/detect/predict</code>目录下找到输出结果</p><a id="more"></a><p><img src="/2024/01/25/yolov8：火灾检测/2.png" alt></p><p>至此，已验证基础环境正常工作。</p><h2 id="训练集"><a href="#训练集" class="headerlink" title="训练集"></a>训练集</h2><p>前往kaggle官网寻找火焰训练集，这里使用Fire and Smoke Dataset数据集（<a href="https://www.kaggle.com/datasets/dataclusterlabs/fire-and-smoke-dataset" target="_blank" rel="noopener">https://www.kaggle.com/datasets/dataclusterlabs/fire-and-smoke-dataset</a>）</p><p>数据集包含烟、火图像共100张</p><p><img src="/2024/01/25/yolov8：火灾检测/3.png" alt></p><p><img src="/2024/01/25/yolov8：火灾检测/4.png" alt></p><h2 id="数据标注"><a href="#数据标注" class="headerlink" title="数据标注"></a>数据标注</h2><h3 id="标注工具：labelme"><a href="#标注工具：labelme" class="headerlink" title="标注工具：labelme"></a>标注工具：labelme</h3><p>对图像中的火焰和烟雾进行矩形标记，将数据的label分为两类：0-火焰；1-烟雾</p><p><img src="/2024/01/25/yolov8：火灾检测/5.png" alt></p><h3 id="数据格式转换"><a href="#数据格式转换" class="headerlink" title="数据格式转换"></a>数据格式转换</h3><p>将labelme数据格式转为yolo格式</p><h3 id="创建训练yaml文件"><a href="#创建训练yaml文件" class="headerlink" title="创建训练yaml文件"></a>创建训练yaml文件</h3><p>在数据集的根目录下新建<code>fire.yaml</code>文件用于记录训练集信息</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">train:</span> <span class="string">/exp/work/video/yolov8/datasets/fire/data/train/images</span> <span class="comment"># 训练集</span></span><br><span class="line"><span class="attr">val:</span> <span class="string">/exp/work/video/yolov8/datasets/fire/data/val/images</span> <span class="comment"># 验证集</span></span><br><span class="line"><span class="attr">test:</span> <span class="string">/exp/work/video/yolov8/datasets/fire/data/test/images</span> <span class="comment"># 测试集</span></span><br><span class="line"><span class="attr">nc:</span> <span class="number">2</span> <span class="comment"># 分类数量</span></span><br><span class="line"></span><br><span class="line"><span class="attr">names:</span> <span class="string">[fire,</span> <span class="string">smoke]</span> <span class="comment"># 类别名称</span></span><br></pre></td></tr></table></figure><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>在项目根目录下创建训练脚本<code>yolov8_train.py</code>，并设置双卡训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ultralytics <span class="keyword">import</span> YOLO</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line">model = YOLO(<span class="string">'yolov8n.pt'</span>)  <span class="comment"># 加载预训练模型</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 双卡训练</span></span><br><span class="line">model.train(</span><br><span class="line">    data=<span class="string">'datasets/fire/data/fire.yaml'</span>,</span><br><span class="line">    epochs=<span class="number">300</span>,</span><br><span class="line">    device=[<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动验证</span></span><br><span class="line">model.val()</span><br></pre></td></tr></table></figure><p>启动训练脚本，开启训练</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python yolov8_train.py</span><br></pre></td></tr></table></figure><p>打印配置信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Ultralytics YOLOv8.1.5 🚀 Python-3.9.18 torch-2.1.2+cu121 CUDA:0 (NVIDIA GeForce RTX 3090, 24260MiB)</span><br><span class="line">                                                          CUDA:1 (NVIDIA GeForce RTX 3090, 24260MiB)</span><br><span class="line">engine/trainer: task=detect, mode=train, model=yolov8n.pt, data=datasets/fire/data/fire.yaml, epochs=300, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=[0, 1], workers=8, project=None, name=train9, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train9</span><br><span class="line">Overriding model.yaml nc=80 with nc=2</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">                   from  n    params  module                                       arguments                     </span><br><span class="line">  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 </span><br><span class="line">  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                </span><br><span class="line">  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             </span><br><span class="line">  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                </span><br><span class="line">  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             </span><br><span class="line">  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               </span><br><span class="line">  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           </span><br><span class="line">  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              </span><br><span class="line">  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           </span><br><span class="line">  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 </span><br><span class="line"> 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          </span><br><span class="line"> 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           </span><br><span class="line"> 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 </span><br><span class="line"> 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          </span><br><span class="line"> 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           </span><br><span class="line"> 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  </span><br><span class="line"> 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                </span><br><span class="line"> 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           </span><br><span class="line"> 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 </span><br><span class="line"> 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              </span><br><span class="line"> 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           </span><br><span class="line"> 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 </span><br><span class="line"> 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           </span><br><span class="line">Model summary: 225 layers, 3011238 parameters, 3011222 gradients, 8.2 GFLOPs</span><br><span class="line"></span><br><span class="line">Transferred 319/355 items from pretrained weights</span><br><span class="line">DDP: debug command /root/anaconda3/envs/py39_yolov8/bin/python -m torch.distributed.run --nproc_per_node 2 --master_port 45027 /root/.config/Ultralytics/DDP/_temp_bfxihrb7140614302513712.py</span><br><span class="line">WARNING:__main__:</span><br><span class="line">*****************************************</span><br><span class="line">Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. </span><br><span class="line">*****************************************</span><br><span class="line">Ultralytics YOLOv8.1.5 🚀 Python-3.9.18 torch-2.1.2+cu121 CUDA:0 (NVIDIA GeForce RTX 3090, 24260MiB)</span><br><span class="line">                                                          CUDA:1 (NVIDIA GeForce RTX 3090, 24260MiB)</span><br><span class="line">Overriding model.yaml nc=80 with nc=2</span><br><span class="line">Transferred 319/355 items from pretrained weights</span><br><span class="line">Freezing layer 'model.22.dfl.conv.weight'</span><br><span class="line">AMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...</span><br><span class="line">AMP: checks passed ✅</span><br></pre></td></tr></table></figure><p>记录模型输出</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Plotting labels to runs/detect/train9/labels.jpg... </span><br><span class="line">optimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... </span><br><span class="line">optimizer: AdamW(lr=0.000714, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)</span><br><span class="line">Image sizes 640 train, 640 val</span><br><span class="line">Using 16 dataloader workers</span><br><span class="line">Logging results to runs/detect/train9</span><br></pre></td></tr></table></figure><p>训练开始</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">Starting training for 300 epochs...</span><br><span class="line"></span><br><span class="line">      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size</span><br><span class="line">      1/300      1.38G      2.132      4.599       2.13          6        640: 100%|██████████| 3/3 [00:04&lt;00:00,  1.53s/it]</span><br><span class="line">                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00&lt;00:00,  3.10it/s]</span><br><span class="line">                   all         41         62    0.00191      0.349     0.0223     0.0121</span><br><span class="line"></span><br><span class="line">      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size</span><br><span class="line">      2/300      1.44G      1.984      4.085      1.942         18        640: 100%|██████████| 3/3 [00:00&lt;00:00,  5.08it/s]</span><br><span class="line">                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00&lt;00:00, 11.57it/s]</span><br><span class="line">                   all         41         62    0.00209      0.373     0.0247     0.0119</span><br><span class="line"></span><br><span class="line">      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size</span><br><span class="line">      3/300      1.48G      2.421      4.407      2.348         14        640: 100%|██████████| 3/3 [00:00&lt;00:00,  5.78it/s]</span><br><span class="line">                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00&lt;00:00, 10.23it/s]</span><br><span class="line">                   all         41         62    0.00205      0.373     0.0335     0.0165</span><br><span class="line"></span><br><span class="line">      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size</span><br><span class="line">      4/300      1.48G      1.795      3.787      1.724         18        640: 100%|██████████| 3/3 [00:00&lt;00:00,  5.61it/s]</span><br><span class="line">                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00&lt;00:00,  9.82it/s]</span><br><span class="line">                   all         41         62    0.00239      0.434     0.0445     0.0219</span><br><span class="line"></span><br><span class="line">      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size</span><br><span class="line">      5/300      1.48G      1.707      3.619      1.785         17        640: 100%|██████████| 3/3 [00:00&lt;00:00,  5.49it/s]</span><br><span class="line">                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00&lt;00:00,  8.87it/s]</span><br><span class="line">                   all         41         62      0.003      0.495     0.0735     0.0283</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size</span><br><span class="line">    295/300       1.5G     0.3742     0.5917     0.8247          7        640: 100%|██████████| 3/3 [00:00&lt;00:00,  7.21it/s]</span><br><span class="line">                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00&lt;00:00, 11.40it/s]</span><br><span class="line">                   all         41         62      0.996          1      0.995      0.946</span><br><span class="line"></span><br><span class="line">      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size</span><br><span class="line">    296/300      1.44G     0.4263     0.6716     0.8004          7        640: 100%|██████████| 3/3 [00:00&lt;00:00,  6.86it/s]</span><br><span class="line">                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00&lt;00:00, 11.97it/s]</span><br><span class="line">                   all         41         62      0.996          1      0.995      0.945</span><br><span class="line"></span><br><span class="line">      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size</span><br><span class="line">    297/300       1.5G      0.473     0.6775     0.8499          7        640: 100%|██████████| 3/3 [00:00&lt;00:00,  7.20it/s]</span><br><span class="line">                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00&lt;00:00, 11.12it/s]</span><br><span class="line">                   all         41         62      0.996          1      0.995      0.945</span><br><span class="line"></span><br><span class="line">      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size</span><br><span class="line">    298/300      1.51G     0.4877     0.6956     0.8936         11        640: 100%|██████████| 3/3 [00:00&lt;00:00,  6.73it/s]</span><br><span class="line">                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00&lt;00:00, 11.03it/s]</span><br><span class="line">                   all         41         62      0.996          1      0.995      0.942</span><br><span class="line"></span><br><span class="line">      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size</span><br><span class="line">    299/300       1.5G     0.4303      0.622     0.8264          6        640: 100%|██████████| 3/3 [00:00&lt;00:00,  6.89it/s]</span><br><span class="line">                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00&lt;00:00, 11.62it/s]</span><br><span class="line">                   all         41         62      0.996          1      0.995      0.941</span><br><span class="line"></span><br><span class="line">      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size</span><br><span class="line">    300/300      1.44G     0.4593     0.6413     0.8419          6        640: 100%|██████████| 3/3 [00:00&lt;00:00,  6.31it/s]</span><br><span class="line">                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00&lt;00:00, 11.83it/s]</span><br><span class="line">                   all         41         62      0.995          1      0.995       0.94</span><br></pre></td></tr></table></figure><p>训练结束，验证</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">300 epochs completed in 0.101 hours.</span><br><span class="line">Optimizer stripped from runs/detect/train9/weights/last.pt, 6.3MB</span><br><span class="line">Optimizer stripped from runs/detect/train9/weights/best.pt, 6.3MB</span><br><span class="line"></span><br><span class="line">Validating runs/detect/train9/weights/best.pt...</span><br><span class="line">Ultralytics YOLOv8.1.5 🚀 Python-3.9.18 torch-2.1.2+cu121 CUDA:0 (NVIDIA GeForce RTX 3090, 24260MiB)</span><br><span class="line">                                                          CUDA:1 (NVIDIA GeForce RTX 3090, 24260MiB)</span><br><span class="line">Model summary (fused): 168 layers, 3006038 parameters, 0 gradients, 8.1 GFLOPs</span><br><span class="line">                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00&lt;00:00, 10.62it/s]</span><br><span class="line">                   all         41         62      0.996          1      0.995      0.946</span><br><span class="line">                  fire         41         41      0.999          1      0.995      0.938</span><br><span class="line">                 smoke         41         21      0.992          1      0.995      0.955</span><br><span class="line">Speed: 0.1ms preprocess, 1.4ms inference, 0.0ms loss, 1.0ms postprocess per image</span><br><span class="line">Results saved to runs/detect/train9</span><br><span class="line">Ultralytics YOLOv8.1.5 🚀 Python-3.9.18 torch-2.1.2+cu121 CUDA:0 (NVIDIA GeForce RTX 3090, 24260MiB)</span><br><span class="line">                                                          CUDA:1 (NVIDIA GeForce RTX 3090, 24260MiB)</span><br><span class="line">Model summary (fused): 168 layers, 3006038 parameters, 0 gradients, 8.1 GFLOPs</span><br><span class="line">                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:04&lt;00:00,  1.41s/it]</span><br><span class="line">                   all         41         62      0.996          1      0.995      0.941</span><br><span class="line">                  fire         41         41      0.999          1      0.995      0.939</span><br><span class="line">                 smoke         41         21      0.992          1      0.995      0.942</span><br><span class="line">Speed: 0.2ms preprocess, 14.3ms inference, 0.0ms loss, 16.8ms postprocess per image</span><br><span class="line">Results saved to runs/detect/train92</span><br></pre></td></tr></table></figure><h2 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h2><p>进入保存训练结果的文件夹内，其中<code>weights</code>文件夹包含了最佳训练模型和最近训练模型，文件夹外存储训练的各项记录和测试曲线等</p><p><img src="/2024/01/25/yolov8：火灾检测/6.png" alt></p><p><img src="/2024/01/25/yolov8：火灾检测/7.png" alt></p><p><code>result.csv</code>文件中记录了300轮epoch的详细结果</p><p><img src="/2024/01/25/yolov8：火灾检测/8.png" alt></p><h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><p>在项目根目录下创建预测脚本<code>yolov8_test.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ultralytics <span class="keyword">import</span> YOLO</span><br><span class="line"></span><br><span class="line">model = YOLO(<span class="string">'runs/detect/train9/weights/best.pt'</span>) <span class="comment"># 指定最佳模型</span></span><br><span class="line">model.predict(<span class="string">'video/fire.mp4'</span>, save=<span class="literal">True</span>, classes=[<span class="number">0</span>, <span class="number">1</span>]) <span class="comment"># 指定输出类别</span></span><br></pre></td></tr></table></figure><p>执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">video 1/1 (1/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 1 fire, 208.5ms</span><br><span class="line">video 1/1 (2/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 (no detections), 9.6ms</span><br><span class="line">video 1/1 (3/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 (no detections), 7.9ms</span><br><span class="line">video 1/1 (4/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 (no detections), 7.9ms</span><br><span class="line">video 1/1 (5/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 (no detections), 8.0ms</span><br><span class="line">video 1/1 (6/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 (no detections), 7.8ms</span><br><span class="line">video 1/1 (7/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 (no detections), 7.7ms</span><br><span class="line">video 1/1 (8/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 (no detections), 7.6ms</span><br><span class="line">video 1/1 (9/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 (no detections), 7.9ms</span><br><span class="line">video 1/1 (10/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 (no detections), 7.8ms</span><br><span class="line">video 1/1 (11/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 (no detections), 8.1ms</span><br><span class="line">video 1/1 (12/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 (no detections), 8.1ms</span><br><span class="line">video 1/1 (13/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 (no detections), 8.2ms</span><br><span class="line">video 1/1 (14/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 (no detections), 8.0ms</span><br><span class="line">video 1/1 (15/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 (no detections), 7.7ms</span><br><span class="line">video 1/1 (16/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 (no detections), 7.8ms</span><br><span class="line">video 1/1 (17/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 (no detections), 7.6ms</span><br><span class="line">video 1/1 (18/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 1 fire, 7.6ms</span><br><span class="line">video 1/1 (19/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 1 fire, 7.8ms</span><br><span class="line">video 1/1 (20/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 1 fire, 7.6ms</span><br><span class="line">video 1/1 (21/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 1 fire, 7.8ms</span><br><span class="line">video 1/1 (22/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 2 fires, 7.7ms</span><br><span class="line">video 1/1 (23/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 1 fire, 7.6ms</span><br><span class="line">video 1/1 (24/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 1 fire, 7.5ms</span><br><span class="line">video 1/1 (25/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 1 fire, 7.7ms</span><br><span class="line">video 1/1 (26/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 1 fire, 7.5ms</span><br><span class="line">video 1/1 (27/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 1 fire, 7.4ms</span><br><span class="line">video 1/1 (28/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 1 fire, 7.6ms</span><br><span class="line">video 1/1 (29/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 1 fire, 7.8ms</span><br><span class="line">video 1/1 (30/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 1 fire, 7.7ms</span><br><span class="line">...</span><br><span class="line">Speed: 2.2ms preprocess, 8.3ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)</span><br><span class="line">Results saved to runs/detect/predict13</span><br></pre></td></tr></table></figure><p><img src="/2024/01/25/yolov8：火灾检测/fire.gif" alt></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;环境&quot;&gt;&lt;a href=&quot;#环境&quot; class=&quot;headerlink&quot; title=&quot;环境&quot;&gt;&lt;/a&gt;环境&lt;/h2&gt;&lt;h3 id=&quot;GPU&quot;&gt;&lt;a href=&quot;#GPU&quot; class=&quot;headerlink&quot; title=&quot;GPU&quot;&gt;&lt;/a&gt;GPU&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;font color=&quot;gold&quot;&gt;NVIDIA 3090*2&lt;/font&gt;&lt;/li&gt;
&lt;li&gt;&lt;font color=&quot;gold&quot;&gt;显卡驱动 535.104.05&lt;/font&gt;&lt;/li&gt;
&lt;li&gt;&lt;font color=&quot;gold&quot;&gt;CUDA版本 12.2&lt;/font&gt;&lt;/li&gt;
&lt;li&gt;&lt;font color=&quot;gold&quot;&gt;CUDAtoolkit&lt;/font&gt; (cuda_12.2.2_535.104.05_linux)&lt;/li&gt;
&lt;li&gt;&lt;font color=&quot;gold&quot;&gt;cuDNN (v8.9.7)&lt;/font&gt;

&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;yolo版本&quot;&gt;&lt;a href=&quot;#yolo版本&quot; class=&quot;headerlink&quot; title=&quot;yolo版本&quot;&gt;&lt;/a&gt;yolo版本&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;font color=&quot;gold&quot;&gt;v8.1.5&lt;/font&gt; (ultralytics yolov8)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;pytorch版本&quot;&gt;&lt;a href=&quot;#pytorch版本&quot; class=&quot;headerlink&quot; title=&quot;pytorch版本&quot;&gt;&lt;/a&gt;pytorch版本&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;font color=&quot;gold&quot;&gt;v2.1.2&lt;/font&gt;

&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;python环境&quot;&gt;&lt;a href=&quot;#python环境&quot; class=&quot;headerlink&quot; title=&quot;python环境&quot;&gt;&lt;/a&gt;python环境&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;font color=&quot;gold&quot;&gt;CentOS7.9&lt;/font&gt;
&lt;/li&gt;
&lt;li&gt;&lt;font color=&quot;gold&quot;&gt;anaconda3&lt;/font&gt;
&lt;/li&gt;
&lt;li&gt;&lt;font color=&quot;gold&quot;&gt;python3.9&lt;/font&gt;

&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;安装&quot;&gt;&lt;a href=&quot;#安装&quot; class=&quot;headerlink&quot; title=&quot;安装&quot;&gt;&lt;/a&gt;安装&lt;/h2&gt;&lt;p&gt;源码主页：&lt;a href=&quot;https://github.com/ultralytics/ultralytics&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/ultralytics/ultralytics&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;官方文档：&lt;a href=&quot;https://docs.ultralytics.com/zh&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://docs.ultralytics.com/zh&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2024/01/25/yolov8：火灾检测/1.png&quot; alt&gt;&lt;/p&gt;
&lt;h3 id=&quot;克隆源码&quot;&gt;&lt;a href=&quot;#克隆源码&quot; class=&quot;headerlink&quot; title=&quot;克隆源码&quot;&gt;&lt;/a&gt;克隆源码&lt;/h3&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;git clone https://hub.nuaa.cf/ultralytics/ultralytics.git&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;安装依赖&quot;&gt;&lt;a href=&quot;#安装依赖&quot; class=&quot;headerlink&quot; title=&quot;安装依赖&quot;&gt;&lt;/a&gt;安装依赖&lt;/h3&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;pip install pip install ultralytics -i https://mirror.baidu.com/pypi/simple&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;环境验证&quot;&gt;&lt;a href=&quot;#环境验证&quot; class=&quot;headerlink&quot; title=&quot;环境验证&quot;&gt;&lt;/a&gt;环境验证&lt;/h3&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;yolo predict model=yolov8n.pt source=ultralytics/assets/zidane.jpg&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;执行完毕后得到输出的结果如下：&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;(py39_yolov8) [root@jdz yolov8]# yolo predict model=yolov8n.pt source=ultralytics/assets/zidane.jpg &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Ultralytics YOLOv8.1.5 🚀 Python-3.9.18 torch-2.1.2+cu121 CUDA:0 (NVIDIA GeForce RTX 3090, 24260MiB)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;image 1/1 /exp/work/video/yolov8/ultralytics/assets/zidane.jpg: 384x640 2 persons, 1 tie, 216.9ms&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Speed: 7.3ms preprocess, 216.9ms inference, 762.4ms postprocess per image at shape (1, 3, 384, 640)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Results saved to runs/detect/predict&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;💡 Learn more at https://docs.ultralytics.com/modes/predict&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;将在&lt;code&gt;Results saved to runs/detect/predict&lt;/code&gt;目录下找到输出结果&lt;/p&gt;</summary>
    
    
    
    <category term="目标检测" scheme="http://yoursite.com/categories/目标检测/"/>
    
    
    <category term="opencv" scheme="http://yoursite.com/tags/opencv/"/>
    
    <category term="yolov8" scheme="http://yoursite.com/tags/yolov8/"/>
    
  </entry>
  
  <entry>
    <title>NetworkX: 图论算法应用</title>
    <link href="http://yoursite.com/2024/01/08/NetworkX-%E5%9B%BE%E8%AE%BA%E7%AE%97%E6%B3%95%E5%BA%94%E7%94%A8/"/>
    <id>http://yoursite.com/2024/01/08/NetworkX-%E5%9B%BE%E8%AE%BA%E7%AE%97%E6%B3%95%E5%BA%94%E7%94%A8/</id>
    <published>2024-01-08T07:48:56.000Z</published>
    <updated>2024-01-25T08:23:02.432Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><h2 id="NetworkX"><a href="#NetworkX" class="headerlink" title="NetworkX"></a>NetworkX</h2><p>NetworkX是一款Python的软件包，用于创造、操作复杂网络，以及学习复杂网络的结构、动力学及其功能。有了NetworkX就可以用标准或者不标准的数据格式加载或者存储网络，它可以产生许多种类的随机网络或经典网络，也可以分析网络结构、建立网络模型、设计新的网络算法、绘制网络等</p><p>参考文献地址: <a href="https://www.osgeo.cn/networkx/reference/index.html" target="_blank" rel="noopener">https://www.osgeo.cn/networkx/reference/index.html</a></p><h2 id="图计算应用方式比较"><a href="#图计算应用方式比较" class="headerlink" title="图计算应用方式比较"></a>图计算应用方式比较</h2><h3 id="1-nebula-spark"><a href="#1-nebula-spark" class="headerlink" title="1.nebula + spark"></a>1.nebula + spark</h3><p>依赖nebula-spark-connector包、nebula-algorithm包和spark集群的数据读取、图计算方式</p><h3 id="2-clickhouse-NetworkX"><a href="#2-clickhouse-NetworkX" class="headerlink" title="2.clickhouse + NetworkX"></a>2.clickhouse + NetworkX</h3><p>由于nebula-algorithm依赖spark集群，且nebula-console原生的数据读取能力不佳，在环境受限且计算量有限的情况下优先考虑跳过spark集群和nebula图库，采用clickhouse + NetworkX的图计算方式，其中clickhouse是存储了nebula源数据的列式分布式表，作用类似于方法1中将nebula集群数据通过nebula-spark-connector包导入为spark-DataFrame，仅用做数据读取，再通过将数据转化为NetworkX的图结构进行图计算</p><p><img src="/2024/01/08/NetworkX-图论算法应用/9.png" alt></p><a id="more"></a><h2 id="应用实例"><a href="#应用实例" class="headerlink" title="应用实例"></a>应用实例</h2><p>nebula集群存储以群聊和群成员、好友关系所组成的关系网，clickhouse集群存储节点源数据和对应关系</p><h3 id="一、集合运算"><a href="#一、集合运算" class="headerlink" title="一、集合运算"></a>一、集合运算</h3><p>交、并、差集</p><p>直接使用clickhouse进行查询，适用于两个群组成员或账号所加群的交并差集</p><p><strong>输入：</strong></p><p><strong><font color="gold">·</font></strong> n个账号（n≥2）</p><p><strong><font color="gold">·</font></strong> n个群组（n≥2）</p><p><img src="/2024/01/08/NetworkX-图论算法应用/1.png" alt></p><h3 id="二、路径探寻"><a href="#二、路径探寻" class="headerlink" title="二、路径探寻"></a>二、路径探寻</h3><p>直接使用nebula进行查询，适用于搜索图谱中任意两个节点的最短可达路径</p><p><strong>输入：</strong></p><p><strong><font color="gold">·</font></strong> 任意类型2个节点</p><p><img src="/2024/01/08/NetworkX-图论算法应用/2.png" alt></p><h3 id="三、中心性"><a href="#三、中心性" class="headerlink" title="三、中心性"></a>三、中心性</h3><p><strong>输入：</strong></p><p><strong><font color="gold">·</font></strong> n个群组（n≥1），分析包括指定n个群组、群组群成员、群成员所加群组成关系网各节点的中心性</p><p><strong><font color="gold">·</font></strong> n个账号（n≥1），分析包括指定n个账号、账号所加群、群成员组成关系网各节点的中心性</p><p><strong><font color="gold">·</font></strong> 指定关系网，分析各节点中心性</p><p><strong>ClickHouse SQL示例（关系图谱）：</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span>  team_id,account_id <span class="keyword">FROM</span> mqv3.ly_team_ship lts </span><br><span class="line">PREWHERE account_id <span class="keyword">GLOBAL</span> <span class="keyword">IN</span> </span><br><span class="line">(<span class="keyword">SELECT</span> account_id <span class="keyword">FROM</span> mqv3.ly_team_ship lts PREWHERE team_id=<span class="string">'&#123;src_vid&#125;'</span>);</span><br></pre></td></tr></table></figure><h4 id="1-度中心性"><a href="#1-度中心性" class="headerlink" title="1.度中心性"></a>1.度中心性</h4><p>衡量节点中心性的指标</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">res = nx.degree_centrality(G)</span><br><span class="line">res = dict(sorted(res.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>))</span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure><p><strong>e.g.</strong> 以群（-1151413367、-1541749047）为指定节点，计算三级关系网各节点度中心性（抛弃度&lt;5的节点，突出结构）</p><p><img src="/2024/01/08/NetworkX-图论算法应用/3.png" alt></p><h4 id="2-接近中心性"><a href="#2-接近中心性" class="headerlink" title="2.接近中心性"></a>2.接近中心性</h4><p>反映在关系网络中某一节点与其他节点之间的接近程度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">res = nx.closeness_centrality(G)</span><br><span class="line">res = dict(sorted(res.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>))</span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure><p><strong>e.g.</strong> 以群（-1151413367、-1541749047）为指定节点，计算三级关系网各节点接近中心性（抛弃度&lt;50的节点，突出结构）</p><p><img src="/2024/01/08/NetworkX-图论算法应用/4.png" alt></p><h4 id="3-中介中心性"><a href="#3-中介中心性" class="headerlink" title="3.中介中心性"></a>3.中介中心性</h4><p>以经过某个节点的最短路径数目来刻画节点重要性的指标</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">res = nx.betweenness_centrality(G, k=<span class="number">1000</span>)</span><br><span class="line">res = dict(sorted(res.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>))</span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure><p><strong>e.g.</strong> 以群（-1151413367、-1541749047）为指定节点，计算三级关系网各节点中介中心性（抛弃度&lt;5的节点，突出结构）</p><p><img src="/2024/01/08/NetworkX-图论算法应用/5.png" alt></p><h4 id="4-特征向量中心性"><a href="#4-特征向量中心性" class="headerlink" title="4.特征向量中心性"></a>4.特征向量中心性</h4><p>关系网络中一个节点的重要性既取决于其邻居节点的数量（即该节点的度），也取决于其邻居节点的重要性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">res = nx.eigenvector_centrality(G)</span><br><span class="line">res = dict(sorted(res.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>))</span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure><p><strong>e.g.</strong> 以群（-1151413367、-1541749047）为指定节点，计算三级关系网各节点特征向量中心性（抛弃度&lt;5的节点，突出结构）</p><p><img src="/2024/01/08/NetworkX-图论算法应用/6.png" alt></p><h3 id="四、重要性"><a href="#四、重要性" class="headerlink" title="四、重要性"></a>四、重要性</h3><p><strong>输入：</strong></p><p><strong><font color="gold">·</font></strong> n个群组（n≥1），分析包括指定n个群组、群组群成员、群成员所加群组成关系网各节点的重要程度</p><p><strong><font color="gold">·</font></strong> n个账号（n≥1），分析包括指定n个账号、账号所加群、群成员组成关系网各节点的重要程度</p><p><strong><font color="gold">·</font></strong> 指定关系网，分析各节点的重要程度</p><h4 id="1-k核"><a href="#1-k核" class="headerlink" title="1.k核"></a>1.k核</h4><p>用于在图中寻找符合一定紧密关系条件（K）的子图结构的算法，要求每个顶点至少与该子图的其他K个顶点关联</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">res1 = nx.core_number(G)</span><br><span class="line">res2 = nx.k_core(G, <span class="number">6</span>)</span><br><span class="line">res1 = dict(sorted(res1.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>))</span><br><span class="line">print(res1)</span><br><span class="line">print(list(res2))</span><br></pre></td></tr></table></figure><p><strong>e.g.</strong> 以群（-1151413367、-1541749047）为指定节点，计算三级关系网各节点k-core值，结合louvain社区上色（全图）</p><p><img src="/2024/01/08/NetworkX-图论算法应用/7.png" alt></p><h4 id="2-PageRank"><a href="#2-PageRank" class="headerlink" title="2.PageRank"></a>2.PageRank</h4><p>衡量图中节点重要性的指标，值越高，图中访问该节点的概率越高</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">res = nx.pagerank(G, alpha=<span class="number">0.85</span>)</span><br><span class="line">res = dict(sorted(res.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>))</span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure><p><strong>e.g.1.</strong> 以群（-1151413367、-1541749047）为指定节点，计算三级关系网各节点pagerank值（抛弃度&lt;5的节点，突出结构）</p><p><img src="/2024/01/08/NetworkX-图论算法应用/8.png" alt></p><p><strong>e.g.2.</strong> 以群（-1151413367、-1541749047）为指定节点，计算三级关系网各节点pagerank值，结合louvain社区（全图）</p><p><img src="/2024/01/08/NetworkX-图论算法应用/9.png" alt></p><h3 id="五、聚类算法"><a href="#五、聚类算法" class="headerlink" title="五、聚类算法"></a>五、聚类算法</h3><p><strong>输入（社区发现）：</strong></p><p><strong><font color="gold">·</font></strong> n个群组（n≥1），划分包括指定n个群组、群组群成员、群成员所加群组成关系网的各个社区</p><p><strong><font color="gold">·</font></strong> n个账号（n≥1），划分包括指定n个账号、账号所加群、群成员组成关系网的各个社区</p><p><strong><font color="gold">·</font></strong> 划分指定关系网各节点的社区</p><p><strong>输入（三角形计数）：</strong></p><p><strong><font color="gold">·</font></strong> n个群组（n≥1），计算包括指定n个群组、群组群成员、群成员好友、群成员所加群组、群成员所加群组成关系网的三角形数量，挖掘团体关系</p><p><strong><font color="gold">·</font></strong> 计算指定关系网各节点的三角形数量，挖掘团体关系</p><p>用于计算图谱中三角关系数量，挖掘关系团体</p><p><strong>ClickHouse SQL示例（三角形计数）：</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">WITH</span> T <span class="keyword">AS</span>(<span class="comment">-- 指定群群成员</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> team_id,account_id <span class="keyword">FROM</span> mqv3.ly_team_ship PREWHERE team_id = <span class="string">'&#123;src_vid&#125;'</span></span><br><span class="line">),T1 <span class="keyword">AS</span>(<span class="comment">-- 指定群群成员的好友</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> account_id,fri_account_id <span class="keyword">FROM</span> mqv3.ly_account_ship PREWHERE account_id <span class="keyword">GLOBAL</span> <span class="keyword">IN</span> (<span class="keyword">SELECT</span> account_id <span class="keyword">FROM</span> T) </span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">DISTINCT</span> </span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> fri_account_id,account_id <span class="keyword">FROM</span> mqv3.ly_account_ship PREWHERE fri_account_id <span class="keyword">GLOBAL</span> <span class="keyword">IN</span> (<span class="keyword">SELECT</span> account_id <span class="keyword">FROM</span> T)</span><br><span class="line">),T2 <span class="keyword">AS</span>(<span class="comment">-- 群成员所加群</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> team_id,account_id <span class="keyword">FROM</span> mqv3.ly_team_ship PREWHERE account_id <span class="keyword">GLOBAL</span> <span class="keyword">IN</span> (<span class="keyword">SELECT</span> account_id <span class="keyword">FROM</span> T)</span><br><span class="line">),T3 <span class="keyword">AS</span>(<span class="comment">-- 群成员所加群和群成员和群成员的好友</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> team_id,account_id,fri_account_id <span class="keyword">FROM</span> T2 <span class="keyword">RIGHT</span> <span class="keyword">JOIN</span> T1 <span class="keyword">ON</span> T2.account_id = T1.account_id</span><br><span class="line">),T4 <span class="keyword">AS</span>(<span class="comment">-- 群成员好友所加群</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> team_id,account_id <span class="keyword">AS</span> fri_account_id <span class="keyword">FROM</span> mqv3.ly_team_ship lts PREWHERE lts.account_id <span class="keyword">GLOBAL</span> <span class="keyword">IN</span> (<span class="keyword">SELECT</span> fri_account_id <span class="keyword">FROM</span> T1)</span><br><span class="line">),T5 <span class="keyword">AS</span>(<span class="comment">-- 共同所加群</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> team_id,account_id,fri_account_id <span class="keyword">FROM</span> T3 <span class="keyword">JOIN</span> T4 <span class="keyword">ON</span> (T3.team_id = T4.team_id <span class="keyword">AND</span> T3.fri_account_id = T4.fri_account_id)</span><br><span class="line">)</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">DISTINCT</span> team_id,account_id,fri_account_id <span class="keyword">from</span> T5</span><br></pre></td></tr></table></figure><h4 id="1-标签传播（LPA社区发现）"><a href="#1-标签传播（LPA社区发现）" class="headerlink" title="1.标签传播（LPA社区发现）"></a>1.标签传播（LPA社区发现）</h4><p>基于lpa的社区划分</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">res = nx.algorithms.community.label_propagation_communities(G)</span><br><span class="line">print(list(res))</span><br></pre></td></tr></table></figure><p><strong>e.g.</strong>以群（-1151413367、-1541749047）为指定节点，划分lpa标签传播社区（抛弃度&lt;5的节点，突出结构）</p><p><img src="/2024/01/08/NetworkX-图论算法应用/10.png" alt></p><h4 id="2-鲁汶（Louvain社区发现）"><a href="#2-鲁汶（Louvain社区发现）" class="headerlink" title="2.鲁汶（Louvain社区发现）"></a>2.鲁汶（Louvain社区发现）</h4><p>基于louvain的社区划分</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">res = nx.algorithms.community.louvain_communities(G, resolution=<span class="number">0.5</span>)</span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure><p><strong>e.g.1.</strong>以群（-1151413367、-1541749047）为指定节点，划分louvain鲁汶社区（resolution=1，抛弃度&lt;5的节点，突出结构）</p><p><img src="/2024/01/08/NetworkX-图论算法应用/11.png" alt></p><p><strong>e.g.2.</strong>以群（-1151413367、-1541749047）为指定节点，划分louvain鲁汶社区（resolution=0.3，抛弃度&lt;5的节点，突出结构）</p><p><img src="/2024/01/08/NetworkX-图论算法应用/12.png" alt></p><h4 id="3-三角形计数"><a href="#3-三角形计数" class="headerlink" title="3.三角形计数"></a>3.三角形计数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">res = nx.triangles(G)</span><br><span class="line">res = dict(sorted(res.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>))</span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure><p><strong>e.g.</strong>以群（-1151413367、-1541749047）为指定节点，计算关系网中各个节点三角形数量（抛弃度&lt;5的节点，突出结构）</p><p><img src="/2024/01/08/NetworkX-图论算法应用/13.png" alt></p><h3 id="六、连通性"><a href="#六、连通性" class="headerlink" title="六、连通性"></a>六、连通性</h3><p><strong>输入：</strong></p><p><strong><font color="gold">·</font></strong> 指定n个群组（n≥2），判断群组、群成员、群成员所加群所组关系网是否为连通图</p><p><strong><font color="gold">·</font></strong> 指定n个账号（n≥2），判断账号、账号所加群、群成员所组关系网是否为连通图</p><p><strong><font color="gold">·</font></strong> 指定关系网</p><h4 id="1-连通性检测"><a href="#1-连通性检测" class="headerlink" title="1.连通性检测"></a>1.连通性检测</h4><p>判断目标图谱是否为连通图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">res1 = nx.is_connected(G)</span><br><span class="line">print(res1)</span><br></pre></td></tr></table></figure><p><strong>e.g.1.</strong>连通图</p><p><img src="/2024/01/08/NetworkX-图论算法应用/14.png" alt></p><p><strong>e.g.2.</strong>非连通图</p><p><img src="/2024/01/08/NetworkX-图论算法应用/15.png" alt></p><h4 id="2-联通组件数"><a href="#2-联通组件数" class="headerlink" title="2.联通组件数"></a>2.联通组件数</h4><p>判断目标图谱中联通组件的数量（n）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">res2 = nx.number_connected_components(G)</span><br><span class="line">print(res2)</span><br></pre></td></tr></table></figure><h4 id="3-联通组件"><a href="#3-联通组件" class="headerlink" title="3.联通组件"></a>3.联通组件</h4><p>显示目标图谱中的全部联通组件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">res3 = nx.connected_components(G)</span><br><span class="line">print(res3)</span><br></pre></td></tr></table></figure><h4 id="4-指定节点联通组件"><a href="#4-指定节点联通组件" class="headerlink" title="4.指定节点联通组件"></a>4.指定节点联通组件</h4><p>显示目标图谱中指定节点所在的联通组件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">res4 = nx.node_connected_component(G, <span class="string">"sample_node"</span>)</span><br><span class="line">print(res4)</span><br></pre></td></tr></table></figure><h3 id="七、几何"><a href="#七、几何" class="headerlink" title="七、几何"></a>七、几何</h3><p><strong>输入：</strong></p><p><strong><font color="gold">·</font></strong> 指定n个群组（n≥2），计算群组、群成员、群成员所加群所组关系网的中心、重心</p><p><strong><font color="gold">·</font></strong> 指定n个账号（n≥2），计算账号、账号所加群、群成员所组关系网的中心、重心</p><p><strong><font color="gold">·</font></strong> 计算指定关系网几何中心、重心</p><h4 id="1-几何重心"><a href="#1-几何重心" class="headerlink" title="1.几何重心"></a>1.几何重心</h4><p>图的重心节点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">res1 = nx.barycenter(G)</span><br><span class="line">print(res1)</span><br></pre></td></tr></table></figure><h4 id="2-几何中心"><a href="#2-几何中心" class="headerlink" title="2.几何中心"></a>2.几何中心</h4><p>图的中心节点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">res2 = nx.center(G)</span><br><span class="line">print(res2)</span><br></pre></td></tr></table></figure><h2 id="绘图"><a href="#绘图" class="headerlink" title="绘图"></a>绘图</h2><p><strong>e.g.</strong> 中心中心性 + LPA标签传播算法聚类</p><p>数据源：<a href="https://www.inetbio.org/wormnet/downloadnetwork.php" target="_blank" rel="noopener">https://www.inetbio.org/wormnet/downloadnetwork.php</a></p><p><strong><font color="gold">·</font></strong> WormNet v.3-GS (<a href="https://www.inetbio.org/wormnet/download.php?type=2" target="_blank" rel="noopener">https://www.inetbio.org/wormnet/download.php?type=2</a>)</p><p> 完整代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> sample</span><br><span class="line"><span class="keyword">import</span> networkx <span class="keyword">as</span> nx</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Gold standard data of positive gene functional associations</span></span><br><span class="line"><span class="comment"># from https://www.inetbio.org/wormnet/downloadnetwork.php</span></span><br><span class="line">G = nx.read_edgelist(<span class="string">"WormNet.v3.benchmark.txt"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># remove randomly selected nodes (to make example fast)</span></span><br><span class="line">num_to_remove = int(len(G) / <span class="number">1.5</span>)</span><br><span class="line">nodes = sample(list(G.nodes), num_to_remove)</span><br><span class="line">G.remove_nodes_from(nodes)</span><br><span class="line"></span><br><span class="line"><span class="comment"># remove low-degree nodes</span></span><br><span class="line">low_degree = [n <span class="keyword">for</span> n, d <span class="keyword">in</span> G.degree() <span class="keyword">if</span> d &lt; <span class="number">10</span>]</span><br><span class="line">G.remove_nodes_from(low_degree)</span><br><span class="line"></span><br><span class="line"><span class="comment"># largest connected component</span></span><br><span class="line">components = nx.connected_components(G)</span><br><span class="line">largest_component = max(components, key=len)</span><br><span class="line">H = G.subgraph(largest_component)</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute centrality</span></span><br><span class="line">centrality = nx.betweenness_centrality(H, k=<span class="number">10</span>, endpoints=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute community structure</span></span><br><span class="line">lpc = nx.community.label_propagation_communities(H)</span><br><span class="line">community_index = &#123;n: i <span class="keyword">for</span> i, com <span class="keyword">in</span> enumerate(lpc) <span class="keyword">for</span> n <span class="keyword">in</span> com&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#### draw graph ####</span></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">20</span>, <span class="number">15</span>))</span><br><span class="line">pos = nx.spring_layout(H, k=<span class="number">0.15</span>, seed=<span class="number">4572321</span>)</span><br><span class="line">node_color = [community_index[n] <span class="keyword">for</span> n <span class="keyword">in</span> H]</span><br><span class="line">node_size = [v * <span class="number">20000</span> <span class="keyword">for</span> v <span class="keyword">in</span> centrality.values()]</span><br><span class="line">nx.draw_networkx(</span><br><span class="line">    H,</span><br><span class="line">    pos=pos,</span><br><span class="line">    with_labels=<span class="literal">False</span>,</span><br><span class="line">    node_color=node_color,</span><br><span class="line">    node_size=node_size,</span><br><span class="line">    edge_color=<span class="string">"gainsboro"</span>,</span><br><span class="line">    alpha=<span class="number">0.4</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Title/legend</span></span><br><span class="line">font = &#123;<span class="string">"color"</span>: <span class="string">"k"</span>, <span class="string">"fontweight"</span>: <span class="string">"bold"</span>, <span class="string">"fontsize"</span>: <span class="number">20</span>&#125;</span><br><span class="line">ax.set_title(<span class="string">"Gene functional association network (C. elegans)"</span>, font)</span><br><span class="line"><span class="comment"># Change font color for legend</span></span><br><span class="line">font[<span class="string">"color"</span>] = <span class="string">"r"</span></span><br><span class="line"></span><br><span class="line">ax.text(</span><br><span class="line">    <span class="number">0.80</span>,</span><br><span class="line">    <span class="number">0.10</span>,</span><br><span class="line">    <span class="string">"node color = community structure"</span>,</span><br><span class="line">    horizontalalignment=<span class="string">"center"</span>,</span><br><span class="line">    transform=ax.transAxes,</span><br><span class="line">    fontdict=font,</span><br><span class="line">)</span><br><span class="line">ax.text(</span><br><span class="line">    <span class="number">0.80</span>,</span><br><span class="line">    <span class="number">0.06</span>,</span><br><span class="line">    <span class="string">"node size = betweeness centrality"</span>,</span><br><span class="line">    horizontalalignment=<span class="string">"center"</span>,</span><br><span class="line">    transform=ax.transAxes,</span><br><span class="line">    fontdict=font,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Resize figure for label readibility</span></span><br><span class="line">ax.margins(<span class="number">0.1</span>, <span class="number">0.05</span>)</span><br><span class="line">fig.tight_layout()</span><br><span class="line">plt.axis(<span class="string">"off"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2024/01/08/NetworkX-图论算法应用/16.png" alt></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;NetworkX&quot;&gt;&lt;a href=&quot;#NetworkX&quot; class=&quot;headerlink&quot; title=&quot;NetworkX&quot;&gt;&lt;/a&gt;NetworkX&lt;/h2&gt;&lt;p&gt;NetworkX是一款Python的软件包，用于创造、操作复杂网络，以及学习复杂网络的结构、动力学及其功能。有了NetworkX就可以用标准或者不标准的数据格式加载或者存储网络，它可以产生许多种类的随机网络或经典网络，也可以分析网络结构、建立网络模型、设计新的网络算法、绘制网络等&lt;/p&gt;
&lt;p&gt;参考文献地址: &lt;a href=&quot;https://www.osgeo.cn/networkx/reference/index.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.osgeo.cn/networkx/reference/index.html&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;图计算应用方式比较&quot;&gt;&lt;a href=&quot;#图计算应用方式比较&quot; class=&quot;headerlink&quot; title=&quot;图计算应用方式比较&quot;&gt;&lt;/a&gt;图计算应用方式比较&lt;/h2&gt;&lt;h3 id=&quot;1-nebula-spark&quot;&gt;&lt;a href=&quot;#1-nebula-spark&quot; class=&quot;headerlink&quot; title=&quot;1.nebula + spark&quot;&gt;&lt;/a&gt;1.nebula + spark&lt;/h3&gt;&lt;p&gt;依赖nebula-spark-connector包、nebula-algorithm包和spark集群的数据读取、图计算方式&lt;/p&gt;
&lt;h3 id=&quot;2-clickhouse-NetworkX&quot;&gt;&lt;a href=&quot;#2-clickhouse-NetworkX&quot; class=&quot;headerlink&quot; title=&quot;2.clickhouse + NetworkX&quot;&gt;&lt;/a&gt;2.clickhouse + NetworkX&lt;/h3&gt;&lt;p&gt;由于nebula-algorithm依赖spark集群，且nebula-console原生的数据读取能力不佳，在环境受限且计算量有限的情况下优先考虑跳过spark集群和nebula图库，采用clickhouse + NetworkX的图计算方式，其中clickhouse是存储了nebula源数据的列式分布式表，作用类似于方法1中将nebula集群数据通过nebula-spark-connector包导入为spark-DataFrame，仅用做数据读取，再通过将数据转化为NetworkX的图结构进行图计算&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2024/01/08/NetworkX-图论算法应用/9.png&quot; alt&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="NetworkX" scheme="http://yoursite.com/categories/NetworkX/"/>
    
    
    <category term="nebula" scheme="http://yoursite.com/tags/nebula/"/>
    
    <category term="图算法" scheme="http://yoursite.com/tags/图算法/"/>
    
    <category term="networkx" scheme="http://yoursite.com/tags/networkx/"/>
    
  </entry>
  
  <entry>
    <title>CentOS-LibreOffice工具包安装</title>
    <link href="http://yoursite.com/2023/12/28/CentOS-LibreOffice%E5%B7%A5%E5%85%B7%E5%8C%85%E5%AE%89%E8%A3%85/"/>
    <id>http://yoursite.com/2023/12/28/CentOS-LibreOffice%E5%B7%A5%E5%85%B7%E5%8C%85%E5%AE%89%E8%A3%85/</id>
    <published>2023-12-28T06:43:49.000Z</published>
    <updated>2023-12-28T07:05:48.623Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><strong>·</strong> <font color="gold">系统：</font> CentOS7</p><p><strong>·</strong> <font color="gold">LibreOffice：</font> 7.4.5.1 稳定版</p><h2 id="资源下载"><a href="#资源下载" class="headerlink" title="资源下载"></a>资源下载</h2><p><strong>·</strong> 官方网站： <a href="https://zh-cn.libreoffice.org/download/libreoffice/" target="_blank" rel="noopener">https://zh-cn.libreoffice.org/download/libreoffice/</a></p><p><strong>·</strong> 下载地址：<a href="https://downloadarchive.documentfoundation.org/libreoffice/old/7.4.5.1/rpm/x86_64/" target="_blank" rel="noopener">https://downloadarchive.documentfoundation.org/libreoffice/old/7.4.5.1/rpm/x86_64/</a></p><p>选择<code>LibreOffice_7.4.5.1_Linux_x86-64_rpm.tar.gz</code>安装包和<code>LibreOffice_7.4.5.1_Linux_x86-64_rpm_langpack_zh-CN.tar.gz</code>中文语言包并下载</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>进入安装包下载目录进行解压，这里为<code>/usr/local/</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/   进入目录</span><br><span class="line">tar -zxvf LibreOffice_7.4.6.1_Linux_x86-64_rpm.tar.gz   解压libreoffice</span><br><span class="line">tar -zxvf LibreOffice7.4.6.1_Linux_x86-64_rpm_langpack_zh-CN.tar.gz   解压中文语言包</span><br></pre></td></tr></table></figure><p>安装<code>libreoffice</code>和语言包的rpm包，默认安装目录为<code>/opt/libreoffice7.4</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/LibreOffice_7.4.6.1_Linux_x86-64_rpm/RPMS/</span><br><span class="line">yum -y install *.rpm</span><br><span class="line">cd /usr/local/LibreOffice_7.4.6.1_Linux_x86-64_rpm_langpack_zh-CN/RPMS</span><br><span class="line">yum -y install *.rpm</span><br></pre></td></tr></table></figure><p>安装<code>soffice</code>，进入<code>/opt/libreoffice7.4/program</code>目录执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/libreoffice7.4/program/</span><br><span class="line">yum install cairo </span><br><span class="line">yum install cups-libs</span><br><span class="line">yum install libSM</span><br></pre></td></tr></table></figure><p>检查</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/opt/libreoffice7.4/program/soffice -help</span><br></pre></td></tr></table></figure><p>正常输出，安装成功，接下来将<code>soffice</code>添加到环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> libreoffice</span><br><span class="line">export LibreOffice_PATH=/opt/libreoffice7.4/program</span><br><span class="line">export PATH=$LibreOffice_PATH:$PATH</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="安装中文字体包"><a href="#安装中文字体包" class="headerlink" title="安装中文字体包"></a>安装中文字体包</h2><p>windows系统字体包位于路径<code>C:\Windows\Font</code>下</p><p>centos系统字体包位于路径<code>/usr/share/fonts</code>下</p><p>进入<code>/usr/share/fonts</code>创建windows系统字体包目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/share/fonts/</span><br><span class="line">mkdir windowsFont</span><br></pre></td></tr></table></figure><p>将windows字体文件全部拷贝进该路径下，执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkfontscale</span><br><span class="line">mkfontdir</span><br><span class="line">fc-cache</span><br></pre></td></tr></table></figure><p>完成！可以在linux系统下顺利操作<code>.docs</code>、<code>.pdf</code>等文档文件，在langchain-chatchat中添加文档形知识库。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;·&lt;/strong&gt; &lt;font color=&quot;gold&quot;&gt;系统：&lt;/font&gt; CentOS7&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;·&lt;/strong&gt; &lt;font color=&quot;gold&quot;&gt;LibreOffice：&lt;/font&gt; 7.4.5.1 稳定版&lt;/p&gt;
&lt;h2 id=&quot;资源下载&quot;&gt;&lt;a href=&quot;#资源下载&quot; class=&quot;headerlink&quot; title=&quot;资源下载&quot;&gt;&lt;/a&gt;资源下载&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;·&lt;/strong&gt; 官方网站： &lt;a href=&quot;https://zh-cn.libreoffice.org/download/libreoffice/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://zh-cn.libreoffice.org/download/libreoffice/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;·&lt;/strong&gt; 下载地址：&lt;a href=&quot;https://downloadarchive.documentfoundation.org/libreoffice/old/7.4.5.1/rpm/x86_64/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://downloadarchive.documentfoundation.org/libreoffice/old/7.4.5.1/rpm/x86_64/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;选择&lt;code&gt;LibreOffice_7.4.5.1_Linux_x86-64_rpm.tar.gz&lt;/code&gt;安装包和&lt;code&gt;LibreOffice_7.4.5.1_Linux_x86-64_rpm_langpack_zh-CN.tar.gz&lt;/code&gt;中文语言包并下载&lt;/p&gt;
&lt;h2 id=&quot;安装&quot;&gt;&lt;a href=&quot;#安装&quot; class=&quot;headerlink&quot; title=&quot;安装&quot;&gt;&lt;/a&gt;安装&lt;/h2&gt;&lt;p&gt;进入安装包下载目录进行解压，这里为&lt;code&gt;/usr/local/&lt;/code&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;cd /usr/local/   进入目录&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;tar -zxvf LibreOffice_7.4.6.1_Linux_x86-64_rpm.tar.gz   解压libreoffice&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;tar -zxvf LibreOffice7.4.6.1_Linux_x86-64_rpm_langpack_zh-CN.tar.gz   解压中文语言包&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;安装&lt;code&gt;libreoffice&lt;/code&gt;和语言包的rpm包，默认安装目录为&lt;code&gt;/opt/libreoffice7.4&lt;/code&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;cd /usr/local/LibreOffice_7.4.6.1_Linux_x86-64_rpm/RPMS/&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;yum -y install *.rpm&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;cd /usr/local/LibreOffice_7.4.6.1_Linux_x86-64_rpm_langpack_zh-CN/RPMS&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;yum -y install *.rpm&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;安装&lt;code&gt;soffice&lt;/code&gt;，进入&lt;code&gt;/opt/libreoffice7.4/program&lt;/code&gt;目录执行&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;cd /opt/libreoffice7.4/program/&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;yum install cairo &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;yum install cups-libs&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;yum install libSM&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;检查&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;/opt/libreoffice7.4/program/soffice -help&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;正常输出，安装成功，接下来将&lt;code&gt;soffice&lt;/code&gt;添加到环境变量&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;vim /etc/profile&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;/span&gt; libreoffice&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;export LibreOffice_PATH=/opt/libreoffice7.4/program&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;export PATH=$LibreOffice_PATH:$PATH&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;source /etc/profile&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
    <category term="linux工具" scheme="http://yoursite.com/tags/linux工具/"/>
    
    <category term="LibreOffice" scheme="http://yoursite.com/tags/LibreOffice/"/>
    
  </entry>
  
  <entry>
    <title>LangChain + ChatGLM2-6B的本地知识问答库</title>
    <link href="http://yoursite.com/2023/10/24/LangChain%20+%20ChatGLM2-6B%E7%9A%84%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%BA%93/"/>
    <id>http://yoursite.com/2023/10/24/LangChain%20+%20ChatGLM2-6B%E7%9A%84%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%BA%93/</id>
    <published>2023-10-24T01:14:34.000Z</published>
    <updated>2023-11-06T08:31:28.442Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p>原项目Github：<a href="https://github.com/imClumsyPanda/langchain-ChatGLM" target="_blank" rel="noopener">https://github.com/imClumsyPanda/langchain-ChatGLM</a></p><h2 id="项目部署"><a href="#项目部署" class="headerlink" title="项目部署"></a>项目部署</h2><p><strong>·</strong> <font color="gold">v 0.2.6</font></p><h3 id="机器配置："><a href="#机器配置：" class="headerlink" title="机器配置："></a>机器配置：</h3><p><strong>·</strong> <font color="gold">python 环境：anaconda3 + python3.10.12</font></p><p><strong>·</strong> <font color="gold">GPU：RTX3090*2 + CUDA11.7</font></p><p><strong>·</strong> <font color="gold">torch：2.0.1（CUDA未升至12）</font></p><p><strong>·</strong> <font color="gold">conda：py310_dtglm</font></p><h3 id="模型下载"><a href="#模型下载" class="headerlink" title="模型下载"></a>模型下载</h3><p><strong>·</strong> <font color="gold">m3e</font> <a href="https://huggingface.co/moka-ai/m3e-base/tree/main" target="_blank" rel="noopener">https://huggingface.co/moka-ai/m3e-base/tree/main</a></p><p><strong>·</strong> <font color="gold">chatglm2-6b</font> <a href="https://huggingface.co/THUDM/chatglm2-6b/tree/main" target="_blank" rel="noopener">https://huggingface.co/THUDM/chatglm2-6b/tree/main</a></p><p>chatglm清华源 <a href="https://cloud.tsinghua.edu.cn/d/674208019e314311ab5c/?p=%2F&amp;mode=list" target="_blank" rel="noopener">https://cloud.tsinghua.edu.cn/d/674208019e314311ab5c/?p=%2F&amp;mode=list</a></p><p>(这里将模型全部下载至<code>/root/huggingface</code>下)</p><h3 id="创建虚拟环境，安装依赖"><a href="#创建虚拟环境，安装依赖" class="headerlink" title="创建虚拟环境，安装依赖"></a>创建虚拟环境，安装依赖</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">conda create -n py310_dtglm python=3.10.12</span><br><span class="line">conda activate py310_dtglm</span><br><span class="line"></span><br><span class="line">pip install --use-pep517 -r requirements.txt -i https://mirror.baidu.com/pypi/simple</span><br><span class="line">pip install --use-pep517 -r requirements_api.txt -i https://mirror.baidu.com/pypi/simple</span><br><span class="line">pip install --use-pep517 -r requirements_webui.txt -i https://mirror.baidu.com/pypi/simple</span><br></pre></td></tr></table></figure><h3 id="修改配置、模型路径"><a href="#修改配置、模型路径" class="headerlink" title="修改配置、模型路径"></a>修改配置、模型路径</h3><p>复制配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python copy_config_example.py</span><br></pre></td></tr></table></figure><p>修改配置文件</p><p><strong>·</strong> <font color="gold">model_config.py</font></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">MODEL_ROOT_PATH = <span class="string">"/root/huggingface"</span></span><br><span class="line"></span><br><span class="line">MODEL_PATH = &#123;</span><br><span class="line">    <span class="string">"embed_model"</span>: &#123;</span><br><span class="line">...</span><br><span class="line">        <span class="string">"m3e-base"</span>: <span class="string">"/root/huggingface/m3e-base"</span>, <span class="comment"># 修改m3e模型路径</span></span><br><span class="line">...</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> add all supported llm models</span></span><br><span class="line">    <span class="string">"llm_model"</span>: &#123;</span><br><span class="line">...</span><br><span class="line">        <span class="string">"chatglm2-6b"</span>: <span class="string">"/root/huggingface/chatglm2-6b"</span>, <span class="comment"># 修改chatglm2-6b模型路径</span></span><br><span class="line">...</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">EMBEDDING_MODEL = <span class="string">"m3e-base"</span> <span class="comment"># 可以尝试最新的嵌入式sota模型：bge-large-zh-v1.5</span></span><br><span class="line">LLM_MODEL = <span class="string">"chatglm2-6b"</span></span><br></pre></td></tr></table></figure><a id="more"></a><p><strong>·</strong> <font color="gold">server_config.py</font></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># webui.py server</span></span><br><span class="line">WEBUI_SERVER = &#123;</span><br><span class="line">    <span class="string">"host"</span>: DEFAULT_BIND_HOST,</span><br><span class="line">    <span class="string">"port"</span>: <span class="number">8501</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># api.py server</span></span><br><span class="line">API_SERVER = &#123;</span><br><span class="line">    <span class="string">"host"</span>: DEFAULT_BIND_HOST,</span><br><span class="line">    <span class="string">"port"</span>: <span class="number">7861</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># fastchat openai_api server</span></span><br><span class="line">FSCHAT_OPENAI_API = &#123;</span><br><span class="line">    <span class="string">"host"</span>: DEFAULT_BIND_HOST,</span><br><span class="line">    <span class="string">"port"</span>: <span class="number">20000</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">FSCHAT_MODEL_WORKERS = &#123;</span><br><span class="line">    <span class="string">"default"</span>: &#123;</span><br><span class="line">        <span class="string">"host"</span>: DEFAULT_BIND_HOST,</span><br><span class="line">        <span class="string">"port"</span>: <span class="number">20002</span>,</span><br><span class="line">        <span class="string">"device"</span>: LLM_DEVICE,</span><br><span class="line">        <span class="string">"infer_turbo"</span>: <span class="literal">False</span>,</span><br><span class="line"></span><br><span class="line">        <span class="comment"># model_worker多卡加载需要配置的参数</span></span><br><span class="line">        <span class="string">"gpus"</span>: <span class="string">"0,1"</span>, <span class="comment"># 使用的GPU，以str的格式指定，如"0,1"，如失效请使用CUDA_VISIBLE_DEVICES="0,1"等形式指定</span></span><br><span class="line">        <span class="string">"num_gpus"</span>: <span class="number">2</span>, <span class="comment"># 使用GPU的数量</span></span><br><span class="line">        <span class="string">"max_gpu_memory"</span>: <span class="string">"20GiB"</span>, <span class="comment"># 每个GPU占用的最大显存</span></span><br><span class="line"></span><br><span class="line">    &#125;,</span><br><span class="line"></span><br><span class="line">    <span class="string">"zhipu-api"</span>: &#123; <span class="comment"># 请为每个要运行的在线API设置不同的端口</span></span><br><span class="line">        <span class="string">"port"</span>: <span class="number">21001</span>,</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># fastchat controller server</span></span><br><span class="line">FSCHAT_CONTROLLER = &#123;</span><br><span class="line">    <span class="string">"host"</span>: DEFAULT_BIND_HOST,</span><br><span class="line">    <span class="string">"port"</span>: <span class="number">20001</span>,</span><br><span class="line">    <span class="string">"dispatch_method"</span>: <span class="string">"shortest_queue"</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="初始化默认知识库"><a href="#初始化默认知识库" class="headerlink" title="初始化默认知识库"></a>初始化默认知识库</h3><p>样例知识库文件位置：<code>knowledge_base/samples/content/test.txt</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python init_database.py --recreate-vs</span><br></pre></td></tr></table></figure><h3 id="启动项目"><a href="#启动项目" class="headerlink" title="启动项目"></a>启动项目</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python startup.py -a</span><br></pre></td></tr></table></figure><p><img src="/2023/10/24/LangChain + ChatGLM2-6B的本地知识问答库/A.png" alt></p><h3 id="通过fastapi接口添加知识库"><a href="#通过fastapi接口添加知识库" class="headerlink" title="通过fastapi接口添加知识库"></a>通过fastapi接口添加知识库</h3><p><code>http://host:7861/knowledge_base/upload_docs</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">curl -X 'POST' \</span><br><span class="line">  'http://host:7861/knowledge_base/upload_docs' \</span><br><span class="line">  -H 'accept: application/json' \</span><br><span class="line">  -H 'Content-Type: multipart/form-data' \</span><br><span class="line">  -F 'to_vector_store=true' \</span><br><span class="line">  -F 'override=false' \</span><br><span class="line">  -F 'not_refresh_vs_cache=false' \</span><br><span class="line">  -F 'chunk_size=250' \</span><br><span class="line">  -F 'chunk_overlap=50' \</span><br><span class="line">  -F 'zh_title_enhance=true' \</span><br><span class="line">  -F 'files=@分体式M录AI智能分析设备建设方案.docx;type=application/vnd.openxmlformats-officedocument.wordprocessingml.document' \</span><br><span class="line">  -F 'knowledge_base_name=琅琊' \</span><br><span class="line">  -F 'docs='</span><br></pre></td></tr></table></figure><p><img src="/2023/10/24/LangChain + ChatGLM2-6B的本地知识问答库/B.png" alt></p><h3 id="选择知识库问答"><a href="#选择知识库问答" class="headerlink" title="选择知识库问答"></a>选择知识库问答</h3><p><img src="/2023/10/24/LangChain + ChatGLM2-6B的本地知识问答库/C.png" alt></p><p><img src="/2023/10/24/LangChain + ChatGLM2-6B的本地知识问答库/D.png" alt></p><h2 id="代码调整"><a href="#代码调整" class="headerlink" title="代码调整"></a>代码调整</h2><h3 id="百川大模型接入"><a href="#百川大模型接入" class="headerlink" title="百川大模型接入"></a>百川大模型接入</h3><p>调整<code>./configs/model_config.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"llm_model"</span>: &#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="string">'baichuan-13b-chat'</span>:<span class="string">'/home/Baichuan2-main/baichuan-inc/Baichuan2-13B-Chat'</span>,</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line">    </span><br><span class="line"><span class="comment"># LLM 名称</span></span><br><span class="line">LLM_MODEL = <span class="string">"baichuan-13b-chat"</span></span><br></pre></td></tr></table></figure><h3 id="接口流式输出"><a href="#接口流式输出" class="headerlink" title="接口流式输出"></a>接口流式输出</h3><p>安装sse_starlette</p><p><code>pip install sse-starlette -i https://mirror.baidu.com/pypi/simple</code></p><p>进入<code>./chat/*.py</code>，修改接口</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sse_starlette.sse <span class="keyword">import</span> EventSourceResponse</span><br><span class="line">...</span><br><span class="line"><span class="keyword">return</span> EventSourceResponse(chat_iterator(query=query,</span><br><span class="line">                                         history=history,</span><br><span class="line">                                         model_name=model_name,</span><br><span class="line">                                         prompt_name=prompt_name))</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*注释内容 */</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;原项目Github：&lt;a href=&quot;https://github.com/imClumsyPanda/langchain-ChatGLM&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/imClumsyPanda/langchain-ChatGLM&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;项目部署&quot;&gt;&lt;a href=&quot;#项目部署&quot; class=&quot;headerlink&quot; title=&quot;项目部署&quot;&gt;&lt;/a&gt;项目部署&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;·&lt;/strong&gt; &lt;font color=&quot;gold&quot;&gt;v 0.2.6&lt;/font&gt;&lt;/p&gt;
&lt;h3 id=&quot;机器配置：&quot;&gt;&lt;a href=&quot;#机器配置：&quot; class=&quot;headerlink&quot; title=&quot;机器配置：&quot;&gt;&lt;/a&gt;机器配置：&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;·&lt;/strong&gt; &lt;font color=&quot;gold&quot;&gt;python 环境：anaconda3 + python3.10.12&lt;/font&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;·&lt;/strong&gt; &lt;font color=&quot;gold&quot;&gt;GPU：RTX3090*2 + CUDA11.7&lt;/font&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;·&lt;/strong&gt; &lt;font color=&quot;gold&quot;&gt;torch：2.0.1（CUDA未升至12）&lt;/font&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;·&lt;/strong&gt; &lt;font color=&quot;gold&quot;&gt;conda：py310_dtglm&lt;/font&gt;&lt;/p&gt;
&lt;h3 id=&quot;模型下载&quot;&gt;&lt;a href=&quot;#模型下载&quot; class=&quot;headerlink&quot; title=&quot;模型下载&quot;&gt;&lt;/a&gt;模型下载&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;·&lt;/strong&gt; &lt;font color=&quot;gold&quot;&gt;m3e&lt;/font&gt; &lt;a href=&quot;https://huggingface.co/moka-ai/m3e-base/tree/main&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://huggingface.co/moka-ai/m3e-base/tree/main&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;·&lt;/strong&gt; &lt;font color=&quot;gold&quot;&gt;chatglm2-6b&lt;/font&gt; &lt;a href=&quot;https://huggingface.co/THUDM/chatglm2-6b/tree/main&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://huggingface.co/THUDM/chatglm2-6b/tree/main&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;chatglm清华源 &lt;a href=&quot;https://cloud.tsinghua.edu.cn/d/674208019e314311ab5c/?p=%2F&amp;amp;mode=list&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://cloud.tsinghua.edu.cn/d/674208019e314311ab5c/?p=%2F&amp;amp;mode=list&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;(这里将模型全部下载至&lt;code&gt;/root/huggingface&lt;/code&gt;下)&lt;/p&gt;
&lt;h3 id=&quot;创建虚拟环境，安装依赖&quot;&gt;&lt;a href=&quot;#创建虚拟环境，安装依赖&quot; class=&quot;headerlink&quot; title=&quot;创建虚拟环境，安装依赖&quot;&gt;&lt;/a&gt;创建虚拟环境，安装依赖&lt;/h3&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;conda create -n py310_dtglm python=3.10.12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;conda activate py310_dtglm&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;pip install --use-pep517 -r requirements.txt -i https://mirror.baidu.com/pypi/simple&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;pip install --use-pep517 -r requirements_api.txt -i https://mirror.baidu.com/pypi/simple&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;pip install --use-pep517 -r requirements_webui.txt -i https://mirror.baidu.com/pypi/simple&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;修改配置、模型路径&quot;&gt;&lt;a href=&quot;#修改配置、模型路径&quot; class=&quot;headerlink&quot; title=&quot;修改配置、模型路径&quot;&gt;&lt;/a&gt;修改配置、模型路径&lt;/h3&gt;&lt;p&gt;复制配置文件&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;python copy_config_example.py&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;修改配置文件&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;·&lt;/strong&gt; &lt;font color=&quot;gold&quot;&gt;model_config.py&lt;/font&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;MODEL_ROOT_PATH = &lt;span class=&quot;string&quot;&gt;&quot;/root/huggingface&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;MODEL_PATH = &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;embed_model&quot;&lt;/span&gt;: &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		...&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;string&quot;&gt;&quot;m3e-base&quot;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&quot;/root/huggingface/m3e-base&quot;&lt;/span&gt;, &lt;span class=&quot;comment&quot;&gt;# 修改m3e模型路径&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		...&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#125;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;# &lt;span class=&quot;doctag&quot;&gt;TODO:&lt;/span&gt; add all supported llm models&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;llm_model&quot;&lt;/span&gt;: &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		...&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;string&quot;&gt;&quot;chatglm2-6b&quot;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&quot;/root/huggingface/chatglm2-6b&quot;&lt;/span&gt;, &lt;span class=&quot;comment&quot;&gt;# 修改chatglm2-6b模型路径&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		...&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#125;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;EMBEDDING_MODEL = &lt;span class=&quot;string&quot;&gt;&quot;m3e-base&quot;&lt;/span&gt; &lt;span class=&quot;comment&quot;&gt;# 可以尝试最新的嵌入式sota模型：bge-large-zh-v1.5&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;LLM_MODEL = &lt;span class=&quot;string&quot;&gt;&quot;chatglm2-6b&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="GPT" scheme="http://yoursite.com/categories/GPT/"/>
    
    
    <category term="langchain-chatchat" scheme="http://yoursite.com/tags/langchain-chatchat/"/>
    
    <category term="chat-glm" scheme="http://yoursite.com/tags/chat-glm/"/>
    
    <category term="baichuan" scheme="http://yoursite.com/tags/baichuan/"/>
    
  </entry>
  
  <entry>
    <title>Nebula3集群版新旧版本多开</title>
    <link href="http://yoursite.com/2023/09/28/Nebula3%E9%9B%86%E7%BE%A4%E7%89%88%E7%89%88%E6%9C%AC%E5%A4%9A%E5%BC%80/"/>
    <id>http://yoursite.com/2023/09/28/Nebula3%E9%9B%86%E7%BE%A4%E7%89%88%E7%89%88%E6%9C%AC%E5%A4%9A%E5%BC%80/</id>
    <published>2023-09-28T02:10:10.000Z</published>
    <updated>2023-11-03T09:32:18.875Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><font color="gold"><strong>·</strong> 系统：CentOS7</font><font color="gold"><strong>·</strong> 已有nebula版本：2.6.1（开源社区版）</font><font color="gold"><strong>·</strong> 已有nebula-console版本：2.6.0</font><font color="gold"><strong>·</strong> 已有nebula-graph-studio版本：3.2.3</font><font color="gold"><strong>·</strong> 多开nebula版本：3.6.0（开源社区版）</font><font color="gold"><strong>·</strong> 多开nebula-graph-studio版本：3.2.3</font><font color="gold"><strong>·</strong> 多开nebula-console版本：3.6.0</font><h2 id="集群部署"><a href="#集群部署" class="headerlink" title="集群部署"></a>集群部署</h2><p><strong>·</strong> 参考单机部署方式，对配置文件<code>--meta_server_addrs</code>做扩展，添加meta机器</p><p><strong>·</strong> 区分2.6.1版本已被占用的端口，找到配置文件默认的<code>9559</code>、<code>19559</code>、<code>9669</code>、<code>19669</code>、<code>9779</code>、<code>19779</code>端口，修改为<code>8559</code>、<code>18559</code>、<code>8669</code>、<code>18669</code>、<code>8779</code>、<code>18779</code></p><p><strong>·</strong> 启动集群</p><p><strong>·</strong> 配置nebula-graph-studio默认端口为<code>7002</code></p><p><font color="gold"><strong>·</strong> 注</font>：双开nebula后使用同版本nebula-graph-studio即使更换了端口，也不能同时运行，可以安装nebula-console来同时启动nebula控制台</p><p><code>chmod 111 nebula-console</code></p><p><code>./nebula-console --addr &lt;host&gt; --port 9669 -u root -p nebula</code></p><p><code>./nebula-console --addr &lt;host&gt; --port 8669 -u root -p nebula</code></p>]]></content>
    
    
      
      
    <summary type="html">&lt;script src=&quot;\assets\js\APlayer.min.js&quot;&gt; &lt;/script&gt;&lt;font color=&quot;gold&quot;&gt;&lt;strong&gt;·&lt;/strong&gt; 系统：CentOS7&lt;/font&gt;

&lt;font color=&quot;gold&quot;&gt;&lt;strong&gt;·&lt;/str</summary>
      
    
    
    
    <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
    
    <category term="database" scheme="http://yoursite.com/tags/database/"/>
    
    <category term="nebula" scheme="http://yoursite.com/tags/nebula/"/>
    
  </entry>
  
  <entry>
    <title>Nebula3单机版快速安装</title>
    <link href="http://yoursite.com/2023/09/27/Nebula3%E5%8D%95%E6%9C%BA%E7%89%88%E5%BF%AB%E9%80%9F%E5%AE%89%E8%A3%85/"/>
    <id>http://yoursite.com/2023/09/27/Nebula3%E5%8D%95%E6%9C%BA%E7%89%88%E5%BF%AB%E9%80%9F%E5%AE%89%E8%A3%85/</id>
    <published>2023-09-27T08:12:31.000Z</published>
    <updated>2023-10-24T02:28:41.838Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><font color="gold"><strong>·</strong> 系统：CentOS7</font><font color="gold"><strong>·</strong> nebula版本：3.6.0（开源社区版）</font><font color="gold"><strong>·</strong> nebula-graph-studio版本：3.2.3</font><h2 id="单机部署"><a href="#单机部署" class="headerlink" title="单机部署"></a>单机部署</h2><h3 id="tar包源码下载"><a href="#tar包源码下载" class="headerlink" title="tar包源码下载"></a>tar包源码下载</h3><p><code>wget https://oss-cdn.nebula-graph.com.cn/package/3.6.0/nebula-graph-3.6.0.el7.x86_64.tar.gz</code></p><h3 id="解压并重命名"><a href="#解压并重命名" class="headerlink" title="解压并重命名"></a>解压并重命名</h3><p><code>tar -xvzf nebula-graph-3.6.0.el7.x86_64.tar.gz</code></p><p><code>mv nebula-graph-3.6.0.el7.x86_64 nebula</code></p><h3 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h3><p><code>cd nebula/etc</code></p><p><code>mv nebula-graphd.conf.default nebula-graphd.conf</code></p><p><code>mv nebula-metad.conf.default nebula-metad.conf</code></p><p><code>mv nebula-storaged.conf nebula-storaged.conf</code></p><p>修改对应文件存储位置、节点ip地址，集群同理</p><a id="more"></a><p><strong>示例：</strong></p><p><strong>·</strong> <code>nebula-graphd.conf</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line">########## basics ##########</span><br><span class="line"># Whether to run as a daemon process</span><br><span class="line">--daemonize=true</span><br><span class="line"># The file to host the process id</span><br><span class="line">--pid_file=pids/nebula-graphd.pid</span><br><span class="line"># Whether to enable optimizer</span><br><span class="line">--enable_optimizer=true</span><br><span class="line"># The default charset when a space is created</span><br><span class="line">--default_charset=utf8</span><br><span class="line"># The default collate when a space is created</span><br><span class="line">--default_collate=utf8_bin</span><br><span class="line"># Whether to use the configuration obtained from the configuration file</span><br><span class="line">--local_config=true</span><br><span class="line"></span><br><span class="line">########## logging ##########</span><br><span class="line"># The directory to host logging files</span><br><span class="line">--log_dir=/home/nebula/graph/logs</span><br><span class="line"># Log level, 0, 1, 2, 3 for INFO, WARNING, ERROR, FATAL respectively</span><br><span class="line">--minloglevel=0</span><br><span class="line"># Verbose log level, 1, 2, 3, 4, the higher of the level, the more verbose of the logging</span><br><span class="line">--v=0</span><br><span class="line"># Maximum seconds to buffer the log messages</span><br><span class="line">--logbufsecs=0</span><br><span class="line"># Whether to redirect stdout and stderr to separate output files</span><br><span class="line">--redirect_stdout=true</span><br><span class="line"># Destination filename of stdout and stderr, which will also reside in log_dir.</span><br><span class="line">--stdout_log_file=graphd-stdout.log</span><br><span class="line">--stderr_log_file=graphd-stderr.log</span><br><span class="line"># Copy log messages at or above this level to stderr in addition to logfiles. The numbers of severity levels INFO, WARNING, ERROR, and FATAL are 0, 1, 2, and 3, respectively.</span><br><span class="line">--stderrthreshold=3</span><br><span class="line"># wether logging files&apos; name contain time stamp.</span><br><span class="line">--timestamp_in_logfile_name=true</span><br><span class="line">########## query ##########</span><br><span class="line"># Whether to treat partial success as an error.</span><br><span class="line"># This flag is only used for Read-only access, and Modify access always treats partial success as an error.</span><br><span class="line">--accept_partial_success=false</span><br><span class="line"># Maximum sentence length, unit byte</span><br><span class="line">--max_allowed_query_size=4194304</span><br><span class="line"></span><br><span class="line">########## networking ##########</span><br><span class="line"># Comma separated Meta Server Addresses</span><br><span class="line">--meta_server_addrs=192.168.9.103:9559</span><br><span class="line"># Local IP used to identify the nebula-graphd process.</span><br><span class="line"># Change it to an address other than loopback if the service is distributed or</span><br><span class="line"># will be accessed remotely.</span><br><span class="line">--local_ip=192.168.9.103</span><br><span class="line"># Network device to listen on</span><br><span class="line">--listen_netdev=any</span><br><span class="line"># Port to listen on</span><br><span class="line">--port=9669</span><br><span class="line"># To turn on SO_REUSEPORT or not</span><br><span class="line">--reuse_port=false</span><br><span class="line"># Backlog of the listen socket, adjust this together with net.core.somaxconn</span><br><span class="line">--listen_backlog=1024</span><br><span class="line"># The number of seconds Nebula service waits before closing the idle connections</span><br><span class="line">--client_idle_timeout_secs=28800</span><br><span class="line"># The number of seconds before idle sessions expire</span><br><span class="line"># The range should be in [1, 604800]</span><br><span class="line">--session_idle_timeout_secs=28800</span><br><span class="line"># The number of threads to accept incoming connections</span><br><span class="line">--num_accept_threads=1</span><br><span class="line"># The number of networking IO threads, 0 for # of CPU cores</span><br><span class="line">--num_netio_threads=0</span><br><span class="line"># Max active connections for all networking threads. 0 means no limit.</span><br><span class="line"># Max connections for each networking thread = num_max_connections / num_netio_threads</span><br><span class="line">--num_max_connections=0</span><br><span class="line"># The number of threads to execute user queries, 0 for # of CPU cores</span><br><span class="line">--num_worker_threads=0</span><br><span class="line"># HTTP service ip</span><br><span class="line">--ws_ip=0.0.0.0</span><br><span class="line"># HTTP service port</span><br><span class="line">--ws_http_port=19669</span><br><span class="line"># storage client timeout</span><br><span class="line">--storage_client_timeout_ms=60000</span><br><span class="line"># slow query threshold in us</span><br><span class="line">--slow_query_threshold_us=200000</span><br><span class="line"># Port to listen on Meta with HTTP protocol, it corresponds to ws_http_port in metad&apos;s configuration file</span><br><span class="line">--ws_meta_http_port=19559</span><br><span class="line"></span><br><span class="line">########## authentication ##########</span><br><span class="line"># Enable authorization</span><br><span class="line">--enable_authorize=false</span><br><span class="line"># User login authentication type, password for nebula authentication, ldap for ldap authentication, cloud for cloud authentication</span><br><span class="line">--auth_type=password</span><br><span class="line"></span><br><span class="line">########## memory ##########</span><br><span class="line"># System memory high watermark ratio, cancel the memory checking when the ratio greater than 1.0</span><br><span class="line">--system_memory_high_watermark_ratio=0.8</span><br><span class="line"></span><br><span class="line">########## metrics ##########</span><br><span class="line">--enable_space_level_metrics=false</span><br><span class="line"></span><br><span class="line">########## experimental feature ##########</span><br><span class="line"># if use experimental features</span><br><span class="line">--enable_experimental_feature=false</span><br><span class="line"></span><br><span class="line"># if use balance data feature, only work if enable_experimental_feature is true</span><br><span class="line">--enable_data_balance=true</span><br><span class="line"></span><br><span class="line"># enable udf, written in c++ only for now</span><br><span class="line">--enable_udf=true</span><br><span class="line"></span><br><span class="line"># set the directory where the .so files of udf are stored, when enable_udf is true</span><br><span class="line">--udf_path=/home/nebula/dev/nebula/udf/</span><br><span class="line"></span><br><span class="line">########## session ##########</span><br><span class="line"># Maximum number of sessions that can be created per IP and per user</span><br><span class="line">--max_sessions_per_ip_per_user=300</span><br><span class="line"></span><br><span class="line">########## memory tracker ##########</span><br><span class="line"># trackable memory ratio (trackable_memory / (total_memory - untracked_reserved_memory) )</span><br><span class="line">--memory_tracker_limit_ratio=0.8</span><br><span class="line"># untracked reserved memory in Mib</span><br><span class="line">--memory_tracker_untracked_reserved_memory_mb=50</span><br><span class="line"></span><br><span class="line"># enable log memory tracker stats periodically</span><br><span class="line">--memory_tracker_detail_log=false</span><br><span class="line"># log memory tacker stats interval in milliseconds</span><br><span class="line">--memory_tracker_detail_log_interval_ms=60000</span><br><span class="line"></span><br><span class="line"># enable memory background purge (if jemalloc is used)</span><br><span class="line">--memory_purge_enabled=true</span><br><span class="line"># memory background purge interval in seconds</span><br><span class="line">--memory_purge_interval_seconds=10</span><br><span class="line"></span><br><span class="line">########## performance optimization ##########</span><br><span class="line"># The max job size in multi job mode</span><br><span class="line">--max_job_size=1</span><br><span class="line"># The min batch size for handling dataset in multi job mode, only enabled when max_job_size is greater than 1</span><br><span class="line">--min_batch_size=8192</span><br><span class="line"># if true, return directly without go through RPC</span><br><span class="line">--optimize_appendvertices=false</span><br><span class="line"># number of paths constructed by each thread</span><br><span class="line">--path_batch_size=10000</span><br></pre></td></tr></table></figure><p><strong>·</strong> <code>nebula-metad.conf</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">########## basics ##########</span><br><span class="line"># Whether to run as a daemon process</span><br><span class="line">--daemonize=true</span><br><span class="line"># The file to host the process id</span><br><span class="line">--pid_file=pids/nebula-metad.pid</span><br><span class="line"></span><br><span class="line">########## logging ##########</span><br><span class="line"># The directory to host logging files</span><br><span class="line">--log_dir=/home/nebula/meta/logs</span><br><span class="line"># Log level, 0, 1, 2, 3 for INFO, WARNING, ERROR, FATAL respectively</span><br><span class="line">--minloglevel=0</span><br><span class="line"># Verbose log level, 1, 2, 3, 4, the higher of the level, the more verbose of the logging</span><br><span class="line">--v=0</span><br><span class="line"># Maximum seconds to buffer the log messages</span><br><span class="line">--logbufsecs=0</span><br><span class="line"># Whether to redirect stdout and stderr to separate output files</span><br><span class="line">--redirect_stdout=true</span><br><span class="line"># Destination filename of stdout and stderr, which will also reside in log_dir.</span><br><span class="line">--stdout_log_file=metad-stdout.log</span><br><span class="line">--stderr_log_file=metad-stderr.log</span><br><span class="line"># Copy log messages at or above this level to stderr in addition to logfiles. The numbers of severity levels INFO, WARNING, ERROR, and FATAL are 0, 1, 2, and 3, respectively.</span><br><span class="line">--stderrthreshold=3</span><br><span class="line"># wether logging files&apos; name contain time stamp, If Using logrotate to rotate logging files, than should set it to true.</span><br><span class="line">--timestamp_in_logfile_name=true</span><br><span class="line"></span><br><span class="line">########## networking ##########</span><br><span class="line"># Comma separated Meta Server addresses</span><br><span class="line">--meta_server_addrs=192.168.9.103:9559</span><br><span class="line"># Local IP used to identify the nebula-metad process.</span><br><span class="line"># Change it to an address other than loopback if the service is distributed or</span><br><span class="line"># will be accessed remotely.</span><br><span class="line">--local_ip=192.168.9.103</span><br><span class="line"># Meta daemon listening port</span><br><span class="line">--port=9559</span><br><span class="line"># HTTP service ip</span><br><span class="line">--ws_ip=0.0.0.0</span><br><span class="line"># HTTP service port</span><br><span class="line">--ws_http_port=19559</span><br><span class="line"># Port to listen on Storage with HTTP protocol, it corresponds to ws_http_port in storage&apos;s configuration file</span><br><span class="line">--ws_storage_http_port=19779</span><br><span class="line"></span><br><span class="line">########## storage ##########</span><br><span class="line"># Root data path, here should be only single path for metad</span><br><span class="line">--data_path=/home/nebula/data/meta</span><br><span class="line"></span><br><span class="line">########## Misc #########</span><br><span class="line"># The default number of parts when a space is created</span><br><span class="line">--default_parts_num=100</span><br><span class="line"># The default replica factor when a space is created</span><br><span class="line">--default_replica_factor=1</span><br><span class="line"></span><br><span class="line">--heartbeat_interval_secs=10</span><br><span class="line">--agent_heartbeat_interval_secs=60</span><br></pre></td></tr></table></figure><p><strong>·</strong> <code>nebula-metad.conf</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line">########## basics ##########</span><br><span class="line"># Whether to run as a daemon process</span><br><span class="line">--daemonize=true</span><br><span class="line"># The file to host the process id</span><br><span class="line">--pid_file=pids/nebula-storaged.pid</span><br><span class="line"># Whether to use the configuration obtained from the configuration file</span><br><span class="line">--local_config=true</span><br><span class="line"></span><br><span class="line">########## logging ##########</span><br><span class="line"># The directory to host logging files</span><br><span class="line">--log_dir=/home/nebula/storage/logs</span><br><span class="line"># Log level, 0, 1, 2, 3 for INFO, WARNING, ERROR, FATAL respectively</span><br><span class="line">--minloglevel=0</span><br><span class="line"># Verbose log level, 1, 2, 3, 4, the higher of the level, the more verbose of the logging</span><br><span class="line">--v=0</span><br><span class="line"># Maximum seconds to buffer the log messages</span><br><span class="line">--logbufsecs=0</span><br><span class="line"># Whether to redirect stdout and stderr to separate output files</span><br><span class="line">--redirect_stdout=true</span><br><span class="line"># Destination filename of stdout and stderr, which will also reside in log_dir.</span><br><span class="line">--stdout_log_file=storaged-stdout.log</span><br><span class="line">--stderr_log_file=storaged-stderr.log</span><br><span class="line"># Copy log messages at or above this level to stderr in addition to logfiles. The numbers of severity levels INFO, WARNING, ERROR, and FATAL are 0, 1, 2, and 3, respectively.</span><br><span class="line">--stderrthreshold=3</span><br><span class="line"># Wether logging files&apos; name contain time stamp.</span><br><span class="line">--timestamp_in_logfile_name=true</span><br><span class="line"></span><br><span class="line">########## networking ##########</span><br><span class="line"># Comma separated Meta server addresses</span><br><span class="line">--meta_server_addrs=192.168.9.103:9559</span><br><span class="line"># Local IP used to identify the nebula-storaged process.</span><br><span class="line"># Change it to an address other than loopback if the service is distributed or</span><br><span class="line"># will be accessed remotely.</span><br><span class="line">--local_ip=192.168.9.103</span><br><span class="line"># Storage daemon listening port</span><br><span class="line">--port=9779</span><br><span class="line"># HTTP service ip</span><br><span class="line">--ws_ip=0.0.0.0</span><br><span class="line"># HTTP service port</span><br><span class="line">--ws_http_port=19779</span><br><span class="line"># heartbeat with meta service</span><br><span class="line">--heartbeat_interval_secs=10</span><br><span class="line"></span><br><span class="line">######### Raft #########</span><br><span class="line"># Raft election timeout</span><br><span class="line">--raft_heartbeat_interval_secs=30</span><br><span class="line"># RPC timeout for raft client (ms)</span><br><span class="line">--raft_rpc_timeout_ms=500</span><br><span class="line">## recycle Raft WAL</span><br><span class="line">--wal_ttl=14400</span><br><span class="line"></span><br><span class="line">########## Disk ##########</span><br><span class="line"># Root data path. Split by comma. e.g. --data_path=/disk1/path1/,/disk2/path2/</span><br><span class="line"># One path per Rocksdb instance.</span><br><span class="line">--data_path=/home/nebula/data/storage</span><br><span class="line"></span><br><span class="line"># Minimum reserved bytes of each data path</span><br><span class="line">--minimum_reserved_bytes=268435456</span><br><span class="line"></span><br><span class="line"># The default reserved bytes for one batch operation</span><br><span class="line">--rocksdb_batch_size=4096</span><br><span class="line"># The default block cache size used in BlockBasedTable.</span><br><span class="line"># The unit is MB.</span><br><span class="line">--rocksdb_block_cache=4</span><br><span class="line"># The type of storage engine, `rocksdb&apos;, `memory&apos;, etc.</span><br><span class="line">--engine_type=rocksdb</span><br><span class="line"></span><br><span class="line"># Compression algorithm, options: no,snappy,lz4,lz4hc,zlib,bzip2,zstd</span><br><span class="line"># For the sake of binary compatibility, the default value is snappy.</span><br><span class="line"># Recommend to use:</span><br><span class="line">#   * lz4 to gain more CPU performance, with the same compression ratio with snappy</span><br><span class="line">#   * zstd to occupy less disk space</span><br><span class="line">#   * lz4hc for the read-heavy write-light scenario</span><br><span class="line">--rocksdb_compression=lz4</span><br><span class="line"></span><br><span class="line"># Set different compressions for different levels</span><br><span class="line"># For example, if --rocksdb_compression is snappy,</span><br><span class="line"># &quot;no:no:lz4:lz4::zstd&quot; is identical to &quot;no:no:lz4:lz4:snappy:zstd:snappy&quot;</span><br><span class="line"># In order to disable compression for level 0/1, set it to &quot;no:no&quot;</span><br><span class="line">--rocksdb_compression_per_level=</span><br><span class="line"></span><br><span class="line"># Whether or not to enable rocksdb&apos;s statistics, disabled by default</span><br><span class="line">--enable_rocksdb_statistics=false</span><br><span class="line"></span><br><span class="line"># Statslevel used by rocksdb to collection statistics, optional values are</span><br><span class="line">#   * kExceptHistogramOrTimers, disable timer stats, and skip histogram stats</span><br><span class="line">#   * kExceptTimers, Skip timer stats</span><br><span class="line">#   * kExceptDetailedTimers, Collect all stats except time inside mutex lock AND time spent on compression.</span><br><span class="line">#   * kExceptTimeForMutex, Collect all stats except the counters requiring to get time inside the mutex lock.</span><br><span class="line">#   * kAll, Collect all stats</span><br><span class="line">--rocksdb_stats_level=kExceptHistogramOrTimers</span><br><span class="line"></span><br><span class="line"># Whether or not to enable rocksdb&apos;s prefix bloom filter, enabled by default.</span><br><span class="line">--enable_rocksdb_prefix_filtering=true</span><br><span class="line"># Whether or not to enable rocksdb&apos;s whole key bloom filter, disabled by default.</span><br><span class="line">--enable_rocksdb_whole_key_filtering=false</span><br><span class="line"></span><br><span class="line">############## rocksdb Options ##############</span><br><span class="line"># rocksdb DBOptions in json, each name and value of option is a string, given as &quot;option_name&quot;:&quot;option_value&quot; separated by comma</span><br><span class="line">--rocksdb_db_options=&#123;&#125;</span><br><span class="line"># rocksdb ColumnFamilyOptions in json, each name and value of option is string, given as &quot;option_name&quot;:&quot;option_value&quot; separated by comma</span><br><span class="line">--rocksdb_column_family_options=&#123;&quot;write_buffer_size&quot;:&quot;67108864&quot;,&quot;max_write_buffer_number&quot;:&quot;4&quot;,&quot;max_bytes_for_level_base&quot;:&quot;268435456&quot;&#125;</span><br><span class="line"># rocksdb BlockBasedTableOptions in json, each name and value of option is string, given as &quot;option_name&quot;:&quot;option_value&quot; separated by comma</span><br><span class="line">--rocksdb_block_based_table_options=&#123;&quot;block_size&quot;:&quot;8192&quot;&#125;</span><br><span class="line"></span><br><span class="line">############### misc ####################</span><br><span class="line"># Whether turn on query in multiple thread</span><br><span class="line">--query_concurrently=true</span><br><span class="line"># Whether remove outdated space data</span><br><span class="line">--auto_remove_invalid_space=true</span><br><span class="line"># Network IO threads number</span><br><span class="line">--num_io_threads=16</span><br><span class="line"># Max active connections for all networking threads. 0 means no limit.</span><br><span class="line"># Max connections for each networking thread = num_max_connections / num_netio_threads</span><br><span class="line">--num_max_connections=0</span><br><span class="line"># Worker threads number to handle request</span><br><span class="line">--num_worker_threads=32</span><br><span class="line"># Maximum subtasks to run admin jobs concurrently</span><br><span class="line">--max_concurrent_subtasks=10</span><br><span class="line"># The rate limit in bytes when leader synchronizes snapshot data</span><br><span class="line">--snapshot_part_rate_limit=10485760</span><br><span class="line"># The amount of data sent in each batch when leader synchronizes snapshot data</span><br><span class="line">--snapshot_batch_size=1048576</span><br><span class="line"># The rate limit in bytes when leader synchronizes rebuilding index</span><br><span class="line">--rebuild_index_part_rate_limit=4194304</span><br><span class="line"># The amount of data sent in each batch when leader synchronizes rebuilding index</span><br><span class="line">--rebuild_index_batch_size=1048576</span><br><span class="line"></span><br><span class="line">########## memory tracker ##########</span><br><span class="line"># trackable memory ratio (trackable_memory / (total_memory - untracked_reserved_memory) )</span><br><span class="line">--memory_tracker_limit_ratio=0.8</span><br><span class="line"># untracked reserved memory in Mib</span><br><span class="line">--memory_tracker_untracked_reserved_memory_mb=50</span><br><span class="line"></span><br><span class="line"># enable log memory tracker stats periodically</span><br><span class="line">--memory_tracker_detail_log=false</span><br><span class="line"># log memory tacker stats interval in milliseconds</span><br><span class="line">--memory_tracker_detail_log_interval_ms=60000</span><br><span class="line"></span><br><span class="line"># enable memory background purge (if jemalloc is used)</span><br><span class="line">--memory_purge_enabled=true</span><br><span class="line"># memory background purge interval in seconds</span><br><span class="line">--memory_purge_interval_seconds=10</span><br></pre></td></tr></table></figure><h3 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h3><p>回到nebula根目录</p><p><code>cd ..</code></p><p>执行启动脚本</p><p><code>./scripts/nebula.service start all</code></p><p>查看服务状态</p><p><code>./scripts/nebula.service status all</code></p><p>重启服务</p><p><code>./scripts/nebula.service restart all</code></p><p>关闭服务</p><p><code>./scripts/nebula.service stop all</code></p><h2 id="nebula-graph-studio部署"><a href="#nebula-graph-studio部署" class="headerlink" title="nebula-graph-studio部署"></a>nebula-graph-studio部署</h2><p>v3.2.3下载地址（免费使用图探索的最高社区版本）</p><p><a href="https://github.com/vesoft-inc/nebula-studio/releases/tag/v3.2.3" target="_blank" rel="noopener">https://github.com/vesoft-inc/nebula-studio/releases/tag/v3.2.3</a></p><p>源码安装后，进入对应目录，执行启动脚本</p><p><code>./service</code></p><p><img src="/2023/09/27/Nebula3单机版快速安装/A.png" alt></p><p><img src="/2023/09/27/Nebula3单机版快速安装/B.png" alt></p>]]></content>
    
    
    <summary type="html">&lt;font color=&quot;gold&quot;&gt;&lt;strong&gt;·&lt;/strong&gt; 系统：CentOS7&lt;/font&gt;

&lt;font color=&quot;gold&quot;&gt;&lt;strong&gt;·&lt;/strong&gt; nebula版本：3.6.0（开源社区版）&lt;/font&gt;

&lt;font color=&quot;gold&quot;&gt;&lt;strong&gt;·&lt;/strong&gt; nebula-graph-studio版本：3.2.3&lt;/font&gt;

&lt;h2 id=&quot;单机部署&quot;&gt;&lt;a href=&quot;#单机部署&quot; class=&quot;headerlink&quot; title=&quot;单机部署&quot;&gt;&lt;/a&gt;单机部署&lt;/h2&gt;&lt;h3 id=&quot;tar包源码下载&quot;&gt;&lt;a href=&quot;#tar包源码下载&quot; class=&quot;headerlink&quot; title=&quot;tar包源码下载&quot;&gt;&lt;/a&gt;tar包源码下载&lt;/h3&gt;&lt;p&gt;&lt;code&gt;wget https://oss-cdn.nebula-graph.com.cn/package/3.6.0/nebula-graph-3.6.0.el7.x86_64.tar.gz&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&quot;解压并重命名&quot;&gt;&lt;a href=&quot;#解压并重命名&quot; class=&quot;headerlink&quot; title=&quot;解压并重命名&quot;&gt;&lt;/a&gt;解压并重命名&lt;/h3&gt;&lt;p&gt;&lt;code&gt;tar -xvzf nebula-graph-3.6.0.el7.x86_64.tar.gz&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;mv nebula-graph-3.6.0.el7.x86_64 nebula&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&quot;修改配置文件&quot;&gt;&lt;a href=&quot;#修改配置文件&quot; class=&quot;headerlink&quot; title=&quot;修改配置文件&quot;&gt;&lt;/a&gt;修改配置文件&lt;/h3&gt;&lt;p&gt;&lt;code&gt;cd nebula/etc&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;mv nebula-graphd.conf.default nebula-graphd.conf&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;mv nebula-metad.conf.default nebula-metad.conf&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;mv nebula-storaged.conf nebula-storaged.conf&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;修改对应文件存储位置、节点ip地址，集群同理&lt;/p&gt;</summary>
    
    
    
    <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
    
    <category term="database" scheme="http://yoursite.com/tags/database/"/>
    
    <category term="nebula" scheme="http://yoursite.com/tags/nebula/"/>
    
  </entry>
  
  <entry>
    <title>paddleDetection Demo</title>
    <link href="http://yoursite.com/2023/08/31/paddleDetection%20Demo/"/>
    <id>http://yoursite.com/2023/08/31/paddleDetection%20Demo/</id>
    <published>2023-08-31T04:20:20.000Z</published>
    <updated>2024-01-25T09:14:54.938Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><h2 id="PPHuman"><a href="#PPHuman" class="headerlink" title="PPHuman"></a>PPHuman</h2><h3 id="行人属性识别"><a href="#行人属性识别" class="headerlink" title="行人属性识别"></a>行人属性识别</h3><h4 id="行人属性"><a href="#行人属性" class="headerlink" title="行人属性"></a>行人属性</h4><p>cfg：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">crop_thresh:</span> <span class="number">0.5</span></span><br><span class="line"><span class="attr">attr_thresh:</span> <span class="number">0.5</span></span><br><span class="line"><span class="attr">kpt_thresh:</span> <span class="number">0.2</span></span><br><span class="line"><span class="attr">visual:</span> <span class="literal">True</span></span><br><span class="line"><span class="attr">warmup_frame:</span> <span class="number">50</span></span><br><span class="line"></span><br><span class="line"><span class="attr">DET:</span></span><br><span class="line">  <span class="attr">model_dir:</span> <span class="string">https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip</span></span><br><span class="line">  <span class="attr">batch_size:</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="attr">MOT:</span></span><br><span class="line">  <span class="attr">model_dir:</span> <span class="string">https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip</span></span><br><span class="line">  <span class="attr">tracker_config:</span> <span class="string">/exp/work/video/PaddleDetection/deploy/pipeline/config/tracker_config.yml</span></span><br><span class="line">  <span class="attr">batch_size:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">skip_frame_num:</span> <span class="number">-1</span> <span class="comment"># preferably no more than 3</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="attr">KPT:</span></span><br><span class="line">  <span class="attr">model_dir:</span> <span class="string">https://bj.bcebos.com/v1/paddledet/models/pipeline/dark_hrnet_w32_256x192.zip</span></span><br><span class="line">  <span class="attr">batch_size:</span> <span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="attr">ATTR:</span></span><br><span class="line">  <span class="attr">model_dir:</span>  <span class="string">https://bj.bcebos.com/v1/paddledet/models/pipeline/PPLCNet_x1_0_person_attribute_945_infer.zip</span></span><br><span class="line">  <span class="attr">batch_size:</span> <span class="number">8</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure><p>cli：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python deploy/pipeline/pipeline.py --config deploy/pipeline/config/cache/cfg_human.yml --device=gpu --video_file=demo_input/human.mp4 --output_dir=demo_output/</span><br></pre></td></tr></table></figure><p><img src="/2023/08/31/paddleDetection Demo/行人属性.gif" alt></p><a id="more"></a><h3 id="行人行为识别"><a href="#行人行为识别" class="headerlink" title="行人行为识别"></a>行人行为识别</h3><h4 id="跌倒"><a href="#跌倒" class="headerlink" title="跌倒"></a>跌倒</h4><p>cfg：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">crop_thresh:</span> <span class="number">0.5</span></span><br><span class="line"><span class="attr">kpt_thresh:</span> <span class="number">0.2</span></span><br><span class="line"><span class="attr">visual:</span> <span class="literal">True</span></span><br><span class="line"><span class="attr">warmup_frame:</span> <span class="number">50</span></span><br><span class="line"></span><br><span class="line"><span class="attr">MOT:</span></span><br><span class="line">  <span class="attr">model_dir:</span> <span class="string">https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_s_36e_pipeline.zip</span></span><br><span class="line">  <span class="attr">tracker_config:</span> <span class="string">/exp/work/video/PaddleDetection/deploy/pipeline/config/tracker_config.yml</span></span><br><span class="line">  <span class="attr">batch_size:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">True</span></span><br><span class="line">  </span><br><span class="line"><span class="attr">KPT:</span></span><br><span class="line">  <span class="attr">model_dir:</span> <span class="string">https://bj.bcebos.com/v1/paddledet/models/pipeline/dark_hrnet_w32_256x192.zip</span></span><br><span class="line">  <span class="attr">batch_size:</span> <span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="attr">SKELETON_ACTION:</span></span><br><span class="line">  <span class="attr">model_dir:</span> <span class="string">https://bj.bcebos.com/v1/paddledet/models/pipeline/STGCN.zip</span></span><br><span class="line">  <span class="attr">batch_size:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">max_frames:</span> <span class="number">50</span></span><br><span class="line">  <span class="attr">display_frames:</span> <span class="number">80</span></span><br><span class="line">  <span class="attr">coord_size:</span> <span class="string">[384,</span> <span class="number">512</span><span class="string">]</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure><p>cli：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python deploy/pipeline/pipeline.py --config deploy/pipeline/config/cache/cfg_fall.yml --device=gpu --video_file=demo_input/fall.mp4 --output_dir=demo_output/</span><br></pre></td></tr></table></figure><p><img src="/2023/08/31/paddleDetection Demo/跌倒.gif" alt></p><h4 id="接打电话"><a href="#接打电话" class="headerlink" title="接打电话"></a>接打电话</h4><p>cfg：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">crop_thresh:</span> <span class="number">0.5</span></span><br><span class="line"><span class="attr">kpt_thresh:</span> <span class="number">0.2</span></span><br><span class="line"><span class="attr">visual:</span> <span class="literal">True</span></span><br><span class="line"><span class="attr">warmup_frame:</span> <span class="number">50</span></span><br><span class="line"></span><br><span class="line"><span class="attr">MOT:</span></span><br><span class="line">  <span class="attr">model_dir:</span> <span class="string">https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_s_36e_pipeline.zip</span></span><br><span class="line">  <span class="attr">tracker_config:</span> <span class="string">/exp/work/video/PaddleDetection/deploy/pipeline/config/tracker_config.yml</span></span><br><span class="line">  <span class="attr">batch_size:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="attr">ID_BASED_CLSACTION:</span></span><br><span class="line">  <span class="attr">model_dir:</span> <span class="string">https://bj.bcebos.com/v1/paddledet/models/pipeline/PPHGNet_tiny_calling_halfbody.zip</span></span><br><span class="line">  <span class="attr">batch_size:</span> <span class="number">8</span></span><br><span class="line">  <span class="attr">threshold:</span> <span class="number">0.8</span></span><br><span class="line">  <span class="attr">display_frames:</span> <span class="number">80</span></span><br><span class="line">  <span class="attr">skip_frame_num:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="attr">KPT:</span></span><br><span class="line">  <span class="attr">model_dir:</span> <span class="string">https://bj.bcebos.com/v1/paddledet/models/pipeline/dark_hrnet_w32_256x192.zip</span></span><br><span class="line">  <span class="attr">batch_size:</span> <span class="number">8</span></span><br></pre></td></tr></table></figure><p>cli：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python deploy/pipeline/pipeline.py --config deploy/pipeline/config/cache/cfg_call.yml --device=gpu --video_file=demo_input/call.mp4 --output_dir=demo_output/</span><br></pre></td></tr></table></figure><p><img src="/2023/08/31/paddleDetection Demo/接打电话.gif" alt></p><h4 id="吸烟"><a href="#吸烟" class="headerlink" title="吸烟"></a>吸烟</h4><p>cfg：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">crop_thresh:</span> <span class="number">0.5</span></span><br><span class="line"><span class="attr">kpt_thresh:</span> <span class="number">0.2</span></span><br><span class="line"><span class="attr">visual:</span> <span class="literal">True</span></span><br><span class="line"><span class="attr">warmup_frame:</span> <span class="number">50</span></span><br><span class="line"></span><br><span class="line"><span class="attr">MOT:</span></span><br><span class="line">  <span class="attr">model_dir:</span> <span class="string">https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_s_36e_pipeline.zip</span></span><br><span class="line">  <span class="attr">tracker_config:</span> <span class="string">/exp/work/video/PaddleDetection/deploy/pipeline/config/tracker_config.yml</span></span><br><span class="line">  <span class="attr">batch_size:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="attr">ID_BASED_DETACTION:</span></span><br><span class="line">  <span class="attr">model_dir:</span>  <span class="string">https://bj.bcebos.com/v1/paddledet/models/pipeline/ppyoloe_crn_s_80e_smoking_visdrone.zip</span></span><br><span class="line">  <span class="attr">batch_size:</span> <span class="number">8</span></span><br><span class="line">  <span class="attr">threshold:</span> <span class="number">0.6</span></span><br><span class="line">  <span class="attr">display_frames:</span> <span class="number">80</span></span><br><span class="line">  <span class="attr">skip_frame_num:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="attr">KPT:</span></span><br><span class="line">  <span class="attr">model_dir:</span> <span class="string">https://bj.bcebos.com/v1/paddledet/models/pipeline/dark_hrnet_w32_256x192.zip</span></span><br><span class="line">  <span class="attr">batch_size:</span> <span class="number">8</span></span><br></pre></td></tr></table></figure><p>cli：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python deploy/pipeline/pipeline.py --config deploy/pipeline/config/cache/cfg_smoke.yml --device=gpu --video_file=demo_input/smoke.mp4 --output_dir=demo_output/</span><br></pre></td></tr></table></figure><p><img src="/2023/08/31/paddleDetection Demo/吸烟.gif" style="zoom:150%;"></p><h4 id="打架"><a href="#打架" class="headerlink" title="打架"></a>打架</h4><p>cfg：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">crop_thresh:</span> <span class="number">0.5</span></span><br><span class="line"><span class="attr">kpt_thresh:</span> <span class="number">0.2</span></span><br><span class="line"><span class="attr">visual:</span> <span class="literal">True</span></span><br><span class="line"><span class="attr">warmup_frame:</span> <span class="number">50</span></span><br><span class="line"></span><br><span class="line"><span class="attr">MOT:</span></span><br><span class="line">  <span class="attr">model_dir:</span> <span class="string">https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_s_36e_pipeline.zip</span></span><br><span class="line">  <span class="attr">tracker_config:</span> <span class="string">/exp/work/video/PaddleDetection/deploy/pipeline/config/tracker_config.yml</span></span><br><span class="line">  <span class="attr">batch_size:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="attr">KPT:</span></span><br><span class="line">  <span class="attr">model_dir:</span> <span class="string">https://bj.bcebos.com/v1/paddledet/models/pipeline/dark_hrnet_w32_256x192.zip</span></span><br><span class="line">  <span class="attr">batch_size:</span> <span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="attr">VIDEO_ACTION:</span></span><br><span class="line">  <span class="attr">model_dir:</span> <span class="string">https://videotag.bj.bcebos.com/PaddleVideo-release2.3/ppTSM_fight.zip</span></span><br><span class="line">  <span class="attr">batch_size:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">frame_len:</span> <span class="number">8</span></span><br><span class="line">  <span class="attr">sample_freq:</span> <span class="number">7</span></span><br><span class="line">  <span class="attr">short_size:</span> <span class="number">340</span></span><br><span class="line">  <span class="attr">target_size:</span> <span class="number">320</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure><p>cli：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python deploy/pipeline/pipeline.py --config deploy/pipeline/config/cache/cfg_fight.yml --device=gpu --video_file=demo_input/fight.mp4 --output_dir=demo_output/</span><br></pre></td></tr></table></figure><p><img src="/2023/08/31/paddleDetection Demo/打架.gif" alt></p><h4 id="行人进入"><a href="#行人进入" class="headerlink" title="行人进入"></a>行人进入</h4><p>cfg：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">crop_thresh:</span> <span class="number">0.5</span></span><br><span class="line"><span class="attr">visual:</span> <span class="literal">True</span></span><br><span class="line"><span class="attr">warmup_frame:</span> <span class="number">50</span></span><br><span class="line"></span><br><span class="line"><span class="attr">MOT:</span></span><br><span class="line">  <span class="attr">model_dir:</span> <span class="string">https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip</span></span><br><span class="line">  <span class="attr">tracker_config:</span> <span class="string">/exp/work/video/PaddleDetection/backend/pp_config/tracker_config.yml</span></span><br><span class="line">  <span class="attr">batch_size:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">skip_frame_num:</span> <span class="number">-1</span> <span class="comment"># preferably no more than 3</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure><p>cli：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python deploy/pipeline/pipeline.py --config deploy/pipeline/config/cache/cfg_entrance.yml --region_type=bottom --do_entrance_counting --device=gpu --video_file=demo_input/entrance.mp4 --output_dir=demo_output/</span><br></pre></td></tr></table></figure><p><img src="/2023/08/31/paddleDetection Demo/行人进入.gif" alt></p><h4 id="禁区"><a href="#禁区" class="headerlink" title="禁区"></a>禁区</h4><p>cfg：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">crop_thresh:</span> <span class="number">0.5</span></span><br><span class="line"><span class="attr">visual:</span> <span class="literal">True</span></span><br><span class="line"><span class="attr">warmup_frame:</span> <span class="number">50</span></span><br><span class="line"></span><br><span class="line"><span class="attr">MOT:</span></span><br><span class="line">  <span class="attr">model_dir:</span> <span class="string">https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip</span></span><br><span class="line">  <span class="attr">tracker_config:</span> <span class="string">/exp/work/video/PaddleDetection/backend/pp_config/tracker_config.yml</span></span><br><span class="line">  <span class="attr">batch_size:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">skip_frame_num:</span> <span class="number">-1</span> <span class="comment"># preferably no more than 3</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure><p>cli：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python deploy/pipeline/pipeline.py --config deploy/pipeline/config/cache/cfg_entrance.yml --do_break_in_counting=true --region_type=custom --region_polygon 355 700 1100 700 915 900 50 900 --device=gpu --video_file=demo_input/forb.mp4 --output_dir=demo_output/</span><br></pre></td></tr></table></figure><p><img src="/2023/08/31/paddleDetection Demo/禁区.gif" alt></p><h2 id="PPVehicle"><a href="#PPVehicle" class="headerlink" title="PPVehicle"></a>PPVehicle</h2><h3 id="车辆属性识别"><a href="#车辆属性识别" class="headerlink" title="车辆属性识别"></a>车辆属性识别</h3><h4 id="车辆属性"><a href="#车辆属性" class="headerlink" title="车辆属性"></a>车辆属性</h4><p>cfg：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">crop_thresh:</span> <span class="number">0.5</span></span><br><span class="line"><span class="attr">visual:</span> <span class="literal">True</span></span><br><span class="line"><span class="attr">warmup_frame:</span> <span class="number">50</span></span><br><span class="line"></span><br><span class="line"><span class="attr">DET:</span></span><br><span class="line">  <span class="attr">model_dir:</span> <span class="string">https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_s_36e_ppvehicle.zip</span></span><br><span class="line">  <span class="attr">batch_size:</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="attr">MOT:</span></span><br><span class="line">  <span class="attr">model_dir:</span> <span class="string">https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_s_36e_ppvehicle.zip</span></span><br><span class="line">  <span class="attr">tracker_config:</span> <span class="string">/exp/work/video/PaddleDetection/backend/pp_config/tracker_config.yml</span></span><br><span class="line">  <span class="attr">batch_size:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">skip_frame_num:</span> <span class="number">3</span> <span class="comment"># preferably no more than 3</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="attr">VEHICLE_PLATE:</span></span><br><span class="line">  <span class="attr">det_model_dir:</span> <span class="string">https://bj.bcebos.com/v1/paddledet/models/pipeline/ch_PP-OCRv3_det_infer.tar.gz</span></span><br><span class="line">  <span class="attr">det_limit_side_len:</span> <span class="number">736</span></span><br><span class="line">  <span class="attr">det_limit_type:</span> <span class="string">"min"</span></span><br><span class="line">  <span class="attr">rec_model_dir:</span> <span class="string">https://bj.bcebos.com/v1/paddledet/models/pipeline/ch_PP-OCRv3_rec_infer.tar.gz</span></span><br><span class="line">  <span class="attr">rec_image_shape:</span> <span class="string">[3,</span> <span class="number">48</span><span class="string">,</span> <span class="number">320</span><span class="string">]</span></span><br><span class="line">  <span class="attr">rec_batch_num:</span> <span class="number">6</span></span><br><span class="line">  <span class="attr">word_dict_path:</span> <span class="string">/exp/work/video/PaddleDetection/deploy/pipeline/ppvehicle/rec_word_dict.txt</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="attr">VEHICLE_ATTR:</span></span><br><span class="line">  <span class="attr">model_dir:</span> <span class="string">https://bj.bcebos.com/v1/paddledet/models/pipeline/vehicle_attribute_model.zip</span></span><br><span class="line">  <span class="attr">batch_size:</span> <span class="number">8</span></span><br><span class="line">  <span class="attr">color_threshold:</span> <span class="number">0.5</span></span><br><span class="line">  <span class="attr">type_threshold:</span> <span class="number">0.5</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="attr">LANE_SEG:</span></span><br><span class="line">  <span class="attr">lane_seg_config:</span> <span class="string">deploy/pipeline/config/lane_seg_config.yml</span></span><br><span class="line">  <span class="attr">model_dir:</span> <span class="string">https://bj.bcebos.com/v1/paddledet/models/pipeline/pp_lite_stdc2_bdd100k.zip</span></span><br></pre></td></tr></table></figure><p>cli：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python deploy/pipeline/pipeline.py --config deploy/pipeline/config/cache/cfg_car.yml --device=gpu --video_file=demo_input/car.mp4 --output_dir=demo_output/</span><br></pre></td></tr></table></figure><p><img src="/2023/08/31/paddleDetection Demo/车辆属性.gif" alt></p><h3 id="车辆行为识别"><a href="#车辆行为识别" class="headerlink" title="车辆行为识别"></a>车辆行为识别</h3><h4 id="禁止停车"><a href="#禁止停车" class="headerlink" title="禁止停车"></a>禁止停车</h4><p>cfg：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">crop_thresh:</span> <span class="number">0.5</span></span><br><span class="line"><span class="attr">visual:</span> <span class="literal">True</span></span><br><span class="line"><span class="attr">warmup_frame:</span> <span class="number">50</span></span><br><span class="line"></span><br><span class="line"><span class="attr">MOT:</span></span><br><span class="line">  <span class="attr">model_dir:</span> <span class="string">https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_s_36e_ppvehicle.zip</span></span><br><span class="line">  <span class="attr">tracker_config:</span> <span class="string">/exp/work/video/PaddleDetection/deploy/pipeline/config/tracker_config.yml</span></span><br><span class="line">  <span class="attr">batch_size:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="attr">VEHICLE_PLATE:</span></span><br><span class="line">  <span class="attr">det_model_dir:</span> <span class="string">https://bj.bcebos.com/v1/paddledet/models/pipeline/ch_PP-OCRv3_det_infer.tar.gz</span></span><br><span class="line">  <span class="attr">det_limit_side_len:</span> <span class="number">736</span></span><br><span class="line">  <span class="attr">det_limit_type:</span> <span class="string">"min"</span></span><br><span class="line">  <span class="attr">rec_model_dir:</span> <span class="string">https://bj.bcebos.com/v1/paddledet/models/pipeline/ch_PP-OCRv3_rec_infer.tar.gz</span></span><br><span class="line">  <span class="attr">rec_image_shape:</span> <span class="string">[3,</span> <span class="number">48</span><span class="string">,</span> <span class="number">320</span><span class="string">]</span></span><br><span class="line">  <span class="attr">rec_batch_num:</span> <span class="number">6</span></span><br><span class="line">  <span class="attr">word_dict_path:</span> <span class="string">/exp/work/video/PaddleDetection/deploy/pipeline/ppvehicle/rec_word_dict.txt</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure><p>cli：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python deploy/pipeline/pipeline.py --config deploy/pipeline/config/cache/cfg_park.yml --region_type=custom --FLAGS.illegal_parking_time=5 --region_polygon 600 200 1400 200 1400 900 600 900 --device=gpu --video_file=demo_input/park.mp4 --output_dir=demo_output/</span><br></pre></td></tr></table></figure><p><img src="/2023/08/31/paddleDetection Demo/禁止停车.gif" alt></p><h2 id="人脸识别"><a href="#人脸识别" class="headerlink" title="人脸识别"></a>人脸识别</h2><h4 id="人脸识别-1"><a href="#人脸识别-1" class="headerlink" title="人脸识别"></a>人脸识别</h4><p>cfg（需paddleDetection集成arcface）：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">crop_thresh:</span> <span class="number">0.5</span></span><br><span class="line"><span class="attr">attr_thresh:</span> <span class="number">0.5</span></span><br><span class="line"><span class="attr">kpt_thresh:</span> <span class="number">0.2</span></span><br><span class="line"><span class="attr">visual:</span> <span class="literal">True</span></span><br><span class="line"><span class="attr">warmup_frame:</span> <span class="number">50</span></span><br><span class="line"></span><br><span class="line"><span class="attr">FACE_DET:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="attr">FACE_REC:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure><p>cli：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python deploy/pipeline/pipeline.py --config deploy/pipeline/config/cache/cfg_face.yml --device=gpu --video_file=demo_input/face.mp4 --output_dir=demo_output/</span><br></pre></td></tr></table></figure><p><img src="/2023/08/31/paddleDetection Demo/人脸识别.gif" style="zoom: 200%;"></p><h2 id="OCR"><a href="#OCR" class="headerlink" title="OCR"></a>OCR</h2><h3 id="视频OCR"><a href="#视频OCR" class="headerlink" title="视频OCR"></a>视频OCR</h3><p>cfg（需paddleDetection集成paddleOCR）：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">crop_thresh:</span> <span class="number">0.5</span></span><br><span class="line"><span class="attr">attr_thresh:</span> <span class="number">0.5</span></span><br><span class="line"><span class="attr">kpt_thresh:</span> <span class="number">0.2</span></span><br><span class="line"><span class="attr">visual:</span> <span class="literal">True</span></span><br><span class="line"><span class="attr">warmup_frame:</span> <span class="number">50</span></span><br><span class="line"></span><br><span class="line"><span class="attr">OCR:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure><p>cli：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python deploy/pipeline/pipeline.py --config deploy/pipeline/config/cache/cfg_ocr.yml --device=gpu --video_file=demo_input/ocr.mp4 --output_dir=demo_output/</span><br></pre></td></tr></table></figure><p><img src="/2023/08/31/paddleDetection Demo/OCR.gif" alt></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;PPHuman&quot;&gt;&lt;a href=&quot;#PPHuman&quot; class=&quot;headerlink&quot; title=&quot;PPHuman&quot;&gt;&lt;/a&gt;PPHuman&lt;/h2&gt;&lt;h3 id=&quot;行人属性识别&quot;&gt;&lt;a href=&quot;#行人属性识别&quot; class=&quot;headerlink&quot; title=&quot;行人属性识别&quot;&gt;&lt;/a&gt;行人属性识别&lt;/h3&gt;&lt;h4 id=&quot;行人属性&quot;&gt;&lt;a href=&quot;#行人属性&quot; class=&quot;headerlink&quot; title=&quot;行人属性&quot;&gt;&lt;/a&gt;行人属性&lt;/h4&gt;&lt;p&gt;cfg：&lt;/p&gt;
&lt;figure class=&quot;highlight yaml&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;crop_thresh:&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0.5&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;attr_thresh:&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0.5&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;kpt_thresh:&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0.2&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;visual:&lt;/span&gt; &lt;span class=&quot;literal&quot;&gt;True&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;warmup_frame:&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;50&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;DET:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;model_dir:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;batch_size:&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;MOT:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;model_dir:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;tracker_config:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;/exp/work/video/PaddleDetection/deploy/pipeline/config/tracker_config.yml&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;batch_size:&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;skip_frame_num:&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt; &lt;span class=&quot;comment&quot;&gt;# preferably no more than 3&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;enable:&lt;/span&gt; &lt;span class=&quot;literal&quot;&gt;True&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;KPT:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;model_dir:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;https://bj.bcebos.com/v1/paddledet/models/pipeline/dark_hrnet_w32_256x192.zip&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;batch_size:&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;8&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;ATTR:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;model_dir:&lt;/span&gt;  &lt;span class=&quot;string&quot;&gt;https://bj.bcebos.com/v1/paddledet/models/pipeline/PPLCNet_x1_0_person_attribute_945_infer.zip&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;batch_size:&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;8&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;enable:&lt;/span&gt; &lt;span class=&quot;literal&quot;&gt;True&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;cli：&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;python deploy/pipeline/pipeline.py --config deploy/pipeline/config/cache/cfg_human.yml --device=gpu --video_file=demo_input/human.mp4 --output_dir=demo_output/&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;img src=&quot;/2023/08/31/paddleDetection Demo/行人属性.gif&quot; alt&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="http://yoursite.com/categories/深度学习/"/>
    
    
    <category term="paddlepaddle" scheme="http://yoursite.com/tags/paddlepaddle/"/>
    
    <category term="opencv" scheme="http://yoursite.com/tags/opencv/"/>
    
  </entry>
  
  <entry>
    <title>LiveGBS国标GB/T28181视频流媒体平台</title>
    <link href="http://yoursite.com/2023/08/18/LiveGBS%E5%9B%BD%E6%A0%87GB-T28181%E8%A7%86%E9%A2%91%E6%B5%81%E5%AA%92%E4%BD%93%E5%B9%B3%E5%8F%B0/"/>
    <id>http://yoursite.com/2023/08/18/LiveGBS%E5%9B%BD%E6%A0%87GB-T28181%E8%A7%86%E9%A2%91%E6%B5%81%E5%AA%92%E4%BD%93%E5%B9%B3%E5%8F%B0/</id>
    <published>2023-08-18T08:34:45.000Z</published>
    <updated>2023-08-18T08:35:21.653Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><h2 id="软件包下载"><a href="#软件包下载" class="headerlink" title="软件包下载"></a>软件包下载</h2><p>LiveGBS GB28181流媒体服务下载地址：<a href="https://www.liveqing.com/docs/download/LiveGBS.html#%E7%89%88%E6%9C%AC%E4%B8%8B%E8%BD%BD" target="_blank" rel="noopener">https://www.liveqing.com/docs/download/LiveGBS.html#%E7%89%88%E6%9C%AC%E4%B8%8B%E8%BD%BD</a></p><p>选择windows版本的<code>LiveGBS 信令服务</code>和<code>LiveGBS</code>流媒体服务，免费版授权周期为26天，届时需要手动更新软件服务</p><p><img src="/2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/A.png" style="zoom:80%;"></p><h2 id="安装LiveGBS-GB28281"><a href="#安装LiveGBS-GB28281" class="headerlink" title="安装LiveGBS GB28281"></a>安装LiveGBS GB28281</h2><p>解压下载好的软件包，分别启动<code>LiveCMS.exe</code>和<code>LiveSMS.exe</code>，如果有默认端口被占用的情况可以修改对应的<code>livecms.ini</code>或<code>livesms.ini</code>配置文件，这里我将LiveGBS的默认端口从10000修改为10005</p><p>成功启动后后台出现livecms和livesms的图标</p><p><img src="/2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/B.png" alt></p><a id="more"></a><h3 id="配置LiveGBS"><a href="#配置LiveGBS" class="headerlink" title="配置LiveGBS"></a>配置LiveGBS</h3><p>进入<code>&lt;host&gt;:10005</code>，点击基础配置，修改信令服务配置和流媒体服务配置</p><p><img src="/2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/C.png" alt></p><p><img src="/2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/D.png" alt></p><h3 id="配置视频设备"><a href="#配置视频设备" class="headerlink" title="配置视频设备"></a>配置视频设备</h3><p>进入网络连接，选择对应的以太网，右键<code>属性</code>，选择<code>Internet 协议版本 4 (TCP/IPv4)</code>，选择<code>高级</code>，添加近端设备和摄像头的网段</p><p><img src="/2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/E.png" alt></p><p>进入设备IP，修改对应的网络配置、近端配置和摄像头配置</p><p><img src="/2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/F.png" alt></p><p><img src="/2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/G.png" alt></p><p><img src="/2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/H.png" alt></p><p>配置完成以后点击用户<code>admin</code>，选择重启设备</p><p><img src="/2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/I.png" alt></p><h2 id="视频拉流"><a href="#视频拉流" class="headerlink" title="视频拉流"></a>视频拉流</h2><p>回到<code>&lt;host&gt;:10005</code>，此时已经可以访问摄像头</p><p><img src="/2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/J.png" alt></p><p>点击国标设备，查看通道</p><p><img src="/2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/K.png" alt></p><p>看到已经配置好的摄像头信息</p><p><img src="/2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/L.png" alt></p><p><img src="/2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/M.png" alt></p><p>可从右下角获取视频拉流</p><p><img src="/2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/N.png" alt></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;软件包下载&quot;&gt;&lt;a href=&quot;#软件包下载&quot; class=&quot;headerlink&quot; title=&quot;软件包下载&quot;&gt;&lt;/a&gt;软件包下载&lt;/h2&gt;&lt;p&gt;LiveGBS GB28181流媒体服务下载地址：&lt;a href=&quot;https://www.liveqing.com/docs/download/LiveGBS.html#%E7%89%88%E6%9C%AC%E4%B8%8B%E8%BD%BD&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.liveqing.com/docs/download/LiveGBS.html#%E7%89%88%E6%9C%AC%E4%B8%8B%E8%BD%BD&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;选择windows版本的&lt;code&gt;LiveGBS 信令服务&lt;/code&gt;和&lt;code&gt;LiveGBS&lt;/code&gt;流媒体服务，免费版授权周期为26天，届时需要手动更新软件服务&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/A.png&quot; style=&quot;zoom:80%;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;安装LiveGBS-GB28281&quot;&gt;&lt;a href=&quot;#安装LiveGBS-GB28281&quot; class=&quot;headerlink&quot; title=&quot;安装LiveGBS GB28281&quot;&gt;&lt;/a&gt;安装LiveGBS GB28281&lt;/h2&gt;&lt;p&gt;解压下载好的软件包，分别启动&lt;code&gt;LiveCMS.exe&lt;/code&gt;和&lt;code&gt;LiveSMS.exe&lt;/code&gt;，如果有默认端口被占用的情况可以修改对应的&lt;code&gt;livecms.ini&lt;/code&gt;或&lt;code&gt;livesms.ini&lt;/code&gt;配置文件，这里我将LiveGBS的默认端口从10000修改为10005&lt;/p&gt;
&lt;p&gt;成功启动后后台出现livecms和livesms的图标&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/B.png&quot; alt&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="计算机视觉" scheme="http://yoursite.com/categories/计算机视觉/"/>
    
    
    <category term="LiveGBS" scheme="http://yoursite.com/tags/LiveGBS/"/>
    
  </entry>
  
  <entry>
    <title>paddleDetection-视频OCR</title>
    <link href="http://yoursite.com/2023/08/14/paddleDetection-%E8%A7%86%E9%A2%91OCR/"/>
    <id>http://yoursite.com/2023/08/14/paddleDetection-%E8%A7%86%E9%A2%91OCR/</id>
    <published>2023-08-14T03:15:55.000Z</published>
    <updated>2023-08-29T09:18:54.671Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><h2 id="PPOCR-V4"><a href="#PPOCR-V4" class="headerlink" title="PPOCR_V4"></a>PPOCR_V4</h2><p>安装百度最新ppocr_v4库，使用虚拟环境为py39_vio，本虚拟环境不可与人脸识别（py38_arcface）兼容（opencv版本不兼容）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install paddleocr --user -i https://mirror.baidu.com/pypi/simple</span><br></pre></td></tr></table></figure><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p><code>cfg_utils.py</code>新增cfg<code>--ocr</code>，设置True为开启，默认False</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parser.add_argument(</span><br><span class="line">    <span class="string">"--ocr"</span>,</span><br><span class="line">    type=bool,</span><br><span class="line">    default=<span class="literal">False</span>,</span><br><span class="line">    help=<span class="string">"use paddlepaddle-ocr"</span>)</span><br></pre></td></tr></table></figure><p><code>pipeline.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> python.visualize <span class="keyword">import</span> visualize_box_mask, visualize_attr, visualize_pose, visualize_action, visualize_vehicleplate, visualize_vehiclepress, visualize_lane, visualize_vehicle_retrograde, visualize_ocr</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PipePredictor</span><span class="params">(object)</span>:</span>  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, args, cfg, is_video=True, multi_camera=False)</span>:</span></span><br><span class="line">    self.ocr = args.ocr</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_video</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                    image_rgb,</span></span></span><br><span class="line"><span class="function"><span class="params">                    result,</span></span></span><br><span class="line"><span class="function"><span class="params">                    collector,</span></span></span><br><span class="line"><span class="function"><span class="params">                    frame_id,</span></span></span><br><span class="line"><span class="function"><span class="params">                    fps,</span></span></span><br><span class="line"><span class="function"><span class="params">                    entrance=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    records=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    center_traj=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    do_illegal_parking_recognition=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                    illegal_parking_dict=None)</span>:</span></span><br><span class="line">    image = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)</span><br><span class="line">    mot_res = copy.deepcopy(result.get(<span class="string">'mot'</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.ocr:</span><br><span class="line">        lock.acquire() <span class="comment"># 加锁，paddleOCR是线程不安全的</span></span><br><span class="line">        ocr_result = ocr.ocr(image, cls=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line">        lock.release()</span><br><span class="line">        ocr_boxes = [line[<span class="number">0</span>] <span class="keyword">for</span> line <span class="keyword">in</span> ocr_result]</span><br><span class="line">        ocr_txts = [line[<span class="number">1</span>][<span class="number">0</span>] <span class="keyword">for</span> line <span class="keyword">in</span> ocr_result]</span><br><span class="line">        ocr_scores = [line[<span class="number">1</span>][<span class="number">1</span>] <span class="keyword">for</span> line <span class="keyword">in</span> ocr_result]</span><br><span class="line">        </span><br><span class="line">        image = visualize_ocr(image, ocr_boxes, ocr_txts, ocr_scores)</span><br></pre></td></tr></table></figure><p><code>visualize.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_ocr</span><span class="params">(im, boxes, texts, score)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(im, str):</span><br><span class="line">        im = Image.open(im)</span><br><span class="line">        im = np.ascontiguousarray(np.copy(im))</span><br><span class="line">        im = cv2.cvtColor(im, cv2.COLOR_RGB2BGR)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        im = np.ascontiguousarray(np.copy(im))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建透明图层，为图像添加文字水印</span></span><br><span class="line">    im = Image.fromarray(im)</span><br><span class="line">    im = im.convert(<span class="string">'RGBA'</span>)</span><br><span class="line">    im_canvas = Image.new(<span class="string">'RGBA'</span>, im.size, (<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, res <span class="keyword">in</span> enumerate(texts):</span><br><span class="line">        <span class="keyword">if</span> boxes <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            box = boxes[i]</span><br><span class="line">            text = res</span><br><span class="line">            <span class="keyword">if</span> text == <span class="string">""</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            text_scale = max(<span class="number">1.0</span>, int(box[<span class="number">2</span>][<span class="number">1</span>] - box[<span class="number">1</span>][<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">            draw = ImageDraw.Draw(im_canvas)</span><br><span class="line">            draw.text(</span><br><span class="line">                (box[<span class="number">0</span>][<span class="number">0</span>], box[<span class="number">0</span>][<span class="number">1</span>]),</span><br><span class="line">                text,</span><br><span class="line">                font=ImageFont.truetype(font_file, size=int(text_scale)),</span><br><span class="line">                fill=(<span class="number">255</span>, <span class="number">255</span>, <span class="number">0</span>, <span class="number">85</span>)) <span class="comment"># 第四位是透明度</span></span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                draw.rectangle(</span><br><span class="line">                    ((box[<span class="number">0</span>][<span class="number">0</span>], box[<span class="number">0</span>][<span class="number">1</span>]), (box[<span class="number">2</span>][<span class="number">0</span>], box[<span class="number">2</span>][<span class="number">1</span>])),</span><br><span class="line">                    fill=<span class="literal">None</span>,</span><br><span class="line">                    outline=(<span class="number">255</span>, <span class="number">255</span>, <span class="number">0</span>),</span><br><span class="line">                    width=<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">except</span> ValueError:</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 复合图层</span></span><br><span class="line">    im = Image.alpha_composite(im, im_canvas)</span><br><span class="line">    im = im.convert(<span class="string">'RGB'</span>)</span><br><span class="line">    <span class="comment"># 还原连续存储数组</span></span><br><span class="line">    im = np.ascontiguousarray(np.copy(im))</span><br><span class="line">    <span class="keyword">return</span> im</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><p>在<code>deploy/pipeline/config</code>下创建视频ocr的yml文件<code>infer_cfg_ppocr.yml</code>，写入基本参数</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">crop_thresh:</span> <span class="number">0.5</span></span><br><span class="line"><span class="attr">visual:</span> <span class="literal">True</span></span><br><span class="line"><span class="attr">warmup_frame:</span> <span class="number">50</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python deploy/pipeline/pipeline.py --config deploy/pipeline/config/infer_cfg_ppocr.yml --device=gpu --video_file=demo_input/car_t1.mp4 --output_dir=demo_output --ocr=True</span><br></pre></td></tr></table></figure><p><img src="/2023/08/14/paddleDetection-视频OCR/A.png" alt></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;PPOCR-V4&quot;&gt;&lt;a href=&quot;#PPOCR-V4&quot; class=&quot;headerlink&quot; title=&quot;PPOCR_V4&quot;&gt;&lt;/a&gt;PPOCR_V4&lt;/h2&gt;&lt;p&gt;安装百度最新ppocr_v4库，使用虚拟环境为py39_vio，本虚拟环境不可与人脸识别（py38_arcface）兼容（opencv版本不兼容）&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;pip install paddleocr --user -i https://mirror.baidu.com/pypi/simple&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;代码&quot;&gt;&lt;a href=&quot;#代码&quot; class=&quot;headerlink&quot; title=&quot;代码&quot;&gt;&lt;/a&gt;代码&lt;/h2&gt;&lt;p&gt;&lt;code&gt;cfg_utils.py&lt;/code&gt;新增cfg&lt;code&gt;--ocr&lt;/code&gt;，设置True为开启，默认False&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;parser.add_argument(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;--ocr&quot;&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    type=bool,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    default=&lt;span class=&quot;literal&quot;&gt;False&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    help=&lt;span class=&quot;string&quot;&gt;&quot;use paddlepaddle-ocr&quot;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;pipeline.py&lt;/code&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; python.visualize &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; visualize_box_mask, visualize_attr, visualize_pose, visualize_action, visualize_vehicleplate, visualize_vehiclepress, visualize_lane, visualize_vehicle_retrograde, visualize_ocr&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;PipePredictor&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(object)&lt;/span&gt;:&lt;/span&gt;  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, args, cfg, is_video=True, multi_camera=False)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    	self.ocr = args.ocr&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;visualize_video&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self,&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;                    image_rgb,&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;                    result,&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;                    collector,&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;                    frame_id,&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;                    fps,&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;                    entrance=None,&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;                    records=None,&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;                    center_traj=None,&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;                    do_illegal_parking_recognition=False,&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;                    illegal_parking_dict=None)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    image = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    mot_res = copy.deepcopy(result.get(&lt;span class=&quot;string&quot;&gt;&#39;mot&#39;&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; self.ocr:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        lock.acquire() &lt;span class=&quot;comment&quot;&gt;# 加锁，paddleOCR是线程不安全的&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ocr_result = ocr.ocr(image, cls=&lt;span class=&quot;literal&quot;&gt;True&lt;/span&gt;)[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        lock.release()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ocr_boxes = [line[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;] &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; line &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; ocr_result]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ocr_txts = [line[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;] &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; line &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; ocr_result]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ocr_scores = [line[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;] &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; line &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; ocr_result]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        image = visualize_ocr(image, ocr_boxes, ocr_txts, ocr_scores)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;visualize.py&lt;/code&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;33&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;34&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;35&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;36&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;37&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;38&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;39&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;40&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;41&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;42&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;43&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;visualize_ocr&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(im, boxes, texts, score)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; isinstance(im, str):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        im = Image.open(im)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        im = np.ascontiguousarray(np.copy(im))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        im = cv2.cvtColor(im, cv2.COLOR_RGB2BGR)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        im = np.ascontiguousarray(np.copy(im))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;# 创建透明图层，为图像添加文字水印&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    im = Image.fromarray(im)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    im = im.convert(&lt;span class=&quot;string&quot;&gt;&#39;RGBA&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    im_canvas = Image.new(&lt;span class=&quot;string&quot;&gt;&#39;RGBA&#39;&lt;/span&gt;, im.size, (&lt;span class=&quot;number&quot;&gt;255&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;255&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;255&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; i, res &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; enumerate(texts):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; boxes &lt;span class=&quot;keyword&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;literal&quot;&gt;None&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            box = boxes[i]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            text = res&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; text == &lt;span class=&quot;string&quot;&gt;&quot;&quot;&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                &lt;span class=&quot;keyword&quot;&gt;continue&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            text_scale = max(&lt;span class=&quot;number&quot;&gt;1.0&lt;/span&gt;, int(box[&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;] - box[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;]))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            draw = ImageDraw.Draw(im_canvas)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            draw.text(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                (box[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;], box[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;]),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                text,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                font=ImageFont.truetype(font_file, size=int(text_scale)),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                fill=(&lt;span class=&quot;number&quot;&gt;255&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;255&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;85&lt;/span&gt;)) &lt;span class=&quot;comment&quot;&gt;# 第四位是透明度&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;try&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                draw.rectangle(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                    ((box[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;], box[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;]), (box[&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;], box[&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;])),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                    fill=&lt;span class=&quot;literal&quot;&gt;None&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                    outline=(&lt;span class=&quot;number&quot;&gt;255&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;255&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                    width=&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;except&lt;/span&gt; ValueError:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                &lt;span class=&quot;keyword&quot;&gt;pass&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;# 复合图层&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    im = Image.alpha_composite(im, im_canvas)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    im = im.convert(&lt;span class=&quot;string&quot;&gt;&#39;RGB&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;# 还原连续存储数组&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    im = np.ascontiguousarray(np.copy(im))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; im&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    
    <category term="OCR" scheme="http://yoursite.com/tags/OCR/"/>
    
    <category term="paddlepaddle" scheme="http://yoursite.com/tags/paddlepaddle/"/>
    
  </entry>
  
  <entry>
    <title>paddleDetection:OpenCV检测框转中文</title>
    <link href="http://yoursite.com/2023/08/09/paddleDetection-OpenCV%E6%A3%80%E6%B5%8B%E6%A1%86%E8%BD%AC%E4%B8%AD%E6%96%87/"/>
    <id>http://yoursite.com/2023/08/09/paddleDetection-OpenCV%E6%A3%80%E6%B5%8B%E6%A1%86%E8%BD%AC%E4%B8%AD%E6%96%87/</id>
    <published>2023-08-09T08:29:45.000Z</published>
    <updated>2024-01-25T09:20:13.155Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><font color="gold">注：OpenCV不能直接显示中文，通过PIL转换会损失一部分算力性能</font><p><img src="/2023/08/09/paddleDetection-OpenCV检测框转中文/A.png" alt></p><h2 id="修改源码（可视化）"><a href="#修改源码（可视化）" class="headerlink" title="修改源码（可视化）"></a>修改源码（可视化）</h2><p><code>./deploy/python/visualize.py</code></p><h3 id="增加导入字体库和字体文件"><a href="#增加导入字体库和字体文件" class="headerlink" title="增加导入字体库和字体文件"></a>增加导入字体库和字体文件</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image, ImageDraw, ImageFile, ImageFont</span><br><span class="line"></span><br><span class="line">font_file = <span class="string">'/exp/work/video/PaddleDetection/deploy/pipeline/SourceHanSansCN-Medium.otf'</span></span><br></pre></td></tr></table></figure><h3 id="visualize-attr"><a href="#visualize-attr" class="headerlink" title="visualize_attr"></a>visualize_attr</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_attr</span><span class="params">(im, results, boxes=None, is_mtmct=False)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(im, str):</span><br><span class="line">        im = Image.open(im)</span><br><span class="line">        im = np.ascontiguousarray(np.copy(im))</span><br><span class="line">        im = cv2.cvtColor(im, cv2.COLOR_RGB2BGR)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        im = np.ascontiguousarray(np.copy(im))</span><br><span class="line"></span><br><span class="line">    line_inter = im.shape[<span class="number">0</span>] / <span class="number">40.</span></span><br><span class="line">    text_scale = max(<span class="number">0.5</span>, im.shape[<span class="number">0</span>] / <span class="number">100.</span>)</span><br><span class="line">    <span class="comment"># 将nparray图像转PIL图像</span></span><br><span class="line">    im = Image.fromarray(im)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, res <span class="keyword">in</span> enumerate(results):</span><br><span class="line">        print(i, res)</span><br><span class="line">        <span class="keyword">if</span> boxes <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            text_w = <span class="number">3</span></span><br><span class="line">            text_h = <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> is_mtmct:</span><br><span class="line">            box = boxes[i]  <span class="comment"># multi camera, bbox shape is x,y, w,h</span></span><br><span class="line">            text_w = int(box[<span class="number">0</span>]) + <span class="number">3</span></span><br><span class="line">            text_h = int(box[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            box = boxes[i]  <span class="comment"># single camera, bbox shape is 0, 0, x,y, w,h</span></span><br><span class="line">            text_w = int(box[<span class="number">2</span>]) + <span class="number">3</span></span><br><span class="line">            text_h = int(box[<span class="number">3</span>])</span><br><span class="line">        <span class="keyword">for</span> text <span class="keyword">in</span> res:</span><br><span class="line">            text_h += int(line_inter)</span><br><span class="line">            text_loc = (text_w, text_h)</span><br><span class="line">            <span class="comment"># 写入</span></span><br><span class="line">            draw = ImageDraw.Draw(im)</span><br><span class="line">            draw.text(</span><br><span class="line">                text_loc,</span><br><span class="line">                text,</span><br><span class="line">                font=ImageFont.truetype(font_file, size=int(text_scale)), <span class="comment"># 字体位置</span></span><br><span class="line">                fill=(<span class="number">0</span>, <span class="number">255</span>, <span class="number">255</span>))</span><br><span class="line">    <span class="comment"># 还原连续存储数组</span></span><br><span class="line">    im = np.ascontiguousarray(np.copy(im))</span><br><span class="line">    <span class="keyword">return</span> im</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="visualize-vehicleplate"><a href="#visualize-vehicleplate" class="headerlink" title="visualize_vehicleplate"></a>visualize_vehicleplate</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_vehicleplate</span><span class="params">(im, results, boxes=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(im, str):</span><br><span class="line">        im = Image.open(im)</span><br><span class="line">        im = np.ascontiguousarray(np.copy(im))</span><br><span class="line">        im = cv2.cvtColor(im, cv2.COLOR_RGB2BGR)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        im = np.ascontiguousarray(np.copy(im))</span><br><span class="line"></span><br><span class="line">    text_scale = max(<span class="number">1.0</span>, im.shape[<span class="number">0</span>] / <span class="number">400.</span>)</span><br><span class="line">    line_inter = im.shape[<span class="number">0</span>] / <span class="number">40.</span></span><br><span class="line">    <span class="comment"># 将nparray图像转PIL图像</span></span><br><span class="line">    im = Image.fromarray(im)</span><br><span class="line">    <span class="keyword">for</span> i, res <span class="keyword">in</span> enumerate(results):</span><br><span class="line">        <span class="keyword">if</span> boxes <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            text_w = <span class="number">3</span></span><br><span class="line">            text_h = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            box = boxes[i]</span><br><span class="line">            text = res</span><br><span class="line">            <span class="keyword">if</span> text == <span class="string">""</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            text_w = int(box[<span class="number">2</span>])</span><br><span class="line">            text_h = int(box[<span class="number">5</span>] + box[<span class="number">3</span>])</span><br><span class="line">            text_loc = (text_w, text_h)</span><br><span class="line">            <span class="comment"># 写入</span></span><br><span class="line">            draw = ImageDraw.Draw(im)</span><br><span class="line">            draw.text(</span><br><span class="line">                text_loc,</span><br><span class="line">                text,</span><br><span class="line">                font=ImageFont.truetype(font_file, size=int(text_scale)), <span class="comment"># 字体位置</span></span><br><span class="line">                fill=(<span class="number">0</span>, <span class="number">255</span>, <span class="number">255</span>))</span><br><span class="line">    <span class="comment"># 还原连续存储数组</span></span><br><span class="line">    im = np.ascontiguousarray(np.copy(im))</span><br><span class="line">    <span class="keyword">return</span> im</span><br></pre></td></tr></table></figure><h3 id="visualize-action"><a href="#visualize-action" class="headerlink" title="visualize_action"></a>visualize_action</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_action</span><span class="params">(im,</span></span></span><br><span class="line"><span class="function"><span class="params">                     mot_boxes,</span></span></span><br><span class="line"><span class="function"><span class="params">                     action_visual_collector=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                     action_text=<span class="string">""</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                     video_action_score=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                     video_action_text=<span class="string">""</span>)</span>:</span></span><br><span class="line">    im = cv2.imread(im) <span class="keyword">if</span> isinstance(im, str) <span class="keyword">else</span> im</span><br><span class="line">    im_h, im_w = im.shape[:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    text_scale = max(<span class="number">1</span>, im.shape[<span class="number">1</span>] / <span class="number">40.</span>)</span><br><span class="line">    text_thickness = <span class="number">2</span></span><br><span class="line">    <span class="comment"># 将nparray图像转PIL图像</span></span><br><span class="line">    im = Image.fromarray(im)</span><br><span class="line">    <span class="keyword">if</span> action_visual_collector:</span><br><span class="line">        id_action_dict = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> collector, action_type <span class="keyword">in</span> zip(action_visual_collector, action_text):</span><br><span class="line">            id_detected = collector.get_visualize_ids()</span><br><span class="line">            <span class="keyword">for</span> pid <span class="keyword">in</span> id_detected:</span><br><span class="line">                id_action_dict[pid] = id_action_dict.get(pid, [])</span><br><span class="line">                id_action_dict[pid].append(action_type)</span><br><span class="line">        <span class="keyword">for</span> mot_box <span class="keyword">in</span> mot_boxes:</span><br><span class="line">            <span class="comment"># mot_box is a format with [mot_id, class, score, xmin, ymin, w, h] </span></span><br><span class="line">            <span class="keyword">if</span> mot_box[<span class="number">0</span>] <span class="keyword">in</span> id_action_dict:</span><br><span class="line">                text_position = (int(mot_box[<span class="number">3</span>] + mot_box[<span class="number">5</span>] * <span class="number">0.75</span>),</span><br><span class="line">                                 int(mot_box[<span class="number">4</span>] - <span class="number">10</span>))</span><br><span class="line">                display_text = <span class="string">', '</span>.join(id_action_dict[mot_box[<span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line">                draw = ImageDraw.Draw(im)</span><br><span class="line">                draw.text(</span><br><span class="line">                    text_position,</span><br><span class="line">                    display_text,</span><br><span class="line">                    font=ImageFont.truetype(size=int(text_scale)),  <span class="comment"># 字体位置</span></span><br><span class="line">                    fill=(<span class="number">0</span>, <span class="number">0</span>, <span class="number">255</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> video_action_score:</span><br><span class="line">        draw = ImageDraw.Draw(im)</span><br><span class="line">        draw.text(</span><br><span class="line">            (int(im_w / <span class="number">2</span>), int(<span class="number">15</span> * text_scale) + <span class="number">5</span>),</span><br><span class="line">            video_action_text + <span class="string">': %.2f'</span> % video_action_score,</span><br><span class="line">            font=ImageFont.truetype(font_file, size=int(text_scale)),  <span class="comment"># 字体位置</span></span><br><span class="line">            fill=(<span class="number">0</span>, <span class="number">0</span>, <span class="number">255</span>))</span><br><span class="line">    <span class="comment"># 还原连续存储数组</span></span><br><span class="line">    im = np.ascontiguousarray(np.copy(im))</span><br><span class="line">    <span class="keyword">return</span> im</span><br></pre></td></tr></table></figure><h2 id="修改源码（属性）"><a href="#修改源码（属性）" class="headerlink" title="修改源码（属性）"></a>修改源码（属性）</h2><p>修改对应中文模块</p><p><code>./deploy/pipeline/</code></p><ul><li>pipeline.py</li></ul><p><code>./deploy/pipeline/ppvehicle</code></p><ul><li>vehicle_attr.py</li><li>vehicle_plate.py</li></ul><p><code>./deploy/pipeline/pphuman</code></p><ul><li>attr_infer.py</li></ul><h2 id="执行脚本"><a href="#执行脚本" class="headerlink" title="执行脚本"></a>执行脚本</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> pphuman</span><br><span class="line">python deploy/pipeline/pipeline.py --config deploy/pipeline/config/infer_cfg_pphuman.yml --video_file=demo_input/act1.mp4 --device=gpu --output_dir=demo_output</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> ppvehicle</span><br><span class="line">python deploy/pipeline/pipeline.py --config deploy/pipeline/config/infer_cfg_ppvehicle.yml --video_file=demo_input/car_t1.mp4 --device=gpu --output_dir=demo_output</span><br></pre></td></tr></table></figure><p><img src="/2023/08/09/paddleDetection-OpenCV检测框转中文/B.png" alt></p>]]></content>
    
    
    <summary type="html">&lt;font color=&quot;gold&quot;&gt;注：OpenCV不能直接显示中文，通过PIL转换会损失一部分算力性能&lt;/font&gt;

&lt;p&gt;&lt;img src=&quot;/2023/08/09/paddleDetection-OpenCV检测框转中文/A.png&quot; alt&gt;&lt;/p&gt;
&lt;h2 id=&quot;修改源码（可视化）&quot;&gt;&lt;a href=&quot;#修改源码（可视化）&quot; class=&quot;headerlink&quot; title=&quot;修改源码（可视化）&quot;&gt;&lt;/a&gt;修改源码（可视化）&lt;/h2&gt;&lt;p&gt;&lt;code&gt;./deploy/python/visualize.py&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&quot;增加导入字体库和字体文件&quot;&gt;&lt;a href=&quot;#增加导入字体库和字体文件&quot; class=&quot;headerlink&quot; title=&quot;增加导入字体库和字体文件&quot;&gt;&lt;/a&gt;增加导入字体库和字体文件&lt;/h3&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; PIL &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; Image, ImageDraw, ImageFile, ImageFont&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;font_file = &lt;span class=&quot;string&quot;&gt;&#39;/exp/work/video/PaddleDetection/deploy/pipeline/SourceHanSansCN-Medium.otf&#39;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;visualize-attr&quot;&gt;&lt;a href=&quot;#visualize-attr&quot; class=&quot;headerlink&quot; title=&quot;visualize_attr&quot;&gt;&lt;/a&gt;visualize_attr&lt;/h3&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;33&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;34&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;35&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;36&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;37&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;38&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;39&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;visualize_attr&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(im, results, boxes=None, is_mtmct=False)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; isinstance(im, str):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        im = Image.open(im)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        im = np.ascontiguousarray(np.copy(im))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        im = cv2.cvtColor(im, cv2.COLOR_RGB2BGR)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        im = np.ascontiguousarray(np.copy(im))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    line_inter = im.shape[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;] / &lt;span class=&quot;number&quot;&gt;40.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    text_scale = max(&lt;span class=&quot;number&quot;&gt;0.5&lt;/span&gt;, im.shape[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;] / &lt;span class=&quot;number&quot;&gt;100.&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;# 将nparray图像转PIL图像&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    im = Image.fromarray(im)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; i, res &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; enumerate(results):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        print(i, res)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; boxes &lt;span class=&quot;keyword&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;literal&quot;&gt;None&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            text_w = &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            text_h = &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;elif&lt;/span&gt; is_mtmct:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            box = boxes[i]  &lt;span class=&quot;comment&quot;&gt;# multi camera, bbox shape is x,y, w,h&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            text_w = int(box[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;]) + &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            text_h = int(box[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            box = boxes[i]  &lt;span class=&quot;comment&quot;&gt;# single camera, bbox shape is 0, 0, x,y, w,h&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            text_w = int(box[&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;]) + &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            text_h = int(box[&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; text &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; res:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            text_h += int(line_inter)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            text_loc = (text_w, text_h)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;comment&quot;&gt;# 写入&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            draw = ImageDraw.Draw(im)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            draw.text(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                text_loc,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                text,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                font=ImageFont.truetype(font_file, size=int(text_scale)), &lt;span class=&quot;comment&quot;&gt;# 字体位置&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                fill=(&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;255&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;255&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;# 还原连续存储数组&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    im = np.ascontiguousarray(np.copy(im))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; im&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="深度学习" scheme="http://yoursite.com/categories/深度学习/"/>
    
    
    <category term="paddlepaddle" scheme="http://yoursite.com/tags/paddlepaddle/"/>
    
    <category term="opencv" scheme="http://yoursite.com/tags/opencv/"/>
    
  </entry>
  
  <entry>
    <title>arcface_paddle</title>
    <link href="http://yoursite.com/2023/08/08/arcface-paddle/"/>
    <id>http://yoursite.com/2023/08/08/arcface-paddle/</id>
    <published>2023-08-08T09:12:11.000Z</published>
    <updated>2023-08-09T10:03:27.504Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><h3 id="GPU（物理）"><a href="#GPU（物理）" class="headerlink" title="GPU（物理）"></a>GPU（物理）</h3><ul><li><p>NVIDIA 3090*2</p></li><li><p>显卡驱动 515.43.04</p></li><li><p>CUDA版本 11.7</p></li><li><p>CUDAtoolkit (cuda_11.7.0_515.43.04_linux)</p></li><li><p>cuDNN (v8.4.1)</p></li><li><p><font color="gold">paddlepaddle 多卡训练需要NCLL支持</font> (ncll v2.12.12 cuda11.7)</p></li></ul><h3 id="paddlepaddle版本"><a href="#paddlepaddle版本" class="headerlink" title="paddlepaddle版本"></a>paddlepaddle版本</h3><ul><li>paddlepaddle-gpu==2.2.0rc0（虚拟环境cuda11.2）</li></ul><h3 id="python环境"><a href="#python环境" class="headerlink" title="python环境"></a>python环境</h3><ul><li><p>CentOS7.9</p></li><li><p>anaconda3</p></li><li><p>python3.8</p></li></ul><h2 id="anaconda安装insightface"><a href="#anaconda安装insightface" class="headerlink" title="anaconda安装insightface"></a>anaconda安装insightface</h2><p><font color="gold">重要：pillow版本建议选择9.5 否则过高会导致安装insightface报错</font> （错误原因：pillow10移除了getsize方法，需要修改对应位置源码为<code>getbbox</code>或 <code>getlength</code>）</p><p>告警信息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tools/test_recognition.py:<span class="number">627</span>: DeprecationWarning: getsize <span class="keyword">is</span> deprecated <span class="keyword">and</span> will be removed <span class="keyword">in</span> Pillow <span class="number">10</span> (<span class="number">2023</span><span class="number">-07</span><span class="number">-01</span>). Use getbbox <span class="keyword">or</span> getlength instead.</span><br><span class="line">  tw = font.getsize(text)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><h4 id="环境安装"><a href="#环境安装" class="headerlink" title="环境安装"></a>环境安装</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> .</span></span><br><span class="line">conda install paddlepaddle-gpu==2.2.0rc0 cudatoolkit=11.2 -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/Paddle/ -c conda-forge</span><br><span class="line"><span class="meta">#</span><span class="bash"> insightface</span></span><br><span class="line">pip install -r requirements.txt -i https://mirror.baidu.com/pypi/simple</span><br><span class="line"><span class="meta">#</span><span class="bash"> insightface/recongition/arcface_paddle/</span></span><br><span class="line">pip install -r requirement.txt -i https://mirror.baidu.com/pypi/simple</span><br><span class="line"><span class="meta">#</span><span class="bash"> insightface-paddle</span></span><br><span class="line">pip install insightface-paddle -i https://mirror.baidu.com/pypi/simple</span><br></pre></td></tr></table></figure><a id="more"></a><h4 id="获取数据集"><a href="#获取数据集" class="headerlink" title="获取数据集"></a>获取数据集</h4><p><a href="https://github.com/deepinsight/insightface/tree/master/recognition/_datasets_" target="_blank" rel="noopener">https://github.com/deepinsight/insightface/tree/master/recognition/_datasets_</a></p><ul><li>MS1M_v2: MS1M-ArcFace</li><li>MS1M_v3: MS1M-RetinaFace</li></ul><p>从 MXNet 格式数据集抽取图像</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python tools/mx_recordio_2_images.py --root_dir ms1m-retinaface-t1/ --output_dir MS1M_v3/</span><br></pre></td></tr></table></figure><p>数据抽取完成后，格式如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">MS1M_v3</span><br><span class="line">|_ images</span><br><span class="line">|  |_ 00000001.jpg</span><br><span class="line">|  |_ ...</span><br><span class="line">|  |_ 05179510.jpg</span><br><span class="line">|_ label.txt</span><br><span class="line">|_ agedb_30.bin</span><br><span class="line">|_ cfp_ff.bin</span><br><span class="line">|_ cfp_fp.bin</span><br><span class="line">|_ lfw.bin</span><br></pre></td></tr></table></figure><p>标签数据格式如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 图像路径与标签的分隔符: &quot;\t&quot;</span><br><span class="line"># 以下是 label.txt 每行的格式</span><br><span class="line">images/00000001.jpg 0</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h4 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h4><p>使用双卡</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export CUDA_VISIBLE_DEVICES=0,1</span><br></pre></td></tr></table></figure><p>训练脚本<code>scripts/train_static.sh</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 降级scipy,scipy版本过高会报错</span></span><br><span class="line">pip install scipy==1.7.1 -i https://mirror.baidu.com/pypi/simple</span><br></pre></td></tr></table></figure><p>训练静态模型</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">python -m paddle.distributed.launch --gpus=1 tools/train.py \</span><br><span class="line">    --config_file configs/ms1mv3_r50.py \</span><br><span class="line">    --is_static True \</span><br><span class="line">    --backbone FresResNet50 \</span><br><span class="line">    --classifier LargeScaleClassifier \</span><br><span class="line">    --embedding_size 512 \</span><br><span class="line">    --model_parallel True \</span><br><span class="line">    --dropout 0.0 \</span><br><span class="line">    --sample_ratio 0.1 \</span><br><span class="line">    --loss ArcFace \</span><br><span class="line">    --batch_size 64 \</span><br><span class="line">    --dataset MS1M_v3 \</span><br><span class="line">    --num_classes 93431 \</span><br><span class="line">    --data_dir MS1M_v3/ \</span><br><span class="line">    --label_file MS1M_v3/label.txt \</span><br><span class="line">    --is_bin False \</span><br><span class="line">    --log_interval_step 100 \</span><br><span class="line">    --validation_interval_step 2000 \</span><br><span class="line">    --fp16 True \</span><br><span class="line">    --use_dynamic_loss_scaling True \</span><br><span class="line">    --init_loss_scaling 27648.0 \</span><br><span class="line">    --num_workers 8 \</span><br><span class="line">    --train_unit 'epoch' \</span><br><span class="line">    --warmup_num 0 \</span><br><span class="line">    --train_num 25 \</span><br><span class="line">    --decay_boundaries "10,16,22" \</span><br><span class="line">    --output MS1M_v3_arcface_static_0.1</span><br></pre></td></tr></table></figure><p><font color="red">3090单卡容易爆显存</font> batch_size可由128调整至64，或开启多卡训练，需ncll</p><p>模型评价<code>sh scripts/validation_static.sh</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">python tools/validation.py \</span><br><span class="line">    --is_static True \</span><br><span class="line">    --backbone FresResNet50 \</span><br><span class="line">    --embedding_size 512 \</span><br><span class="line">    --checkpoint_dir MS1M_v3_arcface_static_0.1/FresResNet50/24 \</span><br><span class="line">    --data_dir MS1M_v3/ \</span><br><span class="line">    --val_targets lfw,cfp_fp,agedb_30 \</span><br><span class="line">    --batch_size 64</span><br></pre></td></tr></table></figure><p>模型导出<code>sh scripts/export_static.sh</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">python tools/export.py \</span><br><span class="line">    --is_static True \</span><br><span class="line">    --export_type paddle \</span><br><span class="line">    --backbone FresResNet50 \</span><br><span class="line">    --embedding_size 512 \</span><br><span class="line">    --checkpoint_dir MS1M_v3_arcface_static_0.1/FresResNet50/24 \</span><br><span class="line">    --output_dir MS1M_v3_arcface_static_0.1/FresResNet50/exported_model</span><br></pre></td></tr></table></figure><p>模型推理<code>sh scripts/inference.sh</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python tools/inference.py \</span><br><span class="line">    --export_type paddle \</span><br><span class="line">    --model_file MS1M_v3_arcface_static_0.1/FresResNet50/exported_model/FresResNet50.pdmodel \</span><br><span class="line">    --params_file MS1M_v3_arcface_static_0.1/FresResNet50/exported_model/FresResNet50.pdiparams \</span><br><span class="line">    --image_path MS1M_v3/images/00000001.jpg</span><br></pre></td></tr></table></figure><h4 id="构建人像索引"><a href="#构建人像索引" class="headerlink" title="构建人像索引"></a>构建人像索引</h4><ul><li>建立图像文件夹</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">fridends</span><br><span class="line">|_ gallery</span><br><span class="line">|  |_ Chandler</span><br><span class="line">|      |_ Chandler01.jpg</span><br><span class="line">|      |_ ...</span><br><span class="line">|      |_ Chandler50.jpg</span><br><span class="line">|  |_ ...</span><br><span class="line">|  |_ Ross</span><br><span class="line">|      |_ Ross01.jpg</span><br><span class="line">|      |_ ...</span><br><span class="line">|      |_ Ross50.jpg</span><br><span class="line">|_ label.txt</span><br></pre></td></tr></table></figure><ul><li>建立索引文件<code>label.txt</code></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">./Chandler/Chandler00037.jpgChandler</span><br><span class="line">./Chandler/Chandler00021.pngChandler</span><br><span class="line">./Chandler/Chandler00040.jpgChandler</span><br><span class="line">./Chandler/Chandler00041.jpgChandler</span><br><span class="line">./Chandler/Chandler00004.pngChandler</span><br><span class="line">./Chandler/Chandler00034.pngChandler</span><br><span class="line">./Chandler/Chandler00008.pngChandler</span><br><span class="line">...</span><br><span class="line">./Ross/Ross00016.jpgRoss</span><br><span class="line">./Ross/Ross00022.jpgRoss</span><br><span class="line">./Ross/Ross00019.jpgRoss</span><br><span class="line">./Ross/Ross00024.jpgRoss</span><br><span class="line">./Ross/Ross00001.jpgRoss</span><br><span class="line">./Ross/Ross00039.jpgRoss</span><br><span class="line">./Ross/Ross00038.jpgRoss</span><br><span class="line">./Ross/Ross00017.jpgRoss</span><br><span class="line">./Ross/Ross00034.jpgRoss</span><br><span class="line">./Ross/Ross00002.pngRoss</span><br></pre></td></tr></table></figure><ul><li>构建索引</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insightfacepaddle --build_index ./demo/friends/index.bin --img_dir ./demo/friends/gallery --label ./demo/friends/gallery/label.txt</span><br></pre></td></tr></table></figure><h4 id="检测图片"><a href="#检测图片" class="headerlink" title="检测图片"></a>检测图片</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python tools/test_recognition.py --det --rec --index=./demo/friends/index.bin --input=./test/测试2.jpg --output=./output</span><br></pre></td></tr></table></figure><p><img src="/2023/08/08/arcface-paddle/A.png" alt></p><h4 id="预测图片"><a href="#预测图片" class="headerlink" title="预测图片"></a>预测图片</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python tools/test_recognition.py --det --rec --index=./demo/friends/index.bin --input=./test/测试2.jpg --output=./output</span><br></pre></td></tr></table></figure><p><img src="/2023/08/08/arcface-paddle/B.png" alt></p><h4 id="预测视频"><a href="#预测视频" class="headerlink" title="预测视频"></a>预测视频</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python tools/test_recognition.py --det --rec --index=./demo/friends/index.bin --input=./test/mp4v.mp4 --output=./output</span><br></pre></td></tr></table></figure><h4 id="python脚本"><a href="#python脚本" class="headerlink" title="python脚本"></a>python脚本</h4><p>脚本封装位置：arcface_paddle/python</p><ul><li>构建索引</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> insightface_paddle <span class="keyword">as</span> face</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line">parser = face.parser()</span><br><span class="line">args = parser.parse_args()</span><br><span class="line">args.build_index = <span class="string">"./demo/friends/index.bin"</span></span><br><span class="line">args.img_dir = <span class="string">"./demo/friends/gallery"</span></span><br><span class="line">args.label = <span class="string">"./demo/friends/gallery/label.txt"</span></span><br><span class="line">predictor = face.InsightFace(args)</span><br><span class="line">predictor.build_index()</span><br></pre></td></tr></table></figure><ul><li>视频</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> insightface_paddle <span class="keyword">as</span> face</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line">PR = os.path.dirname(os.path.abspath(__file__))</span><br><span class="line"></span><br><span class="line">logging.basicConfig(level=logging.INFO)</span><br><span class="line"></span><br><span class="line">parser = face.parser()</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">args.det = <span class="literal">True</span></span><br><span class="line">args.rec = <span class="literal">True</span></span><br><span class="line">args.index = os.path.join(PR, <span class="string">"demo/friends/index.bin"</span>)</span><br><span class="line">args.output = os.path.join(PR, <span class="string">"output"</span>)</span><br><span class="line">input_path = os.path.join(PR, <span class="string">"test/MP4V.mp4"</span>)</span><br><span class="line"></span><br><span class="line">predictor = face.InsightFace(args)</span><br><span class="line">res = predictor.predict(input_path, print_info=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> res:</span><br><span class="line">    print(_.get(<span class="string">'labels'</span>))</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;环境&quot;&gt;&lt;a href=&quot;#环境&quot; class=&quot;headerlink&quot; title=&quot;环境&quot;&gt;&lt;/a&gt;环境&lt;/h2&gt;&lt;h3 id=&quot;GPU（物理）&quot;&gt;&lt;a href=&quot;#GPU（物理）&quot; class=&quot;headerlink&quot; title=&quot;GPU（物理）&quot;&gt;&lt;/a&gt;GPU（物理）&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;NVIDIA 3090*2&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;显卡驱动 515.43.04&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;CUDA版本 11.7&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;CUDAtoolkit (cuda_11.7.0_515.43.04_linux)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;cuDNN (v8.4.1)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;font color=&quot;gold&quot;&gt;paddlepaddle 多卡训练需要NCLL支持&lt;/font&gt; (ncll v2.12.12 cuda11.7)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;paddlepaddle版本&quot;&gt;&lt;a href=&quot;#paddlepaddle版本&quot; class=&quot;headerlink&quot; title=&quot;paddlepaddle版本&quot;&gt;&lt;/a&gt;paddlepaddle版本&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;paddlepaddle-gpu==2.2.0rc0（虚拟环境cuda11.2）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;python环境&quot;&gt;&lt;a href=&quot;#python环境&quot; class=&quot;headerlink&quot; title=&quot;python环境&quot;&gt;&lt;/a&gt;python环境&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;CentOS7.9&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;anaconda3&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;python3.8&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;anaconda安装insightface&quot;&gt;&lt;a href=&quot;#anaconda安装insightface&quot; class=&quot;headerlink&quot; title=&quot;anaconda安装insightface&quot;&gt;&lt;/a&gt;anaconda安装insightface&lt;/h2&gt;&lt;p&gt;&lt;font color=&quot;gold&quot;&gt;重要：pillow版本建议选择9.5 否则过高会导致安装insightface报错&lt;/font&gt; （错误原因：pillow10移除了getsize方法，需要修改对应位置源码为&lt;code&gt;getbbox&lt;/code&gt;或 &lt;code&gt;getlength&lt;/code&gt;）&lt;/p&gt;
&lt;p&gt;告警信息：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;tools/test_recognition.py:&lt;span class=&quot;number&quot;&gt;627&lt;/span&gt;: DeprecationWarning: getsize &lt;span class=&quot;keyword&quot;&gt;is&lt;/span&gt; deprecated &lt;span class=&quot;keyword&quot;&gt;and&lt;/span&gt; will be removed &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; Pillow &lt;span class=&quot;number&quot;&gt;10&lt;/span&gt; (&lt;span class=&quot;number&quot;&gt;2023&lt;/span&gt;&lt;span class=&quot;number&quot;&gt;-07&lt;/span&gt;&lt;span class=&quot;number&quot;&gt;-01&lt;/span&gt;). Use getbbox &lt;span class=&quot;keyword&quot;&gt;or&lt;/span&gt; getlength instead.&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  tw = font.getsize(text)[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h4 id=&quot;环境安装&quot;&gt;&lt;a href=&quot;#环境安装&quot; class=&quot;headerlink&quot; title=&quot;环境安装&quot;&gt;&lt;/a&gt;环境安装&lt;/h4&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; .&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;conda install paddlepaddle-gpu==2.2.0rc0 cudatoolkit=11.2 -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/Paddle/ -c conda-forge&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; insightface&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;pip install -r requirements.txt -i https://mirror.baidu.com/pypi/simple&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; insightface/recongition/arcface_paddle/&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;pip install -r requirement.txt -i https://mirror.baidu.com/pypi/simple&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; insightface-paddle&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;pip install insightface-paddle -i https://mirror.baidu.com/pypi/simple&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="深度学习" scheme="http://yoursite.com/categories/深度学习/"/>
    
    
    <category term="paddlepaddle" scheme="http://yoursite.com/tags/paddlepaddle/"/>
    
  </entry>
  
  <entry>
    <title>paddleDetection前置</title>
    <link href="http://yoursite.com/2023/08/07/paddleDetection%E5%89%8D%E7%BD%AE/"/>
    <id>http://yoursite.com/2023/08/07/paddleDetection%E5%89%8D%E7%BD%AE/</id>
    <published>2023-08-07T08:20:53.000Z</published>
    <updated>2024-01-25T08:50:19.855Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><h3 id="GPU"><a href="#GPU" class="headerlink" title="GPU"></a>GPU</h3><ul><li><p>NVIDIA 3090*2</p></li><li><p>显卡驱动 515.43.04</p></li><li><p>CUDA版本 11.7</p></li><li><p>CUDAtoolkit (cuda_11.7.0_515.43.04_linux)</p></li><li><p>cuDNN (v8.4.1)</p></li><li><p><font color="gold">paddlepaddle 多卡训练需要NCLL支持</font> (ncll v2.12.12 cuda11.7)</p></li></ul><h3 id="paddlepaddle版本"><a href="#paddlepaddle版本" class="headerlink" title="paddlepaddle版本"></a>paddlepaddle版本</h3><ul><li>v2.4.2</li></ul><h3 id="paddleDetection版本"><a href="#paddleDetection版本" class="headerlink" title="paddleDetection版本"></a>paddleDetection版本</h3><ul><li>v2.6.0</li></ul><h3 id="python环境"><a href="#python环境" class="headerlink" title="python环境"></a>python环境</h3><ul><li><p>CentOS7.9</p></li><li><p>anaconda3</p></li><li><p>python3.8</p></li></ul><h2 id="普通视频处理"><a href="#普通视频处理" class="headerlink" title="普通视频处理"></a>普通视频处理</h2><font color="gold">h264格式视频被opencv解析帧率超过65535报错</font><p>源码：</p><p><code>./deploy/pipeline/pipeline.py</code> predict_video</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">out_path = os.path.join(self.output_dir, video_out_name + <span class="string">".mp4"</span>)</span><br><span class="line">fourcc = cv2.VideoWriter_fourcc(* <span class="string">'mp4v'</span>)</span><br><span class="line">writer = cv2.VideoWriter(out_path, fourcc, fps, (width, height))</span><br></pre></td></tr></table></figure><p>输入：cv2.VideoCapture()</p><p>输出：cv2.VideoWriter(）</p><p>本GPU模式下对时长5分钟的行人检测视频处理时间约20分钟（高精度模型），视频体积增大13倍（100M-&gt;1.3G）</p><a id="more"></a><h2 id="视频流"><a href="#视频流" class="headerlink" title="视频流"></a>视频流</h2><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ol><li><p>h264格式存在问题（需要重新编译opencv），实时帧数过高（&gt;65535）cv2.VideoWriter报错，无法保存视频</p><p>问题原因：<code>libx264</code>基于<code>GPL</code>，<code>ffmpeg</code>编码器要使用<code>libx264</code>，必须<code>--enable-gpl</code>，而<code>opencv</code>使用的是<code>MIT</code>许可</p></li><li><p>如果视频流传输加载较慢，获取不到视频报错(修改<code>pipeline.py</code> predict_video)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FrameQueueEmptyException</span><span class="params">(Exception)</span>:</span></span><br><span class="line">    <span class="string">"""An Exception for check frame queue"""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, time)</span>:</span></span><br><span class="line">        self.time = time</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">f"<span class="subst">&#123;self.time&#125;</span>秒无法获取frame队列，请检查摄像头是否正常运转"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">11</span>):</span><br><span class="line">    <span class="keyword">if</span> framequeue.empty():</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br><span class="line">        print(<span class="string">f"Couldn't catch framequeue for <span class="subst">&#123;i&#125;</span> seconds"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">10</span>:</span><br><span class="line">        <span class="keyword">raise</span> FrameQueueEmptyException</span><br></pre></td></tr></table></figure></li></ol><h3 id="摄像头-amp-应用"><a href="#摄像头-amp-应用" class="headerlink" title="摄像头 &amp; 应用"></a>摄像头 &amp; 应用</h3><p>以<code>liveCMS</code>应用为例（国标设备见文档：LiveGBS国标GB-T28181视频流媒体平台）</p><p>每26天需更新一次</p><ul><li><p>liveCMS应用安装在<code>192.168.9.165</code>机器</p></li><li><p>国标设备服务端口：<code>192.168.9.165:10005</code>账号：<code>admin</code>，密码：<code>A12345678</code></p></li><li><p>近端设备地址：<code>192.168.9.207</code></p></li><li><p>设备主板ip：<code>192.168.177.177</code>账户：<code>root</code>，密码：<code>admin</code></p></li><li><p>摄像头设备地址：<code>192.168.1.131</code>、<code>192.168.1.133</code></p></li></ul><h3 id="摄像头推流"><a href="#摄像头推流" class="headerlink" title="摄像头推流"></a>摄像头推流</h3><p>摄像头推流api：<code>http://&lt;livecms_host&gt;:&lt;ip&gt;/api/v1/stream/list</code></p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"StreamCount"</span>: <span class="number">1</span>,</span><br><span class="line">  <span class="attr">"Streams"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">"AudioEnable"</span>: <span class="literal">false</span>,</span><br><span class="line">      <span class="attr">"CDN"</span>: <span class="string">""</span>,</span><br><span class="line">      <span class="attr">"CascadeSize"</span>: <span class="number">0</span>,</span><br><span class="line">      <span class="attr">"ChannelID"</span>: <span class="string">"34020000001320000131"</span>,</span><br><span class="line">      <span class="attr">"ChannelName"</span>: <span class="string">"192.168.1.131"</span>,</span><br><span class="line">      <span class="attr">"CloudRecord"</span>: <span class="literal">false</span>,</span><br><span class="line">      <span class="attr">"DeviceID"</span>: <span class="string">"34020000002000000207"</span>,</span><br><span class="line">      <span class="attr">"Duration"</span>: <span class="number">20520</span>,</span><br><span class="line">      <span class="attr">"FLV"</span>: <span class="string">"http://192.168.9.165:10005/sms/34020000002020000001/flv/hls/34020000002000000207_34020000001320000131.flv"</span>,</span><br><span class="line">      <span class="attr">"HLS"</span>: <span class="string">"http://192.168.9.165:10005/sms/34020000002020000001/hls/34020000002000000207_34020000001320000131/live.m3u8"</span>,</span><br><span class="line">      <span class="attr">"InBitRate"</span>: <span class="number">914</span>,</span><br><span class="line">      <span class="attr">"InBytes"</span>: <span class="number">2196788210</span>,</span><br><span class="line">      <span class="attr">"NumOutputs"</span>: <span class="number">1</span>,</span><br><span class="line">      <span class="attr">"Ondemand"</span>: <span class="literal">true</span>,</span><br><span class="line">      <span class="attr">"OutBytes"</span>: <span class="number">2308731853</span>,</span><br><span class="line">      <span class="attr">"RTMP"</span>: <span class="string">"rtmp://192.168.9.165:11935/hls/34020000002000000207_34020000001320000131"</span>,</span><br><span class="line">      <span class="attr">"RTPCount"</span>: <span class="number">2049812</span>,</span><br><span class="line">      <span class="attr">"RTPLostCount"</span>: <span class="number">3526</span>,</span><br><span class="line">      <span class="attr">"RTPLostRate"</span>: <span class="number">0.6256656017039404</span>,</span><br><span class="line">      <span class="attr">"RTSP"</span>: <span class="string">"rtsp://192.168.9.165:554/34020000002000000207_34020000001320000131"</span>,</span><br><span class="line">      <span class="attr">"RecordStartAt"</span>: <span class="string">""</span>,</span><br><span class="line">      <span class="attr">"RelaySize"</span>: <span class="number">0</span>,</span><br><span class="line">      <span class="attr">"SMSID"</span>: <span class="string">"34020000002020000001"</span>,</span><br><span class="line">      <span class="attr">"SnapURL"</span>: <span class="string">"/sms/34020000002020000001/snap/34020000002000000207/34020000001320000131.jpg?t=1690332332290541800"</span>,</span><br><span class="line">      <span class="attr">"SourceAudioCodecName"</span>: <span class="string">"aac"</span>,</span><br><span class="line">      <span class="attr">"SourceAudioSampleRate"</span>: <span class="number">8000</span>,</span><br><span class="line">      <span class="attr">"SourceVideoCodecName"</span>: <span class="string">"h264"</span>,</span><br><span class="line">      <span class="attr">"SourceVideoFrameRate"</span>: <span class="number">30</span>,</span><br><span class="line">      <span class="attr">"SourceVideoHeight"</span>: <span class="number">720</span>,</span><br><span class="line">      <span class="attr">"SourceVideoWidth"</span>: <span class="number">1280</span>,</span><br><span class="line">      <span class="attr">"StartAt"</span>: <span class="string">"2023-07-26 08:45:30"</span>,</span><br><span class="line">      <span class="attr">"StreamID"</span>: <span class="string">"stream:34020000002000000207:34020000001320000131"</span>,</span><br><span class="line">      <span class="attr">"Transport"</span>: <span class="string">"UDP"</span>,</span><br><span class="line">      <span class="attr">"VideoFrameCount"</span>: <span class="number">605632</span>,</span><br><span class="line">      <span class="attr">"WEBRTC"</span>: <span class="string">"webrtc://192.168.9.165:10005/sms/34020000002020000001/rtc/34020000002000000207_34020000001320000131"</span>,</span><br><span class="line">      <span class="attr">"WS_FLV"</span>: <span class="string">"ws://192.168.9.165:10005/sms/34020000002020000001/ws-flv/hls/34020000002000000207_34020000001320000131.flv"</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="执行paddleDetection-–rtst指令"><a href="#执行paddleDetection-–rtst指令" class="headerlink" title="执行paddleDetection –rtst指令"></a>执行paddleDetection –rtst指令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python deploy/pipeline/pipeline.py --config deploy/pipeline/config/examples/infer_cfg_human_attr.yml --rtsp http://192.168.9.165:10005/sms/34020000002020000001/flv/hls/34020000002000000207_34020000001320000131.flv --device=gpu</span><br></pre></td></tr></table></figure><h3 id="解决rtsp视频流中断问题（样例）"><a href="#解决rtsp视频流中断问题（样例）" class="headerlink" title="解决rtsp视频流中断问题（样例）"></a>解决rtsp视频流中断问题（样例）</h3><p><code>pipline.py - class PipePredictor</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.rtsp = args.rtsp</span><br></pre></td></tr></table></figure><p><code>pipline.py - predict_video</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    flag = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> framequeue.empty():</span><br><span class="line">        ...</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> self.rtsp:</span><br><span class="line">            count = <span class="number">1</span></span><br><span class="line">            capture = cv2.VideoCapture(video_file)</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                fps = int(capture.get(cv2.CAP_PROP_FPS))</span><br><span class="line">            <span class="keyword">except</span> OverflowError:</span><br><span class="line">                fps = <span class="number">20</span></span><br><span class="line">            frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))</span><br><span class="line">            print(<span class="string">"restart camera fps: %d, frame_count: %d"</span> % (fps, frame_count))</span><br><span class="line"></span><br><span class="line">            framequeue = queue.Queue(<span class="number">10</span>)</span><br><span class="line">            thread = threading.Thread(</span><br><span class="line">                target=self.capturevideo, args=(capture, framequeue, execute_time))</span><br><span class="line">            thread.start()</span><br><span class="line">            time.sleep(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> framequeue.empty():</span><br><span class="line">                    time.sleep(<span class="number">1</span>)</span><br><span class="line">                    print(<span class="string">f"Couldn't catch framequeue for <span class="subst">&#123;count&#125;</span> seconds"</span>)</span><br><span class="line">                    count += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">if</span> count == <span class="number">11</span>:</span><br><span class="line">                        flag = <span class="literal">True</span></span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            flag = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">if</span> self.dbconfig:</span><br><span class="line">                <span class="keyword">if</span> self.dbconfig.rtsp:</span><br><span class="line">                    frame_thread_ = threading.Thread(</span><br><span class="line">                        target=self.video_composition, args=(self.frame_dir, self.out_path, video_fps)</span><br><span class="line">                    )</span><br><span class="line">                    <span class="keyword">if</span> <span class="keyword">not</span> os.listdir(os.path.dirname(self.out_path)):</span><br><span class="line">                        <span class="keyword">if</span> <span class="keyword">not</span> self.frame_flag:</span><br><span class="line">                            frame_thread_.start()</span><br><span class="line">        <span class="keyword">if</span> flag:</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure><h2 id="使用Python脚本"><a href="#使用Python脚本" class="headerlink" title="使用Python脚本"></a>使用Python脚本</h2><h3 id="pipeline-pipeline-py"><a href="#pipeline-pipeline-py" class="headerlink" title="pipeline/pipeline.py"></a>pipeline/pipeline.py</h3><p>在对应config文件中将MOT的<code>tracker_config</code>修改为绝对路径</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">MOT:</span></span><br><span class="line">  <span class="attr">model_dir:</span> <span class="string">https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip</span></span><br><span class="line">  <span class="attr">tracker_config:</span> <span class="string">/exp/work/video/PaddleDetection/deploy/pipeline/config/tracker_config.yml</span></span><br><span class="line">  <span class="attr">batch_size:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure><p>修改<code>pipeline.py</code>启动脚本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    paddle.enable_static()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># parse params from command</span></span><br><span class="line">    parser = argsparser()</span><br><span class="line">    FLAGS = parser.parse_args()</span><br><span class="line">    FLAGS.device = <span class="string">'gpu'</span></span><br><span class="line">    FLAGS.config = <span class="string">'/exp/work/video/PaddleDetection/deploy/pipeline/config/examples/infer_cfg_human_attr.yml'</span></span><br><span class="line">    FLAGS.video_file = <span class="string">'/exp/work/video/PaddleDetection/demo_input/t2.mp4'</span></span><br><span class="line">    FLAGS.output_dir = <span class="string">'/exp/work/video/PaddleDetection/demo_output/'</span></span><br><span class="line">    FLAGS.device = FLAGS.device.upper()</span><br><span class="line">    <span class="keyword">assert</span> FLAGS.device <span class="keyword">in</span> [<span class="string">'CPU'</span>, <span class="string">'GPU'</span>, <span class="string">'XPU'</span>, <span class="string">'NPU'</span></span><br><span class="line">                            ], <span class="string">"device should be CPU, GPU, XPU or NPU"</span></span><br><span class="line"></span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>修改<code>cfg_utils.py</code> argsparser；parse_args，修改和注释掉config作为必要条件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_args</span><span class="params">(self, argv=None)</span>:</span></span><br><span class="line">    args = super(ArgsParser, self).parse_args(argv)</span><br><span class="line">    <span class="comment"># assert args.config is not None, \</span></span><br><span class="line">    <span class="comment">#     "Please specify --config=configure_file_path."</span></span><br><span class="line">    args.opt = self._parse_opt(args.opt)</span><br><span class="line">    <span class="keyword">return</span> args</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">parser.add_argument(</span><br><span class="line">    <span class="string">"--config"</span>,</span><br><span class="line">    type=str,</span><br><span class="line">    default=<span class="literal">None</span>,</span><br><span class="line">    help=(<span class="string">"Path of configure"</span>),</span><br><span class="line">    required=<span class="literal">False</span>) <span class="comment"># True ——&gt; False</span></span><br></pre></td></tr></table></figure><p>启动脚本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python pipeline.py</span><br></pre></td></tr></table></figure><p>或将其封装为接口</p><h3 id="python-infer-py"><a href="#python-infer-py" class="headerlink" title="python/infer.py"></a>python/infer.py</h3><p>导出预测模型</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 导出YOLOv3检测模型</span></span><br><span class="line">python tools/export_model.py -c configs/yolov3/yolov3_darknet53_270e_coco.yml --output_dir=./inference_model \</span><br><span class="line"> -o weights=https://paddledet.bj.bcebos.com/models/yolov3_darknet53_270e_coco.pdparams</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 导出HigherHRNet(bottom-up)关键点检测模型</span></span><br><span class="line">python tools/export_model.py -c configs/keypoint/higherhrnet/higherhrnet_hrnet_w32_512.yml -o weights=https://paddledet.bj.bcebos.com/models/keypoint/higherhrnet_hrnet_w32_512.pdparams</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 导出HRNet(top-down)关键点检测模型</span></span><br><span class="line">python tools/export_model.py -c configs/keypoint/hrnet/hrnet_w32_384x288.yml -o weights=https://paddledet.bj.bcebos.com/models/keypoint/hrnet_w32_384x288.pdparams</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 导出FairMOT多目标跟踪模型</span></span><br><span class="line">python tools/export_model.py -c configs/mot/fairmot/fairmot_dla34_30e_1088x608.yml -o weights=https://paddledet.bj.bcebos.com/models/mot/fairmot_dla34_30e_1088x608.pdparams</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 导出ByteTrack多目标跟踪模型(相当于只导出检测器)</span></span><br><span class="line">python tools/export_model.py -c configs/mot/bytetrack/detector/ppyoloe_crn_l_36e_640x640_mot17half.yml -o weights=https://paddledet.bj.bcebos.com/models/mot/ppyoloe_crn_l_36e_640x640_mot17half.pdparams</span><br></pre></td></tr></table></figure><p>预测</p><p>修改<code>deploy/python/infer.py</code>启动脚本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    paddle.enable_static()</span><br><span class="line">    parser = argsparser()</span><br><span class="line">    FLAGS = parser.parse_args()</span><br><span class="line">    FLAGS.device = <span class="string">'gpu'</span></span><br><span class="line">    FLAGS.video_file = <span class="string">'/exp/work/video/PaddleDetection/demo_input/t2.mp4'</span></span><br><span class="line">    FLAGS.output_dir = <span class="string">'/exp/work/video/PaddleDetection/demo_output/'</span></span><br><span class="line">    FLAGS.model_dir = <span class="string">'/exp/work/video/PaddleDetection/inference_model/yolov3_darknet53_270e_coco'</span></span><br><span class="line">    print_arguments(FLAGS)</span><br><span class="line">    FLAGS.device = FLAGS.device.upper()</span><br><span class="line">    <span class="keyword">assert</span> FLAGS.device <span class="keyword">in</span> [<span class="string">'CPU'</span>, <span class="string">'GPU'</span>, <span class="string">'XPU'</span>, <span class="string">'NPU'</span></span><br><span class="line">                            ], <span class="string">"device should be CPU, GPU, XPU or NPU"</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="keyword">not</span> FLAGS.use_gpu, <span class="string">"use_gpu has been deprecated, please use --device"</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> <span class="keyword">not</span> (</span><br><span class="line">        FLAGS.enable_mkldnn == <span class="literal">False</span> <span class="keyword">and</span> FLAGS.enable_mkldnn_bfloat16 == <span class="literal">True</span></span><br><span class="line">    ), <span class="string">'To enable mkldnn bfloat, please turn on both enable_mkldnn and enable_mkldnn_bfloat16'</span></span><br><span class="line"></span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h2 id="pp-human"><a href="#pp-human" class="headerlink" title="pp-human"></a>pp-human</h2><p>行人检测模块，对应配置文件位置：<code>deploy/pipeline/config/examples/infer_cfg_pphuman.yml</code></p><p>通过命令启动检测脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python deploy/pipeline/pipeline.py --config deploy/pipeline/config/examples/infer_cfg_human_attr.yml --device=gpu --video_file=demo_input/t1.mp4 --output_dir=demo_output/</span><br></pre></td></tr></table></figure><p><img src="/2023/08/07/paddleDetection前置/PaddleDetection前置/A.png" alt></p><h2 id="pp-vehicle"><a href="#pp-vehicle" class="headerlink" title="pp-vehicle"></a>pp-vehicle</h2><p>车辆检测模块，对应配置文件位置：<code>deploy/pipeline/config/examples/infer_cfg_ppvehicle.yml</code></p><p>通过命令启动检测脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python deploy/pipeline/pipeline.py --config deploy/pipeline/config/infer_cfg_ppvehicle.yml --video_file=demo_input/car.mp4 --device=gpu --output_dir=demo_output</span><br></pre></td></tr></table></figure><p><img src="/2023/08/07/paddleDetection前置/PaddleDetection前置/B.png" alt></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;环境&quot;&gt;&lt;a href=&quot;#环境&quot; class=&quot;headerlink&quot; title=&quot;环境&quot;&gt;&lt;/a&gt;环境&lt;/h2&gt;&lt;h3 id=&quot;GPU&quot;&gt;&lt;a href=&quot;#GPU&quot; class=&quot;headerlink&quot; title=&quot;GPU&quot;&gt;&lt;/a&gt;GPU&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;NVIDIA 3090*2&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;显卡驱动 515.43.04&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;CUDA版本 11.7&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;CUDAtoolkit (cuda_11.7.0_515.43.04_linux)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;cuDNN (v8.4.1)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;font color=&quot;gold&quot;&gt;paddlepaddle 多卡训练需要NCLL支持&lt;/font&gt; (ncll v2.12.12 cuda11.7)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;paddlepaddle版本&quot;&gt;&lt;a href=&quot;#paddlepaddle版本&quot; class=&quot;headerlink&quot; title=&quot;paddlepaddle版本&quot;&gt;&lt;/a&gt;paddlepaddle版本&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;v2.4.2&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;paddleDetection版本&quot;&gt;&lt;a href=&quot;#paddleDetection版本&quot; class=&quot;headerlink&quot; title=&quot;paddleDetection版本&quot;&gt;&lt;/a&gt;paddleDetection版本&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;v2.6.0&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;python环境&quot;&gt;&lt;a href=&quot;#python环境&quot; class=&quot;headerlink&quot; title=&quot;python环境&quot;&gt;&lt;/a&gt;python环境&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;CentOS7.9&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;anaconda3&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;python3.8&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;普通视频处理&quot;&gt;&lt;a href=&quot;#普通视频处理&quot; class=&quot;headerlink&quot; title=&quot;普通视频处理&quot;&gt;&lt;/a&gt;普通视频处理&lt;/h2&gt;&lt;font color=&quot;gold&quot;&gt;h264格式视频被opencv解析帧率超过65535报错&lt;/font&gt;

&lt;p&gt;源码：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;./deploy/pipeline/pipeline.py&lt;/code&gt; predict_video&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;out_path = os.path.join(self.output_dir, video_out_name + &lt;span class=&quot;string&quot;&gt;&quot;.mp4&quot;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;fourcc = cv2.VideoWriter_fourcc(* &lt;span class=&quot;string&quot;&gt;&#39;mp4v&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;writer = cv2.VideoWriter(out_path, fourcc, fps, (width, height))&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;输入：cv2.VideoCapture()&lt;/p&gt;
&lt;p&gt;输出：cv2.VideoWriter(）&lt;/p&gt;
&lt;p&gt;本GPU模式下对时长5分钟的行人检测视频处理时间约20分钟（高精度模型），视频体积增大13倍（100M-&amp;gt;1.3G）&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="http://yoursite.com/categories/深度学习/"/>
    
    
    <category term="paddlepaddle" scheme="http://yoursite.com/tags/paddlepaddle/"/>
    
    <category term="opencv" scheme="http://yoursite.com/tags/opencv/"/>
    
  </entry>
  
  <entry>
    <title>搭建nebula-CentOS集群（极速版）</title>
    <link href="http://yoursite.com/2023/07/10/%E6%90%AD%E5%BB%BAnebula-CentOS%E9%9B%86%E7%BE%A4%EF%BC%88%E6%9E%81%E9%80%9F%E7%89%88%EF%BC%89/"/>
    <id>http://yoursite.com/2023/07/10/%E6%90%AD%E5%BB%BAnebula-CentOS%E9%9B%86%E7%BE%A4%EF%BC%88%E6%9E%81%E9%80%9F%E7%89%88%EF%BC%89/</id>
    <published>2023-07-10T01:05:06.000Z</published>
    <updated>2023-08-10T01:38:30.425Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><ul><li>系统</li></ul><p>CentOS7.9</p><ul><li>机器</li></ul><p>三台，分别为<code>nebula01</code>、<code>nebula02</code>、<code>nebula03</code>、</p><ul><li>安装位置</li></ul><p><code>/usr/local</code></p><ul><li>nebula版本</li></ul><p><code>2.6.1</code></p><ul><li>nebula-graph-studio版本</li></ul><p><code>3.2.3</code></p><h2 id="快速开始"><a href="#快速开始" class="headerlink" title="快速开始"></a>快速开始</h2><ul><li>下载<code>tar.gz</code>文件</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local</span><br><span class="line">wget https://oss-cdn.nebula-graph.com.cn/package/2.6.1/nebula-graph-2.6.1.el7.x86_64.tar.gz</span><br><span class="line">wget https://oss-cdn.nebula-graph.com.cn/nebula-graph-studio/3.2.3/nebula-graph-studio-3.2.3.x86_64.tar.gz</span><br></pre></td></tr></table></figure><ul><li>解压缩并重命名文件</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf nebula-graph-2.6.1.el7.x86_64.tar.gz &amp;&amp; mv nebula-graph-2.6.1.el7.x86_64 nebula</span><br><span class="line">tar -zxvf nebula-graph-studio-3.2.3.x86_64.tar.gz &amp;&amp; mv nebula-graph-studio-3.2.3.x86_64 nebula-graph-studio</span><br></pre></td></tr></table></figure><ul><li>复制配置文件</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd nebula/etc</span><br><span class="line">cp nebula-graphd.conf.default nebula-graphd.conf</span><br><span class="line">cp nebula-metad.conf.default nebula-metad.conf</span><br><span class="line">cp nebula-storaged.conf.default nebula-storaged.conf</span><br></pre></td></tr></table></figure><a id="more"></a><ul><li>修改配置文件</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim nebula-graphd.conf</span><br></pre></td></tr></table></figure><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line"><span class="comment"># The directory to host logging files</span></span><br><span class="line"><span class="string">--log_dir=/data/nebula/logs</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="comment"># Comma separated Meta Server Addresses</span></span><br><span class="line"><span class="string">--meta_server_addrs=nebula01:9559,nebula02:9559,nebula03:9559</span></span><br><span class="line"><span class="comment"># Local IP used to identify the nebula-graphd process.</span></span><br><span class="line"><span class="comment"># Change it to an address other than loopback if the service is distributed or</span></span><br><span class="line"><span class="comment"># will be accessed remotely.</span></span><br><span class="line"><span class="string">--local_ip=nebula01</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim nebula-metad.conf</span><br></pre></td></tr></table></figure><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line"><span class="comment"># The directory to host logging files</span></span><br><span class="line"><span class="string">--log_dir=/data/nebula/logs</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="comment"># Comma separated Meta Server addresses</span></span><br><span class="line"><span class="string">--meta_server_addrs=nebula01:9559,nebula02:9559,nebula03:9559</span></span><br><span class="line"><span class="comment"># Local IP used to identify the nebula-metad process.</span></span><br><span class="line"><span class="comment"># Change it to an address other than loopback if the service is distributed or</span></span><br><span class="line"><span class="comment"># will be accessed remotely.</span></span><br><span class="line"><span class="string">--local_ip=nebula01</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="comment"># Root data path, here should be only single path for metad</span></span><br><span class="line"><span class="string">--data_path=/data/nebula/meta</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim nebula-storaged.conf</span><br></pre></td></tr></table></figure><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line"><span class="comment"># The directory to host logging files</span></span><br><span class="line"><span class="string">--log_dir=/data/nebula/logs</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="comment"># Comma separated Meta server addresses</span></span><br><span class="line"><span class="string">--meta_server_addrs=nebula01:9559,nebula02:9559,nebula03:9559</span></span><br><span class="line"><span class="comment"># Local IP used to identify the nebula-storaged process.</span></span><br><span class="line"><span class="comment"># Change it to an address other than loopback if the service is distributed or</span></span><br><span class="line"><span class="comment"># will be accessed remotely.</span></span><br><span class="line"><span class="string">--local_ip=nebula01</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="comment"># Root data path. Split by comma. e.g. --data_path=/disk1/path1/,/disk2/path2/</span></span><br><span class="line"><span class="comment"># One path per Rocksdb instance.</span></span><br><span class="line"><span class="string">--data_path=/data/nebula/storage</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure><ul><li>分发nebula文件，并修改对应的<code>--local_ip</code></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd ../..</span><br><span class="line">scp -r nebula nebula02:`pwd`/</span><br><span class="line">scp -r nebula nebula03:`pwd`/</span><br></pre></td></tr></table></figure><ul><li>启动集群</li></ul><p><font color="gold">注：</font>因为meta机器选择了nebula01、02、03共三台机器，启动时需要将三台同时执行命令拉起</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nebula/scripts/nebula.service start all</span><br></pre></td></tr></table></figure><ul><li>检查集群状态</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nebula/scripts/nebula.service status all</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[INFO] nebula-metad(de03025): Running as 24680, Listening on 9559 </span><br><span class="line">[INFO] nebula-graphd(de03025): Running as 25453, Listening on 9669 </span><br><span class="line">[INFO] nebula-storaged(de03025): Running as 24787, Listening on 9779</span><br></pre></td></tr></table></figure><p>此时<code>nebula</code>文件夹下生成<code>cluster.id</code>文件，集群成功启动</p><ul><li>启动nebula-graph-studio</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd nebula-graph-studio</span><br><span class="line">nohup ./server &amp;</span><br></pre></td></tr></table></figure><p><img src="/2023/07/10/搭建nebula-CentOS集群（极速版）/A.png" alt></p><p><font color="gold">注：</font>同样可用于集群数据恢复（<code>/data</code>下的数据已备份）</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;环境准备&quot;&gt;&lt;a href=&quot;#环境准备&quot; class=&quot;headerlink&quot; title=&quot;环境准备&quot;&gt;&lt;/a&gt;环境准备&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;系统&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;CentOS7.9&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;机器&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;三台，分别为&lt;code&gt;nebula01&lt;/code&gt;、&lt;code&gt;nebula02&lt;/code&gt;、&lt;code&gt;nebula03&lt;/code&gt;、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;安装位置&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;/usr/local&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;nebula版本&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;2.6.1&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;nebula-graph-studio版本&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;3.2.3&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&quot;快速开始&quot;&gt;&lt;a href=&quot;#快速开始&quot; class=&quot;headerlink&quot; title=&quot;快速开始&quot;&gt;&lt;/a&gt;快速开始&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;下载&lt;code&gt;tar.gz&lt;/code&gt;文件&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;cd /usr/local&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;wget https://oss-cdn.nebula-graph.com.cn/package/2.6.1/nebula-graph-2.6.1.el7.x86_64.tar.gz&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;wget https://oss-cdn.nebula-graph.com.cn/nebula-graph-studio/3.2.3/nebula-graph-studio-3.2.3.x86_64.tar.gz&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;解压缩并重命名文件&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;tar -zxvf nebula-graph-2.6.1.el7.x86_64.tar.gz &amp;amp;&amp;amp; mv nebula-graph-2.6.1.el7.x86_64 nebula&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;tar -zxvf nebula-graph-studio-3.2.3.x86_64.tar.gz &amp;amp;&amp;amp; mv nebula-graph-studio-3.2.3.x86_64 nebula-graph-studio&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;复制配置文件&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;cd nebula/etc&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;cp nebula-graphd.conf.default nebula-graphd.conf&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;cp nebula-metad.conf.default nebula-metad.conf&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;cp nebula-storaged.conf.default nebula-storaged.conf&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
    
    <category term="database" scheme="http://yoursite.com/tags/database/"/>
    
    <category term="nebua" scheme="http://yoursite.com/tags/nebua/"/>
    
  </entry>
  
  <entry>
    <title>Nebula-Spark和图算法</title>
    <link href="http://yoursite.com/2023/03/24/Nebula-Spark/"/>
    <id>http://yoursite.com/2023/03/24/Nebula-Spark/</id>
    <published>2023-03-24T06:34:47.000Z</published>
    <updated>2024-01-08T07:52:12.236Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><h2 id="Nebula-Spark-Connector"><a href="#Nebula-Spark-Connector" class="headerlink" title="Nebula Spark Connector"></a>Nebula Spark Connector</h2><p>下载地址&amp;官方文档：【<a href="https://github.com/vesoft-inc/nebula-spark-connector" target="_blank" rel="noopener">https://github.com/vesoft-inc/nebula-spark-connector</a>】</p><h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><p><strong>·</strong> nebula：<font color="orange">2.6.1</font><br><strong>·</strong> hadoop：<font color="orange">2.7</font><br><strong>·</strong> spark：<font color="orange">2.4.7</font><br><strong>·</strong> pyspark：<font color="orange">2.4.7</font><br><strong>·</strong> python：<font color="orange">3.7.16</font><br><strong>·</strong> nebula-spark-connector：<font color="orange">2.6.1</font></p><h3 id="编译打包nebula-spark-connector"><a href="#编译打包nebula-spark-connector" class="headerlink" title="编译打包nebula-spark-connector"></a>编译打包nebula-spark-connector</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> nebula-spark-connector-2.6.1/nebula-spark-connector</span><br><span class="line">$ mvn clean package -Dmaven.test.skip=<span class="literal">true</span> -Dgpg.skip -Dmaven.javadoc.skip=<span class="literal">true</span></span><br></pre></td></tr></table></figure><p>成功后在<code>nebula-spark-connector/target/</code> 目录下得到 <code>nebula-spark-connector-2.6.1.jar</code>文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@root target]<span class="comment"># ll</span></span><br><span class="line">total 106792</span><br><span class="line">drwxr-xr-x 3 root root        17 Mar 11 14:14 classes</span><br><span class="line">-rw-r--r-- 1 root root         1 Mar 11 14:14 classes.-497386701.timestamp</span><br><span class="line">-rw-r--r-- 1 root root         1 Mar 11 14:14 classes.timestamp</span><br><span class="line">-rw-r--r-- 1 root root     30701 Mar 11 14:15 jacoco.exec</span><br><span class="line">drwxr-xr-x 2 root root        28 Mar 11 14:15 maven-archiver</span><br><span class="line">-rw-r--r-- 1 root root 108375457 Mar 11 14:16 nebula-spark-connector-2.6.1.jar</span><br><span class="line">-rw-r--r-- 1 root root    583482 Mar 11 14:16 nebula-spark-connector-2.6.1-javadoc.jar</span><br><span class="line">-rw-r--r-- 1 root root     36358 Mar 11 14:16 nebula-spark-connector-2.6.1-sources.jar</span><br><span class="line">-rw-r--r-- 1 root root    315392 Mar 11 14:15 original-nebula-spark-connector-2.6.1.jar</span><br><span class="line">drwxr-xr-x 4 root root        37 Mar 11 14:15 site</span><br></pre></td></tr></table></figure><h3 id="PySpark-读取-NebulaGraph-数据"><a href="#PySpark-读取-NebulaGraph-数据" class="headerlink" title="PySpark 读取 NebulaGraph 数据"></a>PySpark 读取 NebulaGraph 数据</h3><p>从 <code>metaAddress</code> 为 <code>&quot;metad0:9559&quot;</code> 的 Nebula Graph 中读取整个 tag 下的数据为一个 dataframe：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">df = spark.read.format(</span><br><span class="line">  <span class="string">"com.vesoft.nebula.connector.NebulaDataSource"</span>).option(</span><br><span class="line">    <span class="string">"type"</span>, <span class="string">"vertex"</span>).option(</span><br><span class="line">    <span class="string">"spaceName"</span>, <span class="string">"basketballplayer"</span>).option(</span><br><span class="line">    <span class="string">"label"</span>, <span class="string">"player"</span>).option(</span><br><span class="line">    <span class="string">"returnCols"</span>, <span class="string">"name,age"</span>).option(</span><br><span class="line">    <span class="string">"metaAddress"</span>, <span class="string">"metad0:9559"</span>).option(</span><br><span class="line">    <span class="string">"partitionNumber"</span>, <span class="number">1</span>).load()</span><br></pre></td></tr></table></figure><p>然后可以像这样 <code>show</code> 这个 dataframe：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>df.show(n=<span class="number">2</span>)</span><br><span class="line">+---------+--------------+---+</span><br><span class="line">|_vertexId|          name|age|</span><br><span class="line">+---------+--------------+---+</span><br><span class="line">|player105|   Danny Green| <span class="number">31</span>|</span><br><span class="line">|player109|Tiago Splitter| <span class="number">34</span>|</span><br><span class="line">+---------+--------------+---+</span><br><span class="line">only showing top <span class="number">2</span> rows</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="PySpark-写-NebulaGraph-数据"><a href="#PySpark-写-NebulaGraph-数据" class="headerlink" title="PySpark 写 NebulaGraph 数据"></a>PySpark 写 NebulaGraph 数据</h3><p>默认不指定的情况下 <code>writeMode</code> 是 <code>insert</code>：</p><h4 id="写入点"><a href="#写入点" class="headerlink" title="写入点"></a>写入点</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">df.write.format(<span class="string">"com.vesoft.nebula.connector.NebulaDataSource"</span>).option(</span><br><span class="line">    <span class="string">"type"</span>, <span class="string">"vertex"</span>).option(</span><br><span class="line">    <span class="string">"spaceName"</span>, <span class="string">"basketballplayer"</span>).option(</span><br><span class="line">    <span class="string">"label"</span>, <span class="string">"player"</span>).option(</span><br><span class="line">    <span class="string">"vidPolicy"</span>, <span class="string">""</span>).option(</span><br><span class="line">    <span class="string">"vertexField"</span>, <span class="string">"_vertexId"</span>).option(</span><br><span class="line">    <span class="string">"batch"</span>, <span class="number">1</span>).option(</span><br><span class="line">    <span class="string">"metaAddress"</span>, <span class="string">"metad0:9559"</span>).option(</span><br><span class="line">    <span class="string">"graphAddress"</span>, <span class="string">"graphd1:9669"</span>).option(</span><br><span class="line">    <span class="string">"passwd"</span>, <span class="string">"nebula"</span>).option(</span><br><span class="line">    <span class="string">"user"</span>, <span class="string">"root"</span>).save()</span><br></pre></td></tr></table></figure><h4 id="删除点"><a href="#删除点" class="headerlink" title="删除点"></a>删除点</h4><p>如果想指定 <code>delete</code> 或者 <code>update</code> 的非默认写入模式，增加 <code>writeMode</code> 的配置，比如 <code>delete</code> 的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">df.write.format(<span class="string">"com.vesoft.nebula.connector.NebulaDataSource"</span>).option(</span><br><span class="line">    <span class="string">"type"</span>, <span class="string">"vertex"</span>).option(</span><br><span class="line">    <span class="string">"spaceName"</span>, <span class="string">"basketballplayer"</span>).option(</span><br><span class="line">    <span class="string">"label"</span>, <span class="string">"player"</span>).option(</span><br><span class="line">    <span class="string">"vidPolicy"</span>, <span class="string">""</span>).option(</span><br><span class="line">    <span class="string">"vertexField"</span>, <span class="string">"_vertexId"</span>).option(</span><br><span class="line">    <span class="string">"batch"</span>, <span class="number">1</span>).option(</span><br><span class="line">    <span class="string">"metaAddress"</span>, <span class="string">"metad0:9559"</span>).option(</span><br><span class="line">    <span class="string">"graphAddress"</span>, <span class="string">"graphd1:9669"</span>).option(</span><br><span class="line">    <span class="string">"passwd"</span>, <span class="string">"nebula"</span>).option(</span><br><span class="line">    <span class="string">"writeMode"</span>, <span class="string">"delete"</span>).option(</span><br><span class="line">    <span class="string">"user"</span>, <span class="string">"root"</span>).save()</span><br></pre></td></tr></table></figure><h4 id="写入边"><a href="#写入边" class="headerlink" title="写入边"></a>写入边</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">df.write.format(<span class="string">"com.vesoft.nebula.connector.NebulaDataSource"</span>)\</span><br><span class="line">    .mode(<span class="string">"overwrite"</span>)\</span><br><span class="line">    .option(<span class="string">"srcPolicy"</span>, <span class="string">""</span>)\</span><br><span class="line">    .option(<span class="string">"dstPolicy"</span>, <span class="string">""</span>)\</span><br><span class="line">    .option(<span class="string">"metaAddress"</span>, <span class="string">"metad0:9559"</span>)\</span><br><span class="line">    .option(<span class="string">"graphAddress"</span>, <span class="string">"graphd:9669"</span>)\</span><br><span class="line">    .option(<span class="string">"user"</span>, <span class="string">"root"</span>)\</span><br><span class="line">    .option(<span class="string">"passwd"</span>, <span class="string">"nebula"</span>)\</span><br><span class="line">    .option(<span class="string">"type"</span>, <span class="string">"edge"</span>)\</span><br><span class="line">    .option(<span class="string">"spaceName"</span>, <span class="string">"basketballplayer"</span>)\</span><br><span class="line">    .option(<span class="string">"label"</span>, <span class="string">"server"</span>)\</span><br><span class="line">    .option(<span class="string">"srcVertexField"</span>, <span class="string">"srcid"</span>)\</span><br><span class="line">    .option(<span class="string">"dstVertexField"</span>, <span class="string">"dstid"</span>)\</span><br><span class="line">    .option(<span class="string">"rankField"</span>, <span class="string">""</span>)\</span><br><span class="line">    .option(<span class="string">"batch"</span>, <span class="number">100</span>)\</span><br><span class="line">    .option(<span class="string">"writeMode"</span>, <span class="string">"insert"</span>).save()   <span class="comment"># delete to delete edge, update to update edge</span></span><br></pre></td></tr></table></figure><h4 id="删除边"><a href="#删除边" class="headerlink" title="删除边"></a>删除边</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">df.write.format(<span class="string">"com.vesoft.nebula.connector.NebulaDataSource"</span>)\</span><br><span class="line">    .mode(<span class="string">"overwrite"</span>)\</span><br><span class="line">    .option(<span class="string">"srcPolicy"</span>, <span class="string">""</span>)\</span><br><span class="line">    .option(<span class="string">"dstPolicy"</span>, <span class="string">""</span>)\</span><br><span class="line">    .option(<span class="string">"metaAddress"</span>, <span class="string">"metad0:9559"</span>)\</span><br><span class="line">    .option(<span class="string">"graphAddress"</span>, <span class="string">"graphd:9669"</span>)\</span><br><span class="line">    .option(<span class="string">"user"</span>, <span class="string">"root"</span>)\</span><br><span class="line">    .option(<span class="string">"passwd"</span>, <span class="string">"nebula"</span>)\</span><br><span class="line">    .option(<span class="string">"type"</span>, <span class="string">"edge"</span>)\</span><br><span class="line">    .option(<span class="string">"spaceName"</span>, <span class="string">"basketballplayer"</span>)\</span><br><span class="line">    .option(<span class="string">"label"</span>, <span class="string">"server"</span>)\</span><br><span class="line">    .option(<span class="string">"srcVertexField"</span>, <span class="string">"srcid"</span>)\</span><br><span class="line">    .option(<span class="string">"dstVertexField"</span>, <span class="string">"dstid"</span>)\</span><br><span class="line">    .option(<span class="string">"randkField"</span>, <span class="string">""</span>)\</span><br><span class="line">    .option(<span class="string">"batch"</span>, <span class="number">100</span>)\</span><br><span class="line">    .option(<span class="string">"writeMode"</span>, <span class="string">"delete"</span>).save()   <span class="comment"># delete to delete edge, update to update edge</span></span><br></pre></td></tr></table></figure><h3 id="关于-PySpark-读写的-option"><a href="#关于-PySpark-读写的-option" class="headerlink" title="关于 PySpark 读写的 option"></a>关于 PySpark 读写的 option</h3><p>对于其他的 option，比如删除点的时候的 <code>withDeleteEdge</code> 可以参考 [nebula/connector/NebulaOptions.scala</p><p>](<a href="https://github.com/vesoft-inc/nebula-spark-connector/blob/master/nebula-spark-connector/src/main/scala/com/vesoft/nebula/connector/NebulaOptions.scala" target="_blank" rel="noopener">https://github.com/vesoft-inc/nebula-spark-connector/blob/master/nebula-spark-connector/src/main/scala/com/vesoft/nebula/connector/NebulaOptions.scala</a>) 的字符串配置定义，我们可以看到它的字符串定义字段是 <code>deleteEdge</code> ：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** write config */</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">RATE_LIMIT</span>: <span class="type">String</span>   = <span class="string">"rateLimit"</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">VID_POLICY</span>: <span class="type">String</span>   = <span class="string">"vidPolicy"</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">SRC_POLICY</span>: <span class="type">String</span>   = <span class="string">"srcPolicy"</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">DST_POLICY</span>: <span class="type">String</span>   = <span class="string">"dstPolicy"</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">VERTEX_FIELD</span>         = <span class="string">"vertexField"</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">SRC_VERTEX_FIELD</span>     = <span class="string">"srcVertexField"</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">DST_VERTEX_FIELD</span>     = <span class="string">"dstVertexField"</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">RANK_FIELD</span>           = <span class="string">"randkField"</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">BATCH</span>: <span class="type">String</span>        = <span class="string">"batch"</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">VID_AS_PROP</span>: <span class="type">String</span>  = <span class="string">"vidAsProp"</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">SRC_AS_PROP</span>: <span class="type">String</span>  = <span class="string">"srcAsProp"</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">DST_AS_PROP</span>: <span class="type">String</span>  = <span class="string">"dstAsProp"</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">RANK_AS_PROP</span>: <span class="type">String</span> = <span class="string">"rankAsProp"</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">WRITE_MODE</span>: <span class="type">String</span>   = <span class="string">"writeMode"</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">DELETE_EDGE</span>: <span class="type">String</span>  = <span class="string">"deleteEdge"</span></span><br></pre></td></tr></table></figure><h2 id="PySpark-调用-Nebula-Spark-Connector"><a href="#PySpark-调用-Nebula-Spark-Connector" class="headerlink" title="PySpark 调用 Nebula Spark Connector"></a>PySpark 调用 Nebula Spark Connector</h2><h3 id="Pycharm"><a href="#Pycharm" class="headerlink" title="Pycharm"></a>Pycharm</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入SparkSession、findspark自动获取$&#123;SPARK_HOME&#125;，定义虚拟环境</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> findspark</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">"HADOOP_CONF_DIR"</span>] = <span class="string">"/exp/server/hadoop-2.7.7/etc/hadoop"</span></span><br><span class="line">os.environ[<span class="string">"YARN_CONF_DIR"</span>] = <span class="string">"/exp/server/hadoop-2.7.7/etc/hadoop"</span></span><br><span class="line">os.environ[<span class="string">"SPARK_HOME"</span>] = <span class="string">"/exp/server/spark-2.4.7-bin-hadoop2.7"</span></span><br><span class="line">os.environ[<span class="string">"PYSPARK_PYTHON"</span>] = <span class="string">"/root/anaconda3/envs/pyspark38/bin/python"</span></span><br><span class="line">os.environ[<span class="string">"PYSPARK_DRIVER_PYTHON"</span>] = <span class="string">"/root/anaconda3/envs/pyspark38/bin/python"</span></span><br><span class="line">findspark.init()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建SparkSession对象，导入target下的jar包</span></span><br><span class="line">spark = SparkSession.builder.config(</span><br><span class="line">    <span class="string">"spark.jars"</span>,</span><br><span class="line">    <span class="string">"/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar"</span>).config(</span><br><span class="line">    <span class="string">"spark.driver.extraClassPath"</span>,</span><br><span class="line">    <span class="string">"/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar"</span>).appName(</span><br><span class="line">    <span class="string">"nebula-connector"</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取nebula-graph数据</span></span><br><span class="line"><span class="comment"># read vertex</span></span><br><span class="line">df_tag = spark.read.format(</span><br><span class="line">    <span class="string">"com.vesoft.nebula.connector.NebulaDataSource"</span>).option(</span><br><span class="line">    <span class="string">"sep"</span>, <span class="string">"\t"</span>).option(</span><br><span class="line">    <span class="string">"type"</span>, <span class="string">"vertex"</span>).option(</span><br><span class="line">    <span class="string">"spaceName"</span>, <span class="string">"space"</span>).option(</span><br><span class="line">    <span class="string">"label"</span>, <span class="string">"tag"</span>).option(</span><br><span class="line">    <span class="string">"returnCols"</span>, <span class="string">""</span>).option(</span><br><span class="line">    <span class="string">"metaAddress"</span>, <span class="string">"metahost:9559"</span>).option(</span><br><span class="line">    <span class="string">"partitionNumber"</span>, <span class="number">1</span>).load()</span><br><span class="line"></span><br><span class="line"><span class="comment"># read edge</span></span><br><span class="line">df_edge = spark.read.format(</span><br><span class="line">    <span class="string">"com.vesoft.nebula.connector.NebulaDataSource"</span>).option(</span><br><span class="line">    <span class="string">"sep"</span>, <span class="string">"\t"</span>).option(</span><br><span class="line">    <span class="string">"type"</span>, <span class="string">"edge"</span>).option(</span><br><span class="line">    <span class="string">"spaceName"</span>, <span class="string">"space"</span>).option(</span><br><span class="line">    <span class="string">"label"</span>, <span class="string">"edge"</span>).option(</span><br><span class="line">    <span class="string">"returnCols"</span>, <span class="string">""</span>).option(</span><br><span class="line">    <span class="string">"metaAddress"</span>, <span class="string">"metahost:9559"</span>).option(</span><br><span class="line">    <span class="string">"partitionNumber"</span>, <span class="number">1</span>).load()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 写回nebula-graph</span></span><br><span class="line"><span class="comment"># write vertex</span></span><br><span class="line">df_tag.write.format(<span class="string">"com.vesoft.nebula.connector.NebulaDataSource"</span>).option(</span><br><span class="line">    <span class="string">"type"</span>, <span class="string">"vertex"</span>).option(</span><br><span class="line">    <span class="string">"spaceName"</span>, <span class="string">"space"</span>).option(</span><br><span class="line">    <span class="string">"label"</span>, <span class="string">"tag"</span>).option(</span><br><span class="line">    <span class="string">"vidPolicy"</span>, <span class="string">""</span>).option(</span><br><span class="line">    <span class="string">"vertexField"</span>, <span class="string">"_vertexId"</span>).option(</span><br><span class="line">    <span class="string">"batch"</span>, <span class="number">1</span>).option(</span><br><span class="line">    <span class="string">"metaAddress"</span>, <span class="string">"metahost:9559"</span>).option(</span><br><span class="line">    <span class="string">"graphAddress"</span>, <span class="string">"graphhost:9669"</span>).option(</span><br><span class="line">    <span class="string">"passwd"</span>, <span class="string">"nebula"</span>).option(</span><br><span class="line">    <span class="string">"writeMode"</span>, <span class="string">"update"</span>).option(</span><br><span class="line">    <span class="string">"user"</span>, <span class="string">"root"</span>).save()</span><br><span class="line"></span><br><span class="line">df_tag.show()</span><br><span class="line">df_edge.show()</span><br></pre></td></tr></table></figure><h3 id="Spark-submit"><a href="#Spark-submit" class="headerlink" title="Spark-submit"></a>Spark-submit</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$&#123;SPARK_HOME&#125;</span>/bin/spark-submit --master <span class="built_in">local</span>[*] \</span><br><span class="line">--deploy-mode client  \</span><br><span class="line">--jars file:/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar \</span><br><span class="line">/exp/work/pyspark/nebula/nebula_reader.py</span><br></pre></td></tr></table></figure><h2 id="NebulaAlgorithm"><a href="#NebulaAlgorithm" class="headerlink" title="NebulaAlgorithm"></a>NebulaAlgorithm</h2><p>下载地址&amp;官方文档：【&lt;<a href="https://github.com/vesoft-inc/nebula-algorithm】" target="_blank" rel="noopener">https://github.com/vesoft-inc/nebula-algorithm】</a></p><h3 id="环境-1"><a href="#环境-1" class="headerlink" title="环境"></a>环境</h3><p><strong>·</strong> nebula：<font color="orange">2.6.1</font><br><strong>·</strong> hadoop：<font color="orange">2.7</font><br><strong>·</strong> spark：<font color="orange">2.4.7</font><br><strong>·</strong> pyspark：<font color="orange">2.4.7</font><br><strong>·</strong> python：<font color="orange">3.7.16</font><br><strong>·</strong> nebula-spark-connector：<font color="orange">2.6.1</font><br><strong>·</strong> nebula-algorithm：<font color="orange">2.6.1</font></p><h3 id="编译打包nebula-spark-connector-1"><a href="#编译打包nebula-spark-connector-1" class="headerlink" title="编译打包nebula-spark-connector"></a>编译打包nebula-spark-connector</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> nebula-algorithm-2.6.1/nebula-algorithm</span><br><span class="line">$ mvn clean package -Dgpg.skip -Dmaven.javadoc.skip=<span class="literal">true</span> -Dmaven.test.skip=<span class="literal">true</span></span><br></pre></td></tr></table></figure><p>成功后在<code>nebula-algorithm/target/</code> 目录下得到 <code>nebula-algorithm-2.6.1.jar</code>文件</p><h2 id="PySpark-调用-Nebula-Algorithm"><a href="#PySpark-调用-Nebula-Algorithm" class="headerlink" title="PySpark 调用 Nebula Algorithm"></a>PySpark 调用 Nebula Algorithm</h2><h3 id="Pycharm-1"><a href="#Pycharm-1" class="headerlink" title="Pycharm"></a>Pycharm</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> findspark</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession, DataFrame</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.window <span class="keyword">import</span> Window</span><br><span class="line"><span class="keyword">from</span> py4j.java_gateway <span class="keyword">import</span> java_import</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">"HADOOP_CONF_DIR"</span>] = <span class="string">"/exp/server/hadoop-2.7.7/etc/hadoop"</span></span><br><span class="line">os.environ[<span class="string">"YARN_CONF_DIR"</span>] = <span class="string">"/exp/server/hadoop-2.7.7/etc/hadoop"</span></span><br><span class="line">os.environ[<span class="string">"SPARK_HOME"</span>] = <span class="string">"/exp/server/spark-2.4.7-bin-hadoop2.7"</span></span><br><span class="line">os.environ[<span class="string">"PYSPARK_PYTHON"</span>] = <span class="string">"/root/anaconda3/envs/pyspark37/bin/python"</span></span><br><span class="line">os.environ[<span class="string">"PYSPARK_DRIVER_PYTHON"</span>] = <span class="string">"/root/anaconda3/envs/pyspark37/bin/python"</span></span><br><span class="line">findspark.init()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.config(</span><br><span class="line">    <span class="string">"spark.jars"</span>,</span><br><span class="line">    <span class="string">"/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar"</span>).config(</span><br><span class="line">    <span class="string">"spark.driver.extraClassPath"</span>,</span><br><span class="line">    <span class="string">"/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar"</span>).config(</span><br><span class="line">    <span class="string">"spark.jars"</span>,</span><br><span class="line">    <span class="string">"/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar"</span>).config(</span><br><span class="line">    <span class="string">"spark.driver.extraClassPath"</span>,</span><br><span class="line">    <span class="string">"/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar"</span>).appName(</span><br><span class="line">    <span class="string">"nebula-connector"</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># spark = SparkSession.builder.appName("PageRankExample").getOrCreate()</span></span><br><span class="line">jspark = spark._jsparkSession</span><br><span class="line"></span><br><span class="line"><span class="comment"># import "com.vesoft.nebula.algorithm.config.SparkConfig"</span></span><br><span class="line">java_import(spark._jvm, <span class="string">"com.vesoft.nebula.algorithm.config.SparkConfig"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># import "com.vesoft.nebula.algorithm.config.PRConfig"</span></span><br><span class="line">java_import(spark._jvm, <span class="string">"com.vesoft.nebula.algorithm.config.PRConfig"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># import "com.vesoft.nebula.algorithm.lib.PageRankAlgo"</span></span><br><span class="line">java_import(spark._jvm, <span class="string">"com.vesoft.nebula.algorithm.lib.PageRankAlgo"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将string类型vid转int类型vid</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_string_id_to_long_id</span><span class="params">(df)</span>:</span></span><br><span class="line">    df = df.drop(<span class="string">"display_desc"</span>).drop(<span class="string">"creation_time"</span>)</span><br><span class="line">    src_id_df = df.select(<span class="string">"_srcId"</span>).withColumnRenamed(<span class="string">"_srcId"</span>, <span class="string">"id"</span>)</span><br><span class="line">    dst_id_df = df.select(<span class="string">"_dstId"</span>).withColumnRenamed(<span class="string">"_dstId"</span>, <span class="string">"id"</span>)</span><br><span class="line">    id_df = src_id_df.union(dst_id_df).distinct()</span><br><span class="line">    encode_id = id_df.withColumn(<span class="string">"encodedId"</span>, dense_rank().over(Window.orderBy(<span class="string">"id"</span>)))</span><br><span class="line">    <span class="comment"># encode_id.write.option("header", True).csv("file:///tmp/encodeId.csv")</span></span><br><span class="line">    src_join_df = df.join(encode_id, df._srcId == encode_id.id) \</span><br><span class="line">        .drop(<span class="string">"_srcId"</span>) \</span><br><span class="line">        .drop(<span class="string">"id"</span>) \</span><br><span class="line">        .withColumnRenamed(<span class="string">"encodedId"</span>, <span class="string">"_srcId"</span>)</span><br><span class="line"></span><br><span class="line">    df_sv = df.join(encode_id, df._srcId == encode_id.id) \</span><br><span class="line">        .drop(<span class="string">"_srcId"</span>) \</span><br><span class="line">        .drop(<span class="string">"_rank"</span>) \</span><br><span class="line">        .drop(<span class="string">"_dstId"</span>) \</span><br><span class="line">        .withColumnRenamed(<span class="string">"id"</span>, <span class="string">"src_vid"</span>) \</span><br><span class="line">        .withColumnRenamed(<span class="string">"encodedId"</span>, <span class="string">"_srcId"</span>)</span><br><span class="line"></span><br><span class="line">    df_dv = src_join_df.join(encode_id, src_join_df._dstId == encode_id.id) \</span><br><span class="line">        .drop(<span class="string">"_dstId"</span>) \</span><br><span class="line">        .drop(<span class="string">"_rank"</span>) \</span><br><span class="line">        .drop(<span class="string">"degree"</span>) \</span><br><span class="line">        .withColumnRenamed(<span class="string">"encodedId"</span>, <span class="string">"_dstId"</span>) \</span><br><span class="line">        .withColumnRenamed(<span class="string">"_srcID"</span>, <span class="string">"_src"</span>) \</span><br><span class="line">        .withColumnRenamed(<span class="string">"id"</span>, <span class="string">"dst_vid"</span>)</span><br><span class="line"></span><br><span class="line">    dst_join_df = src_join_df.join(encode_id, src_join_df._dstId == encode_id.id) \</span><br><span class="line">        .drop(<span class="string">"_dstId"</span>) \</span><br><span class="line">        .drop(<span class="string">"_rank"</span>) \</span><br><span class="line">        .drop(<span class="string">"degree"</span>) \</span><br><span class="line">        .withColumnRenamed(<span class="string">"encodedId"</span>, <span class="string">"_dstId"</span>).drop(<span class="string">"id"</span>)</span><br><span class="line"></span><br><span class="line">    df_v = df_dv.join(df_sv, df_dv._src == df_sv._srcId).drop(<span class="string">"_src"</span>)</span><br><span class="line">    <span class="keyword">return</span> dst_join_df, df_v</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">df = spark.read.format(</span><br><span class="line">    <span class="string">"com.vesoft.nebula.connector.NebulaDataSource"</span>).option(</span><br><span class="line">    <span class="comment"># "sep", "\t").option(</span></span><br><span class="line">    <span class="string">"type"</span>, <span class="string">"edge"</span>).option(</span><br><span class="line">    <span class="string">"spaceName"</span>, <span class="string">"DWD_GRAPH_LY_V3_2023"</span>).option(</span><br><span class="line">    <span class="string">"label"</span>, <span class="string">"edge_phone"</span>).option(</span><br><span class="line">    <span class="string">"returnCols"</span>, <span class="string">""</span>).option(</span><br><span class="line">    <span class="string">"metaAddress"</span>, <span class="string">"192.168.100.45:9559"</span>).option(</span><br><span class="line">    <span class="string">"partitionNumber"</span>, <span class="number">1</span>).load()</span><br><span class="line"></span><br><span class="line">df.orderBy(<span class="string">"creation_time"</span>).show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">df_int, df_v = convert_string_id_to_long_id(df)</span><br><span class="line">prConfig = spark._jvm.PRConfig(<span class="number">1</span>, <span class="number">0.8</span>)</span><br><span class="line">prResult = spark._jvm.PageRankAlgo.apply(jspark, df_int._jdf, prConfig, <span class="literal">False</span>)</span><br><span class="line">df_v.show(<span class="number">20</span>, <span class="literal">False</span>)</span><br><span class="line">prResult.show(<span class="number">20</span>, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将jdf转df</span></span><br><span class="line">prResult = prResult.toDF()</span><br><span class="line">pagerank_df = DataFrame(prResult, spark)</span><br><span class="line">pagerank_df.sort(col(<span class="string">"pagerank"</span>).desc()).show(<span class="number">60</span>, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> PYSPARK (pyspark=2.4.7, python=3.7)</span></span><br><span class="line"><span class="comment"># conda activate pyspark37</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> SPARK-SUBMIT</span></span><br><span class="line"><span class="comment"># $&#123;SPARK_HOME&#125;/bin/spark-submit --master local[*] \</span></span><br><span class="line"><span class="comment"># --deploy-mode client \</span></span><br><span class="line"><span class="comment"># --driver-class-path file:/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar \</span></span><br><span class="line"><span class="comment"># --driver-class-path file:/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar \</span></span><br><span class="line"><span class="comment"># --jars file:/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar \</span></span><br><span class="line"><span class="comment"># --jars file:/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar \</span></span><br><span class="line"><span class="comment"># /exp/work/pyspark/nebula/pagerank.py</span></span><br></pre></td></tr></table></figure><h3 id="Spark-submit-1"><a href="#Spark-submit-1" class="headerlink" title="Spark-submit"></a>Spark-submit</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$&#123;SPARK_HOME&#125;</span>/bin/spark-submit --master <span class="built_in">local</span>[*] \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">--driver-class-path file:/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar \</span><br><span class="line">--driver-class-path file:/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar \</span><br><span class="line">--jars file:/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar \</span><br><span class="line">--jars file:/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar \</span><br><span class="line">/exp/work/pyspark/nebula/pagerank.py</span><br></pre></td></tr></table></figure><h3 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br></pre></td><td class="code"><pre><span class="line">ssh://root@192.168.100.43:22/root/anaconda3/envs/pyspark37/bin/python -u /exp/work/pyspark/nebula/pagerank.py</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/exp/server/spark-2.4.7-bin-hadoop2.7/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]</span><br><span class="line">23/03/24 14:42:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Setting default log level to &quot;WARN&quot;.</span><br><span class="line">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</span><br><span class="line">+--------------------+--------------------+-----+-------------+------------+</span><br><span class="line">|              _srcId|              _dstId|_rank|creation_time|display_desc|</span><br><span class="line">+--------------------+--------------------+-----+-------------+------------+</span><br><span class="line">|dwd_accounttelegr...|dwd_phone66802018560|    0|   1678692201|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone12366827389|    0|   1678692201|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone86151162...|    0|   1678692201|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone79653470394|    0|   1678692210|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone99890918...|    0|   1678692210|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone99890535...|    0|   1678692210|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone85620578...|    0|   1678692210|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone99899666...|    0|   1678692210|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone88802607465|    0|   1678692219|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone62831556...|    0|   1678692228|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone95965067...|    0|   1678692228|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone85595212324|    0|   1678692228|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone86132873...|    0|   1678692279|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone63948774...|    0|   1678692279|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone85590645678|    0|   1678692279|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone63948229...|    0|   1678692288|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone99899913...|    0|   1678692288|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone99890302...|    0|   1678692288|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone99890986...|    0|   1678692288|  注册手机号|</span><br><span class="line">|dwd_accounttelegr...|dwd_phone16728042478|    0|   1678692297|  注册手机号|</span><br><span class="line">+--------------------+--------------------+-----+-------------+------------+</span><br><span class="line">only showing top 20 rows</span><br><span class="line"></span><br><span class="line">23/03/24 14:42:39 WARN window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.</span><br><span class="line">23/03/24 14:42:39 WARN window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.</span><br><span class="line">[Stage 3:&gt;                  (0 + 1) / 1][Stage 4:&gt;                  (0 + 1) / 1]23/03/24 14:42:49 WARN storage.BlockManager: Block rdd_29_0 already exists on this machine; not re-adding it</span><br><span class="line">23/03/24 14:42:50 WARN window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.</span><br><span class="line">23/03/24 14:42:50 WARN window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.</span><br><span class="line">23/03/24 14:42:50 WARN window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.</span><br><span class="line">+----------------------+------+---------------------------------+------+</span><br><span class="line">|dst_vid               |_dstId|src_vid                          |_srcId|</span><br><span class="line">+----------------------+------+---------------------------------+------+</span><br><span class="line">|dwd_phone8613951884225|18141 |dwd_accountctrip1187326021       |1     |</span><br><span class="line">|dwd_phone8618705166775|19486 |dwd_accountctrip18705166775      |2     |</span><br><span class="line">|dwd_phone8613006584273|17582 |dwd_accountctripM2295153681      |3     |</span><br><span class="line">|dwd_phone8615668271999|18526 |dwd_accountctripM2494074526      |4     |</span><br><span class="line">|dwd_phone8616653556969|18757 |dwd_accountctripM2682271769      |5     |</span><br><span class="line">|dwd_phone8613562835888|17938 |dwd_accountctripM3011519721      |6     |</span><br><span class="line">|dwd_phone8618506413343|19344 |dwd_accountctripM4761127290      |7     |</span><br><span class="line">|dwd_phone8618112957512|19178 |dwd_accountctripM4915141465      |8     |</span><br><span class="line">|dwd_phone8615119464037|18222 |dwd_accountctripM548327259       |9     |</span><br><span class="line">|dwd_phone8613123368388|17719 |dwd_accountctripM601536478       |10    |</span><br><span class="line">|dwd_phone8617605442050|18905 |dwd_accountctrip_WeChat2269232661|11    |</span><br><span class="line">|dwd_phone8613123368388|17719 |dwd_accountdidi101108284         |12    |</span><br><span class="line">|dwd_phone8616653556969|18757 |dwd_accountdidi1641492054156     |13    |</span><br><span class="line">|dwd_phone8615851895366|18609 |dwd_accountdidi1746082933402     |14    |</span><br><span class="line">|dwd_phone8617561929739|18889 |dwd_accountdidi17598416475664    |15    |</span><br><span class="line">|dwd_phone8615119464037|18222 |dwd_accountdidi25441524          |16    |</span><br><span class="line">|dwd_phone8613006584273|17582 |dwd_accountdidi2881218873540     |17    |</span><br><span class="line">|dwd_phone8613951884225|18141 |dwd_accountdidi3099925           |18    |</span><br><span class="line">|dwd_phone8618600764544|19397 |dwd_accountdidi486186360832      |19    |</span><br><span class="line">|dwd_phone8618705166775|19486 |dwd_accountdidi772722            |20    |</span><br><span class="line">+----------------------+------+---------------------------------+------+</span><br><span class="line">only showing top 20 rows</span><br><span class="line"></span><br><span class="line">+-----+------------------+</span><br><span class="line">|_id  |pagerank          |</span><br><span class="line">+-----+------------------+</span><br><span class="line">|19021|1.1105466561585526|</span><br><span class="line">|9831 |0.8884373249268421|</span><br><span class="line">|5354 |0.8884373249268421|</span><br><span class="line">|4926 |0.8884373249268421|</span><br><span class="line">|21377|1.1105466561585526|</span><br><span class="line">|14609|1.1105466561585526|</span><br><span class="line">|11852|1.1105466561585526|</span><br><span class="line">|8390 |0.8884373249268421|</span><br><span class="line">|10837|0.8884373249268421|</span><br><span class="line">|4992 |0.8884373249268421|</span><br><span class="line">|20894|1.1105466561585526|</span><br><span class="line">|21780|1.1105466561585526|</span><br><span class="line">|1813 |0.8884373249268421|</span><br><span class="line">|9025 |0.8884373249268421|</span><br><span class="line">|14554|1.1105466561585526|</span><br><span class="line">|1780 |0.8884373249268421|</span><br><span class="line">|16132|1.1105466561585526|</span><br><span class="line">|22467|1.1105466561585526|</span><br><span class="line">|2117 |0.8884373249268421|</span><br><span class="line">|16321|1.1105466561585526|</span><br><span class="line">+-----+------------------+</span><br><span class="line">only showing top 20 rows</span><br><span class="line"></span><br><span class="line">+-----+------------------+</span><br><span class="line">|_id  |pagerank          |</span><br><span class="line">+-----+------------------+</span><br><span class="line">|18222|3.331639968475657 |</span><br><span class="line">|19178|2.8874213060122367|</span><br><span class="line">|19486|2.8874213060122367|</span><br><span class="line">|12765|1.9989839810853947|</span><br><span class="line">|13332|1.9989839810853947|</span><br><span class="line">|13505|1.9989839810853947|</span><br><span class="line">|17582|1.9989839810853947|</span><br><span class="line">|18889|1.776874649853684 |</span><br><span class="line">|19176|1.776874649853684 |</span><br><span class="line">|12470|1.776874649853684 |</span><br><span class="line">|19397|1.5547653186219736|</span><br><span class="line">|18141|1.5547653186219736|</span><br><span class="line">|11750|1.5547653186219736|</span><br><span class="line">|18555|1.5547653186219736|</span><br><span class="line">|13216|1.5547653186219736|</span><br><span class="line">|18905|1.5547653186219736|</span><br><span class="line">|17688|1.5547653186219736|</span><br><span class="line">|19734|1.5547653186219736|</span><br><span class="line">|12768|1.5547653186219736|</span><br><span class="line">|18609|1.5547653186219736|</span><br><span class="line">|18347|1.332655987390263 |</span><br><span class="line">|19473|1.332655987390263 |</span><br><span class="line">|12007|1.332655987390263 |</span><br><span class="line">|12100|1.332655987390263 |</span><br><span class="line">|16268|1.332655987390263 |</span><br><span class="line">|19298|1.332655987390263 |</span><br><span class="line">|23258|1.332655987390263 |</span><br><span class="line">|17938|1.332655987390263 |</span><br><span class="line">|20042|1.332655987390263 |</span><br><span class="line">|12639|1.332655987390263 |</span><br><span class="line">|13309|1.332655987390263 |</span><br><span class="line">|12739|1.332655987390263 |</span><br><span class="line">|18037|1.332655987390263 |</span><br><span class="line">|18757|1.332655987390263 |</span><br><span class="line">|18344|1.332655987390263 |</span><br><span class="line">|12461|1.332655987390263 |</span><br><span class="line">|20037|1.332655987390263 |</span><br><span class="line">|13754|1.332655987390263 |</span><br><span class="line">|18864|1.332655987390263 |</span><br><span class="line">|21713|1.332655987390263 |</span><br><span class="line">|16872|1.332655987390263 |</span><br><span class="line">|15710|1.332655987390263 |</span><br><span class="line">|12229|1.332655987390263 |</span><br><span class="line">|19183|1.332655987390263 |</span><br><span class="line">|17928|1.332655987390263 |</span><br><span class="line">|19118|1.332655987390263 |</span><br><span class="line">|20243|1.332655987390263 |</span><br><span class="line">|12223|1.332655987390263 |</span><br><span class="line">|23219|1.332655987390263 |</span><br><span class="line">|12529|1.332655987390263 |</span><br><span class="line">|20865|1.332655987390263 |</span><br><span class="line">|16538|1.332655987390263 |</span><br><span class="line">|13253|1.332655987390263 |</span><br><span class="line">|13329|1.332655987390263 |</span><br><span class="line">|13198|1.332655987390263 |</span><br><span class="line">|17719|1.332655987390263 |</span><br><span class="line">|14661|1.1105466561585526|</span><br><span class="line">|18584|1.1105466561585526|</span><br><span class="line">|12532|1.1105466561585526|</span><br><span class="line">|20466|1.1105466561585526|</span><br><span class="line">+-----+------------------+</span><br><span class="line">only showing top 60 rows</span><br></pre></td></tr></table></figure><h3 id="封装并写回nebula-graph实例"><a href="#封装并写回nebula-graph实例" class="headerlink" title="封装并写回nebula-graph实例"></a>封装并写回nebula-graph实例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> findspark</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession, DataFrame</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.window <span class="keyword">import</span> Window</span><br><span class="line"><span class="keyword">from</span> py4j.java_gateway <span class="keyword">import</span> java_import</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span> nebula_config</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> environment variables</span></span><br><span class="line">os.environ[<span class="string">"HADOOP_CONF_DIR"</span>] = <span class="string">"/exp/server/hadoop-2.7.7/etc/hadoop"</span></span><br><span class="line">os.environ[<span class="string">"YARN_CONF_DIR"</span>] = <span class="string">"/exp/server/hadoop-2.7.7/etc/hadoop"</span></span><br><span class="line">os.environ[<span class="string">"SPARK_HOME"</span>] = <span class="string">"/exp/server/spark-2.4.7-bin-hadoop2.7"</span></span><br><span class="line">os.environ[<span class="string">"PYSPARK_PYTHON"</span>] = <span class="string">"/root/anaconda3/envs/pyspark37/bin/python"</span></span><br><span class="line">os.environ[<span class="string">"PYSPARK_DRIVER_PYTHON"</span>] = <span class="string">"/root/anaconda3/envs/pyspark37/bin/python"</span></span><br><span class="line">findspark.init()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PageRank</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    pagerank</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.df = <span class="literal">None</span></span><br><span class="line">        self.dfc = <span class="literal">None</span></span><br><span class="line">        self.tag = <span class="literal">None</span></span><br><span class="line">        self.id_df = <span class="literal">None</span></span><br><span class="line">        self.df_int = <span class="literal">None</span></span><br><span class="line">        self.encode_id = <span class="literal">None</span></span><br><span class="line">        self.src_id_df = <span class="literal">None</span></span><br><span class="line">        self.dst_id_df = <span class="literal">None</span></span><br><span class="line">        self.src_join_df = <span class="literal">None</span></span><br><span class="line">        self.dst_join_df = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> spark object with nebula-spark-connector &amp; nebula-algorithm</span></span><br><span class="line">        self.spark = SparkSession.builder.appName(</span><br><span class="line">            <span class="string">"pagerank"</span>)\</span><br><span class="line">            .master(</span><br><span class="line">            <span class="string">"local[*]"</span>)\</span><br><span class="line">            .config(</span><br><span class="line">            <span class="string">"spark.jars"</span>,</span><br><span class="line">            <span class="string">"/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar"</span>)\</span><br><span class="line">            .config(</span><br><span class="line">            <span class="string">"spark.driver.extraClassPath"</span>,</span><br><span class="line">            <span class="string">"/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar"</span>)\</span><br><span class="line">            .config(</span><br><span class="line">            <span class="string">"spark.jars"</span>,</span><br><span class="line">            <span class="string">"/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar"</span>)\</span><br><span class="line">            .config(</span><br><span class="line">            <span class="string">"spark.driver.extraClassPath"</span>,</span><br><span class="line">            <span class="string">"/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar"</span>)\</span><br><span class="line">            .getOrCreate()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> java import</span></span><br><span class="line">        self.jspark = self.spark._jsparkSession</span><br><span class="line"></span><br><span class="line">        <span class="comment"># import "com.vesoft.nebula.algorithm.config.SparkConfig"</span></span><br><span class="line">        java_import(self.spark._jvm, <span class="string">"com.vesoft.nebula.algorithm.config.SparkConfig"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># import "com.vesoft.nebula.algorithm.config.PRConfig"</span></span><br><span class="line">        java_import(self.spark._jvm, <span class="string">"com.vesoft.nebula.algorithm.config.PRConfig"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># import "com.vesoft.nebula.algorithm.lib.PageRankAlgo"</span></span><br><span class="line">        java_import(self.spark._jvm, <span class="string">"com.vesoft.nebula.algorithm.lib.PageRankAlgo"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_tag</span><span class="params">(self, tag: list)</span>:</span></span><br><span class="line">        self.tag = tag</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">exe</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.sdf_create()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> covert fixed_string vid 2 long vid</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">convert</span><span class="params">(self, df)</span>:</span></span><br><span class="line">        self.df = df.drop(<span class="string">"creation_time"</span>)</span><br><span class="line">        self.src_id_df = self.df.select(<span class="string">"_srcId"</span>).withColumnRenamed(<span class="string">"_srcId"</span>, <span class="string">"id"</span>)</span><br><span class="line">        self.dst_id_df = self.df.select(<span class="string">"_dstId"</span>).withColumnRenamed(<span class="string">"_dstId"</span>, <span class="string">"id"</span>)</span><br><span class="line">        self.id_df = self.src_id_df.union(self.dst_id_df).distinct()</span><br><span class="line">        self.encode_id = self.id_df.withColumn(<span class="string">"encodedId"</span>, dense_rank().over(Window.orderBy(<span class="string">"id"</span>)))</span><br><span class="line">        self.src_join_df = self.df.join(self.encode_id, self.df._srcId == self.encode_id.id) \</span><br><span class="line">            .drop(<span class="string">"_srcId"</span>) \</span><br><span class="line">            .drop(<span class="string">"id"</span>) \</span><br><span class="line">            .withColumnRenamed(<span class="string">"encodedId"</span>, <span class="string">"_srcId"</span>)</span><br><span class="line"></span><br><span class="line">        self.dst_join_df = self.src_join_df.join(self.encode_id, self.src_join_df._dstId == self.encode_id.id) \</span><br><span class="line">            .drop(<span class="string">"_dstId"</span>) \</span><br><span class="line">            .drop(<span class="string">"_rank"</span>) \</span><br><span class="line">            .drop(<span class="string">"degree"</span>) \</span><br><span class="line">            .withColumnRenamed(<span class="string">"encodedId"</span>, <span class="string">"_dstId"</span>).drop(<span class="string">"id"</span>)</span><br><span class="line">        <span class="comment"># self.dst_join_df.write.option("header", True).csv("file:/exp/work/pyspark/nebula/pr_file/1.csv")</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> sdf build by nebula_reader</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sdf_create</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.tag)):</span><br><span class="line">            self.dfc = self.spark.read.format(</span><br><span class="line">                <span class="string">"com.vesoft.nebula.connector.NebulaDataSource"</span>).option(</span><br><span class="line">                <span class="string">"type"</span>, <span class="string">"edge"</span>).option(</span><br><span class="line">                <span class="string">"spaceName"</span>, nebula_config.get(<span class="string">"space"</span>)).option(</span><br><span class="line">                <span class="string">"label"</span>, <span class="string">f"<span class="subst">&#123;self.tag[i]&#125;</span>"</span>).option(</span><br><span class="line">                <span class="string">"returnCols"</span>, <span class="string">"creation_time"</span>).option(</span><br><span class="line">                <span class="string">"metaAddress"</span>, nebula_config.get(<span class="string">"metaAddress"</span>)).option(</span><br><span class="line">                <span class="string">"partitionNumber"</span>, nebula_config.get(<span class="string">"partitionNumber"</span>)).load()</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">                self.df = self.dfc</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.df = self.df.unionByName(self.dfc)</span><br><span class="line"></span><br><span class="line">        self.convert(self.df)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> NEBULA-READER</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nebula_reader</span><span class="params">(sth, tag)</span>:</span></span><br><span class="line">    dataframe = sth.spark.read.format(</span><br><span class="line">        <span class="string">"com.vesoft.nebula.connector.NebulaDataSource"</span>).option(</span><br><span class="line">        <span class="string">"type"</span>, <span class="string">"vertex"</span>).option(</span><br><span class="line">        <span class="string">"spaceName"</span>, nebula_config.get(<span class="string">"space"</span>)).option(</span><br><span class="line">        <span class="string">"label"</span>, <span class="string">f"<span class="subst">&#123;tag&#125;</span>"</span>).option(</span><br><span class="line">        <span class="string">"returnCols"</span>, <span class="string">"pagerank"</span>).option(</span><br><span class="line">        <span class="string">"metaAddress"</span>, nebula_config.get(<span class="string">"metaAddress"</span>)).option(</span><br><span class="line">        <span class="string">"partitionNumber"</span>, nebula_config.get(<span class="string">"partitionNumber"</span>)).load().drop(<span class="string">"pagerank"</span>)</span><br><span class="line">    <span class="keyword">return</span> dataframe</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> UPDATE NEBULA-GRAPH</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nebula_writer</span><span class="params">(tag, dataframe)</span>:</span></span><br><span class="line">    dataframe.write.format(<span class="string">"com.vesoft.nebula.connector.NebulaDataSource"</span>).option(</span><br><span class="line">        <span class="string">"type"</span>, <span class="string">"vertex"</span>).option(</span><br><span class="line">        <span class="string">"spaceName"</span>, nebula_config.get(<span class="string">"space"</span>)).option(</span><br><span class="line">        <span class="string">"label"</span>, <span class="string">f"<span class="subst">&#123;tag&#125;</span>"</span>).option(</span><br><span class="line">        <span class="string">"vidPolicy"</span>, <span class="string">""</span>).option(</span><br><span class="line">        <span class="string">"vertexField"</span>, <span class="string">"_vertexId"</span>).option(</span><br><span class="line">        <span class="string">"writeMode"</span>, <span class="string">"update"</span>).option(</span><br><span class="line">        <span class="string">"batch"</span>, nebula_config.get(<span class="string">"batch"</span>)).option(</span><br><span class="line">        <span class="string">"metaAddress"</span>, nebula_config.get(<span class="string">"metaAddress"</span>)).option(</span><br><span class="line">        <span class="string">"graphAddress"</span>, nebula_config.get(<span class="string">"graphAddress"</span>)).option(</span><br><span class="line">        <span class="string">"passwd"</span>, nebula_config.get(<span class="string">"passwd"</span>)).option(</span><br><span class="line">        <span class="string">"user"</span>, nebula_config.get(<span class="string">"user"</span>)).save()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> process data</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(tag: list, *args)</span>:</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> algorithm object</span></span><br><span class="line">    obj = PageRank()</span><br><span class="line">    obj.set_tag(tag)</span><br><span class="line">    obj.exe()</span><br><span class="line"></span><br><span class="line">    encode_id = obj.encode_id</span><br><span class="line">    df_int = obj.dst_join_df</span><br><span class="line">    df_spark = df_int</span><br><span class="line"></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> nebula-algorithm</span></span><br><span class="line">    config = obj.spark._jvm.PRConfig(<span class="number">1</span>, <span class="number">0.8</span>)</span><br><span class="line">    result = obj.spark._jvm.PageRankAlgo.apply(obj.jspark, df_spark._jdf, config, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> jdf to sdf</span></span><br><span class="line">    result = result.toDF()</span><br><span class="line">    algo_df = DataFrame(result, obj.spark)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> long vid mapping fixed_string vid</span></span><br><span class="line">    df = encode_id.join(algo_df, encode_id.encodedId == algo_df._id) \</span><br><span class="line">        .drop(<span class="string">"encodedId"</span>) \</span><br><span class="line">        .drop(<span class="string">"_id"</span>) \</span><br><span class="line">        .withColumnRenamed(<span class="string">"id"</span>, <span class="string">"_vertexId"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> arg <span class="keyword">in</span> args:</span><br><span class="line">        df_ = nebula_reader(obj, arg)</span><br><span class="line">        df_ = df_.join(df, df_._vertexId == df._vertexId, <span class="string">"leftsemi"</span>)</span><br><span class="line">        df__ = df.join(df_, df._vertexId == df_._vertexId, <span class="string">"leftsemi"</span>)</span><br><span class="line">        nebula_writer(arg, df__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    process([<span class="string">"edge_phone"</span>], <span class="string">"dwd_phone"</span>, <span class="string">"dwd_account"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> PYSPARK (pyspark=2.4.7, python=3.7)</span></span><br><span class="line"><span class="comment"># conda activate pyspark37</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> SPARK-SUBMIT</span></span><br><span class="line"><span class="comment"># $&#123;SPARK_HOME&#125;/bin/spark-submit --master local[*] \</span></span><br><span class="line"><span class="comment"># --deploy-mode client \</span></span><br><span class="line"><span class="comment"># --driver-class-path file:/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar \</span></span><br><span class="line"><span class="comment"># --driver-class-path file:/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar \</span></span><br><span class="line"><span class="comment"># --jars file:/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar \</span></span><br><span class="line"><span class="comment"># --jars file:/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar \</span></span><br><span class="line"><span class="comment"># /exp/work/pyspark/nebula/pagerank.py</span></span><br></pre></td></tr></table></figure><h3 id="fastapi接口实例"><a href="#fastapi接口实例" class="headerlink" title="fastapi接口实例"></a>fastapi接口实例</h3><p><code>`</code>python<br>import os</p><p>import findspark</p><h1 id="from-pyspark-sql-import-SparkSession"><a href="#from-pyspark-sql-import-SparkSession" class="headerlink" title="from pyspark.sql import SparkSession"></a>from pyspark.sql import SparkSession</h1><p>from pyspark.sql import DataFrame<br>from pyspark.sql.functions import *<br>from typing import List<br>from fastapi import FastAPI, Query<br>from fastapi.middleware.cors import CORSMiddleware<br>from pydantic import BaseModel</p><p>from config import nebula_config, pagerank_dict<br>from algorithm.pagerank import PageRank<br>from algorithm.louvain import Louvain</p><p>app_router = FastAPI(docs_url=None, redoc_url=None)</p><h1 id="CORS"><a href="#CORS" class="headerlink" title="CORS"></a>CORS</h1><p>origins = [<br>    “*”<br>]</p><p>app_router.add_middleware(<br>    CORSMiddleware,<br>    allow_origins=origins,<br>    allow_credentials=True,<br>    allow_methods=[“<em>“],<br>    allow_headers=[“</em>“]<br>)</p><p>class NebulaAlgorithm(BaseModel):<br>    algo: str<br>    tag_list: List[str]</p><p>algorithm_dict = {<br>    “pagerank”: PageRank(),<br>    “louvain”: Louvain()<br>}</p><p>@app_router.post(“/nebula/algo”)<br>async def nebula_algorithm(<br>        args: NebulaAlgorithm<br>):</p><pre><code># TODO: algorithm objecttags = []for line in args.tag_list:    for item in pagerank_dict.get(line):        tags.append(item)algo = algorithm_dict.get(args.algo)obj = algoobj.set_tag(tags)obj.exe()# TODO: nebula-algorithmencode_id = obj.encode_iddf_int = obj.dst_join_dfif algo.__doc__.strip() == &apos;pagerank&apos;:    if len(tags) == 1:        df_spark = df_int    else:        df_pd = df_int.toPandas()        values = df_pd.values.tolist()        columns = df_pd.columns.tolist()        df_spark = obj.spark.createDataFrame(values, columns)    config = obj.spark._jvm.PRConfig(1, 0.8)    result = obj.spark._jvm.PageRankAlgo.apply(obj.jspark, df_spark._jdf, config, False)elif algo.__doc__.strip() == &apos;louvain&apos;:    if len(tags) == 1:        df_spark = df_int    else:        df_pd = df_int.toPandas()        values = df_pd.values.tolist()        columns = df_pd.columns.tolist()        df_spark = obj.spark.createDataFrame(values, columns)    config = obj.spark._jvm.LouvainConfig(20, 10, 0.5)    result = obj.spark._jvm.LouvainAlgo.apply(obj.jspark, df_spark._jdf, config, False)else:    return# TODO: jdf to sdfresult = result.toDF()algo_df = DataFrame(result, obj.spark)# TODO: long vid mapping fixed_string viddf = encode_id.join(algo_df, encode_id.encodedId == algo_df._id) \    .drop(&quot;encodedId&quot;) \    .drop(&quot;_id&quot;) \    .withColumnRenamed(&quot;id&quot;, &quot;_vertexId&quot;)df.show()</code></pre><p>if <strong>name</strong> == “<strong>main</strong>“:<br>    import uvicorn<br>    uvicorn.run(app=”main:app_router”, reload=True, host=’0.0.0.0’, port=7792)</p><h1 id="TODO-PYSPARK-pyspark-2-4-7-python-3-7"><a href="#TODO-PYSPARK-pyspark-2-4-7-python-3-7" class="headerlink" title="TODO: PYSPARK (pyspark=2.4.7, python=3.7)"></a>TODO: PYSPARK (pyspark=2.4.7, python=3.7)</h1><h1 id="conda-activate-pyspark37"><a href="#conda-activate-pyspark37" class="headerlink" title="conda activate pyspark37"></a>conda activate pyspark37</h1><h1 id="TODO-SPARK-SUBMIT"><a href="#TODO-SPARK-SUBMIT" class="headerlink" title="TODO: SPARK-SUBMIT"></a>TODO: SPARK-SUBMIT</h1><h1 id="SPARK-HOME-bin-spark-submit-–master-local"><a href="#SPARK-HOME-bin-spark-submit-–master-local" class="headerlink" title="${SPARK_HOME}/bin/spark-submit –master local[*] \"></a>${SPARK_HOME}/bin/spark-submit –master local[*] \</h1><h1 id="–deploy-mode-client"><a href="#–deploy-mode-client" class="headerlink" title="–deploy-mode client \"></a>–deploy-mode client \</h1><h1 id="–driver-class-path-file-exp-work-pyspark-nebula-spark-connector-nebula-spark-connector-target-nebula-spark-connector-2-6-1-jar"><a href="#–driver-class-path-file-exp-work-pyspark-nebula-spark-connector-nebula-spark-connector-target-nebula-spark-connector-2-6-1-jar" class="headerlink" title="–driver-class-path file:/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar \"></a>–driver-class-path file:/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar \</h1><h1 id="–driver-class-path-file-exp-work-pyspark-nebula-algorithm-nebula-algorithm-target-nebula-algorithm-2-6-1-jar"><a href="#–driver-class-path-file-exp-work-pyspark-nebula-algorithm-nebula-algorithm-target-nebula-algorithm-2-6-1-jar" class="headerlink" title="–driver-class-path file:/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar \"></a>–driver-class-path file:/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar \</h1><h1 id="–jars-file-exp-work-pyspark-nebula-spark-connector-nebula-spark-connector-target-nebula-spark-connector-2-6-1-jar"><a href="#–jars-file-exp-work-pyspark-nebula-spark-connector-nebula-spark-connector-target-nebula-spark-connector-2-6-1-jar" class="headerlink" title="–jars file:/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar \"></a>–jars file:/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar \</h1><h1 id="–jars-file-exp-work-pyspark-nebula-algorithm-nebula-algorithm-target-nebula-algorithm-2-6-1-jar"><a href="#–jars-file-exp-work-pyspark-nebula-algorithm-nebula-algorithm-target-nebula-algorithm-2-6-1-jar" class="headerlink" title="–jars file:/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar \"></a>–jars file:/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar \</h1><h1 id="exp-work-pyspark-nebula-main-py"><a href="#exp-work-pyspark-nebula-main-py" class="headerlink" title="/exp/work/pyspark/nebula/main.py"></a>/exp/work/pyspark/nebula/main.py</h1>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;Nebula-Spark-Connector&quot;&gt;&lt;a href=&quot;#Nebula-Spark-Connector&quot; class=&quot;headerlink&quot; title=&quot;Nebula Spark Connector&quot;&gt;&lt;/a&gt;Nebula Spark Connector&lt;/h2&gt;&lt;p&gt;下载地址&amp;amp;官方文档：【&lt;a href=&quot;https://github.com/vesoft-inc/nebula-spark-connector&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/vesoft-inc/nebula-spark-connector&lt;/a&gt;】&lt;/p&gt;
&lt;h3 id=&quot;环境&quot;&gt;&lt;a href=&quot;#环境&quot; class=&quot;headerlink&quot; title=&quot;环境&quot;&gt;&lt;/a&gt;环境&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;·&lt;/strong&gt; nebula：&lt;font color=&quot;orange&quot;&gt;2.6.1&lt;/font&gt;&lt;br&gt;&lt;strong&gt;·&lt;/strong&gt; hadoop：&lt;font color=&quot;orange&quot;&gt;2.7&lt;/font&gt;&lt;br&gt;&lt;strong&gt;·&lt;/strong&gt; spark：&lt;font color=&quot;orange&quot;&gt;2.4.7&lt;/font&gt;&lt;br&gt;&lt;strong&gt;·&lt;/strong&gt; pyspark：&lt;font color=&quot;orange&quot;&gt;2.4.7&lt;/font&gt;&lt;br&gt;&lt;strong&gt;·&lt;/strong&gt; python：&lt;font color=&quot;orange&quot;&gt;3.7.16&lt;/font&gt;&lt;br&gt;&lt;strong&gt;·&lt;/strong&gt; nebula-spark-connector：&lt;font color=&quot;orange&quot;&gt;2.6.1&lt;/font&gt;&lt;/p&gt;
&lt;h3 id=&quot;编译打包nebula-spark-connector&quot;&gt;&lt;a href=&quot;#编译打包nebula-spark-connector&quot; class=&quot;headerlink&quot; title=&quot;编译打包nebula-spark-connector&quot;&gt;&lt;/a&gt;编译打包nebula-spark-connector&lt;/h3&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ &lt;span class=&quot;built_in&quot;&gt;cd&lt;/span&gt; nebula-spark-connector-2.6.1/nebula-spark-connector&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ mvn clean package -Dmaven.test.skip=&lt;span class=&quot;literal&quot;&gt;true&lt;/span&gt; -Dgpg.skip -Dmaven.javadoc.skip=&lt;span class=&quot;literal&quot;&gt;true&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;成功后在&lt;code&gt;nebula-spark-connector/target/&lt;/code&gt; 目录下得到 &lt;code&gt;nebula-spark-connector-2.6.1.jar&lt;/code&gt;文件&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;(base) [root@root target]&lt;span class=&quot;comment&quot;&gt;# ll&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;total 106792&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;drwxr-xr-x 3 root root        17 Mar 11 14:14 classes&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;-rw-r--r-- 1 root root         1 Mar 11 14:14 classes.-497386701.timestamp&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;-rw-r--r-- 1 root root         1 Mar 11 14:14 classes.timestamp&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;-rw-r--r-- 1 root root     30701 Mar 11 14:15 jacoco.exec&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;drwxr-xr-x 2 root root        28 Mar 11 14:15 maven-archiver&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;-rw-r--r-- 1 root root 108375457 Mar 11 14:16 nebula-spark-connector-2.6.1.jar&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;-rw-r--r-- 1 root root    583482 Mar 11 14:16 nebula-spark-connector-2.6.1-javadoc.jar&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;-rw-r--r-- 1 root root     36358 Mar 11 14:16 nebula-spark-connector-2.6.1-sources.jar&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;-rw-r--r-- 1 root root    315392 Mar 11 14:15 original-nebula-spark-connector-2.6.1.jar&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;drwxr-xr-x 4 root root        37 Mar 11 14:15 site&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;PySpark-读取-NebulaGraph-数据&quot;&gt;&lt;a href=&quot;#PySpark-读取-NebulaGraph-数据&quot; class=&quot;headerlink&quot; title=&quot;PySpark 读取 NebulaGraph 数据&quot;&gt;&lt;/a&gt;PySpark 读取 NebulaGraph 数据&lt;/h3&gt;&lt;p&gt;从 &lt;code&gt;metaAddress&lt;/code&gt; 为 &lt;code&gt;&amp;quot;metad0:9559&amp;quot;&lt;/code&gt; 的 Nebula Graph 中读取整个 tag 下的数据为一个 dataframe：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;df = spark.read.format(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;string&quot;&gt;&quot;com.vesoft.nebula.connector.NebulaDataSource&quot;&lt;/span&gt;).option(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;type&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;vertex&quot;&lt;/span&gt;).option(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;spaceName&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;basketballplayer&quot;&lt;/span&gt;).option(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;label&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;player&quot;&lt;/span&gt;).option(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;returnCols&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;name,age&quot;&lt;/span&gt;).option(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;metaAddress&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;metad0:9559&quot;&lt;/span&gt;).option(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;partitionNumber&quot;&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;).load()&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;然后可以像这样 &lt;code&gt;show&lt;/code&gt; 这个 dataframe：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;&amp;gt;&amp;gt;&amp;gt; &lt;/span&gt;df.show(n=&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;+---------+--------------+---+&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|_vertexId|          name|age|&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;+---------+--------------+---+&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|player105|   Danny Green| &lt;span class=&quot;number&quot;&gt;31&lt;/span&gt;|&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;|player109|Tiago Splitter| &lt;span class=&quot;number&quot;&gt;34&lt;/span&gt;|&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;+---------+--------------+---+&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;only showing top &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt; rows&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
    
    <category term="nebula" scheme="http://yoursite.com/tags/nebula/"/>
    
    <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
    <category term="图算法" scheme="http://yoursite.com/tags/图算法/"/>
    
  </entry>
  
  <entry>
    <title>ElasticSearch环境搭建</title>
    <link href="http://yoursite.com/2023/02/16/ElasticSearch%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    <id>http://yoursite.com/2023/02/16/ElasticSearch%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</id>
    <published>2023-02-16T03:16:07.000Z</published>
    <updated>2023-03-24T06:52:51.416Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p>* 配合nebula全文检索测试环境而搭建的es单机环境</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><h3 id="下载地址"><a href="#下载地址" class="headerlink" title="下载地址"></a>下载地址</h3><p>【<a href="https://www.elastic.co/cn/downloads/elasticsearch#ga-release" target="_blank" rel="noopener">https://www.elastic.co/cn/downloads/elasticsearch#ga-release</a>】 (或前往elastic中文社区下载中心【<a href="https://elasticsearch.cn/download/" target="_blank" rel="noopener">https://elasticsearch.cn/download/</a>】)</p><ul><li>选择linux版本</li></ul><p><img src="/2023/02/16/ElasticSearch环境搭建/A.png" alt></p><h3 id="安装ES"><a href="#安装ES" class="headerlink" title="安装ES"></a>安装ES</h3><ul><li>解压缩</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> tar xf elasticsearch-7.14.2-linux-x86_64.tar.gz</span><br></pre></td></tr></table></figure><ul><li>创建es用户</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> useradd es &amp;&amp; passwd es</span><br></pre></td></tr></table></figure><ul><li>更名</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> mv elasticsearch-7.14.2 elasticsearch</span><br></pre></td></tr></table></figure><ul><li>赋予es用户权限</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> chown -R es:es elasticsearch</span><br></pre></td></tr></table></figure><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><pre><code>* 可使用es自带的java环境：ES_JAVA_HOME</code></pre><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> vim /etc/profile</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ES_JAVA_HOME</span></span><br><span class="line"><span class="built_in">export</span> ES_JAVA_HOME=/data/elasticsearch/jdk/</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$ES_JAVA_HOME</span>/bin:<span class="variable">$PATH</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">source</span> /etc/profile</span></span><br></pre></td></tr></table></figure><ul><li>elasticsearch config</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> elasticsearch/config</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vim elasticsearch.yml</span></span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">node.name:</span> <span class="string">node-1</span>                          <span class="comment">##节点名称</span></span><br><span class="line"><span class="attr">path.data:</span> <span class="string">/usr/local/elasticsearch/data</span>        <span class="comment">##数据存放路径</span></span><br><span class="line"><span class="attr">path.logs:</span> <span class="string">/usr/local/elasticsearch/logs</span>        <span class="comment">##日志存放路径 </span></span><br><span class="line"><span class="attr">bootstrap.memory_lock:</span> <span class="literal">true</span>                <span class="comment">##避免es使用swap交换分区</span></span><br><span class="line"><span class="attr">indices.requests.cache.size:</span> <span class="number">5</span><span class="string">%</span>            <span class="comment">##缓存配置</span></span><br><span class="line"><span class="attr">indices.queries.cache.size:</span> <span class="number">10</span><span class="string">%</span>            <span class="comment">##缓存配置</span></span><br><span class="line"><span class="attr">network.host:</span> <span class="number">192.168</span><span class="number">.80</span><span class="number">.128</span>               <span class="comment">##本机IP</span></span><br><span class="line"><span class="attr">http.port:</span> <span class="number">9200</span>                            <span class="comment">##默认端口</span></span><br><span class="line"><span class="attr">cluster.initial_master_nodes:</span> <span class="string">["node-1"]</span>   <span class="comment">##设置符合主节点条件的节点的主机名或 IP 地址来引导启动集群</span></span><br><span class="line"><span class="attr">http.cors.enabled:</span> <span class="literal">true</span>                    <span class="comment">##跨域</span></span><br><span class="line"><span class="attr">http.cors.allow-origin:</span> <span class="string">"*"</span></span><br></pre></td></tr></table></figure><ul><li>将当前用户软硬限制调大</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vim /etc/security/limits.conf</span></span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">es</span> <span class="string">soft</span> <span class="string">nofile</span> <span class="number">65535</span></span><br><span class="line"><span class="string">es</span> <span class="string">hard</span> <span class="string">nofile</span> <span class="number">65537</span></span><br><span class="line"><span class="string">es</span> <span class="string">soft</span> <span class="string">memlock</span> <span class="string">unlimited</span></span><br><span class="line"><span class="string">es</span> <span class="string">hard</span> <span class="string">memlock</span> <span class="string">unlimited</span></span><br></pre></td></tr></table></figure><ul><li>修改vm.max_map_count内存</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysctl.conf</span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">vm.max_map_count=655360</span></span><br></pre></td></tr></table></figure><h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> su es</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> ../bin</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ./elasticsearch -d</span></span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="Kibana"><a href="#Kibana" class="headerlink" title="Kibana"></a>Kibana</h2><h3 id="下载地址-1"><a href="#下载地址-1" class="headerlink" title="下载地址"></a>下载地址</h3><p>[<a href="https://elasticsearch.cn/download/]" target="_blank" rel="noopener">https://elasticsearch.cn/download/]</a></p><ul><li>选择和es相同版本</li></ul><h3 id="安装kibana"><a href="#安装kibana" class="headerlink" title="安装kibana"></a>安装kibana</h3><ul><li>解压缩</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> tar -zxvf kibana-7.14.2-linux-x86_64.tar.gz</span></span><br></pre></td></tr></table></figure><ul><li>更名</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mv kibana-7.14.2-linux-x86_64 kibana</span></span><br></pre></td></tr></table></figure><h3 id="配置-1"><a href="#配置-1" class="headerlink" title="配置"></a>配置</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd kibana</span><br><span class="line">vim config/kibana.yml</span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">server.port:</span> <span class="number">5601</span></span><br><span class="line"><span class="attr">server.host:</span> <span class="string">"0.0.0.0"</span></span><br><span class="line"><span class="attr">elasticsearch.hosts:</span> <span class="string">"http://192.168.80.128:9200"</span></span><br><span class="line"><span class="attr">kibana.index:</span> <span class="string">".kibana"</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 修改/etc/sudoers文件，进入超级用户, 给予es用户写权限</span></span><br><span class="line">(base) [root@localhost kibana]# chmod u+w /etc/sudoers</span><br><span class="line"><span class="meta">#</span><span class="bash"> 编辑/etc/sudoers文件</span></span><br><span class="line">(base) [root@localhost kibana]# vim /etc/sudoers</span><br><span class="line"><span class="meta">#</span><span class="bash"> 赋予es用户权限</span></span><br><span class="line">es    ALL=(ALL)       ALL</span><br><span class="line">(base) [root@localhost kibana]# chmod u-w /etc/sudoers</span><br><span class="line"><span class="meta">#</span><span class="bash"> 撤销es用户文件写权限</span></span><br><span class="line">(base) [root@localhost kibana]# sudo chown -R es:es /usr/local/kibana</span><br><span class="line">(base) [root@localhost kibana]# su es</span><br></pre></td></tr></table></figure><h3 id="启动-1"><a href="#启动-1" class="headerlink" title="启动"></a>启动</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[es@localhost kibana]$ cd bin</span><br><span class="line">[es@localhost bin]$ ./kibana</span><br></pre></td></tr></table></figure><ul><li>进入kibana</li></ul><p><code>http:192.168.80.128:5601</code></p><p><img src="/2023/02/16/ElasticSearch环境搭建/B.png" alt></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;* 配合nebula全文检索测试环境而搭建的es单机环境&lt;/p&gt;
&lt;h2 id=&quot;安装&quot;&gt;&lt;a href=&quot;#安装&quot; class=&quot;headerlink&quot; title=&quot;安装&quot;&gt;&lt;/a&gt;安装&lt;/h2&gt;&lt;h3 id=&quot;下载地址&quot;&gt;&lt;a href=&quot;#下载地址&quot; class=&quot;headerlink&quot; title=&quot;下载地址&quot;&gt;&lt;/a&gt;下载地址&lt;/h3&gt;&lt;p&gt;【&lt;a href=&quot;https://www.elastic.co/cn/downloads/elasticsearch#ga-release&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.elastic.co/cn/downloads/elasticsearch#ga-release&lt;/a&gt;】 (或前往elastic中文社区下载中心【&lt;a href=&quot;https://elasticsearch.cn/download/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://elasticsearch.cn/download/&lt;/a&gt;】)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;选择linux版本&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/2023/02/16/ElasticSearch环境搭建/A.png&quot; alt&gt;&lt;/p&gt;
&lt;h3 id=&quot;安装ES&quot;&gt;&lt;a href=&quot;#安装ES&quot; class=&quot;headerlink&quot; title=&quot;安装ES&quot;&gt;&lt;/a&gt;安装ES&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;解压缩&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; tar xf elasticsearch-7.14.2-linux-x86_64.tar.gz&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;创建es用户&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; useradd es &amp;amp;&amp;amp; passwd es&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;更名&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; mv elasticsearch-7.14.2 elasticsearch&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;赋予es用户权限&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; chown -R es:es elasticsearch&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;配置&quot;&gt;&lt;a href=&quot;#配置&quot; class=&quot;headerlink&quot; title=&quot;配置&quot;&gt;&lt;/a&gt;配置&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;* 可使用es自带的java环境：ES_JAVA_HOME
&lt;/code&gt;&lt;/pre&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; vim /etc/profile&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# ES_JAVA_HOME&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;export&lt;/span&gt; ES_JAVA_HOME=/data/elasticsearch/jdk/&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;export&lt;/span&gt; PATH=&lt;span class=&quot;variable&quot;&gt;$ES_JAVA_HOME&lt;/span&gt;/bin:&lt;span class=&quot;variable&quot;&gt;$PATH&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; &lt;span class=&quot;built_in&quot;&gt;source&lt;/span&gt; /etc/profile&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;elasticsearch config&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; &lt;span class=&quot;built_in&quot;&gt;cd&lt;/span&gt; elasticsearch/config&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; vim elasticsearch.yml&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight yaml&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;node.name:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;node-1&lt;/span&gt;                          &lt;span class=&quot;comment&quot;&gt;##节点名称&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;path.data:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;/usr/local/elasticsearch/data&lt;/span&gt;        &lt;span class=&quot;comment&quot;&gt;##数据存放路径&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;path.logs:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;/usr/local/elasticsearch/logs&lt;/span&gt;        &lt;span class=&quot;comment&quot;&gt;##日志存放路径 &lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;bootstrap.memory_lock:&lt;/span&gt; &lt;span class=&quot;literal&quot;&gt;true&lt;/span&gt;                &lt;span class=&quot;comment&quot;&gt;##避免es使用swap交换分区&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;indices.requests.cache.size:&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;string&quot;&gt;%&lt;/span&gt;            &lt;span class=&quot;comment&quot;&gt;##缓存配置&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;indices.queries.cache.size:&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;string&quot;&gt;%&lt;/span&gt;            &lt;span class=&quot;comment&quot;&gt;##缓存配置&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;network.host:&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;192.168&lt;/span&gt;&lt;span class=&quot;number&quot;&gt;.80&lt;/span&gt;&lt;span class=&quot;number&quot;&gt;.128&lt;/span&gt;               &lt;span class=&quot;comment&quot;&gt;##本机IP&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;http.port:&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;9200&lt;/span&gt;                            &lt;span class=&quot;comment&quot;&gt;##默认端口&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;cluster.initial_master_nodes:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;[&quot;node-1&quot;]&lt;/span&gt;   &lt;span class=&quot;comment&quot;&gt;##设置符合主节点条件的节点的主机名或 IP 地址来引导启动集群&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;http.cors.enabled:&lt;/span&gt; &lt;span class=&quot;literal&quot;&gt;true&lt;/span&gt;                    &lt;span class=&quot;comment&quot;&gt;##跨域&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;http.cors.allow-origin:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;*&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;将当前用户软硬限制调大&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; vim /etc/security/limits.conf&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight yaml&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;es&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;soft&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;nofile&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;65535&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;es&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;hard&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;nofile&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;65537&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;es&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;soft&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;memlock&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;unlimited&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;es&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;hard&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;memlock&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;unlimited&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;修改vm.max_map_count内存&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;vim /etc/sysctl.conf&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight yaml&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;vm.max_map_count=655360&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;启动&quot;&gt;&lt;a href=&quot;#启动&quot; class=&quot;headerlink&quot; title=&quot;启动&quot;&gt;&lt;/a&gt;启动&lt;/h3&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; su es&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; &lt;span class=&quot;built_in&quot;&gt;cd&lt;/span&gt; ../bin&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; ./elasticsearch -d&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
    
    <category term="elastic search" scheme="http://yoursite.com/tags/elastic-search/"/>
    
  </entry>
  
  <entry>
    <title>Spark-SQL</title>
    <link href="http://yoursite.com/2022/12/08/Spark-SQL/"/>
    <id>http://yoursite.com/2022/12/08/Spark-SQL/</id>
    <published>2022-12-08T08:50:52.000Z</published>
    <updated>2022-12-09T08:58:34.011Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><h2 id="SparkSQL"><a href="#SparkSQL" class="headerlink" title="SparkSQL"></a>SparkSQL</h2><p><strong>SparkSQL</strong>是spark的一个用于处理海量<strong>结构化数据</strong>的模块</p><ul><li>支持SQL语言</li><li>自动优化</li><li>性能强</li><li>兼容HIVE</li><li>API流程简单</li><li>支持标准化JDBC和ODBC连接</li><li>…</li></ul><h3 id="SparkSQL数据抽象"><a href="#SparkSQL数据抽象" class="headerlink" title="SparkSQL数据抽象"></a>SparkSQL数据抽象</h3><ul><li><p><strong><font color="orange">Pandas · DataFrame</font></strong></p><p><strong><font color="orange">·</font></strong> 二维表数据结构</p><p><strong><font color="orange">·</font></strong> 单机（本地）集合</p></li><li><p><strong><font color="orange">SparkCore · RDD</font></strong></p><p><strong><font color="orange">·</font></strong> 无标准数据结构</p><p><strong><font color="orange">·</font></strong> 分布式（分区）集合</p></li><li><p><strong><font color="orange">SparkSQL · DataFrame</font></strong></p><p><strong><font color="orange">·</font></strong> 二维表数据结构</p><p><strong><font color="orange">·</font></strong> 分布式（分区）集合</p></li></ul><h3 id="SparkSession对象"><a href="#SparkSession对象" class="headerlink" title="SparkSession对象"></a>SparkSession对象</h3><p>RDD程序的执行入口对象：<strong>SparkContext</strong></p><p>在Spark2.0以后，推出了<strong>SparkSession</strong>对象，来作为Spark编码的统一入口对象。<strong>SparkSession</strong>：</p><ul><li>用于SparkSQL编程，作为入口对象</li><li>用于SparkCore编程，通过SparkSession对象获取SparkContext</li></ul><p>构建SparkSession对象：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过builder方法来构建SparkSession对象</span></span><br><span class="line"><span class="comment"># appName：设置程序名称</span></span><br><span class="line"><span class="comment"># config：配置常用属性</span></span><br><span class="line"><span class="comment"># getOrCreate：完成创建SparkSession对象</span></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).config(<span class="string">"spark.sql.shuffle.partitions"</span>, <span class="string">"4"</span>).getOrCreate()</span><br></pre></td></tr></table></figure><p>通过SparkSesion对象获取SparkContext对象：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).config(<span class="string">"spark.sql.shuffle.partitions"</span>, <span class="string">"4"</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line">sc = spark.sparkContext</span><br></pre></td></tr></table></figure><a id="more"></a><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).config(<span class="string">"spark.sql.shuffle.partitions"</span>, <span class="string">"4"</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">df1 = spark.read.csv(<span class="string">"hdfs://master:8020/input/pip_file.txt"</span>, sep=<span class="string">','</span>, header=<span class="literal">False</span>)</span><br><span class="line">df2 = df1.toDF(<span class="string">"id"</span>, <span class="string">"package"</span>, <span class="string">"version"</span>)</span><br><span class="line">df2.printSchema()         <span class="comment"># 表结构</span></span><br><span class="line">df2.show()                <span class="comment"># 展示表</span></span><br><span class="line">df2.createTempView(<span class="string">"pip"</span>) <span class="comment"># 创建"pip"表,保存在内存中</span></span><br><span class="line"><span class="comment"># SQL风格</span></span><br><span class="line">spark.sql(<span class="string">"""</span></span><br><span class="line"><span class="string">SELECT * FROM pip WHERE package='pyspark'</span></span><br><span class="line"><span class="string">"""</span>).show</span><br><span class="line"><span class="comment"># DSL风格</span></span><br><span class="line">df2.where(<span class="string">"package='pyspark'"</span>).limit(<span class="number">5</span>).show()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- id: string (nullable = true)</span><br><span class="line"> |-- package: string (nullable = true)</span><br><span class="line"> |-- version: string (nullable = true)</span><br><span class="line"></span><br><span class="line">+---+-------------+--------+</span><br><span class="line">| id|      package| version|</span><br><span class="line">+---+-------------+--------+</span><br><span class="line">|  1|        flask|   1.1.4|</span><br><span class="line">|  2|      fastapi|  0.78.0|</span><br><span class="line">|  3|         h5py|  2.10.0|</span><br><span class="line">|  4|        keras|   2.6.0|</span><br><span class="line">|  5|        jieba|  0.42.1|</span><br><span class="line">|  6|        numpy|  1.23.3|</span><br><span class="line">|  7|opencv-python|4.6.0.66|</span><br><span class="line">|  8|       pandas|   1.4.3|</span><br><span class="line">|  9|       pillow|   9.2.0|</span><br><span class="line">| 10|         py4j|0.10.9.5|</span><br><span class="line">| 11|      pyspark|   3.3.1|</span><br><span class="line">| 12|      sklearn|     0.0|</span><br><span class="line">| 13|   tensorfolw|   2.6.2|</span><br><span class="line">| 14|     requests|  2.28.1|</span><br><span class="line">| 15|        redis|   3.5.3|</span><br><span class="line">+---+-------------+--------+</span><br><span class="line"></span><br><span class="line">+---+-------+-------+</span><br><span class="line">| id|package|version|</span><br><span class="line">+---+-------+-------+</span><br><span class="line">| 11|pyspark|  3.3.1|</span><br><span class="line">+---+-------+-------+</span><br><span class="line"></span><br><span class="line">+---+-------+-------+</span><br><span class="line">| id|package|version|</span><br><span class="line">+---+-------+-------+</span><br><span class="line">| 11|pyspark|  3.3.1|</span><br><span class="line">+---+-------+-------+</span><br></pre></td></tr></table></figure><h2 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h2><h3 id="DataFrame的组成"><a href="#DataFrame的组成" class="headerlink" title="DataFrame的组成"></a>DataFrame的组成</h3><p>DataFrame是一个二维表结构，在结构层面：</p><ul><li>StructureType对象描述整个DataFrame的表结构</li><li>StructField对象描述一个列的信息</li></ul><p>在数据层面：</p><ul><li>Row对象记录一行数据</li><li>Column对象记录一列数据并包含列的信息</li></ul><h3 id="DataFrame的构建"><a href="#DataFrame的构建" class="headerlink" title="DataFrame的构建"></a>DataFrame的构建</h3><h4 id="RDD转换"><a href="#RDD转换" class="headerlink" title="RDD转换"></a>RDD转换</h4><p>DataFrame对象可以从RDD转换而来</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = spark.createDataFrame(rdd, schema=[<span class="string">'id'</span>, <span class="string">'package'</span>])</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">rdd = sc.textFile(<span class="string">"hdfs://master:8020/input/pip_file.txt"</span>).map(<span class="keyword">lambda</span> x: x.split(<span class="string">","</span>)).map(<span class="keyword">lambda</span> x:(int(x[<span class="number">0</span>]), x[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">df = spark.createDataFrame(rdd, schema=[<span class="string">'id'</span>, <span class="string">'package'</span>])</span><br><span class="line">df.printSchema()</span><br><span class="line">df.show(<span class="number">10</span>, <span class="literal">False</span>) <span class="comment"># 输出前十行数据，要全部显示默认设置为True</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">"pip"</span>)</span><br><span class="line">spark.sql(<span class="string">"SELECT * FROM pip WHERE id &lt; 5"</span>).show()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- id: long (nullable = true)</span><br><span class="line"> |-- package: string (nullable = true)</span><br><span class="line"></span><br><span class="line">+---+-------------+</span><br><span class="line">|id |package      |</span><br><span class="line">+---+-------------+</span><br><span class="line">|1  |flask        |</span><br><span class="line">|2  |fastapi      |</span><br><span class="line">|3  |h5py         |</span><br><span class="line">|4  |keras        |</span><br><span class="line">|5  |jieba        |</span><br><span class="line">|6  |numpy        |</span><br><span class="line">|7  |opencv-python|</span><br><span class="line">|8  |pandas       |</span><br><span class="line">|9  |pillow       |</span><br><span class="line">|10 |py4j         |</span><br><span class="line">+---+-------------+</span><br><span class="line">only showing top 10 rows</span><br><span class="line"></span><br><span class="line">+---+-------+</span><br><span class="line">| id|package|</span><br><span class="line">+---+-------+</span><br><span class="line">|  1|  flask|</span><br><span class="line">|  2|fastapi|</span><br><span class="line">|  3|   h5py|</span><br><span class="line">|  4|  keras|</span><br><span class="line">+---+-------+</span><br></pre></td></tr></table></figure><h4 id="StructType"><a href="#StructType" class="headerlink" title="StructType"></a>StructType</h4><p>通过StructType对象来定义DataFrame的表结构，转换RDD</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入StructType对象和类型</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType, IntegerType</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义表结构</span></span><br><span class="line">schema = StructType().add(<span class="string">"id"</span>, IntegerType(), nullable=<span class="literal">True</span>).add(<span class="string">"package"</span>, StringType(), nullable=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType, IntegerType</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">rdd = sc.textFile(<span class="string">"hdfs://master:8020/input/pip_file.txt"</span>).map(<span class="keyword">lambda</span> x: x.split(<span class="string">","</span>)).map(<span class="keyword">lambda</span> x:(int(x[<span class="number">0</span>]), x[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">schema = StructType().add(<span class="string">"id"</span>, IntegerType(), nullable=<span class="literal">True</span>).add(<span class="string">"package"</span>, StringType(), nullable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">df = spark.createDataFrame(rdd, schema=schema)</span><br><span class="line">df.printSchema()</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- id: integer (nullable = true)</span><br><span class="line"> |-- package: string (nullable = true)</span><br><span class="line"></span><br><span class="line">+---+-------------+</span><br><span class="line">| id|      package|</span><br><span class="line">+---+-------------+</span><br><span class="line">|  1|        flask|</span><br><span class="line">|  2|      fastapi|</span><br><span class="line">|  3|         h5py|</span><br><span class="line">|  4|        keras|</span><br><span class="line">|  5|        jieba|</span><br><span class="line">|  6|        numpy|</span><br><span class="line">|  7|opencv-python|</span><br><span class="line">|  8|       pandas|</span><br><span class="line">|  9|       pillow|</span><br><span class="line">| 10|         py4j|</span><br><span class="line">| 11|      pyspark|</span><br><span class="line">| 12|      sklearn|</span><br><span class="line">| 13|   tensorfolw|</span><br><span class="line">| 14|     requests|</span><br><span class="line">| 15|        redis|</span><br><span class="line">+---+-------------+</span><br></pre></td></tr></table></figure><h4 id="toDF"><a href="#toDF" class="headerlink" title="toDF"></a>toDF</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType, IntegerType</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">rdd = sc.textFile(<span class="string">"hdfs://master:8020/input/pip_file.txt"</span>).map(<span class="keyword">lambda</span> x: x.split(<span class="string">","</span>)).map(<span class="keyword">lambda</span> x:(int(x[<span class="number">0</span>]), x[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># toDF快速构建DataFrame</span></span><br><span class="line"><span class="comment"># 对列类型不敏感，默认string类型</span></span><br><span class="line">df1 = rdd.toDF([<span class="string">"id"</span>, <span class="string">"package"</span>])</span><br><span class="line">df1.printSchema()</span><br><span class="line">df1.show(<span class="number">5</span>, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置schema通过toDF构建DataFrame</span></span><br><span class="line">schema = StructType().add(<span class="string">"id"</span>, IntegerType(), nullable=<span class="literal">True</span>).add(<span class="string">"package"</span>, StringType(), nullable=<span class="literal">True</span>)</span><br><span class="line">df2 = rdd.toDF(schema=schema)</span><br><span class="line">df2.printSchema()</span><br><span class="line">df2.show(<span class="number">5</span>, <span class="literal">False</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- id: long (nullable = true)</span><br><span class="line"> |-- package: string (nullable = true)</span><br><span class="line"></span><br><span class="line">+---+-------+</span><br><span class="line">|id |package|</span><br><span class="line">+---+-------+</span><br><span class="line">|1  |flask  |</span><br><span class="line">|2  |fastapi|</span><br><span class="line">|3  |h5py   |</span><br><span class="line">|4  |keras  |</span><br><span class="line">|5  |jieba  |</span><br><span class="line">+---+-------+</span><br><span class="line">only showing top 5 rows</span><br><span class="line"></span><br><span class="line">root</span><br><span class="line"> |-- id: integer (nullable = true)</span><br><span class="line"> |-- package: string (nullable = true)</span><br><span class="line"></span><br><span class="line">+---+-------+</span><br><span class="line">|id |package|</span><br><span class="line">+---+-------+</span><br><span class="line">|1  |flask  |</span><br><span class="line">|2  |fastapi|</span><br><span class="line">|3  |h5py   |</span><br><span class="line">|4  |keras  |</span><br><span class="line">|5  |jieba  |</span><br><span class="line">+---+-------+</span><br><span class="line">only showing top 5 rows</span><br></pre></td></tr></table></figure><h4 id="Pandas"><a href="#Pandas" class="headerlink" title="Pandas"></a>Pandas</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">pddf = pd.DataFrame(</span><br><span class="line">&#123;</span><br><span class="line">        <span class="string">"id"</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        <span class="string">"package"</span>: [<span class="string">"flask"</span>, <span class="string">"fastapi"</span>, <span class="string">"h5py"</span>],</span><br><span class="line">        <span class="string">"version"</span>: [<span class="string">"1.1.4"</span>, <span class="string">"0.78.0"</span>, <span class="string">"2.10.0"</span>]</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">df = spark.createDataFrame(pddf)</span><br><span class="line">df.printSchema()</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- id: long (nullable = true)</span><br><span class="line"> |-- package: string (nullable = true)</span><br><span class="line"> |-- version: string (nullable = true)</span><br><span class="line"></span><br><span class="line">+---+-------+-------+</span><br><span class="line">| id|package|version|</span><br><span class="line">+---+-------+-------+</span><br><span class="line">|  1|  flask|  1.1.4|</span><br><span class="line">|  2|fastapi| 0.78.0|</span><br><span class="line">|  3|   h5py| 2.10.0|</span><br><span class="line">+---+-------+-------+</span><br></pre></td></tr></table></figure><h3 id="通过文件构建DataFrame"><a href="#通过文件构建DataFrame" class="headerlink" title="通过文件构建DataFrame"></a>通过文件构建DataFrame</h3><h4 id="text"><a href="#text" class="headerlink" title="text"></a>text</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sparksession.read.format(<span class="string">"text|csv|json|jdbc|..."</span>).option(<span class="string">"K"</span>, <span class="string">"V"</span>).schema(StructType|String).load(<span class="string">"文件路径，支持本地文件系统和HDFS"</span>)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建StructType，text数据源，读取数据的特点是将一整行当作一列来读取，默认列名为value，类型为string</span></span><br><span class="line">schema = StructType().add(<span class="string">"data"</span>, StringType(), nullable=<span class="literal">True</span>)</span><br><span class="line">df = spark.read.format(<span class="string">"text"</span>).schema(schema=schema).load(<span class="string">"hdfs://master:8020/input/pip_file.txt"</span>)</span><br><span class="line">df.printSchema()</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- data: string (nullable = true)</span><br><span class="line"></span><br><span class="line">+--------------------+</span><br><span class="line">|                data|</span><br><span class="line">+--------------------+</span><br><span class="line">|       1,flask,1.1.4|</span><br><span class="line">|    2,fastapi,0.78.0|</span><br><span class="line">|       3,h5py,2.10.0|</span><br><span class="line">|       4,keras,2.6.0|</span><br><span class="line">|      5,jieba,0.42.1|</span><br><span class="line">|      6,numpy,1.23.3|</span><br><span class="line">|7,opencv-python,4...|</span><br><span class="line">|      8,pandas,1.4.3|</span><br><span class="line">|      9,pillow,9.2.0|</span><br><span class="line">|    10,py4j,0.10.9.5|</span><br><span class="line">|    11,pyspark,3.3.1|</span><br><span class="line">|      12,sklearn,0.0|</span><br><span class="line">| 13,tensorfolw,2.6.2|</span><br><span class="line">|  14,requests,2.28.1|</span><br><span class="line">|      15,redis,3.5.3|</span><br><span class="line">+--------------------+</span><br></pre></td></tr></table></figure><h4 id="json"><a href="#json" class="headerlink" title="json"></a>json</h4><p>json文件自带一定数据结构</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">df = spark.read.format(<span class="string">"json"</span>).load(<span class="string">"hdfs://master:8020/input/pip_file.json"</span>)</span><br><span class="line">df.printSchema()</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- id: long (nullable = true)</span><br><span class="line"> |-- package: string (nullable = true)</span><br><span class="line"> |-- version: string (nullable = true)</span><br><span class="line"></span><br><span class="line">+---+-------+-------+</span><br><span class="line">| id|package|version|</span><br><span class="line">+---+-------+-------+</span><br><span class="line">|  1|  flask|  1.1.4|</span><br><span class="line">|  2|fastapi| 0.78.0|</span><br><span class="line">|  3|   h5py| 2.10.0|</span><br><span class="line">|  4|  keras|  2.6.0|</span><br><span class="line">|  5|  jieba| 0.42.1|</span><br><span class="line">+---+-------+-------+</span><br></pre></td></tr></table></figure><h4 id="csv"><a href="#csv" class="headerlink" title="csv"></a>csv</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过.option指定属性</span></span><br><span class="line">df = spark.read.format(<span class="string">"csv"</span>).option(<span class="string">"sep"</span>, <span class="string">";"</span>).option(<span class="string">"header"</span>, <span class="literal">True</span>).option(<span class="string">"encoding"</span>, <span class="string">"utf-8"</span>).schema(<span class="string">"id INT, package STRING, version STRING"</span>).load(<span class="string">"hdfs://master:8020/input/pip_file.csv"</span>)</span><br><span class="line">df.printSchema()</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- id: integer (nullable = true)</span><br><span class="line"> |-- package: string (nullable = true)</span><br><span class="line"> |-- version: string (nullable = true)</span><br><span class="line"></span><br><span class="line">+---+-------------+--------+</span><br><span class="line">| id|      package| version|</span><br><span class="line">+---+-------------+--------+</span><br><span class="line">|  1|        flask|   1.1.4|</span><br><span class="line">|  2|      fastapi|  0.78.0|</span><br><span class="line">|  3|         h5py|  2.10.0|</span><br><span class="line">|  4|        keras|   2.6.0|</span><br><span class="line">|  5|        jieba|  0.42.1|</span><br><span class="line">|  6|        numpy|  1.23.3|</span><br><span class="line">|  7|opencv-python|4.6.0.66|</span><br><span class="line">|  8|       pandas|   1.4.3|</span><br><span class="line">|  9|       pillow|   9.2.0|</span><br><span class="line">| 10|         py4j|0.10.9.5|</span><br><span class="line">| 11|      pyspark|   3.3.1|</span><br><span class="line">| 12|      sklearn|     0.0|</span><br><span class="line">| 13|   tensorfolw|   2.6.2|</span><br><span class="line">| 14|     requests|  2.28.1|</span><br><span class="line">| 15|        redis|   3.5.3|</span><br><span class="line">+---+-------------+--------+</span><br></pre></td></tr></table></figure><h4 id="parquet"><a href="#parquet" class="headerlink" title="parquet"></a>parquet</h4><p>parquet是spark中常用的一种列示存储文件格式</p><ul><li>内置schema（列名，列类型，是否为空）</li><li>存储是以列作为存储格式</li><li>存储是序列化存储在文件中（有压缩属性体积小）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过.option指定属性</span></span><br><span class="line">df = spark.read.format(<span class="string">"parquet"</span>).load(<span class="string">"hdfs://master:8020/input/suers.parquet"</span>)</span><br><span class="line">df.printSchema()</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- name: string (nullable = true)</span><br><span class="line"> |-- favorite_color: string (nullable = true)</span><br><span class="line"> |-- favorite_numbers: array (nullable = true)</span><br><span class="line"> |    |-- element: integer (containsNull = true)</span><br><span class="line"></span><br><span class="line">+------+--------------+----------------+</span><br><span class="line">|  name|favorite_color|favorite_numbers|</span><br><span class="line">+------+--------------+----------------+</span><br><span class="line">|Alyssa|          null|  [3, 9, 15, 20]|</span><br><span class="line">|   Ben|           red|              []|</span><br><span class="line">+------+--------------+----------------+</span><br></pre></td></tr></table></figure><h3 id="DataFrame编程"><a href="#DataFrame编程" class="headerlink" title="DataFrame编程"></a>DataFrame编程</h3><p>DataFrame支持两种编程风格</p><ul><li><p><strong>DSL风格</strong></p><p>被称为领域特定语言，是DataFrame的特有API</p></li><li><p><strong>SQL风格</strong></p><p>使用SQL语句来直接处理DataFrame</p></li></ul><h4 id="DSL风格"><a href="#DSL风格" class="headerlink" title="DSL风格"></a>DSL风格</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">df = spark.read.format(<span class="string">"csv"</span>).schema(<span class="string">"id INT, package STRING, version STRING"</span>).load(<span class="string">"hdfs://master:8020/input/pip_file.txt"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取Colum对象</span></span><br><span class="line">id_colum = df[<span class="string">'id'</span>]</span><br><span class="line">package_colum = df[<span class="string">'package'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># DSL风格</span></span><br><span class="line"><span class="comment"># select API</span></span><br><span class="line">df.select([<span class="string">'id'</span>, <span class="string">'package'</span>]).show()       <span class="comment"># list</span></span><br><span class="line">df.select(<span class="string">'id'</span>, <span class="string">'package'</span>).show()         <span class="comment"># 可变参数</span></span><br><span class="line">df.select(id_colum, package_colum).show() <span class="comment"># Colum对象</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># filter API</span></span><br><span class="line">df.filter(<span class="string">'id &lt; 5'</span>).show()</span><br><span class="line">df.filter(df[<span class="string">'id'</span>] &lt; <span class="number">5</span>).show()            <span class="comment"># Colum对象</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># where API</span></span><br><span class="line">df.where(<span class="string">'id &lt; 5'</span>).show()</span><br><span class="line">df.where(df[<span class="string">'id'</span>] &lt; <span class="number">5</span>).show()             <span class="comment"># Colum对象</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># groupBy API</span></span><br><span class="line">df.groupBy(<span class="string">'package'</span>).count().show()</span><br><span class="line">df.groupBy(df[<span class="string">'package'</span>]).count().show()  <span class="comment"># Colum对象</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回值不是DataFrame，而是GroupedData对象</span></span><br><span class="line"><span class="comment"># 是一个有分组关系的数据结构，提供API做分组聚合</span></span><br><span class="line"><span class="comment"># SQL：group by后接聚合：sum、avg、count、min、max</span></span><br><span class="line"><span class="comment"># GroupedData类似于SQL分组后的数据结构，同样拥有上述5种聚合方法</span></span><br><span class="line"><span class="comment"># GroupedData调用聚合方法后，返回值依旧是DataFrame对象</span></span><br><span class="line"><span class="comment"># GroupedData只是一个中转对象，最终还是要获得DataFrame对象</span></span><br><span class="line">r = df.groupBy(<span class="string">'package'</span>)</span><br><span class="line">print(r.sum().show(), r.avg().show(), r.count().show(), r.min().show(), r.max().show())</span><br></pre></td></tr></table></figure><h4 id="SQL风格"><a href="#SQL风格" class="headerlink" title="SQL风格"></a>SQL风格</h4><p>使用SQL风格，需要将DataFrame提前注册成表</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df.createTempView(<span class="string">"pip"</span>)            <span class="comment"># 注册一个临时视图（表）</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">"pip"</span>)   <span class="comment"># 注册一个临时表，如果已存在则进行替换</span></span><br><span class="line">df.createGlobalTempView(<span class="string">"pip"</span>)      <span class="comment"># 注册一个全局表</span></span><br></pre></td></tr></table></figure><ul><li><p><strong>全局表</strong>：可跨SparkSession对象使用，在一个程序内的多个SparkSession中均可调用，查询时带上前缀<code>global_temp</code></p></li><li><p><strong>临时表</strong>：仅在当前SparkSession中可用</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">df = spark.read.format(<span class="string">"csv"</span>).schema(<span class="string">"id INT, package STRING, version STRING"</span>).load(<span class="string">"hdfs://master:8020/input/pip_file.txt"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注册临时表</span></span><br><span class="line">df.createTempView(<span class="string">"pip"</span>)</span><br><span class="line">df.createOrReplaceTempView(<span class="string">"pip"</span>)</span><br><span class="line">df.createGlobalTempView(<span class="string">"pip_2"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过SparkSession SQL API执行sql语句</span></span><br><span class="line">spark.sql(<span class="string">"SELECT package, COUNT(*) AS cnt FROM pip GROUP BY package"</span>).show()</span><br><span class="line">spark.sql(<span class="string">"SELECT package, COUNT(*) AS cnt FROM global_temp.pip_2 GROUP BY package"</span>).show()</span><br></pre></td></tr></table></figure><h3 id="pyspark-sql-functions"><a href="#pyspark-sql-functions" class="headerlink" title="pyspark.sql.functions"></a>pyspark.sql.functions</h3><p>PySpark提供的pyspark.sql.functions包包含一系列可供SparkSQL使用的计算函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br></pre></td></tr></table></figure><p>即可调用F对象调用函数进行计算。这些功能函数的返回值大多是Colum对象</p><h3 id="WordCount示例"><a href="#WordCount示例" class="headerlink" title="WordCount示例"></a>WordCount示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line"><span class="comment"># SQL风格处理</span></span><br><span class="line">rdd = sc.textFile(<span class="string">"hdfs://master:8020/input/words.txt"</span>).flatMap(<span class="keyword">lambda</span> x: x.split(<span class="string">" "</span>)).map(<span class="keyword">lambda</span> x: [x])</span><br><span class="line">df = rdd.toDF([<span class="string">"word"</span>])</span><br><span class="line">df.createTempView(<span class="string">"words"</span>)</span><br><span class="line">spark.sql(<span class="string">"SELECT word, COUNT(*) AS cnt FROM words GROUP BY word ORDER BY cnt DESC"</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># DSL风格处理</span></span><br><span class="line">df = spark.read.format(<span class="string">"text"</span>).load(<span class="string">"hdfs://master:8020/input/words.txt"</span>)</span><br><span class="line"><span class="comment"># withColumn方法</span></span><br><span class="line"><span class="comment"># ：对已存在的列进行操作，返回与一个新的列，如果名字和之前相同则替换，否则新建列</span></span><br><span class="line">df.withColumn(<span class="string">"value"</span>, F.explode(F.split(df[<span class="string">'value'</span>], <span class="string">" "</span>))).show()</span><br><span class="line">df2 = df.withColumn(<span class="string">"value"</span>, F.explode(F.split(df[<span class="string">'value'</span>], <span class="string">" "</span>))).show()</span><br><span class="line">df2.groupBy(<span class="string">"value"</span>).count().show()</span><br><span class="line">df2.groupBy(<span class="string">"value"</span>).count().withColumnRenamed(<span class="string">"value"</span>, <span class="string">"name"</span>).withColumnRenamed(<span class="string">"count"</span>, <span class="string">"cnt"</span>).orderBy(<span class="string">"cnt"</span>, ascending=<span class="literal">False</span>).show()</span><br></pre></td></tr></table></figure><h2 id="SparkSQL-Shuffle"><a href="#SparkSQL-Shuffle" class="headerlink" title="SparkSQL Shuffle"></a>SparkSQL Shuffle</h2><p>spark.sql.shuffle.partitions参数是在spark sql计算过程中，shuffle算子阶段默认的分区数是200个。对于集群模式，200较为合适，如果在local模式下运行，200较多，会在调度上带来额外的损耗，所以在local模式下建议修改较低，例如2/4/10。这个参数和RDD中设置并行度的参数相互独立</p><p>可以按优先级在三处设置：</p><ul><li><p>代码设置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).config(<span class="string">"spark.sql.shuffle.partitions"</span>, <span class="string">"2"</span>).getOrCreate()</span><br></pre></td></tr></table></figure></li><li><p>客户端参数设置</p><p><code>bin/spark-submit --conf &quot;spark.sql.shuffle.partitions=100&quot;</code></p></li><li><p>配置文件设置</p><p><font color="#008080">conf/spark-defaults.conf</font> <code>spark.sql.shuffle.partitions 100</code></p></li></ul><h2 id="SparkSQL数据清洗"><a href="#SparkSQL数据清洗" class="headerlink" title="SparkSQL数据清洗"></a>SparkSQL数据清洗</h2><h3 id="数据去重API"><a href="#数据去重API" class="headerlink" title="数据去重API"></a>数据去重API</h3><p><strong>dropDuplicates</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DataFrame API</span></span><br><span class="line"><span class="comment"># 不设置参数，对全局的列联合起来进行比较，去除重复值，只保留一条</span></span><br><span class="line">df.dropDuplicates().show()</span><br><span class="line">df.dropDuplicates([<span class="string">'id'</span>, <span class="string">'version'</span>]).show()</span><br></pre></td></tr></table></figure><h3 id="缺失值处理API"><a href="#缺失值处理API" class="headerlink" title="缺失值处理API"></a>缺失值处理API</h3><p><strong>dropna</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DataFrame API</span></span><br><span class="line"><span class="comment"># 对缺失值的数据进行删除</span></span><br><span class="line"><span class="comment"># 不设置参数，只要列中存在null即删除整行</span></span><br><span class="line">df.dropna().show()</span><br><span class="line"><span class="comment"># tresh=3表示，最少满足3个有效列，不满足即删除当前数据</span></span><br><span class="line">df.dropna(thresh=<span class="number">3</span>).show()</span><br><span class="line">df.dropna(tresh=<span class="number">2</span>,subset[<span class="string">'id'</span>, <span class="string">'version'</span>]).show()</span><br></pre></td></tr></table></figure><p><strong>fillna</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DataFrame API</span></span><br><span class="line"><span class="comment"># 对缺失值的数据进行填充</span></span><br><span class="line">df.fillna(<span class="string">"loss"</span>).show()</span><br><span class="line"><span class="comment"># 对指定列进行填充</span></span><br><span class="line">df.fillna(<span class="string">"M/A"</span>, subset[<span class="string">'version'</span>]).show()</span><br><span class="line"><span class="comment"># 指定一个字典，对所有的列提供填充依据</span></span><br><span class="line">df.fillna(&#123;<span class="string">"id"</span>: <span class="string">"unknown"</span>, <span class="string">"package"</span>: <span class="string">"unknown"</span>, <span class="string">"version"</span>: <span class="string">"none"</span>&#125;).show()</span><br></pre></td></tr></table></figure><h2 id="DataFrame数据写出"><a href="#DataFrame数据写出" class="headerlink" title="DataFrame数据写出"></a>DataFrame数据写出</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mode：传入模式，append-追加，overwrite-覆盖，ignore-忽略，error-重复异常（默认）</span></span><br><span class="line"><span class="comment"># format：传入格式：text/csv/json/parquet/orc/avro/jdbc</span></span><br><span class="line"><span class="comment"># （text源仅支持单列df写出）</span></span><br><span class="line"><span class="comment"># option：设置属性，如.option("sep", ",")</span></span><br><span class="line"><span class="comment"># save：保存路径，支持本地文件系统和HDFS</span></span><br><span class="line">df.write.mode().format().option(K, V).save(PATH)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType, IntegerType</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 0. 构建执行环境入口对象SparkSession</span></span><br><span class="line">spark = SparkSession.builder.\</span><br><span class="line">    appName(<span class="string">"test"</span>).\</span><br><span class="line">    master(<span class="string">"local[*]"</span>).\</span><br><span class="line">    config(<span class="string">"spark.sql.shuffle.partitions"</span>, <span class="number">2</span>).\</span><br><span class="line">    getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 读取数据集</span></span><br><span class="line">schema = StructType().add(<span class="string">"user_id"</span>, StringType(), nullable=<span class="literal">True</span>). \</span><br><span class="line">    add(<span class="string">"movie_id"</span>, IntegerType(), nullable=<span class="literal">True</span>). \</span><br><span class="line">    add(<span class="string">"rank"</span>, IntegerType(), nullable=<span class="literal">True</span>). \</span><br><span class="line">    add(<span class="string">"ts"</span>, StringType(), nullable=<span class="literal">True</span>)</span><br><span class="line">df = spark.read.format(<span class="string">"csv"</span>). \</span><br><span class="line">    option(<span class="string">"sep"</span>, <span class="string">"\t"</span>). \</span><br><span class="line">    option(<span class="string">"header"</span>, <span class="literal">False</span>). \</span><br><span class="line">    option(<span class="string">"encoding"</span>, <span class="string">"utf-8"</span>). \</span><br><span class="line">    schema(schema=schema). \</span><br><span class="line">    load(<span class="string">"hdfs://master:8020/input/u.data"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Write text 写出, 只能写出一个列的数据, 需要将df转换为单列df</span></span><br><span class="line">df.select(F.concat_ws(<span class="string">"---"</span>, <span class="string">"user_id"</span>, <span class="string">"movie_id"</span>, <span class="string">"rank"</span>, <span class="string">"ts"</span>)).\</span><br><span class="line">    write.\</span><br><span class="line">    mode(<span class="string">"overwrite"</span>).\</span><br><span class="line">    format(<span class="string">"text"</span>).\</span><br><span class="line">    save(<span class="string">"hdfs://master:8020/output/sql/text"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Write csv</span></span><br><span class="line">df.write.mode(<span class="string">"overwrite"</span>).\</span><br><span class="line">    format(<span class="string">"csv"</span>).\</span><br><span class="line">    option(<span class="string">"sep"</span>, <span class="string">";"</span>).\</span><br><span class="line">    option(<span class="string">"header"</span>, <span class="literal">True</span>).\</span><br><span class="line">    save(<span class="string">"hdfs://master:8020/output/sql/csv"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Write json</span></span><br><span class="line">df.write.mode(<span class="string">"overwrite"</span>).\</span><br><span class="line">    format(<span class="string">"json"</span>).\</span><br><span class="line">    save(<span class="string">"hdfs://master:8020/output/sql/json"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Write parquet</span></span><br><span class="line">df.write.mode(<span class="string">"overwrite"</span>).\</span><br><span class="line">    format(<span class="string">"parquet"</span>).\</span><br><span class="line">    save(<span class="string">"hdfs://master:8020/output/sql/parquet"</span>)</span><br></pre></td></tr></table></figure><h2 id="DataFrame-JDBC"><a href="#DataFrame-JDBC" class="headerlink" title="DataFrame JDBC"></a>DataFrame JDBC</h2><p>将mysql包添加进pyspark</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/anaconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/jars/</span><br><span class="line">rz</span><br></pre></td></tr></table></figure><h3 id="DataFrame读写数据库"><a href="#DataFrame读写数据库" class="headerlink" title="DataFrame读写数据库"></a>DataFrame读写数据库</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将DataFrame通过JDBC写入mysql</span></span><br><span class="line">df.write.mode(<span class="string">"overwrite"</span>).\</span><br><span class="line">format(<span class="string">"jdbc"</span>).\</span><br><span class="line">option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://master:3306/..."</span>).\</span><br><span class="line">option(<span class="string">"dbtable"</span>, <span class="string">"test"</span>).\</span><br><span class="line">option(<span class="string">"user"</span>, <span class="string">"root"</span>).\</span><br><span class="line">option(<span class="string">"password"</span>, <span class="string">"123456"</span>).\</span><br><span class="line">save()</span><br><span class="line"></span><br><span class="line"><span class="comment"># JDBC会自动建表，因为DataFrame中含有表结构的信息</span></span><br><span class="line"><span class="comment"># 读mysql</span></span><br><span class="line">df = spark.read.format(<span class="string">"jdbc"</span>).\</span><br><span class="line">option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://master:3306/..."</span>).\</span><br><span class="line">option(<span class="string">"dbtable"</span>, <span class="string">"test"</span>).\</span><br><span class="line">option(<span class="string">"user"</span>, <span class="string">"root"</span>).\</span><br><span class="line">option(<span class="string">"password"</span>, <span class="string">"123456"</span>).\</span><br><span class="line">load()</span><br><span class="line"></span><br><span class="line">df.printSchema()</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure><h2 id="SparkSQL函数定义"><a href="#SparkSQL函数定义" class="headerlink" title="SparkSQL函数定义"></a>SparkSQL函数定义</h2><h3 id="SparkSQL定义UDF函数"><a href="#SparkSQL定义UDF函数" class="headerlink" title="SparkSQL定义UDF函数"></a>SparkSQL定义UDF函数</h3><p>SparkSQL模块自带实现公共方法的位置在pyspark.sql.functions中，同时SparkSQL和Hive一样支持自定义函数：UDF和UDAF</p><p>目前python仅支持SparkSQL UDF自定函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注册的UDF可用于DSL和SQL风格</span></span><br><span class="line"><span class="comment"># 返回值用于DSL风格，传参内的名称用于SQL风格</span></span><br><span class="line"><span class="comment"># arg1：注册的UDF名称，仅可用于SQL风格</span></span><br><span class="line"><span class="comment"># arg2：UDF处理逻辑，是一个单独的方法</span></span><br><span class="line"><span class="comment"># arg3：声明UDF的返回值类型，UDF注册时，必须声明返回值类型，并且UDF的真实返回值一定要和声明的返回值一致</span></span><br><span class="line"><span class="comment"># 返回值对象：是一个UDF对象，仅可用于DSL语法</span></span><br><span class="line"><span class="comment"># 这种方式定义的UDF，可以通过arg1的名称用于SQL风格，通过返回值对象用于DSL风格</span></span><br><span class="line">sparksession.udf.register(arg1, arg2, arg3)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 仅能用于DSL风格</span></span><br><span class="line">pyspark.sql.functions.udf</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> IntegerType</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).config(<span class="string">"spark.sql.shuffle.partitions"</span>, <span class="number">2</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]).map(<span class="keyword">lambda</span> x: [x])</span><br><span class="line">df = rdd.toDF([<span class="string">'num'</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># sparksession.udf.register()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">num_ride_10</span><span class="params">(num)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> num * <span class="number">10</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">udf1 = spark.udf.register(<span class="string">"udf1"</span>, num_ride_10, IntegerType())</span><br><span class="line"></span><br><span class="line"><span class="comment"># SQL风格使用</span></span><br><span class="line"><span class="comment"># selectExpr：以SELECT的表达式执行（SQL字符串）</span></span><br><span class="line"><span class="comment"># 普通select方法接受普通字符串字段名，或者返回值是Column对象的计算</span></span><br><span class="line">df.selectExpr(<span class="string">"udf1(num)"</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># DSL风格</span></span><br><span class="line"><span class="comment"># 返回值UDF对象如果作为方法使用，传入的参数一定是Column对象</span></span><br><span class="line">df.select(udf1(df[<span class="string">'num'</span>])).show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># pyspark.sql.functions.udf</span></span><br><span class="line">udf2 = F.udf(num_ride_10, IntegerType())</span><br><span class="line">df.select(udf2(df[<span class="string">'num'</span>])).show()</span><br><span class="line">df.selectExpr(<span class="string">"udf2(num)"</span>).show()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">+---------+</span><br><span class="line">|udf1(num)|</span><br><span class="line">+---------+</span><br><span class="line">|       10|</span><br><span class="line">|       20|</span><br><span class="line">|       30|</span><br><span class="line">|       40|</span><br><span class="line">|       50|</span><br><span class="line">|       60|</span><br><span class="line">|       70|</span><br><span class="line">|       80|</span><br><span class="line">|       90|</span><br><span class="line">|      100|</span><br><span class="line">+---------+</span><br><span class="line"></span><br><span class="line">+---------+</span><br><span class="line">|udf1(num)|</span><br><span class="line">+---------+</span><br><span class="line">|       10|</span><br><span class="line">|       20|</span><br><span class="line">|       30|</span><br><span class="line">|       40|</span><br><span class="line">|       50|</span><br><span class="line">|       60|</span><br><span class="line">|       70|</span><br><span class="line">|       80|</span><br><span class="line">|       90|</span><br><span class="line">|      100|</span><br><span class="line">+---------+</span><br><span class="line"></span><br><span class="line">+----------------+</span><br><span class="line">|num_ride_10(num)|</span><br><span class="line">+----------------+</span><br><span class="line">|              10|</span><br><span class="line">|              20|</span><br><span class="line">|              30|</span><br><span class="line">|              40|</span><br><span class="line">|              50|</span><br><span class="line">|              60|</span><br><span class="line">|              70|</span><br><span class="line">+----------------+</span><br></pre></td></tr></table></figure><h3 id="注册返回值为数组类型的UDF"><a href="#注册返回值为数组类型的UDF" class="headerlink" title="注册返回值为数组类型的UDF"></a>注册返回值为数组类型的UDF</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StringType, ArrayType</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 0. 构建执行环境入口对象SparkSession</span></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).config(<span class="string">"spark.sql.shuffle.partitions"</span>, <span class="number">2</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建一个RDD</span></span><br><span class="line">rdd = sc.parallelize([[<span class="string">"hadoop spark flink"</span>], [<span class="string">"hadoop flink java"</span>]])</span><br><span class="line">df = rdd.toDF([<span class="string">"line"</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注册UDF, UDF的执行函数定义</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_line</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> data.split(<span class="string">" "</span>)  <span class="comment"># 返回值是一个Array对象</span></span><br><span class="line"><span class="comment"># TODO1 方式1 构建UDF</span></span><br><span class="line">udf2 = spark.udf.register(<span class="string">"udf1"</span>, split_line, ArrayType(StringType()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># DLS风格</span></span><br><span class="line">df.select(udf2(df[<span class="string">'line'</span>])).show()</span><br><span class="line"><span class="comment"># SQL风格</span></span><br><span class="line">df.createTempView(<span class="string">"lines"</span>)</span><br><span class="line">spark.sql(<span class="string">"SELECT udf1(line) FROM lines"</span>).show(truncate=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TODO 2 方式2的形式构建UDF</span></span><br><span class="line">udf3 = F.udf(split_line, ArrayType(StringType()))</span><br><span class="line">df.select(udf3(df[<span class="string">'line'</span>])).show(truncate=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">+--------------------+</span><br><span class="line">|          udf1(line)|</span><br><span class="line">+--------------------+</span><br><span class="line">|[hadoop, spark, f...|</span><br><span class="line">|[hadoop, flink, j...|</span><br><span class="line">+--------------------+</span><br><span class="line"></span><br><span class="line">+----------------------+</span><br><span class="line">|udf1(line)            |</span><br><span class="line">+----------------------+</span><br><span class="line">|[hadoop, spark, flink]|</span><br><span class="line">|[hadoop, flink, java] |</span><br><span class="line">+----------------------+</span><br><span class="line"></span><br><span class="line">+----------------------+</span><br><span class="line">|split_line(line)      |</span><br><span class="line">+----------------------+</span><br><span class="line">|[hadoop, spark, flink]|</span><br><span class="line">|[hadoop, flink, java] |</span><br><span class="line">+----------------------+</span><br></pre></td></tr></table></figure><h3 id="注册返回值为字典类型的UDF"><a href="#注册返回值为字典类型的UDF" class="headerlink" title="注册返回值为字典类型的UDF"></a>注册返回值为字典类型的UDF</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType, IntegerType, ArrayType</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 0. 构建执行环境入口对象SparkSession</span></span><br><span class="line">spark = SparkSession.builder.master(<span class="string">"local[*]"</span>).config(<span class="string">"spark.sql.shuffle.partitions"</span>, <span class="number">2</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设 有三个数字  1 2 3  我们传入数字 ,返回数字所在序号对应的 字母 然后和数字结合形成dict返回</span></span><br><span class="line"><span class="comment"># 比如传入1 我们返回 &#123;"num":1, "letters": "a"&#125;</span></span><br><span class="line">rdd = sc.parallelize([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]])</span><br><span class="line">df = rdd.toDF([<span class="string">"num"</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注册UDF</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">"num"</span>: data, <span class="string">"letters"</span>: string.ascii_letters[data]&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">UDF的返回值是字典的话, 需要用StructType来接收</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">udf1 = spark.udf.register(<span class="string">"udf1"</span>, process, StructType().add(<span class="string">"num"</span>, IntegerType(), nullable=<span class="literal">True</span>).\</span><br><span class="line">                          add(<span class="string">"letters"</span>, StringType(), nullable=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">df.selectExpr(<span class="string">"udf1(num)"</span>).show(truncate=<span class="literal">False</span>)</span><br><span class="line">df.select(udf1(df[<span class="string">'num'</span>])).show(truncate=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">+---------+</span><br><span class="line">|udf1(num)|</span><br><span class="line">+---------+</span><br><span class="line">|&#123;1, b&#125;   |</span><br><span class="line">|&#123;2, c&#125;   |</span><br><span class="line">|&#123;3, d&#125;   |</span><br><span class="line">+---------+</span><br><span class="line"></span><br><span class="line">+---------+</span><br><span class="line">|udf1(num)|</span><br><span class="line">+---------+</span><br><span class="line">|&#123;1, b&#125;   |</span><br><span class="line">|&#123;2, c&#125;   |</span><br><span class="line">|&#123;3, d&#125;   |</span><br><span class="line">+---------+</span><br></pre></td></tr></table></figure><h3 id="通过RDD模拟UDAF"><a href="#通过RDD模拟UDAF" class="headerlink" title="通过RDD模拟UDAF"></a>通过RDD模拟UDAF</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="comment"># 0. 构建执行环境入口对象SparkSession</span></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"test"</span>).master(<span class="string">"local[*]"</span>).config(<span class="string">"spark.sql.shuffle.partitions"</span>, <span class="number">2</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], <span class="number">3</span>)</span><br><span class="line">df = rdd.map(<span class="keyword">lambda</span> x: [x]).toDF([<span class="string">'num'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 折中的方式 就是使用RDD的mapPartitions 算子来完成聚合操作</span></span><br><span class="line"><span class="comment"># 如果用mapPartitions API 完成UDAF聚合, 一定要单分区</span></span><br><span class="line">single_partition_rdd = df.rdd.repartition(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(iter)</span>:</span></span><br><span class="line">    sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> iter:</span><br><span class="line">        sum += row[<span class="string">'num'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> [sum]    <span class="comment"># 一定要嵌套list, 因为mapPartitions方法要求的返回值是list对象</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(single_partition_rdd.mapPartitions(process).collect())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[15]</span><br></pre></td></tr></table></figure><h2 id="SparkSQL窗口函数"><a href="#SparkSQL窗口函数" class="headerlink" title="SparkSQL窗口函数"></a>SparkSQL窗口函数</h2><h3 id="开窗函数"><a href="#开窗函数" class="headerlink" title="开窗函数"></a>开窗函数</h3><p>开窗函数的引入是为了既显示聚集前的数据又显示聚集后的数据，即在每一行的最后一列添加聚合函数的结果</p><p>开窗用于为行为定义一个窗口（运算将要操作的行为集合），对一组值进行操作，不需要使用<font color="orange">GROUP BY</font>字句对数据进行分组，能够在同一行中同时返回基础行的列和聚合列</p><h3 id="聚合函数和开窗函数"><a href="#聚合函数和开窗函数" class="headerlink" title="聚合函数和开窗函数"></a>聚合函数和开窗函数</h3><ul><li><p>聚合函数是将多行变为一行，count、avg…；如果要显示其他的列必须将列加入到<font color="orange">GROUP BY</font>中</p></li><li><p>开窗函数是将一行变成多行，可以不使用<font color="orange">GROUP BY</font>直接显示所有数据</p></li></ul><h3 id="开窗函数分类"><a href="#开窗函数分类" class="headerlink" title="开窗函数分类"></a>开窗函数分类</h3><ul><li>聚合开窗函数</li><li>排序开窗函数</li><li>分区类型NTILE的窗口函数</li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;SparkSQL&quot;&gt;&lt;a href=&quot;#SparkSQL&quot; class=&quot;headerlink&quot; title=&quot;SparkSQL&quot;&gt;&lt;/a&gt;SparkSQL&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;SparkSQL&lt;/strong&gt;是spark的一个用于处理海量&lt;strong&gt;结构化数据&lt;/strong&gt;的模块&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;支持SQL语言&lt;/li&gt;
&lt;li&gt;自动优化&lt;/li&gt;
&lt;li&gt;性能强&lt;/li&gt;
&lt;li&gt;兼容HIVE&lt;/li&gt;
&lt;li&gt;API流程简单&lt;/li&gt;
&lt;li&gt;支持标准化JDBC和ODBC连接&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;SparkSQL数据抽象&quot;&gt;&lt;a href=&quot;#SparkSQL数据抽象&quot; class=&quot;headerlink&quot; title=&quot;SparkSQL数据抽象&quot;&gt;&lt;/a&gt;SparkSQL数据抽象&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;font color=&quot;orange&quot;&gt;Pandas · DataFrame&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;font color=&quot;orange&quot;&gt;·&lt;/font&gt;&lt;/strong&gt; 二维表数据结构&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;font color=&quot;orange&quot;&gt;·&lt;/font&gt;&lt;/strong&gt; 单机（本地）集合&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;font color=&quot;orange&quot;&gt;SparkCore · RDD&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;font color=&quot;orange&quot;&gt;·&lt;/font&gt;&lt;/strong&gt; 无标准数据结构&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;font color=&quot;orange&quot;&gt;·&lt;/font&gt;&lt;/strong&gt; 分布式（分区）集合&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;font color=&quot;orange&quot;&gt;SparkSQL · DataFrame&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;font color=&quot;orange&quot;&gt;·&lt;/font&gt;&lt;/strong&gt; 二维表数据结构&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;font color=&quot;orange&quot;&gt;·&lt;/font&gt;&lt;/strong&gt; 分布式（分区）集合&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;SparkSession对象&quot;&gt;&lt;a href=&quot;#SparkSession对象&quot; class=&quot;headerlink&quot; title=&quot;SparkSession对象&quot;&gt;&lt;/a&gt;SparkSession对象&lt;/h3&gt;&lt;p&gt;RDD程序的执行入口对象：&lt;strong&gt;SparkContext&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在Spark2.0以后，推出了&lt;strong&gt;SparkSession&lt;/strong&gt;对象，来作为Spark编码的统一入口对象。&lt;strong&gt;SparkSession&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;用于SparkSQL编程，作为入口对象&lt;/li&gt;
&lt;li&gt;用于SparkCore编程，通过SparkSession对象获取SparkContext&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;构建SparkSession对象：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; pyspark.sql &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; SparkSession&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 通过builder方法来构建SparkSession对象&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# appName：设置程序名称&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# config：配置常用属性&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# getOrCreate：完成创建SparkSession对象&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;spark = SparkSession.builder.appName(&lt;span class=&quot;string&quot;&gt;&quot;test&quot;&lt;/span&gt;).master(&lt;span class=&quot;string&quot;&gt;&quot;local[*]&quot;&lt;/span&gt;).config(&lt;span class=&quot;string&quot;&gt;&quot;spark.sql.shuffle.partitions&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;4&quot;&lt;/span&gt;).getOrCreate()&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;通过SparkSesion对象获取SparkContext对象：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; pyspark.sql &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; SparkSession&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;spark = SparkSession.builder.appName(&lt;span class=&quot;string&quot;&gt;&quot;test&quot;&lt;/span&gt;).master(&lt;span class=&quot;string&quot;&gt;&quot;local[*]&quot;&lt;/span&gt;).config(&lt;span class=&quot;string&quot;&gt;&quot;spark.sql.shuffle.partitions&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;4&quot;&lt;/span&gt;).getOrCreate()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sc = spark.sparkContext&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
    
    <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark-Core</title>
    <link href="http://yoursite.com/2022/12/07/Spark-Core/"/>
    <id>http://yoursite.com/2022/12/07/Spark-Core/</id>
    <published>2022-12-07T08:58:49.000Z</published>
    <updated>2022-12-08T09:05:29.171Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><h2 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h2><p>本地对象被发送到同个Executor内每个分区的处理线程上使用，这样每个分区实际上存放了重复的数据。而Executor本质上是进程，进程内资源共享，没必要将本地对象分发给所有分区，造成内存浪费</p><p><strong>解决方案</strong>：将本地对象设置为广播变量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.将本地对象标记为广播变量</span></span><br><span class="line">broadcast = sc.broadcast(var)</span><br><span class="line"><span class="comment"># 2.使用广播变量，从broadcast对象中取出本地对象</span></span><br><span class="line">value = broadcast.value</span><br><span class="line"><span class="comment"># 当传输的是广播对象时，spark会只给每个Executor分发一份数据</span></span><br></pre></td></tr></table></figure><p>当本地集合对象和分布式集合对象（RDD）进行关联时，需要将本地集合对象封装为广播变量</p><ul><li>节省网络IO次数</li><li>降低Executor内存占用</li></ul><h2 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h2><p>当执行累加操作时，各个分区累加自身的内容</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># spark提供累加器变量，参数是初始值</span></span><br><span class="line">acmlt = sc.accumulator(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">acmlt = sc.accumulator(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">counts</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> acmlt</span><br><span class="line">    acmlt += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">rdd1 = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], <span class="number">3</span>)</span><br><span class="line">rdd2 = rdd1.map(counts)</span><br><span class="line">print(rdd2.collect())</span><br><span class="line">print(acmlt)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[1, 2, 3]</span><br><span class="line">3</span><br></pre></td></tr></table></figure><p><font color="gold">注</font>：累加器可能因血缘关系导致重复的累加，例如一个RDD被释放后累加已经完成，此时再使用该RDD将会导致重复累加。可通过cache缓存机制来解决</p><a id="more"></a> <h2 id="DAG"><a href="#DAG" class="headerlink" title="DAG"></a>DAG</h2><p>Spark的核心是根据RDD来实现的，Spark Scheduler（spark任务调度）是spark核心实现的重要一环，其功能是组织处理RDD中每个分区的数据，根据RDD的依赖关系构建DAG有向无环图，再基于DAG划分Stage，将每个Stage中的任务发送到指定节点运行，合理规划资源的利用</p><h3 id="DAG标准定义"><a href="#DAG标准定义" class="headerlink" title="DAG标准定义"></a>DAG标准定义</h3><p>有向无环图：有方向而没有形成闭环的执行流程图</p><ul><li>有向：具有执行方向</li><li>无环：没有闭环</li></ul><h3 id="Action算子和Job"><a href="#Action算子和Job" class="headerlink" title="Action算子和Job"></a>Action算子和Job</h3><p>一个Action会产生一个DAG，如果代码中存在3个Action则会产生3个DAG；</p><p>每个DAG在应用程序运行时产生一个Job（应用程序内的子任务）</p><p><strong><font color="orange">1个Action = 1个DAG = 1个Job</font></strong></p><p>这样的代码运行起来在spark中被称为<strong><font color="orange">Application</font></strong></p><h3 id="DAG和分区"><a href="#DAG和分区" class="headerlink" title="DAG和分区"></a>DAG和分区</h3><p>DAG的最终作用是为了构建spark详细执行的物理计划，由于spark是分布式多分区的，所以DAG和分区间也具有关联</p><h3 id="DAG的宽窄依赖和阶段划分"><a href="#DAG的宽窄依赖和阶段划分" class="headerlink" title="DAG的宽窄依赖和阶段划分"></a>DAG的宽窄依赖和阶段划分</h3><p>在Spark RDD前后之间的血缘关系，分为：</p><ul><li>窄依赖：父RDD的一个分区，将全部数据发送给子RDD的一个分区；</li><li>宽依赖：父RDD的一个分区，将数据发送给子RDD的多个分区，别名：shuffle</li></ul><p>对于spark，会根据DAG，按照宽依赖划分不同的DAG阶段。划分依据：从后向前，每遇到宽依赖就划分出一个阶段，称之为stage。在stage内部，一定是窄依赖</p><p><img src="/2022/12/07/Spark-Core/A.png" alt></p><h2 id="Spark的内存迭代计算"><a href="#Spark的内存迭代计算" class="headerlink" title="Spark的内存迭代计算"></a>Spark的内存迭代计算</h2><p>窄依赖同一线程内走管道交互，进入宽依赖走网络IO交互</p><p>Spark默认收到全局并行度的限制，除了个别算子有特殊分区的情况，大部分算子都会遵循全局并行度的要求来划分自己的分区数。例如全局并行度是3，大部分算子的默认分区都是3-&gt;不建议再独立通过arg来指定分区数</p><h2 id="Spark并行度"><a href="#Spark并行度" class="headerlink" title="Spark并行度"></a>Spark并行度</h2><p>Spark的并行：在同一时间内，有多少task在同时运行</p><p>Spark的并行度：并行能力，当设置为6，即共有6个task在并行运行，RDD的分区被规划为6个分区</p><p>Spark并行度设置（优先级由高到低）：</p><ul><li>代码</li><li>客户端参数</li><li>配置文件</li><li>默认值（1），并不会全部以1来运行，多数情况下基于读取文件的分片数量来作为默认并行度</li></ul><p>全局并行度配置参数：</p><p><code>spark.default.parallelism</code></p><ul><li>代码中设置：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conf = SparkConf()</span><br><span class="line">conf.set(<span class="string">"spark.default.parallelism"</span>, <span class="string">"100"</span>)</span><br></pre></td></tr></table></figure><ul><li><p>客户端提交参数中设置<code>bin/spark-submit --conf &quot;spark.default.parallelism=100&quot;</code></p></li><li><p>配置文件<font color="#008080">conf/spark-defaults.conf</font>中设置<code>spark.default.parallelism 100</code></p></li></ul><p><font color="gold">注</font>：全局并行度是推荐设置，不要针对RDD更改分区，可能会影响内存迭代管道的构建，或者产生额外的shuffle</p><p>针对RDD并行度的设置（不推荐）：</p><p><strong><font color="orange">·</font></strong> repartition算子</p><p><strong><font color="orange">·</font></strong> coalesce算子</p><p><strong><font color="orange">·</font></strong> partitionBy算子</p><h3 id="规划Spark集群并行度"><a href="#规划Spark集群并行度" class="headerlink" title="规划Spark集群并行度"></a>规划Spark集群并行度</h3><p>设置为CPU总核心的<strong><font color="orange">2~10倍</font></strong>（或更高）* </p><p>比如集群可用的CPU核心数量为100个，建议并行度200~1000（确保是CPU核心的整数倍）</p><ul><li><p>设置为最少2倍：</p><p>CPU的一个核心同一时间只能做一件事，当拥有100个核心的情况下，设置100并行度，能利用全部的CPU，但task的压力不均衡，一旦某个task先执行完毕，会导致某个CPU核心的空闲。所以将task并行分配的数量增多，例如设置1000并行度，同一时间内有100个task在运行，900个在等待，但可以确保某个task运行完毕后会不断有task补上，不让CPU处于空闲状态，最大程度利用集群的资源</p></li></ul><h2 id="Spark任务调度"><a href="#Spark任务调度" class="headerlink" title="Spark任务调度"></a>Spark任务调度</h2><h3 id="Spark任务由Driver进行调度"><a href="#Spark任务由Driver进行调度" class="headerlink" title="Spark任务由Driver进行调度"></a>Spark任务由Driver进行调度</h3><p>包括：</p><ul><li>逻辑DAG产生</li><li>分区DAG产生</li><li>基于分区DAG构建线程task并划分</li><li>将task分配给Executor并监控其工作</li></ul><h3 id="Spark程序调度流程"><a href="#Spark程序调度流程" class="headerlink" title="Spark程序调度流程"></a>Spark程序调度流程</h3><ul><li>构建Driver（<font color="orange">Driver</font>）</li><li>构建SparkContext执行环境入口对象（<font color="orange">Driver</font>）</li><li>基于DAG scheduler调度器构建逻辑task分配（<font color="orange">Driver</font>）</li><li>基于task scheduler调度器将逻辑task分配到各个Executor上执行并监控（<font color="orange">Driver</font>）</li><li>Worker（Executor）被task scheduler管理监控，遵从指令干活并汇报进度（<font color="orange">Worker</font>）</li></ul><h3 id="Driver内部组件"><a href="#Driver内部组件" class="headerlink" title="Driver内部组件"></a>Driver内部组件</h3><ul><li><p><strong>DAG调度器</strong></p><p>将逻辑DAG进行处理，最终得到逻辑上的task划分</p></li><li><p><strong>Task调度器</strong></p><p>基于DAG调度器的产出，来规划这些逻辑的task应该在哪些物理的Executor上运行，以及监控它们</p></li></ul><h3 id="层级关系梳理"><a href="#层级关系梳理" class="headerlink" title="层级关系梳理"></a>层级关系梳理</h3><ul><li>1个spark环境可运行多个Application；</li><li>1个代码成功运行生成一个Application；</li><li>1个Application内部有多个Job；</li><li>1个Action算子产生1个Job，每个Job有自己的DAG执行图；</li><li>1个Job的DAG基于宽窄依赖划分不同的阶段；</li><li>1个阶段里基于分区数量形成多个并行的内存迭代管道；</li><li>1个内存迭代管道形成1个task（DAG调度器划分将Job内划分出具体的task任务，1个Job被划分出的task在逻辑上被称为这个job的taskset）</li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;广播变量&quot;&gt;&lt;a href=&quot;#广播变量&quot; class=&quot;headerlink&quot; title=&quot;广播变量&quot;&gt;&lt;/a&gt;广播变量&lt;/h2&gt;&lt;p&gt;本地对象被发送到同个Executor内每个分区的处理线程上使用，这样每个分区实际上存放了重复的数据。而Executor本质上是进程，进程内资源共享，没必要将本地对象分发给所有分区，造成内存浪费&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;解决方案&lt;/strong&gt;：将本地对象设置为广播变量&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 1.将本地对象标记为广播变量&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;broadcast = sc.broadcast(var)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 2.使用广播变量，从broadcast对象中取出本地对象&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;value = broadcast.value&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 当传输的是广播对象时，spark会只给每个Executor分发一份数据&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;当本地集合对象和分布式集合对象（RDD）进行关联时，需要将本地集合对象封装为广播变量&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;节省网络IO次数&lt;/li&gt;
&lt;li&gt;降低Executor内存占用&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;累加器&quot;&gt;&lt;a href=&quot;#累加器&quot; class=&quot;headerlink&quot; title=&quot;累加器&quot;&gt;&lt;/a&gt;累加器&lt;/h2&gt;&lt;p&gt;当执行累加操作时，各个分区累加自身的内容&lt;/p&gt;
 &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# spark提供累加器变量，参数是初始值&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;acmlt = sc.accumulator(&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;e.g.&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; pyspark &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; SparkConf, SparkContext&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;conf = SparkConf().setAppName(&lt;span class=&quot;string&quot;&gt;&quot;test&quot;&lt;/span&gt;).setMaster(&lt;span class=&quot;string&quot;&gt;&quot;local[*]&quot;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sc = SparkContext(conf=conf)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;acmlt = sc.accumulator(&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;counts&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(data)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;global&lt;/span&gt; acmlt&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    acmlt += &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; data&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;rdd1 = sc.parallelize([&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;], &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;rdd2 = rdd1.map(counts)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(rdd2.collect())&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(acmlt)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[1, 2, 3]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;font color=&quot;gold&quot;&gt;注&lt;/font&gt;：累加器可能因血缘关系导致重复的累加，例如一个RDD被释放后累加已经完成，此时再使用该RDD将会导致重复累加。可通过cache缓存机制来解决&lt;/p&gt;</summary>
    
    
    
    <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
    
    <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Pyspark-RDD</title>
    <link href="http://yoursite.com/2022/12/07/Pyspark-RDD/"/>
    <id>http://yoursite.com/2022/12/07/Pyspark-RDD/</id>
    <published>2022-12-07T05:45:49.000Z</published>
    <updated>2022-12-08T06:09:52.881Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><p>RDD（Resilient Distributed Dataset）弹性分布式数据集，是spark中最基本的数据抽象，代表一个不可变、可分区、其中元素可并行计算的集合</p><ul><li>Resilient：RDD中的数据可存储再内存或磁盘中</li><li>Distributed：分布式存储数据（跨机器/跨进程），用于分布式计算</li><li>Dataset：一个用于存放数据的数据集合</li></ul><h3 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h3><h4 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h4><p>RDD分区是RDD数据存储的最小单位，一份RDD数据本质上分隔成了多个分区</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 存储9个数字，设立三个分区</span></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], <span class="number">3</span>)</span><br><span class="line">rdd.glom().collect()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[1,2,3],[4,5,6],[7,8,9]]</span><br></pre></td></tr></table></figure><h4 id="RDD方法会作用在其所有方法上"><a href="#RDD方法会作用在其所有方法上" class="headerlink" title="RDD方法会作用在其所有方法上"></a>RDD方法会作用在其所有方法上</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.map(<span class="keyword">lambda</span> x: x * <span class="number">10</span>).collect()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[10,20,30,40,50,60,70,80,90]</span><br></pre></td></tr></table></figure><h4 id="RDD之间具有依赖关系"><a href="#RDD之间具有依赖关系" class="headerlink" title="RDD之间具有依赖关系"></a>RDD之间具有依赖关系</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line">rdd1 = sc.textFile(<span class="string">"../test.text"</span>)</span><br><span class="line">rdd2 = rdd1.flatMap(<span class="keyword">lambda</span> x: x.split(<span class="string">' '</span>))</span><br><span class="line">rdd3 = rdd2.map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))</span><br><span class="line">rdd4 = rdd3.reduceByKey(<span class="keyword">lambda</span> a, b: a+b)</span><br><span class="line">print(rdd4.collect())</span><br></pre></td></tr></table></figure><h4 id="Key-Value型RDD可以有分区器"><a href="#Key-Value型RDD可以有分区器" class="headerlink" title="Key-Value型RDD可以有分区器"></a>Key-Value型RDD可以有分区器</h4><p>KV型RDD：RDD内存储的数据是只有两个元素的二元元组</p><p>默认分区器：Hash分区规则，也可手动设置分区器：rdd.partitionBy()方法</p><p><font color="gold">注</font>：不是所有RDD都是KV型</p><h4 id="RDD的分区规划：会尽量靠近数据所在的服务器"><a href="#RDD的分区规划：会尽量靠近数据所在的服务器" class="headerlink" title="RDD的分区规划：会尽量靠近数据所在的服务器"></a>RDD的分区规划：会尽量靠近数据所在的服务器</h4><p>在初始RDD读取数据规划阶段，分区会尽量规划到存储数据所在服务器，直接读取本地数据，避免从网络读取数据</p><p>Spark会在确保并行计算能力的前提下，尽量确保本地读取</p><h3 id="RDD创建"><a href="#RDD创建" class="headerlink" title="RDD创建"></a>RDD创建</h3><ul><li>通过并行化集合创建（本地对象转化为分布式RDD）</li><li>读取外部数据源（读文件）</li></ul><h4 id="并行化创建"><a href="#并行化创建" class="headerlink" title="并行化创建"></a>并行化创建</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># arg1: 集合对象，如：list</span></span><br><span class="line"><span class="comment"># arg2：可选，指定分区数量</span></span><br><span class="line">rdd = SparkContext.parallelize(arg1, arg2)</span><br></pre></td></tr></table></figure><h4 id="读取文件"><a href="#读取文件" class="headerlink" title="读取文件"></a>读取文件</h4><p>通过textFile API来读取本地或者hdfs的数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># arg1: 文件路径</span></span><br><span class="line"><span class="comment"># arg2：可选，最小分区数量</span></span><br><span class="line"><span class="comment"># 当arg2超出spark允许范围，参数失效</span></span><br><span class="line">SparkContext.textFile(arg1, arg2)</span><br></pre></td></tr></table></figure><p>通过wholeTextFile API来读取小文件，这个api偏向于少量分区读取数据，是pyspark基于小文件的优化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># arg1：文件路径</span></span><br><span class="line"><span class="comment"># arg2：可选，最小分区数量</span></span><br><span class="line"><span class="comment"># 当arg2超出spark允许范围，参数失效</span></span><br><span class="line">SparkContext.wholeTextFiles(arg1, arg2)</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="RDD算子"><a href="#RDD算子" class="headerlink" title="RDD算子"></a>RDD算子</h3><p>方法、函数：本地对象的API</p><p>算子：分布式集合对象的API</p><p>RDD的算子分为两类：</p><ul><li><p>Transformation：转换算子</p><p>返回值仍旧是RDD的算子，构建执行计划</p></li><li><p>Action：行动算子</p><p>返回值不再是RDD，使执行计划开始工作</p></li></ul><h3 id="Transformation算子"><a href="#Transformation算子" class="headerlink" title="Transformation算子"></a>Transformation算子</h3><h4 id="map"><a href="#map" class="headerlink" title="map"></a>map</h4><p>将RDD的数据一条条处理（处理逻辑基于map算子接收的处理函数），返回新的RDD</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.map(func)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"TEST"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> data * <span class="number">10</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(rdd.map(add).collect())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[10, 20, 30, 40, 50, 60]</span><br></pre></td></tr></table></figure><h4 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h4><p>对RDD执行map操作，接着进行解除嵌套：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 嵌套</span></span><br><span class="line">lst = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]</span><br><span class="line"><span class="comment"># 解除嵌套</span></span><br><span class="line">lst = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([<span class="string">"hadoop spark hadoop"</span>, <span class="string">"spark hadoop hadoop"</span>, <span class="string">"hadoop flink spark"</span>])</span><br><span class="line"><span class="comment"># 得到所有的单词, 组成RDD, flatMap的传入参数 和map一致, 就是给map逻辑用的, 解除嵌套无需逻辑(传参)</span></span><br><span class="line">rdd2 = rdd.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">" "</span>))</span><br><span class="line">print(rdd2.collect())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&apos;hadoop&apos;, &apos;spark&apos;, &apos;hadoop&apos;, &apos;spark&apos;, &apos;hadoop&apos;, &apos;hadoop&apos;, &apos;hadoop&apos;, &apos;flink&apos;, &apos;spark&apos;]</span><br></pre></td></tr></table></figure><h4 id="reduceByKey"><a href="#reduceByKey" class="headerlink" title="reduceByKey"></a>reduceByKey</h4><p>针对KV型RDD自动按照key进行分组，然后根据提供的聚合逻辑完成组内数据（value）聚合操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 接受两个类型一致的传入参数，返回聚合值</span></span><br><span class="line">rdd.reduceByKey(func)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([(<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'b'</span>, <span class="number">1</span>), (<span class="string">'b'</span>, <span class="number">1</span>), (<span class="string">'a'</span>, <span class="number">1</span>)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># reduceByKey 对相同key 的数据执行聚合相加</span></span><br><span class="line">print(rdd.reduceByKey(<span class="keyword">lambda</span> a, b: a + b).collect())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(&apos;b&apos;, 2), (&apos;a&apos;, 3)]</span><br></pre></td></tr></table></figure><h4 id="mapValues"><a href="#mapValues" class="headerlink" title="mapValues"></a>mapValues</h4><p>针对二元元组RDD，对其内部的二元元组value值进行map</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 传入二元元组的value值，func只对value进行处理</span></span><br><span class="line">rdd.mapValues(func)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([(<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'b'</span>, <span class="number">1</span>), (<span class="string">'b'</span>, <span class="number">1</span>), (<span class="string">'a'</span>, <span class="number">1</span>)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># reduceByKey 对相同key 的数据执行聚合相加</span></span><br><span class="line">print(rdd.mapValues(<span class="keyword">lambda</span> x: x * <span class="number">10</span>).collect())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(&apos;a&apos;, 10), (&apos;a&apos;, 10), (&apos;b&apos;, 10), (&apos;b&apos;, 10), (&apos;a&apos;, 10)]</span><br></pre></td></tr></table></figure><h4 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h4><p>将RDD的数据进行分组</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># func要求传入一个参数，返回一个值，类型不做要求。相同的返回值将被放入同一个组中。</span></span><br><span class="line"><span class="comment"># 分组完成后，每一个组是一个二元元组，key就是返回值，所有同组数据放入一个迭代器对象中作为value</span></span><br><span class="line">rdd.groupBy(func)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([(<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'b'</span>, <span class="number">1</span>), (<span class="string">'b'</span>, <span class="number">2</span>), (<span class="string">'b'</span>, <span class="number">3</span>)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过groupBy对数据进行分组</span></span><br><span class="line"><span class="comment"># groupBy传入的函数的 意思是: 通过这个函数, 确定按照谁来分组(返回谁即可)</span></span><br><span class="line"><span class="comment"># 分组规则 和SQL是一致的, 也就是相同的在一个组(Hash分组)</span></span><br><span class="line">result = rdd.groupBy(<span class="keyword">lambda</span> t: t[<span class="number">0</span>])</span><br><span class="line">print(result.map(<span class="keyword">lambda</span> t:(t[<span class="number">0</span>], list(t[<span class="number">1</span>]))).collect())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(&apos;b&apos;, [(&apos;b&apos;, 1), (&apos;b&apos;, 2), (&apos;b&apos;, 3)]), (&apos;a&apos;, [(&apos;a&apos;, 1), (&apos;a&apos;, 1)])]</span><br></pre></td></tr></table></figure><h4 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h4><p>过滤数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># func返回值为True的参数保留，False丢弃</span></span><br><span class="line">rdd.filter(func)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过Filter算子, 过滤奇数</span></span><br><span class="line">result = rdd.filter(<span class="keyword">lambda</span> x: x % <span class="number">2</span> == <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(result.collect())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1, 3, 5]</span><br></pre></td></tr></table></figure><h4 id="distinct"><a href="#distinct" class="headerlink" title="distinct"></a>distinct</h4><p>对RDD数据进行去重</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># arg：去重分区数量，一般省略</span></span><br><span class="line">rdd.distinct(arg)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># distinct 进行RDD数据去重操作</span></span><br><span class="line">print(rdd.distinct().collect())</span><br><span class="line"></span><br><span class="line">rdd2 = sc.parallelize([(<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'a'</span>, <span class="number">3</span>)])</span><br><span class="line">print(rdd2.distinct().collect())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[2, 1, 3]</span><br><span class="line">[(&apos;a&apos;, 1), (&apos;a&apos;, 3)]</span><br></pre></td></tr></table></figure><h4 id="union"><a href="#union" class="headerlink" title="union"></a>union</h4><p>将两个RDD合并成一个RDD返回</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 仅合并，不会去重</span></span><br><span class="line"><span class="comment"># 可以合并不同类型的RDD</span></span><br><span class="line">rdd.union(other_rdd)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd1 = sc.parallelize([<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">rdd2 = sc.parallelize([<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"a"</span>])</span><br><span class="line"></span><br><span class="line">rdd3 = rdd1.union(rdd2)</span><br><span class="line">print(rdd3.collect())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1, 1, 3, 3, &apos;a&apos;, &apos;b&apos;, &apos;a&apos;]</span><br></pre></td></tr></table></figure><h4 id="join"><a href="#join" class="headerlink" title="join"></a>join</h4><p>对两个RDD执行JOIN操作（可实现SQL的内/外连接）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># jion算子只能用于二元元组</span></span><br><span class="line">rdd.join(other_rdd)            <span class="comment"># 内连接</span></span><br><span class="line">rdd.leftOuterJoin(other_rdd)   <span class="comment"># 左外连接</span></span><br><span class="line">rdd.rightOuterJoin(other_rdd)  <span class="comment"># 右外连接</span></span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd1 = sc.parallelize([ (<span class="number">1001</span>, <span class="string">"zhangsan"</span>), (<span class="number">1002</span>, <span class="string">"lisi"</span>), (<span class="number">1003</span>, <span class="string">"wangwu"</span>), (<span class="number">1004</span>, <span class="string">"zhaoliu"</span>) ])</span><br><span class="line">rdd2 = sc.parallelize([ (<span class="number">1001</span>, <span class="string">"销售部"</span>), (<span class="number">1002</span>, <span class="string">"科技部"</span>)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过join算子来进行rdd之间的关联</span></span><br><span class="line"><span class="comment"># 对于join算子来说 关联条件 按照二元元组的key来进行关联</span></span><br><span class="line">print(rdd1.join(rdd2).collect())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 左外连接, 右外连接 可以更换一下rdd的顺序 或者调用rightOuterJoin即可</span></span><br><span class="line">print(rdd1.leftOuterJoin(rdd2).collect())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[(1001, (&apos;zhangsan&apos;, &apos;销售部&apos;)), (1002, (&apos;lisi&apos;, &apos;科技部&apos;))]</span><br><span class="line">[(1004, (&apos;zhaoliu&apos;, None)), (1001, (&apos;zhangsan&apos;, &apos;销售部&apos;)), (1002, (&apos;lisi&apos;, &apos;科技部&apos;)), (1003, (&apos;wangwu&apos;, None))]</span><br></pre></td></tr></table></figure><h4 id="intersection"><a href="#intersection" class="headerlink" title="intersection"></a>intersection</h4><p>求两个RDD的交集，返回一个新RDD</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.intersection(other_rdd)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd1 = sc.parallelize([(<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'a'</span>, <span class="number">3</span>)])</span><br><span class="line">rdd2 = sc.parallelize([(<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'b'</span>, <span class="number">3</span>)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过intersection算子求RDD之间的交集, 将交集取出 返回新RDD</span></span><br><span class="line">rdd3 = rdd1.intersection(rdd2)</span><br><span class="line"></span><br><span class="line">print(rdd3.collect())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(&apos;a&apos;, 1)]</span><br></pre></td></tr></table></figure><h4 id="glom"><a href="#glom" class="headerlink" title="glom"></a>glom</h4><p>将RDD的数据按照分区加上嵌套</p><p>例如RDD数据[1,2,3,4,5]有两个分区，经过glom处理后变成：[[1,2,3]],[4,5]]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.glom()</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">print(rdd.glom().collect())</span><br><span class="line">print(rdd.glom().flatMap(<span class="keyword">lambda</span> x: x).collect()) <span class="comment"># tips：解嵌套</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[1, 2, 3, 4], [5, 6, 7, 8, 9]]</span><br><span class="line">[1, 2, 3, 4, 5, 6, 7, 8, 9]</span><br></pre></td></tr></table></figure><h4 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey"></a>groupByKey</h4><p>针对KV型RDD自动按key分组</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.groupByKey()</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([(<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'b'</span>, <span class="number">1</span>), (<span class="string">'b'</span>, <span class="number">1</span>), (<span class="string">'b'</span>, <span class="number">1</span>)])</span><br><span class="line"></span><br><span class="line">rdd2 = rdd.groupByKey()</span><br><span class="line"></span><br><span class="line">print(rdd2.map(<span class="keyword">lambda</span> x: (x[<span class="number">0</span>], list(x[<span class="number">1</span>]))).collect())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(&apos;b&apos;, [1, 1, 1]), (&apos;a&apos;, [1, 1])]</span><br></pre></td></tr></table></figure><h4 id="sortBy"><a href="#sortBy" class="headerlink" title="sortBy"></a>sortBy</h4><p>基于指定的排序函数对RDD数据进行排序</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ascending：True-升序，False-降序</span></span><br><span class="line"><span class="comment"># numPartitions：用于排序的分区数量，要进行全局排序，设置为1</span></span><br><span class="line">rdd.sortBy(func, ascending=<span class="literal">False</span>, numPartitions=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([(<span class="string">'c'</span>, <span class="number">3</span>), (<span class="string">'f'</span>, <span class="number">1</span>), (<span class="string">'b'</span>, <span class="number">11</span>), (<span class="string">'c'</span>, <span class="number">3</span>), (<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'c'</span>, <span class="number">5</span>), (<span class="string">'e'</span>, <span class="number">1</span>), (<span class="string">'n'</span>, <span class="number">9</span>), (<span class="string">'a'</span>, <span class="number">1</span>)], <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用sortBy对rdd执行排序</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 按照value 数字进行排序</span></span><br><span class="line"><span class="comment"># 参数1函数，告知Spark 按照数据的哪个列进行排序</span></span><br><span class="line"><span class="comment"># 参数2: True表示升序 False表示降序</span></span><br><span class="line"><span class="comment"># 参数3: 排序的分区数</span></span><br><span class="line"><span class="string">"""注意: 如果要全局有序, 排序分区数需设置为1"""</span></span><br><span class="line">print(rdd.sortBy(<span class="keyword">lambda</span> x: x[<span class="number">1</span>], ascending=<span class="literal">True</span>, numPartitions=<span class="number">1</span>).collect())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按照key来进行排序</span></span><br><span class="line">print(rdd.sortBy(<span class="keyword">lambda</span> x: x[<span class="number">0</span>], ascending=<span class="literal">False</span>, numPartitions=<span class="number">1</span>).collect())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[(&apos;f&apos;, 1), (&apos;a&apos;, 1), (&apos;e&apos;, 1), (&apos;a&apos;, 1), (&apos;c&apos;, 3), (&apos;c&apos;, 3), (&apos;c&apos;, 5), (&apos;n&apos;, 9), (&apos;b&apos;, 11)]</span><br><span class="line">[(&apos;n&apos;, 9), (&apos;f&apos;, 1), (&apos;e&apos;, 1), (&apos;c&apos;, 3), (&apos;c&apos;, 3), (&apos;c&apos;, 5), (&apos;b&apos;, 11), (&apos;a&apos;, 1), (&apos;a&apos;, 1)]</span><br></pre></td></tr></table></figure><h4 id="sortByKey"><a href="#sortByKey" class="headerlink" title="sortByKey"></a>sortByKey</h4><p>针对KV型RDD按照key进行排序</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ascending：True-升序，False-降序</span></span><br><span class="line"><span class="comment"># numPartitions：用于排序的分区数量，要进行全局排序，设置为1</span></span><br><span class="line"><span class="comment"># keyfunc：在排序前对key进行的处理</span></span><br><span class="line">rdd.sortByKey(ascending=<span class="literal">True</span>, numPartitions=<span class="literal">None</span>, keyfunc=&lt;function RDD.&lt;<span class="keyword">lambda</span>&gt;&gt;)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([(<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'E'</span>, <span class="number">1</span>), (<span class="string">'C'</span>, <span class="number">1</span>), (<span class="string">'D'</span>, <span class="number">1</span>), (<span class="string">'b'</span>, <span class="number">1</span>), (<span class="string">'g'</span>, <span class="number">1</span>), (<span class="string">'f'</span>, <span class="number">1</span>),</span><br><span class="line">                      (<span class="string">'y'</span>, <span class="number">1</span>), (<span class="string">'u'</span>, <span class="number">1</span>), (<span class="string">'i'</span>, <span class="number">1</span>), (<span class="string">'o'</span>, <span class="number">1</span>), (<span class="string">'p'</span>, <span class="number">1</span>),</span><br><span class="line">                      (<span class="string">'m'</span>, <span class="number">1</span>), (<span class="string">'n'</span>, <span class="number">1</span>), (<span class="string">'j'</span>, <span class="number">1</span>), (<span class="string">'k'</span>, <span class="number">1</span>), (<span class="string">'l'</span>, <span class="number">1</span>)], <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">print(rdd.sortByKey(ascending=<span class="literal">True</span>, numPartitions=<span class="number">1</span>, keyfunc=<span class="keyword">lambda</span> key: str(key).lower()).collect())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(&apos;a&apos;, 1), (&apos;b&apos;, 1), (&apos;C&apos;, 1), (&apos;D&apos;, 1), (&apos;E&apos;, 1), (&apos;f&apos;, 1), (&apos;g&apos;, 1), (&apos;i&apos;, 1), (&apos;j&apos;, 1), (&apos;k&apos;, 1), (&apos;l&apos;, 1), (&apos;m&apos;, 1), (&apos;n&apos;, 1), (&apos;o&apos;, 1), (&apos;p&apos;, 1), (&apos;u&apos;, 1), (&apos;y&apos;, 1)]</span><br></pre></td></tr></table></figure><h4 id="repartition-amp-coalesce"><a href="#repartition-amp-coalesce" class="headerlink" title="repartition &amp; coalesce"></a>repartition &amp; coalesce</h4><p>对RDD的分区执行重新分区（仅数量）</p><p><font color="gold">注</font>：尽量避免使用，影响并行计算性能。在合并到1个分区进行全局排序等场景下使用，尽可能避免增加分区，可能破坏内存迭代的计算管道</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># n：决定新的分区数量</span></span><br><span class="line"><span class="comment"># coalesce中增加分区必须指定shuffle=True</span></span><br><span class="line">rdd.repartition(n)</span><br><span class="line">rdd.coalesce(n, shuffle)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># repartition 修改分区</span></span><br><span class="line">print(rdd.repartition(<span class="number">1</span>).getNumPartitions())</span><br><span class="line"></span><br><span class="line">print(rdd.repartition(<span class="number">5</span>).getNumPartitions())</span><br><span class="line"></span><br><span class="line"><span class="comment"># coalesce 修改分区</span></span><br><span class="line">print(rdd.coalesce(<span class="number">1</span>).getNumPartitions())</span><br><span class="line"></span><br><span class="line">print(rdd.coalesce(<span class="number">5</span>, shuffle=<span class="literal">True</span>).getNumPartitions())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">5</span><br><span class="line">1</span><br><span class="line">5</span><br></pre></td></tr></table></figure><h3 id="Action算子"><a href="#Action算子" class="headerlink" title="Action算子"></a>Action算子</h3><h4 id="countByKey"><a href="#countByKey" class="headerlink" title="countByKey"></a>countByKey</h4><p>用于统计key出现的次数（一般适用于KV型RDD）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.countByKey()</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.textFile(<span class="string">"hdfs://master:8020/input/words.txt"</span>)</span><br><span class="line">rdd2 = rdd.flatMap(<span class="keyword">lambda</span> x: x.split(<span class="string">" "</span>)).map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过countByKey来对key进行计数, 这是一个Action算子</span></span><br><span class="line">result = rdd2.countByKey()</span><br><span class="line"></span><br><span class="line">print(result)</span><br><span class="line">print(type(result))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">defaultdict(&lt;class &apos;int&apos;&gt;, &#123;&apos;hadoop&apos;: 7, &apos;spark&apos;: 5, &apos;flink&apos;: 3&#125;)</span><br><span class="line">&lt;class &apos;collections.defaultdict&apos;&gt;</span><br></pre></td></tr></table></figure><h4 id="collect"><a href="#collect" class="headerlink" title="collect"></a>collect</h4><p>将RDD各个分区内的数据统一收集到Driver中，形成一个list对象</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.collect()</span><br></pre></td></tr></table></figure><p><font color="gold">注</font>：数据集大小不能超过Driver内存</p><h4 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h4><p>对RDD数据集按照func逻辑进行聚合</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对func：传入2个参数得到1个返回值，要求返回值和参数的类型保持一致</span></span><br><span class="line">rdd.reduce(func)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line">print(rdd.reduce(<span class="keyword">lambda</span> a, b: a + b))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">15</span><br></pre></td></tr></table></figure><h4 id="fold"><a href="#fold" class="headerlink" title="fold"></a>fold</h4><p>同reduce，接受传入逻辑进行聚合，但是聚合是带有初始值的。这个初始值的聚合作用在：</p><ul><li>分区内聚合</li><li>分区间聚合</li></ul><p>例如：[[1, 2, 3], [4, 5, 6], [7, 8, 9]]</p><p>数据分布在三个<strong>分区</strong>上</p><p><strong><font color="orange">分区1</font></strong>：123聚合时带上10作为初始值得到16</p><p><strong><font color="orange">分区2</font></strong>：456聚合时带上10作为初始值得到25</p><p><strong><font color="orange">分区3</font></strong>：789聚合时带上10作为初始值得到34</p><p>最后再做3个分区间的聚合：16+25+34得到85</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.fold(src, func)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">print(rdd.fold(<span class="number">10</span>, <span class="keyword">lambda</span> a, b: a + b))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">85</span><br></pre></td></tr></table></figure><h4 id="first·take·count·top"><a href="#first·take·count·top" class="headerlink" title="first·take·count·top"></a>first·take·count·top</h4><p><strong><font color="orange">first</font></strong></p><p>取出RDD的第一个元素</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;sc.parallelize([<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]).first()</span><br><span class="line"><span class="number">3</span></span><br></pre></td></tr></table></figure><p><strong><font color="orange">take</font></strong></p><p>取出RDD的前n个元素，组合成list返回</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]).take(<span class="number">5</span>)</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br></pre></td></tr></table></figure><p> <strong><font color="orange">count</font></strong></p><p>计算RDD有多少条数据，返回值是一个数字</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;sc.parallelize([<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]).count()</span><br><span class="line"><span class="number">6</span></span><br></pre></td></tr></table></figure><p><strong><font color="orange">top</font></strong></p><p>对RDD数据集进行降序排序，取结果前n个</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;sc.parallelize([<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]).top(<span class="number">3</span>)</span><br><span class="line">[<span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>]</span><br></pre></td></tr></table></figure><h4 id="takeSample"><a href="#takeSample" class="headerlink" title="takeSample"></a>takeSample</h4><p>随机抽样RDD数据，可用于数据检查</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># arg1：True表示运行取同一个数据，False表示不允许取同一个数据（和数据内容无关，是否重复表示的是同一个位置的数据）</span></span><br><span class="line"><span class="comment"># arg2：抽样数目</span></span><br><span class="line"><span class="comment"># arg3：可选，随机数种子，随意传进一个数字</span></span><br><span class="line">takeSample(arg1：<span class="literal">True</span>/<span class="literal">False</span>, arg2:采样数, arg3:随机数种子)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">6</span>], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(rdd.takeSample(<span class="literal">False</span>, <span class="number">5</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[2, 7, 6, 6, 3]</span><br></pre></td></tr></table></figure><h4 id="takeOrdered"><a href="#takeOrdered" class="headerlink" title="takeOrdered"></a>takeOrdered</h4><p>对RDD进行排序，取结果前n个</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># arg1：需要几个数据</span></span><br><span class="line"><span class="comment"># arg2：对排序的数据进行更改（不会更改数据本身，仅在排序时使用）</span></span><br><span class="line">rdd.takeOrdered(arg1, arg2)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">6</span>], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(rdd.takeOrdered(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">print(rdd.takeOrdered(<span class="number">3</span>, <span class="keyword">lambda</span> x: -x))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[1, 2, 3]</span><br><span class="line">[9, 7, 6]</span><br></pre></td></tr></table></figure><h4 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h4><p>对RDD的每一个元素执行提供的逻辑操作（同map），无返回值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.foreach(func)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">6</span>], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">result = rdd.foreach(<span class="keyword">lambda</span> x: print(x * <span class="number">10</span>))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">10</span><br><span class="line">30</span><br><span class="line">20</span><br><span class="line">40</span><br><span class="line">70</span><br><span class="line">90</span><br><span class="line">60</span><br></pre></td></tr></table></figure><p>特性：由Executor直接输出</p><h4 id="saveAsTextFile"><a href="#saveAsTextFile" class="headerlink" title="saveAsTextFile"></a>saveAsTextFile</h4><p>将RDD数据写入文本文件</p><p>支持：本地写出或hdfs等文件系统</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.saveAsTextFile()</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">6</span>], <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">rdd.saveAsTextFile(<span class="string">"hdfs://master:8020/output/out_test1"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -ls /output/out_test1</span><br><span class="line">hadoop fs -cat /output/out_test1/*</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Found 4 items</span><br><span class="line">-rw-r--r--   3 root supergroup          0 2022-12-06 17:44 /output/out_test1/_SUCCESS</span><br><span class="line">-rw-r--r--   3 root supergroup          4 2022-12-06 17:44 /output/out_test1/part-00000</span><br><span class="line">-rw-r--r--   3 root supergroup          4 2022-12-06 17:44 /output/out_test1/part-00001</span><br><span class="line">-rw-r--r--   3 root supergroup          6 2022-12-06 17:44 /output/out_test1/part-00002</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">3</span><br><span class="line">2</span><br><span class="line">4</span><br><span class="line">7</span><br><span class="line">9</span><br><span class="line">6</span><br></pre></td></tr></table></figure><p>特性：由Executor直接写入文件</p><h4 id="mapPartitions"><a href="#mapPartitions" class="headerlink" title="mapPartitions"></a>mapPartitions</h4><p>不同于map每次操作一个分区的单一对象，mapPartitions一次操作一整个分区的数据，作为一个迭代器对象传入进来</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.mapPartitions(func)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">6</span>], <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(iter)</span>:</span></span><br><span class="line">    result = list()</span><br><span class="line">    <span class="keyword">for</span> it <span class="keyword">in</span> iter:</span><br><span class="line">        result.append(it * <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">print(rdd.mapPartitions(process).collect())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[10, 20, 70]</span><br></pre></td></tr></table></figure><h4 id="foreachPartition"><a href="#foreachPartition" class="headerlink" title="foreachPartition"></a>foreachPartition</h4><p>和foreach一致，foreach一条条处理，而foreachPartition一次处理一整个分区的数据，类似于没有返回值的mapPartitions</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.foreachPartition(func)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">6</span>], <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(iter)</span>:</span></span><br><span class="line">    result = list()</span><br><span class="line">    <span class="keyword">for</span> it <span class="keyword">in</span> iter:</span><br><span class="line">        result.append(it * <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">        print(result)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">rdd.foreachPartition(process)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[20]</span><br><span class="line">[20, 40]</span><br><span class="line">[10]</span><br><span class="line">[10, 30]</span><br><span class="line">[70]</span><br><span class="line">[70, 90]</span><br><span class="line">[70, 90, 60]</span><br></pre></td></tr></table></figure><h4 id="partitionBy"><a href="#partitionBy" class="headerlink" title="partitionBy"></a>partitionBy</h4><p>对RDD进行自定义分区操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># arg1：重新分区后的分区数量</span></span><br><span class="line"><span class="comment"># arg2：自定义分区规则，通过函数传入，函数返回值必须位int型（分区编号从0开始，不得超过分区数-1）</span></span><br><span class="line">rdd.partitionBy(arg1, arg2)</span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([(<span class="string">'hadoop'</span>, <span class="number">1</span>), (<span class="string">'spark'</span>, <span class="number">1</span>), (<span class="string">'hello'</span>, <span class="number">1</span>), (<span class="string">'flink'</span>, <span class="number">1</span>), (<span class="string">'hadoop'</span>, <span class="number">1</span>), (<span class="string">'spark'</span>, <span class="number">1</span>)])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用partitionBy 自定义 分区</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(k)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">'hadoop'</span> == k <span class="keyword">or</span> <span class="string">'hello'</span> == k: <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"><span class="keyword">if</span> <span class="string">'spark'</span> == k: <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(rdd.partitionBy(<span class="number">3</span>, process).glom().collect())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[(&apos;hadoop&apos;, 1), (&apos;hello&apos;, 1), (&apos;hadoop&apos;, 1)], [(&apos;spark&apos;, 1), (&apos;spark&apos;, 1)], [(&apos;flink&apos;, 1)]]</span><br></pre></td></tr></table></figure><h2 id="RDD的数据是过程数据"><a href="#RDD的数据是过程数据" class="headerlink" title="RDD的数据是过程数据"></a>RDD的数据是过程数据</h2><p>RDD之间进行相互迭代计算（Transformation的转换），当执行开启以后，新RDD生成，旧RDD消失。所以RDD的数据是过程数据，仅在处理的过程中存在，一旦处理完成便会被释放，旨在最大化的合理利用系统资源</p><p>当RDD被释放后需要被重新使用，会从头开始执行</p><h3 id="RDD缓存"><a href="#RDD缓存" class="headerlink" title="RDD缓存"></a>RDD缓存</h3><p>防止当RDD被释放而又要被重新调用的情况下，避免从头执行代码，使用RDD缓存API</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">rdd.cache()                                  <span class="comment"># 缓存到内存中</span></span><br><span class="line">rdd.persist(StorageLevel.MEMORY_ONLY)        <span class="comment"># 仅在内存缓存</span></span><br><span class="line">rdd.persist(StorageLevel.MEMORY_ONLY_2)      <span class="comment"># 仅在内存缓存，生成2个副本</span></span><br><span class="line">rdd.persist(StorageLevel.DISK_ONLY)          <span class="comment"># 仅缓存到硬盘</span></span><br><span class="line">rdd.persist(StorageLevel.DISK_ONLY_2)        <span class="comment"># 仅缓存到硬盘，生成2个副本</span></span><br><span class="line">rdd.persist(StorageLevel.DISK_ONLY_3)        <span class="comment"># 仅缓存到硬盘，生成3个副本</span></span><br><span class="line">rdd.persist(StorageLevel.MEMORY_AND_DISK)    <span class="comment"># 先在内存缓存，内存不够缓存到硬盘</span></span><br><span class="line">rdd.persist(StorageLevel.MEMORY_AND_DISK_2)  <span class="comment"># 先在内存缓存，内存不够缓存到硬盘，生成2个副本</span></span><br><span class="line">rdd.persist(StorageLevel.OFF_HEAP)           <span class="comment"># 堆外内存</span></span><br><span class="line">rdd.unpersist()                              <span class="comment"># 主动清理缓存</span></span><br></pre></td></tr></table></figure><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.storagelevel <span class="keyword">import</span> StorageLevel</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">rdd1 = sc.textFile(<span class="string">"hdfs://master:8020/input/words.txt"</span>)</span><br><span class="line">rdd2 = rdd1.flatMap(<span class="keyword">lambda</span> x: x.split(<span class="string">" "</span>))</span><br><span class="line">rdd3 = rdd2.map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">rdd3.cache()</span><br><span class="line">rdd3.persist(StorageLevel.MEMORY_AND_DISK_2)</span><br><span class="line"></span><br><span class="line">rdd4 = rdd3.reduceByKey(<span class="keyword">lambda</span> a, b: a + b)</span><br><span class="line">print(rdd4.collect())</span><br><span class="line"></span><br><span class="line">rdd5 = rdd3.groupByKey()</span><br><span class="line">rdd6 = rdd5.mapValues(<span class="keyword">lambda</span> x: sum(x))</span><br><span class="line">print(rdd6.collect())</span><br><span class="line"></span><br><span class="line">rdd3.unpersist()</span><br><span class="line">time.sleep(<span class="number">100000</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[(&apos;hadoop&apos;, 1), (&apos;hello&apos;, 3), (&apos;spark&apos;, 1), (&apos;flink&apos;, 1)]</span><br><span class="line">[(&apos;hadoop&apos;, 1), (&apos;hello&apos;, 3), (&apos;spark&apos;, 1), (&apos;flink&apos;, 1)]</span><br></pre></td></tr></table></figure><p><font color="gold">注</font>：缓存分散存储在各Executor所在服务器中。缓存从设计上来说是不安全的，缓存一旦丢失，需要重新计算缓存，必须保留被缓存RDD的前置血缘关系</p><h3 id="RDD-CheckPoint"><a href="#RDD-CheckPoint" class="headerlink" title="RDD CheckPoint"></a>RDD CheckPoint</h3><p>用于保存RDD数据，仅支持硬盘存储，且可以写入HDFS（cache不行），从设计上来说是安全的，不保留RDD的前置血缘关系</p><p>ChickPoint集中收集各个分区的数据进行存储，而非cache的分散存储</p><p>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.storagelevel <span class="keyword">import</span> StorageLevel</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 告知spark, 开启CheckPoint功能</span></span><br><span class="line">sc.setCheckpointDir(<span class="string">"hdfs://master:8020/output/ckp"</span>)</span><br><span class="line">rdd1 = sc.textFile(<span class="string">"hdfs://master:8020/input/words.txt"</span>)</span><br><span class="line">rdd2 = rdd1.flatMap(<span class="keyword">lambda</span> x: x.split(<span class="string">" "</span>))</span><br><span class="line">rdd3 = rdd2.map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用checkpoint API 保存数据即可</span></span><br><span class="line">rdd3.checkpoint()</span><br><span class="line"></span><br><span class="line">rdd4 = rdd3.reduceByKey(<span class="keyword">lambda</span> a, b: a + b)</span><br><span class="line">print(rdd4.collect())</span><br><span class="line"></span><br><span class="line">rdd5 = rdd3.groupByKey()</span><br><span class="line">rdd6 = rdd5.mapValues(<span class="keyword">lambda</span> x: sum(x))</span><br><span class="line">print(rdd6.collect())</span><br><span class="line"></span><br><span class="line">rdd3.unpersist()</span><br><span class="line">time.sleep(<span class="number">100000</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[(&apos;hadoop&apos;, 1), (&apos;hello&apos;, 3), (&apos;spark&apos;, 1), (&apos;flink&apos;, 1)]</span><br><span class="line">[(&apos;hadoop&apos;, 1), (&apos;hello&apos;, 3), (&apos;spark&apos;, 1), (&apos;flink&apos;, 1)]</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;RDD&quot;&gt;&lt;a href=&quot;#RDD&quot; class=&quot;headerlink&quot; title=&quot;RDD&quot;&gt;&lt;/a&gt;RDD&lt;/h2&gt;&lt;p&gt;RDD（Resilient Distributed Dataset）弹性分布式数据集，是spark中最基本的数据抽象，代表一个不可变、可分区、其中元素可并行计算的集合&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Resilient：RDD中的数据可存储再内存或磁盘中&lt;/li&gt;
&lt;li&gt;Distributed：分布式存储数据（跨机器/跨进程），用于分布式计算&lt;/li&gt;
&lt;li&gt;Dataset：一个用于存放数据的数据集合&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;特性&quot;&gt;&lt;a href=&quot;#特性&quot; class=&quot;headerlink&quot; title=&quot;特性&quot;&gt;&lt;/a&gt;特性&lt;/h3&gt;&lt;h4 id=&quot;分区&quot;&gt;&lt;a href=&quot;#分区&quot; class=&quot;headerlink&quot; title=&quot;分区&quot;&gt;&lt;/a&gt;分区&lt;/h4&gt;&lt;p&gt;RDD分区是RDD数据存储的最小单位，一份RDD数据本质上分隔成了多个分区&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 存储9个数字，设立三个分区&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;rdd = sc.parallelize([&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;6&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;7&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;8&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;9&lt;/span&gt;], &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;rdd.glom().collect()&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[[1,2,3],[4,5,6],[7,8,9]]&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h4 id=&quot;RDD方法会作用在其所有方法上&quot;&gt;&lt;a href=&quot;#RDD方法会作用在其所有方法上&quot; class=&quot;headerlink&quot; title=&quot;RDD方法会作用在其所有方法上&quot;&gt;&lt;/a&gt;RDD方法会作用在其所有方法上&lt;/h4&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;rdd.map(&lt;span class=&quot;keyword&quot;&gt;lambda&lt;/span&gt; x: x * &lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;).collect()&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[10,20,30,40,50,60,70,80,90]&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h4 id=&quot;RDD之间具有依赖关系&quot;&gt;&lt;a href=&quot;#RDD之间具有依赖关系&quot; class=&quot;headerlink&quot; title=&quot;RDD之间具有依赖关系&quot;&gt;&lt;/a&gt;RDD之间具有依赖关系&lt;/h4&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;sc = SparkContext(conf=conf)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;rdd1 = sc.textFile(&lt;span class=&quot;string&quot;&gt;&quot;../test.text&quot;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;rdd2 = rdd1.flatMap(&lt;span class=&quot;keyword&quot;&gt;lambda&lt;/span&gt; x: x.split(&lt;span class=&quot;string&quot;&gt;&#39; &#39;&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;rdd3 = rdd2.map(&lt;span class=&quot;keyword&quot;&gt;lambda&lt;/span&gt; x: (x, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;rdd4 = rdd3.reduceByKey(&lt;span class=&quot;keyword&quot;&gt;lambda&lt;/span&gt; a, b: a+b)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(rdd4.collect())&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h4 id=&quot;Key-Value型RDD可以有分区器&quot;&gt;&lt;a href=&quot;#Key-Value型RDD可以有分区器&quot; class=&quot;headerlink&quot; title=&quot;Key-Value型RDD可以有分区器&quot;&gt;&lt;/a&gt;Key-Value型RDD可以有分区器&lt;/h4&gt;&lt;p&gt;KV型RDD：RDD内存储的数据是只有两个元素的二元元组&lt;/p&gt;
&lt;p&gt;默认分区器：Hash分区规则，也可手动设置分区器：rdd.partitionBy()方法&lt;/p&gt;
&lt;p&gt;&lt;font color=&quot;gold&quot;&gt;注&lt;/font&gt;：不是所有RDD都是KV型&lt;/p&gt;
&lt;h4 id=&quot;RDD的分区规划：会尽量靠近数据所在的服务器&quot;&gt;&lt;a href=&quot;#RDD的分区规划：会尽量靠近数据所在的服务器&quot; class=&quot;headerlink&quot; title=&quot;RDD的分区规划：会尽量靠近数据所在的服务器&quot;&gt;&lt;/a&gt;RDD的分区规划：会尽量靠近数据所在的服务器&lt;/h4&gt;&lt;p&gt;在初始RDD读取数据规划阶段，分区会尽量规划到存储数据所在服务器，直接读取本地数据，避免从网络读取数据&lt;/p&gt;
&lt;p&gt;Spark会在确保并行计算能力的前提下，尽量确保本地读取&lt;/p&gt;
&lt;h3 id=&quot;RDD创建&quot;&gt;&lt;a href=&quot;#RDD创建&quot; class=&quot;headerlink&quot; title=&quot;RDD创建&quot;&gt;&lt;/a&gt;RDD创建&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;通过并行化集合创建（本地对象转化为分布式RDD）&lt;/li&gt;
&lt;li&gt;读取外部数据源（读文件）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;并行化创建&quot;&gt;&lt;a href=&quot;#并行化创建&quot; class=&quot;headerlink&quot; title=&quot;并行化创建&quot;&gt;&lt;/a&gt;并行化创建&lt;/h4&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# arg1: 集合对象，如：list&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# arg2：可选，指定分区数量&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;rdd = SparkContext.parallelize(arg1, arg2)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h4 id=&quot;读取文件&quot;&gt;&lt;a href=&quot;#读取文件&quot; class=&quot;headerlink&quot; title=&quot;读取文件&quot;&gt;&lt;/a&gt;读取文件&lt;/h4&gt;&lt;p&gt;通过textFile API来读取本地或者hdfs的数据&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# arg1: 文件路径&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# arg2：可选，最小分区数量&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 当arg2超出spark允许范围，参数失效&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;SparkContext.textFile(arg1, arg2)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;通过wholeTextFile API来读取小文件，这个api偏向于少量分区读取数据，是pyspark基于小文件的优化&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# arg1：文件路径&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# arg2：可选，最小分区数量&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 当arg2超出spark允许范围，参数失效&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;SparkContext.wholeTextFiles(arg1, arg2)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
    
    <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
    <category term="pyspark" scheme="http://yoursite.com/tags/pyspark/"/>
    
  </entry>
  
  <entry>
    <title>Pyspark</title>
    <link href="http://yoursite.com/2022/12/05/Pyspark/"/>
    <id>http://yoursite.com/2022/12/05/Pyspark/</id>
    <published>2022-12-05T08:45:40.000Z</published>
    <updated>2022-12-07T05:47:29.586Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><h2 id="Pyspark库安装"><a href="#Pyspark库安装" class="headerlink" title="Pyspark库安装"></a>Pyspark库安装</h2><p>（本文基于上文spark基础）</p><h3 id="Python库安装"><a href="#Python库安装" class="headerlink" title="Python库安装"></a>Python库安装</h3><p>在三台机器分别安装pyspark</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda activate pyspark</span><br><span class="line">pip install pyspark -i https://mirror.baidu.com/pypi/simple</span><br></pre></td></tr></table></figure><h3 id="Windows补丁"><a href="#Windows补丁" class="headerlink" title="Windows补丁"></a>Windows补丁</h3><p>将hadoop.dll置于C:/windows/system32/目录下，然后配置hadoop工具包的环境变量</p><p><img src="/2022/12/05/Pyspark/A.png" alt></p><p>安装相关python库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pyspark pyhive pymysql jieba -i https://mirror.baidu.com/pypi/simple</span><br></pre></td></tr></table></figure><h3 id="为pycharm添加ssh解释器环境"><a href="#为pycharm添加ssh解释器环境" class="headerlink" title="为pycharm添加ssh解释器环境"></a>为pycharm添加ssh解释器环境</h3><p><img src="/2022/12/05/Pyspark/B.png" alt></p><p><img src="/2022/12/05/Pyspark/C.png" alt></p><p><img src="/2022/12/05/Pyspark/D.png" alt></p><a id="more"></a><h2 id="SparkContext对象"><a href="#SparkContext对象" class="headerlink" title="SparkContext对象"></a>SparkContext对象</h2><p>Spark Application程序的入口为SparkContext。任何一个spark应用都要先构建SparkContext对象：</p><ul><li>创建SparkConf对象</li><li>基于SparkConf创建SparkContext</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conf = SparkConf().setAppName(appName).setMaster(master)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br></pre></td></tr></table></figure><h2 id="WorldCount测试程序"><a href="#WorldCount测试程序" class="headerlink" title="WorldCount测试程序"></a>WorldCount测试程序</h2><h3 id="Windows"><a href="#Windows" class="headerlink" title="Windows"></a>Windows</h3><p>在pycharm中新建python脚本，通过解释器执行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"><span class="comment"># 提交到yarn集群执行时，需配置环境变量</span></span><br><span class="line"><span class="comment"># import os</span></span><br><span class="line"><span class="comment"># os.environ["HADOOP_CONF_DIR"] = "/usr/local/hadoop/etc/hadoop"</span></span><br><span class="line"><span class="comment"># os.environ["YARN_CONF_DIR"] = "/usr/local/hadoop/etc/hadoop"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># 通过脚本执行时无需在代码中指定Master</span></span><br><span class="line">    <span class="comment"># conf = SparkConf().setAppName("WordCountHelloWorld")</span></span><br><span class="line">    <span class="comment"># 直接在pycharm执行</span></span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">"WordCountHelloWorld"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">    <span class="comment"># 通过SparkConf对象构建SparkContext对象</span></span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 需求 : wordcount单词计数, 读取HDFS上的words.txt文件, 对其内部的单词统计出现 的数量</span></span><br><span class="line">    <span class="comment"># 读取hdfs文件</span></span><br><span class="line">    file_rdd = sc.textFile(<span class="string">"hdfs://master:8020/input/words.txt"</span>)</span><br><span class="line">    <span class="comment"># 读取本地文件</span></span><br><span class="line">    <span class="comment"># file_rdd = sc.textFile("/usr/local/words.txt")</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将单词进行切割, 得到一个存储全部单词的集合对象</span></span><br><span class="line">    words_rdd = file_rdd.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将单词转换为元组对象, key是单词, value是数字1</span></span><br><span class="line">    words_with_one_rdd = words_rdd.map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将元组的value 按照key来分组, 对所有的value执行聚合操作(相加)</span></span><br><span class="line">    result_rdd = words_with_one_rdd.reduceByKey(<span class="keyword">lambda</span> a, b: a + b)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 通过collect方法收集RDD的数据打印输出结果</span></span><br><span class="line">    print(result_rdd.collect())</span><br></pre></td></tr></table></figure><h3 id="CentOS"><a href="#CentOS" class="headerlink" title="CentOS"></a>CentOS</h3><p>在根目录创建一份py脚本，通过spark客户端执行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/spark/bin/spark-submit --master local[*] /root/helloworld.py</span><br><span class="line">/usr/local/spark/bin/spark-submit --master yarn /root/helloworld.py</span><br></pre></td></tr></table></figure><h2 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h2><h3 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h3><p>构建SparkContext对象等非任务处理由Driver执行，RDD数据任务处理由Executor执行，再由Driver处理分布式计算结果</p><h3 id="Master-Node"><a href="#Master-Node" class="headerlink" title="Master Node"></a>Master Node</h3><p>spark自身的JVM框架JVM Driver和JVM Executor之间可以相互通讯，Python通过构建SparkContext对象与JVM Driver进行连接（Python的Driver代码翻译成JVM代码-py4j库，变成JVM Driver）</p><h3 id="Worker-Node"><a href="#Worker-Node" class="headerlink" title="Worker Node"></a>Worker Node</h3><p>Driver的操作指令发送给JVM Executor（RPC），JVM Executor再通过pyspark守护进程将指令发送给pyspark守护进程，pyspark守护进程将指令调度到运行的python进程中去。Executor端本质上是由python进程再工作</p><p>Driver段是直接由py4j直接翻译过去，Executor端则是转发</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;Pyspark库安装&quot;&gt;&lt;a href=&quot;#Pyspark库安装&quot; class=&quot;headerlink&quot; title=&quot;Pyspark库安装&quot;&gt;&lt;/a&gt;Pyspark库安装&lt;/h2&gt;&lt;p&gt;（本文基于上文spark基础）&lt;/p&gt;
&lt;h3 id=&quot;Python库安装&quot;&gt;&lt;a href=&quot;#Python库安装&quot; class=&quot;headerlink&quot; title=&quot;Python库安装&quot;&gt;&lt;/a&gt;Python库安装&lt;/h3&gt;&lt;p&gt;在三台机器分别安装pyspark&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;conda activate pyspark&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;pip install pyspark -i https://mirror.baidu.com/pypi/simple&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;Windows补丁&quot;&gt;&lt;a href=&quot;#Windows补丁&quot; class=&quot;headerlink&quot; title=&quot;Windows补丁&quot;&gt;&lt;/a&gt;Windows补丁&lt;/h3&gt;&lt;p&gt;将hadoop.dll置于C:/windows/system32/目录下，然后配置hadoop工具包的环境变量&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2022/12/05/Pyspark/A.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;安装相关python库&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;pip install pyspark pyhive pymysql jieba -i https://mirror.baidu.com/pypi/simple&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;为pycharm添加ssh解释器环境&quot;&gt;&lt;a href=&quot;#为pycharm添加ssh解释器环境&quot; class=&quot;headerlink&quot; title=&quot;为pycharm添加ssh解释器环境&quot;&gt;&lt;/a&gt;为pycharm添加ssh解释器环境&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;/2022/12/05/Pyspark/B.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2022/12/05/Pyspark/C.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2022/12/05/Pyspark/D.png&quot; alt&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
    
    <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
    <category term="pyspark" scheme="http://yoursite.com/tags/pyspark/"/>
    
  </entry>
  
  <entry>
    <title>Spark基础</title>
    <link href="http://yoursite.com/2022/12/01/Spark%E5%9F%BA%E7%A1%80/"/>
    <id>http://yoursite.com/2022/12/01/Spark%E5%9F%BA%E7%A1%80/</id>
    <published>2022-12-01T07:40:17.000Z</published>
    <updated>2023-04-13T01:43:42.844Z</updated>
    
    <content type="html"><![CDATA[<script src="\assets\js\APlayer.min.js"> </script><p><font color="gold">定义</font>：<font color="gold">Apache Spark是用于大规模数据处理的统一分析引擎。</font>其核心数据结构：弹性分布式数据集（RDD）能够在大规模集群中做内存运算，且具有一定容错方式。</p><h2 id="Spark框架"><a href="#Spark框架" class="headerlink" title="Spark框架"></a>Spark框架</h2><h3 id="组成"><a href="#组成" class="headerlink" title="组成"></a>组成</h3><p><font color="orange"><strong>· Spark Core</strong></font>：以RDD为数据抽象，提供Python、Java、Scala、R语言的API和Spark的核心功能，是Spark运行的基础；</p><p><font color="orange"><strong>· SparkSQL</strong></font>：基于SparkCore，提供机构化数据处理模块，支持以SQL语言对数据进行处理；同时可以作为StructuredStreaming模块的基础，进行数据流式计算；</p><p><font color="orange"><strong>· SparkStreaming</strong></font>：基于SparkCore，提供数据流式计算；</p><p><font color="orange"><strong>· MLlib</strong></font>：基于SparkCore，内置大量机器学习库和算法API，进行机器学习计算；</p><p><font color="orange"><strong>· GraphX</strong></font>：基于SparkCore，提供了大量图计算API，用于分布式图计算</p><h3 id="运行模式"><a href="#运行模式" class="headerlink" title="运行模式"></a>运行模式</h3><p><strong>· 本地模式（单机/local）</strong>：以一个独立进程，通过内部的多线程模拟Spark运行环境</p><p><strong>· Standalone模式（集群）</strong>：Spark的各个角色以独立进程形式存在，组成集群环境</p><p><strong>· Hadoop YARN模式（集群）</strong>：Spark的各个角色运行在YARN容器内部，组成集群环境</p><p><strong>· Kubernetes模式（容器集群）</strong>：Spark的各个角色运行在Kubernetes容器内部，组成集群环境</p><p><strong>· 云服务模式</strong></p><h3 id="角色"><a href="#角色" class="headerlink" title="角色"></a>角色</h3><ul><li><strong>资源</strong></li></ul><p><strong>· Master角色</strong>：集群资源管理</p><p><strong>· Worker角色</strong>：单机资源管理(所在服务器资源管理)</p><ul><li><strong>任务</strong></li></ul><p><strong>· Driver角色</strong>：单个任务管理</p><p><strong>· Executor角色</strong>：单个任务计算</p><a id="more"></a><h2 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h2><h3 id="Hadoop伪分布式搭建"><a href="#Hadoop伪分布式搭建" class="headerlink" title="Hadoop伪分布式搭建"></a>Hadoop伪分布式搭建</h3><h4 id="准备master虚拟机"><a href="#准备master虚拟机" class="headerlink" title="准备master虚拟机"></a>准备master虚拟机</h4><ul><li>配置静态ip</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysconfig/network-scripts/ifcfg-ens33</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#修改</span><br><span class="line">BOOTPROTO=&quot;static&quot;</span><br><span class="line">ONBOOT=&quot;yes&quot;</span><br><span class="line"></span><br><span class="line"># 新增</span><br><span class="line">IPADDR=&quot;192.168.80.129&quot;</span><br><span class="line">GATEWAY=&quot;192.168.80.2&quot;</span><br><span class="line">NETMASK=&quot;255.255.255.0&quot;</span><br><span class="line">DNS1=&quot;8.8.8.8&quot;</span><br><span class="line">DNS2=&quot;114.114.114.114&quot;</span><br><span class="line">IPV6_PRIVACY=&quot;no&quot;</span><br></pre></td></tr></table></figure><ul><li>重启网卡服务</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service network restart</span><br></pre></td></tr></table></figure><ul><li>关闭防火墙，并禁止防火墙开机自启</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br></pre></td></tr></table></figure><ul><li>关闭selinux</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/selinux/config</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># This file controls the state of SELinux on the system.</span><br><span class="line"># SELINUX= can take one of these three values:</span><br><span class="line">#     enforcing - SELinux security policy is enforced.</span><br><span class="line">#     permissive - SELinux prints warnings instead of enforcing.</span><br><span class="line">#     disabled - No SELinux policy is loaded.</span><br><span class="line">SELINUX=disabled</span><br><span class="line"># SELINUXTYPE= can take one of three values:</span><br><span class="line">#     targeted - Targeted processes are protected,</span><br><span class="line">#     minimum - Modification of targeted policy. Only selected processes are protected. </span><br><span class="line">#     mls - Multi Level Security protection.</span><br><span class="line">SELINUXTYPE=targeted</span><br></pre></td></tr></table></figure><ul><li>重启master</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reboot</span><br></pre></td></tr></table></figure><p>重启完成后将相关软件包（jdk、hadoop、spark等）导入至/usr/local目录下并解压</p><ul><li>解压jdk</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf jdk-8u211-linux-x64.tar.gz -C /usr/local</span><br></pre></td></tr></table></figure><p>配置jdk环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#set java environment </span><br><span class="line">JAVA_HOME=/usr/local/jdk1.8.0_211</span><br><span class="line">CLASSPATH=.:$JAVA_HOME/lib </span><br><span class="line">PATH=$JAVA_HOME/bin:$PATH </span><br><span class="line">export JAVA_HOME CLASSPATH PATH</span><br></pre></td></tr></table></figure><p>重启环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><p>检查安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">java version &quot;1.8.0_211&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_211-b12)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.211-b12, mixed mode)</span><br></pre></td></tr></table></figure><h4 id="配置linux集群"><a href="#配置linux集群" class="headerlink" title="配置linux集群"></a>配置linux集群</h4><p>将master完整克隆两台node1、node2机器，配置静态ip</p><p>配置主机名，分别执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hostname</span><br></pre></td></tr></table></figure><p>将三台机器的主机名设置为master、node1、node2</p><p>配置三台虚拟机的域名映射</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hosts</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"></span><br><span class="line">192.168.80.129 master</span><br><span class="line">192.168.80.130 node1</span><br><span class="line">192.168.80.131 node2</span><br></pre></td></tr></table></figure><p>此时三台机器已经可以互相ping通</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ping master</span><br><span class="line">ping node1</span><br><span class="line">ping node2</span><br></pre></td></tr></table></figure><p>分别重启三台机器</p><p>生成三台机器的公钥和私钥，分别执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure><p>反复回车，在/root/.ssh隐藏目录下生成私钥id_rsa和公钥id_rsa.pub</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Generating public/private rsa key pair.</span><br><span class="line">Enter file in which to save the key (/root/.ssh/id_rsa): </span><br><span class="line">Created directory &apos;/root/.ssh&apos;.</span><br><span class="line">Enter passphrase (empty for no passphrase): </span><br><span class="line">Enter same passphrase again: </span><br><span class="line">Your identification has been saved in /root/.ssh/id_rsa.</span><br><span class="line">Your public key has been saved in /root/.ssh/id_rsa.pub.</span><br><span class="line">The key fingerprint is:</span><br><span class="line">SHA256:/Vd3sEsHhz3CcxGwbRp5lj6+srZ7OpwAvyKS+qoTi9Q root@master</span><br><span class="line">The key&apos;s randomart image is:</span><br><span class="line">+---[RSA 2048]----+</span><br><span class="line">|             ..o.|</span><br><span class="line">|            . +oo|</span><br><span class="line">|             B+Oo|</span><br><span class="line">|         o    @=.|</span><br><span class="line">|  .     S +  .oo=|</span><br><span class="line">| o E       + ..++|</span><br><span class="line">|o o   .     = +. |</span><br><span class="line">|.o   o . . . B ..|</span><br><span class="line">| .oo+.. . . .=O. |</span><br><span class="line">+----[SHA256]-----+</span><br></pre></td></tr></table></figure><p>在三台虚拟机执行命令将公钥拷贝到master</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id master</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">The authenticity of host &apos;master (192.168.80.129)&apos; can&apos;t be established.</span><br><span class="line">ECDSA key fingerprint is SHA256:9UGNQgh5SWXh/1Z9iWTOzBSbqXf8kjbxc5SC73j9ct4.</span><br><span class="line">ECDSA key fingerprint is MD5:ee:b1:5d:3c:a5:2b:2e:08:cd:85:44:68:fe:c7:29:d9.</span><br><span class="line">Are you sure you want to continue connecting (yes/no)? yes</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys</span><br><span class="line">root@master&apos;s password: </span><br><span class="line"></span><br><span class="line">Number of key(s) added: 1</span><br><span class="line"></span><br><span class="line">Now try logging into the machine, with:   &quot;ssh &apos;master&apos;&quot;</span><br><span class="line">and check to make sure that only the key(s) you wanted were added.</span><br></pre></td></tr></table></figure><p>将master的公钥拷贝到node上</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp /root/.ssh/authorized_keys node1:/root/.ssh</span><br><span class="line">scp /root/.ssh/authorized_keys node2:/root/.ssh</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">The authenticity of host &apos;node1 (192.168.80.130)&apos; can&apos;t be established.</span><br><span class="line">ECDSA key fingerprint is SHA256:9UGNQgh5SWXh/1Z9iWTOzBSbqXf8kjbxc5SC73j9ct4.</span><br><span class="line">ECDSA key fingerprint is MD5:ee:b1:5d:3c:a5:2b:2e:08:cd:85:44:68:fe:c7:29:d9.</span><br><span class="line">Are you sure you want to continue connecting (yes/no)? yes</span><br><span class="line">Warning: Permanently added &apos;node1,192.168.80.130&apos; (ECDSA) to the list of known hosts.</span><br><span class="line">root@node1&apos;s password: </span><br><span class="line">authorized_keys                                      100% 1177     1.2MB/s   00:00</span><br></pre></td></tr></table></figure><p>测试ssh免密登录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh node1</span><br><span class="line">ssh node2</span><br><span class="line">exit</span><br></pre></td></tr></table></figure><h4 id="安装hadoop"><a href="#安装hadoop" class="headerlink" title="安装hadoop"></a>安装hadoop</h4><p>解压安装包并重命名</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hadoop-3.3.1.tar.gz -C /usr/local</span><br><span class="line">mv hadoop-3.3.1 hadoop</span><br></pre></td></tr></table></figure><p>修改配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hadoop/etc/hadoop</span><br></pre></td></tr></table></figure><p>hadoop-env.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim hadoop-env.sh</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_211</span><br><span class="line">export HDFS_NAMENODE_USER=root</span><br><span class="line">export HDFS_DATANODE_USER=root</span><br><span class="line">export HDFS_SECONDARYNAMENODE_USER=root</span><br><span class="line">export YARN_RESOURCEMANAGER_USER=root</span><br><span class="line">export YARN_NODEMANAGER_USER=root</span><br></pre></td></tr></table></figure><p>core-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim core-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://master:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 设置Hadoop本地保存数据路径 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/data/hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 设置HDFS web UI用户身份 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 整合hive 用户代理设置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>hdfs-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim hdfs-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置SNN进程运行机器位置信息 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>node1:9868<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>mapred-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim mapred-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置MR程序默认运行模式： yarn集群模式 local本地模式 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- MR程序历史服务器端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 历史服务器web端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.app.mapreduce.am.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>yarn-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim yarn-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置YARN集群主角色运行机器位置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 是否将对容器实施物理内存限制 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 是否将对容器实施虚拟内存限制。 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 开启日志聚集 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 设置yarn历史服务器地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log.server.url<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>http://master:19888/jobhistory/logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 保存的时间7天 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>workers</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim workers</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">master</span><br><span class="line">node1</span><br><span class="line">node2</span><br></pre></td></tr></table></figure><p>slaves</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim workers</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">master</span><br><span class="line">node1</span><br><span class="line">node2</span><br></pre></td></tr></table></figure><p>将安装包分发至其他机器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local</span><br><span class="line">scp -r hadoop root@node1:/usr/local</span><br><span class="line">scp -r hadoop root@node2:/usr/local</span><br></pre></td></tr></table></figure><p>配置hadoop环境变量，分发至其他机器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># set hadoop env</span><br><span class="line">export HADOOP_HOME=/usr/local/hadoop</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp /etc/profile node1:/etc/</span><br><span class="line">scp /etc/profile node2:/etc/</span><br></pre></td></tr></table></figure><p>在三台机器分别重启环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><p>首次启动，先格式化namenode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin hdfs namenode -format</span><br></pre></td></tr></table></figure><p>启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br><span class="line">start-yarn.sh</span><br><span class="line">mapred --daemon start historyserver</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">start-all.sh</span><br><span class="line">mapred --daemon start historyserver</span><br></pre></td></tr></table></figure><p>查看任务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">61488 Jps</span><br><span class="line">60417 NameNode</span><br><span class="line">60597 DataNode</span><br><span class="line">60981 ResourceManager</span><br><span class="line">61133 NodeManager</span><br><span class="line">61567 JobHistoryServer</span><br></pre></td></tr></table></figure><p>查看web页面（hadoop3.0版本以后web端口跟改为9870）</p><p><code>master:9870</code></p><p><img src="/2022/12/01/Spark基础/A.png" alt></p><p>关闭任务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stop-dfs.sh</span><br><span class="line">stop-yarn.sh</span><br><span class="line">mapred --daemon stop historyserver</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">stop-all.sh</span><br><span class="line">mapred --daemon stop historyserver</span><br></pre></td></tr></table></figure><h3 id="Anaconda3安装"><a href="#Anaconda3安装" class="headerlink" title="Anaconda3安装"></a>Anaconda3安装</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh ./Anaconda3-2021.05-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><p>配置清华源</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.condarc</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">channels:</span><br><span class="line">  - defaults</span><br><span class="line">show_channel_urls: true</span><br><span class="line">default_channels:</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2</span><br><span class="line">custom_channels:</span><br><span class="line">  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br></pre></td></tr></table></figure><p>创建pyspark虚拟环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n pyspark python=3.8</span><br></pre></td></tr></table></figure><h3 id="Spark-local模式搭建"><a href="#Spark-local模式搭建" class="headerlink" title="Spark local模式搭建"></a>Spark local模式搭建</h3><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>解压spark安装包并重命名</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /usr/local</span><br><span class="line">mv spark-3.2.0-bin-hadoop3.2 spark</span><br></pre></td></tr></table></figure><p>配置环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_HOME=/usr/local/spark</span><br><span class="line">export PYSPARK_PYTHON=/usr/local/anaconda3/envs/pyspark/bin/python3.8</span><br><span class="line">export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><p><font color="gold">注</font>：查找pyspark虚拟环境位置：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/anaconda3/envs/pyspark/bin</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bashrc</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_211</span><br><span class="line">export PYSPARK_PYTHON=/usr/local/anaconda3/envs/pyspark/bin/python3.8</span><br></pre></td></tr></table></figure><h4 id="启动pyspark交互式解释器"><a href="#启动pyspark交互式解释器" class="headerlink" title="启动pyspark交互式解释器"></a>启动pyspark交互式解释器</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/pyspark</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Python 3.8.8 (default, Apr 13 2021, 19:58:26) </span><br><span class="line">[GCC 7.3.0] :: Anaconda, Inc. on linux</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line">Using Spark&apos;s default log4j profile: org/apache/spark/log4j-defaults.properties</span><br><span class="line">Setting default log level to &quot;WARN&quot;.</span><br><span class="line">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</span><br><span class="line">22/12/02 17:38:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  &apos;_/</span><br><span class="line">   /__ / .__/\_,_/_/ /_/\_\   version 3.2.0</span><br><span class="line">      /_/</span><br><span class="line"></span><br><span class="line">Using Python version 3.8.8 (default, Apr 13 2021 19:58:26)</span><br><span class="line">Spark context Web UI available at http://master:4040</span><br><span class="line">Spark context available as &apos;sc&apos; (master = local[*], app id = local-1669973937905).</span><br><span class="line">SparkSession available as &apos;spark&apos;.</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure><p>进入浏览器任务页面<code>master:4040</code>，可以查看信息</p><p><img src="/2022/12/01/Spark基础/B.png" alt></p><p>执行一条pyspark指令后：</p><p><img src="/2022/12/01/Spark基础/C.png" alt></p><h3 id="Spark-StandAlone模式搭建"><a href="#Spark-StandAlone模式搭建" class="headerlink" title="Spark StandAlone模式搭建"></a>Spark StandAlone模式搭建</h3><h4 id="StandAlone"><a href="#StandAlone" class="headerlink" title="StandAlone"></a>StandAlone</h4><p>StandAlone模式是Spark自带的集群模式，Master角色以Master进程形式存在，Worker角色以Worker进程形式存在。其中Driver角色运行在Master进程内，Executor角色运行在Worker进程内。此外，还可以开启第三个进程：历史服务器（HistoryServer），用于保存Spark app运行后的事件日志。</p><h4 id="环境分发"><a href="#环境分发" class="headerlink" title="环境分发"></a>环境分发</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp Anaconda3-2021.05-Linux-x86_64.sh node1:`pwd`/</span><br><span class="line">scp Anaconda3-2021.05-Linux-x86_64.sh node2:`pwd`/</span><br></pre></td></tr></table></figure><p>进入node1、node2安装anaconda，同master：配置conda源，创建虚拟环境</p><p>将master:/etc/profile和bashrc中的环境变量复制到node1、2中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scp /etc/profile node1:/etc/profile</span><br><span class="line">scp /etc/profile node2:/etc/profile</span><br><span class="line">scp ~/.bashrc node1:~/</span><br><span class="line">scp ~/.bashrc node2:~/</span><br></pre></td></tr></table></figure><h4 id="创建hadoop用户（仅有root用户可跳过）"><a href="#创建hadoop用户（仅有root用户可跳过）" class="headerlink" title="创建hadoop用户（仅有root用户可跳过）"></a>创建hadoop用户（仅有root用户可跳过）</h4><p>hadoop用户拥有yarn的最高权限</p><p>新建用户</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo adduser hadoop</span><br><span class="line">passwd hadoop</span><br></pre></td></tr></table></figure><p>添加用户组</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo usermod -a -G hadoop hadoop</span><br></pre></td></tr></table></figure><p>赋予root权限</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sudoers</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">## Allow root to run any commands anywhere </span><br><span class="line">root    ALL=(ALL)       ALL</span><br><span class="line">hadoop    ALL=(ALL)       ALL</span><br></pre></td></tr></table></figure><p>添加权限</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/</span><br><span class="line">chown -R hadoop:hadoop hadoop*</span><br><span class="line">chown -R hadoop:hadoop spark*</span><br></pre></td></tr></table></figure><h4 id="配置spark配置文件"><a href="#配置spark配置文件" class="headerlink" title="配置spark配置文件"></a>配置spark配置文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">su - hadoop</span><br><span class="line">cd /usr/local/spark/conf</span><br><span class="line">mv workers.template workers</span><br><span class="line">vim workers</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">master</span><br><span class="line">node1</span><br><span class="line">node2</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mv spark-env.sh.template spark-env.sh</span><br><span class="line">vim spark-env.sh</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 设置JAVA安装目录</span></span></span><br><span class="line">JAVA_HOME=/usr/local/jdk1.8.0_211</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群</span></span></span><br><span class="line">HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop</span><br><span class="line">YARN_CONF_DIR=/usr/local/hadoop/etc/hadoop</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 指定spark master的IP和提交任务的通信端口</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 告知spark的master运行在哪个机器上</span></span><br><span class="line">export SPARK_MASTER_HOST=master</span><br><span class="line"><span class="meta">#</span><span class="bash"> 告知sparkmaster的通讯端口</span></span><br><span class="line">export SPARK_MASTER_PORT=7077</span><br><span class="line"><span class="meta">#</span><span class="bash"> 告知spark master的 webui端口</span></span><br><span class="line">SPARK_MASTER_WEBUI_PORT=8080</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> worker cpu可用核数</span></span><br><span class="line">SPARK_WORKER_CORES=1</span><br><span class="line"><span class="meta">#</span><span class="bash"> worker可用内存</span></span><br><span class="line">SPARK_WORKER_MEMORY=1g</span><br><span class="line"><span class="meta">#</span><span class="bash"> worker的工作通讯地址</span></span><br><span class="line">SPARK_WORKER_PORT=7078</span><br><span class="line"><span class="meta">#</span><span class="bash"> worker的 webui地址</span></span><br><span class="line">SPARK_WORKER_WEBUI_PORT=8081</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 设置历史服务器</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置的意思是  将spark程序运行的历史日志 存到hdfs的/sparklog文件夹中</span></span><br><span class="line">SPARK_HISTORY_OPTS="-Dspark.history.fs.logDirectory=hdfs://master:8020/sparklog/ -Dspark.history.fs.cleaner.enabled=true"</span><br></pre></td></tr></table></figure><p>启动hadoop</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -ls /</span><br></pre></td></tr></table></figure><p>此时没有sparklog文件。创建sparklog文件,赋予权限</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir /sparklog</span><br><span class="line">hadoop fs -chmod 777 /sparklog</span><br></pre></td></tr></table></figure><p>继续配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mv spark-defaults.conf.template spark-defaults.conf</span><br><span class="line">vim spark-defaults.conf</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 开启spark的日期记录功能</span><br><span class="line">spark.eventLog.enabled true</span><br><span class="line"># 设置spark日志记录的路径</span><br><span class="line">spark.eventLog.dir hdfs://master:8020/sparklog/ </span><br><span class="line"># 设置spark日志是否启动压缩</span><br><span class="line">spark.eventLog.compress true</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mv log4j.properties.template log4j.properties</span><br><span class="line">vim log4j.properties</span><br></pre></td></tr></table></figure><p>将INFO改为WARN，减少冗余日志</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Set everything to be logged to the console</span><br><span class="line">log4j.rootCategory=WARN, console</span><br></pre></td></tr></table></figure><h4 id="分发spark配置文件"><a href="#分发spark配置文件" class="headerlink" title="分发spark配置文件"></a>分发spark配置文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local</span><br><span class="line">scp -r spark node1:`pwd`/</span><br><span class="line">scp -r spark node2:`pwd`/</span><br></pre></td></tr></table></figure><h3 id="启动spark集群"><a href="#启动spark集群" class="headerlink" title="启动spark集群"></a>启动spark集群</h3><p>启动历史服务器进程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd spark</span><br><span class="line">sbin/start-history-server.sh</span><br><span class="line">jps</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">49297 JobHistoryServer</span><br><span class="line">49778 HistoryServer</span><br><span class="line">48296 DataNode</span><br><span class="line">49835 Jps</span><br><span class="line">48734 ResourceManager</span><br><span class="line">48910 NodeManager</span><br><span class="line">48127 NameNode</span><br></pre></td></tr></table></figure><p>启动集群</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-all.sh</span><br><span class="line">jps</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">49297 JobHistoryServer</span><br><span class="line">50162 Jps</span><br><span class="line">50020 Master</span><br><span class="line">48296 DataNode</span><br><span class="line">50107 Worker</span><br><span class="line">48734 ResourceManager</span><br><span class="line">48910 NodeManager</span><br><span class="line">48127 NameNode</span><br><span class="line">49903 HistoryServer</span><br></pre></td></tr></table></figure><p>进入<code>master:8080</code>web端口可以看到spark集群界面</p><p><img src="/2022/12/01/Spark基础/D.png" alt></p><h4 id="StandAlone集群测试"><a href="#StandAlone集群测试" class="headerlink" title="StandAlone集群测试"></a>StandAlone集群测试</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/spark/bin</span><br><span class="line">./pyspark --master spark://master:7077</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Python 3.8.15 (default, Nov 24 2022, 15:19:38) </span><br><span class="line">[GCC 11.2.0] :: Anaconda, Inc. on linux</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line">22/12/05 11:54:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  &apos;_/</span><br><span class="line">   /__ / .__/\_,_/_/ /_/\_\   version 3.2.0</span><br><span class="line">      /_/</span><br><span class="line"></span><br><span class="line">Using Python version 3.8.15 (default, Nov 24 2022 15:19:38)</span><br><span class="line">Spark context Web UI available at http://master:4040</span><br><span class="line">Spark context available as &apos;sc&apos; (master = spark://master:7077, app id = app-20221205115438-0000).</span><br><span class="line">SparkSession available as &apos;spark&apos;.</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure><p><img src="/2022/12/01/Spark基础/E.png" alt></p><p>在/usr/local下创建一个words.txt文件，写入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop spark flink</span><br><span class="line">hadoop spark flink hadoop hadoop</span><br><span class="line">hadoop spark flink hadoop hadoop spark spark</span><br></pre></td></tr></table></figure><p>创建input文件夹，将文件上传至hdfs</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir /input/</span><br><span class="line">hdfs dfs -put /usr/local/words.txt /input/</span><br><span class="line">hadoop fs -ls /input</span><br><span class="line">hadoop fs -cat /input/words.txt</span><br></pre></td></tr></table></figure><p>执行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sc.textFile(<span class="string">"hdfs://master:8020/input/words.txt"</span>).flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">" "</span>)).map(<span class="keyword">lambda</span> x:(x,<span class="number">1</span>)).reduceByKey(<span class="keyword">lambda</span> a,b:a+b).collect()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(&apos;hadoop&apos;, 7), (&apos;spark&apos;, 5), (&apos;flink&apos;, 3)]</span><br></pre></td></tr></table></figure><p><img src="/2022/12/01/Spark基础/F.png" alt></p><p><img src="/2022/12/01/Spark基础/G.png" alt></p><p><img src="/2022/12/01/Spark基础/H.png" alt></p><h2 id="Spark-on-YARN"><a href="#Spark-on-YARN" class="headerlink" title="Spark on YARN"></a>Spark on YARN</h2><ul><li><p>将spark部署到yarn集群中可以提高对资源的利用率，无需部署spark集群，只需要一台充当spark客户端的服务器即可提交任务到yarn集群运行</p></li><li><p>Master角色由yarn的ResourceManager担任</p></li><li>Worker角色由yarn的NodeManager担任</li><li>Driver角色运行在yarn容器内或提交任务的客户端进程中</li><li>Executor运行在yarn提供的容器内</li></ul><p>让spark计算任务运行在yarn容器内部，资源管理交友yarn的ResourceManager和NodeManager代替</p><h3 id="启动Spark-on-YARN"><a href="#启动Spark-on-YARN" class="headerlink" title="启动Spark on YARN"></a>启动Spark on YARN</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/spark</span><br><span class="line">./sbin/stop-all.sh #关闭standalone集群</span><br><span class="line">bin/pyspark --master yarn</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Python 3.8.15 (default, Nov 24 2022, 15:19:38) </span><br><span class="line">[GCC 11.2.0] :: Anaconda, Inc. on linux</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line">22/12/05 15:27:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">22/12/05 15:27:49 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.</span><br><span class="line"> Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  &apos;_/</span><br><span class="line">   /__ / .__/\_,_/_/ /_/\_\   version 3.2.0</span><br><span class="line">      /_/</span><br><span class="line"></span><br><span class="line">Using Python version 3.8.15 (default, Nov 24 2022 15:19:38)</span><br><span class="line">Spark context Web UI available at http://master:4040</span><br><span class="line">Spark context available as &apos;sc&apos; (master = yarn, app id = application_1670210110128_0001).</span><br><span class="line">SparkSession available as &apos;spark&apos;.</span><br><span class="line">&gt;&gt;&gt; sc.parallelize([1,2,3,4,5]).map(lambda x:x*10).collect()</span><br><span class="line">[10, 20, 30, 40, 50]                                                            </span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure><p>执行程序测试，或通过spark客户端spark-submit提交代码，spark‘算法会运行在容器中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit --master yarn /usr/local/spark/examples/src/main/python/pi.py 100</span><br></pre></td></tr></table></figure><p><img src="/2022/12/01/Spark基础/I.png" alt></p><p><img src="/2022/12/01/Spark基础/J.png" alt></p><h3 id="Spark-on-YARN部署模式"><a href="#Spark-on-YARN部署模式" class="headerlink" title="Spark on YARN部署模式"></a>Spark on YARN部署模式</h3><ul><li><p>Cluster（集群模式）</p><p>Driver运行在yarn容器内部，和ApplicationMaster在同一个容器内</p></li><li><p>Client（客户端模式）</p><p>Driver运行在客户端进程中，例如Driver运行在spark-submit客户端的进程中</p></li></ul><p>其中集群模式在容器内进行通讯，效率高，但是日志同样存放于容器内部</p><h4 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --master yarn --deploy-mode client /usr/local/spark/examples/src/main/python/pi.py 100</span><br></pre></td></tr></table></figure><h4 id="Cluster"><a href="#Cluster" class="headerlink" title="Cluster"></a>Cluster</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --master yarn --deploy-mode cluster /usr/local/spark/examples/src/main/python/pi.py 100</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;font color=&quot;gold&quot;&gt;定义&lt;/font&gt;：&lt;font color=&quot;gold&quot;&gt;Apache Spark是用于大规模数据处理的统一分析引擎。&lt;/font&gt;其核心数据结构：弹性分布式数据集（RDD）能够在大规模集群中做内存运算，且具有一定容错方式。&lt;/p&gt;
&lt;h2 id=&quot;Spark框架&quot;&gt;&lt;a href=&quot;#Spark框架&quot; class=&quot;headerlink&quot; title=&quot;Spark框架&quot;&gt;&lt;/a&gt;Spark框架&lt;/h2&gt;&lt;h3 id=&quot;组成&quot;&gt;&lt;a href=&quot;#组成&quot; class=&quot;headerlink&quot; title=&quot;组成&quot;&gt;&lt;/a&gt;组成&lt;/h3&gt;&lt;p&gt;&lt;font color=&quot;orange&quot;&gt;&lt;strong&gt;· Spark Core&lt;/strong&gt;&lt;/font&gt;：以RDD为数据抽象，提供Python、Java、Scala、R语言的API和Spark的核心功能，是Spark运行的基础；&lt;/p&gt;
&lt;p&gt;&lt;font color=&quot;orange&quot;&gt;&lt;strong&gt;· SparkSQL&lt;/strong&gt;&lt;/font&gt;：基于SparkCore，提供机构化数据处理模块，支持以SQL语言对数据进行处理；同时可以作为StructuredStreaming模块的基础，进行数据流式计算；&lt;/p&gt;
&lt;p&gt;&lt;font color=&quot;orange&quot;&gt;&lt;strong&gt;· SparkStreaming&lt;/strong&gt;&lt;/font&gt;：基于SparkCore，提供数据流式计算；&lt;/p&gt;
&lt;p&gt;&lt;font color=&quot;orange&quot;&gt;&lt;strong&gt;· MLlib&lt;/strong&gt;&lt;/font&gt;：基于SparkCore，内置大量机器学习库和算法API，进行机器学习计算；&lt;/p&gt;
&lt;p&gt;&lt;font color=&quot;orange&quot;&gt;&lt;strong&gt;· GraphX&lt;/strong&gt;&lt;/font&gt;：基于SparkCore，提供了大量图计算API，用于分布式图计算&lt;/p&gt;
&lt;h3 id=&quot;运行模式&quot;&gt;&lt;a href=&quot;#运行模式&quot; class=&quot;headerlink&quot; title=&quot;运行模式&quot;&gt;&lt;/a&gt;运行模式&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;· 本地模式（单机/local）&lt;/strong&gt;：以一个独立进程，通过内部的多线程模拟Spark运行环境&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;· Standalone模式（集群）&lt;/strong&gt;：Spark的各个角色以独立进程形式存在，组成集群环境&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;· Hadoop YARN模式（集群）&lt;/strong&gt;：Spark的各个角色运行在YARN容器内部，组成集群环境&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;· Kubernetes模式（容器集群）&lt;/strong&gt;：Spark的各个角色运行在Kubernetes容器内部，组成集群环境&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;· 云服务模式&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;角色&quot;&gt;&lt;a href=&quot;#角色&quot; class=&quot;headerlink&quot; title=&quot;角色&quot;&gt;&lt;/a&gt;角色&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;资源&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;· Master角色&lt;/strong&gt;：集群资源管理&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;· Worker角色&lt;/strong&gt;：单机资源管理(所在服务器资源管理)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;任务&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;· Driver角色&lt;/strong&gt;：单个任务管理&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;· Executor角色&lt;/strong&gt;：单个任务计算&lt;/p&gt;</summary>
    
    
    
    <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
    
    <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
  </entry>
  
</feed>
