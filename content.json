{"meta":{"title":"青域","subtitle":null,"description":"personal IT related station","author":"tianL.R","url":"http://yoursite.com","root":"/"},"pages":[{"title":"about","date":"2022-06-27T07:18:21.000Z","updated":"2022-06-30T03:43:24.840Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"Thanks For Visit！ （阿汤宝~汤阿宝~ ^^）"},{"title":"categories","date":"2022-06-27T06:58:23.000Z","updated":"2022-06-27T07:12:49.129Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2022-06-27T07:18:12.000Z","updated":"2022-06-27T07:21:10.521Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"关键信息提取（KIE）","slug":"关键信息提取（KIE）","date":"2024-11-26T11:24:34.000Z","updated":"2025-02-08T07:47:57.131Z","comments":true,"path":"2024/11/26/关键信息提取（KIE）/","link":"","permalink":"http://yoursite.com/2024/11/26/关键信息提取（KIE）/","excerpt":"环境准备服务器· OS: CentOS7.9· GPU: RTX3090 24G*2· CUDA: 11.7· CUDNN: 8.9.2 飞桨· paddlepaddle: paddlepaddle-gpu==2.4.2（cudatoolkit=11.7，建议conda安装）1conda install paddlepaddle-gpu==2.4.2 cudatoolkit=11.7 -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/Paddle/ -c conda-forge· paddleocr: 2.9.1· paddlenlp: 2.5.2 项目依赖requirements.txt 123456789101112131415161718192021222324252627# paddlepaddle 2.4.2 注：必须# conda install paddlepaddle-gpu==2.4.2 cudatoolkit=11.7 -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/Paddle/ -c conda-forgeshapelyscikit-imagepyclipperlmdbtqdmnumpyrapidfuzzopencv-pythonopencv-contrib-pythoncythonPillowpyyamlrequestsalbumentations==1.4.10# to be compatible with albumentationsalbucore==0.0.13sentencepieceyacsseqevalpypandocattrdict3python_docxpaddlenlp==2.5.2 注：bugfix 1.ModuleNotFoundError: No module named &#39;ppocr.***&#39;前往官方github主页（https://github.com/PaddlePaddle/PaddleOCR）补齐源码到python环境e.g.conda 虚拟环境下，需要补齐的源码根目录为：/root/anaconda3/envs/py39_kie/lib/python3.9/site-packages/paddleocr/ppocr2.ModuleNotFoundError: No module named ‘paddle.fluid’安装较低版本paddle paddlepaddle2.4.2/2.5.03.类型错误：123InvalidArgumentError: The type of data we are trying to retrieve does not match the type of data currently contained in the container. [Hint: Expected dtype() == paddle::experimental::CppTypeToDataType&lt;T&gt;::Type(), but received dtype():10 != paddle::experimental::CppTypeToDataType&lt;T&gt;::Type():9.] (at /paddle/paddle/phi/core/dense_tensor.cc:143) [operator &lt; less_equal &gt; error]官方示例bug，需要添加参数项--use_visual_backbone = False","text":"环境准备服务器· OS: CentOS7.9· GPU: RTX3090 24G*2· CUDA: 11.7· CUDNN: 8.9.2 飞桨· paddlepaddle: paddlepaddle-gpu==2.4.2（cudatoolkit=11.7，建议conda安装）1conda install paddlepaddle-gpu==2.4.2 cudatoolkit=11.7 -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/Paddle/ -c conda-forge· paddleocr: 2.9.1· paddlenlp: 2.5.2 项目依赖requirements.txt 123456789101112131415161718192021222324252627# paddlepaddle 2.4.2 注：必须# conda install paddlepaddle-gpu==2.4.2 cudatoolkit=11.7 -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/Paddle/ -c conda-forgeshapelyscikit-imagepyclipperlmdbtqdmnumpyrapidfuzzopencv-pythonopencv-contrib-pythoncythonPillowpyyamlrequestsalbumentations==1.4.10# to be compatible with albumentationsalbucore==0.0.13sentencepieceyacsseqevalpypandocattrdict3python_docxpaddlenlp==2.5.2 注：bugfix 1.ModuleNotFoundError: No module named &#39;ppocr.***&#39;前往官方github主页（https://github.com/PaddlePaddle/PaddleOCR）补齐源码到python环境e.g.conda 虚拟环境下，需要补齐的源码根目录为：/root/anaconda3/envs/py39_kie/lib/python3.9/site-packages/paddleocr/ppocr2.ModuleNotFoundError: No module named ‘paddle.fluid’安装较低版本paddle paddlepaddle2.4.2/2.5.03.类型错误：123InvalidArgumentError: The type of data we are trying to retrieve does not match the type of data currently contained in the container. [Hint: Expected dtype() == paddle::experimental::CppTypeToDataType&lt;T&gt;::Type(), but received dtype():10 != paddle::experimental::CppTypeToDataType&lt;T&gt;::Type():9.] (at /paddle/paddle/phi/core/dense_tensor.cc:143) [operator &lt; less_equal &gt; error]官方示例bug，需要添加参数项--use_visual_backbone = False 模型参考：https://paddlepaddle.github.io/PaddleOCR/latest/ppstructure/model_train/train_kie.html 准备（inference模型）SER任务模型下载链接：https://paddleocr.bj.bcebos.com/ppstructure/models/vi_layoutxlm/ser_vi_layoutxlm_xfund_infer.tar RE任务模型下载链接：https://paddleocr.bj.bcebos.com/ppstructure/models/vi_layoutxlm/re_vi_layoutxlm_xfund_infer.tar 创建项目根目录demo并添加测试图片1.png，在demo目录下新建models文件夹用于存放OCR、SER、RE模型、字典文件、字体文件。其中OCR模型可使用ppocrv3、v4版本下检测和识别（det/rec）模型，字典文件复制源码目录下paddleocr/ppocr/utils/dict/kie_dict/xfund_class_list.txt，字体文件使用simfang.ttf SER/RE模型串联1.创建官方代码示例demo/test.py用于测试和显示图像（源码：paddleocr/ppstructure/kie/predict_kie_token_ser_re.py）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.## Licensed under the Apache License, Version 2.0 (the \"License\");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.import osimport sys__dir__ = os.path.dirname(os.path.abspath(__file__))sys.path.append(__dir__)sys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"../..\")))os.environ[\"FLAGS_allocator_strategy\"] = \"auto_growth\"import cv2import jsonimport numpy as npimport timeimport paddleocr.tools.infer.utility as utilityfrom paddleocr.tools.infer_kie_token_ser_re import make_inputfrom paddleocr.ppocr.postprocess import build_post_processfrom paddleocr.ppocr.utils.logging import get_loggerfrom paddleocr.ppocr.utils.visual import draw_ser_results, draw_re_resultsfrom paddleocr.ppocr.utils.utility import get_image_file_list, check_and_readfrom paddleocr.ppstructure.utility import parse_argsfrom paddleocr.ppstructure.kie.predict_kie_token_ser import SerPredictorlogger = get_logger()class SerRePredictor(object): def __init__(self, args): self.use_visual_backbone = args.use_visual_backbone self.ser_engine = SerPredictor(args) if args.re_model_dir is not None: postprocess_params = &#123;\"name\": \"VQAReTokenLayoutLMPostProcess\"&#125; self.postprocess_op = build_post_process(postprocess_params) ( self.predictor, self.input_tensor, self.output_tensors, self.config, ) = utility.create_predictor(args, \"re\", logger) else: self.predictor = None def __call__(self, img): starttime = time.time() ser_results, ser_inputs, ser_elapse = self.ser_engine(img) if self.predictor is None: return ser_results, ser_elapse re_input, entity_idx_dict_batch = make_input(ser_inputs, ser_results) if self.use_visual_backbone == False: re_input.pop(4) for idx in range(len(self.input_tensor)): self.input_tensor[idx].copy_from_cpu(re_input[idx]) self.predictor.run() outputs = [] for output_tensor in self.output_tensors: output = output_tensor.copy_to_cpu() outputs.append(output) preds = dict( loss=outputs[1], pred_relations=outputs[2], hidden_states=outputs[0], ) post_result = self.postprocess_op( preds, ser_results=ser_results, entity_idx_dict_batch=entity_idx_dict_batch ) elapse = time.time() - starttime return post_result, elapsedef main(args): image_file_list = get_image_file_list(args.image_dir) ser_re_predictor = SerRePredictor(args) count = 0 total_time = 0 os.makedirs(args.output, exist_ok=True) with open( os.path.join(args.output, \"infer.txt\"), mode=\"w\", encoding=\"utf-8\" ) as f_w: for image_file in image_file_list: img, flag, _ = check_and_read(image_file) if not flag: img = cv2.imread(image_file) img = img[:, :, ::-1] if img is None: logger.info(\"error in loading image:&#123;&#125;\".format(image_file)) continue re_res, elapse = ser_re_predictor(img) re_res = re_res[0] res_str = \"&#123;&#125;\\t&#123;&#125;\\n\".format( image_file, json.dumps( &#123; \"ocr_info\": re_res, &#125;, ensure_ascii=False, ), ) f_w.write(res_str) if ser_re_predictor.predictor is not None: img_res = draw_re_results( image_file, re_res, font_path=args.vis_font_path ) img_save_path = os.path.join( args.output, os.path.splitext(os.path.basename(image_file))[0] + \"_ser_re.jpg\", ) else: img_res = draw_ser_results( image_file, re_res, font_path=args.vis_font_path ) img_save_path = os.path.join( args.output, os.path.splitext(os.path.basename(image_file))[0] + \"_ser.jpg\", ) cv2.imwrite(img_save_path, img_res) logger.info(\"save vis result to &#123;&#125;\".format(img_save_path)) if count &gt; 0: total_time += elapse count += 1 logger.info(\"Predict time of &#123;&#125;: &#123;&#125;\".format(image_file, elapse))if __name__ == \"__main__\": args = parse_args() args.mode = 'kie' args.use_visual_backbone = False args.kie_algorithm = 'LayoutXLM' args.re_model_dir = './models/re_vi_layoutxlm_xfund_infer' args.ser_model_dir = './models/ser_vi_layoutxlm_xfund_infer' args.ser_dict_path = './models/xfund_class_list.txt' args.vis_font_path = './models/simfang.ttf' args.ocr_order_method = \"tb-yx\" args.det_model_dir = './models/ch_PP-OCRv3_det_infer' args.rec_model_dir = './models/ch_PP-OCRv4_rec_infer' args.image_dir = './1.png' main(args)2.创建demo/main.py用于测试1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import cv2import timefrom paddleocr.ppstructure.kie.predict_kie_token_ser_re import SerRePredictorfrom paddleocr.ppstructure.utility import parse_argsfrom paddleocr.tools.infer_kie_token_ser_re import make_input# 继承SerRePredictor类，重写__call__方法，实现同时输出Ser模型结果class KIE(SerRePredictor): def __init__(self, para): super().__init__(para) def __call__(self, im): start_time = time.time() ser_results, ser_inputs, ser_elapse = self.ser_engine(im) if self.predictor is None: return ser_results, ser_elapse re_input, entity_idx_dict_batch = make_input(ser_inputs, ser_results) if not self.use_visual_backbone: re_input.pop(4) for idx in range(len(self.input_tensor)): self.input_tensor[idx].copy_from_cpu(re_input[idx]) self.predictor.run() outputs = [] for output_tensor in self.output_tensors: output = output_tensor.copy_to_cpu() outputs.append(output) preds = dict( loss=outputs[1], pred_relations=outputs[2], hidden_states=outputs[0], ) post_result = self.postprocess_op( preds, ser_results=ser_results, entity_idx_dict_batch=entity_idx_dict_batch ) elapse_time = time.time() - start_time return ser_results, post_result, elapse_timeif __name__ == \"__main__\": args = parse_args() args.mode = 'kie' args.show_log = True args.use_visual_backbone = False args.kie_algorithm = 'LayoutXLM' args.re_model_dir = './models/re_vi_layoutxlm_xfund_infer' args.ser_model_dir = './models/ser_vi_layoutxlm_xfund_infer' args.ser_dict_path = './models/xfund_class_list.txt' args.vis_font_path = './models/simfang.ttf' args.ocr_order_method = \"tb-yx\" args.det_model_dir = './models/ch_PP-OCRv3_det_infer' args.rec_model_dir = './models/ch_PP-OCRv4_rec_infer' args.det_limit_side_len = 1600, kie = KIE(args) img = cv2.imread('./1.png') ser_res, re_res, elapse = kie(img) print(ser_res, '\\n', re_res, '\\n', elapse)输出： 123[[&#123;&apos;transcription&apos;: &apos;证号&apos;, &apos;bbox&apos;: [239, 113, 267, 128], &apos;points&apos;: [[239.0, 113.0], [267.0, 113.0], [267.0, 128.0], [239.0, 128.0]], &apos;pred_id&apos;: 0, &apos;pred&apos;: &apos;O&apos;&#125;, &#123;&apos;transcription&apos;: &apos;T4105051990090&apos;, &apos;bbox&apos;: [240, 135, 441, 157], &apos;points&apos;: [[240.0, 135.0], [441.0, 136.0], [440.0, 157.0], [240.0, 156.0]], &apos;pred_id&apos;: 3, &apos;pred&apos;: &apos;ANSWER&apos;&#125;, &#123;&apos;transcription&apos;: &apos;姓名&apos;, &apos;bbox&apos;: [239, 162, 266, 178], &apos;points&apos;: [[239.0, 162.0], [266.0, 162.0], [266.0, 178.0], [239.0, 178.0]], &apos;pred_id&apos;: 1, &apos;pred&apos;: &apos;QUESTION&apos;&#125;, &#123;&apos;transcription&apos;: &apos;作业类别&apos;, &apos;bbox&apos;: [431, 162, 486, 179], &apos;points&apos;: [[431.0, 162.0], [486.0, 162.0], [486.0, 179.0], [431.0, 179.0]], &apos;pred_id&apos;: 1, &apos;pred&apos;: &apos;QUESTION&apos;&#125;, &#123;&apos;transcription&apos;: &apos;毕宏&apos;, &apos;bbox&apos;: [237, 182, 296, 203], &apos;points&apos;: [[237.0, 182.0], [296.0, 182.0], [296.0, 203.0], [237.0, 203.0]], &apos;pred_id&apos;: 3, &apos;pred&apos;: &apos;ANSWER&apos;&#125;, &#123;&apos;transcription&apos;: &apos;焊接与热切割作业&apos;, &apos;bbox&apos;: [432, 185, 586, 204], &apos;points&apos;: [[432.0, 185.0], [586.0, 185.0], [586.0, 204.0], [432.0, 204.0]], &apos;pred_id&apos;: 3, &apos;pred&apos;: &apos;ANSWER&apos;&#125;, &#123;&apos;transcription&apos;: &apos;男&apos;, &apos;bbox&apos;: [237, 246, 259, 269], &apos;points&apos;: [[237.0, 246.0], [259.0, 246.0], [259.0, 269.0], [237.0, 269.0]], &apos;pred_id&apos;: 3, &apos;pred&apos;: &apos;ANSWER&apos;&#125;, &#123;&apos;transcription&apos;: &apos;性别&apos;, &apos;bbox&apos;: [238, 227, 269, 246], &apos;points&apos;: [[238.0, 227.0], [269.0, 227.0], [269.0, 246.0], [238.0, 246.0]], &apos;pred_id&apos;: 1, &apos;pred&apos;: &apos;QUESTION&apos;&#125;, &#123;&apos;transcription&apos;: &apos;操作项目&apos;, &apos;bbox&apos;: [430, 228, 484, 245], &apos;points&apos;: [[430.0, 228.0], [484.0, 228.0], [484.0, 245.0], [430.0, 245.0]], &apos;pred_id&apos;: 1, &apos;pred&apos;: &apos;QUESTION&apos;&#125;, &#123;&apos;transcription&apos;: &apos;熔化焊接与热切割作业&apos;, &apos;bbox&apos;: [432, 251, 622, 268], &apos;points&apos;: [[432.0, 251.0], [622.0, 251.0], [622.0, 268.0], [432.0, 268.0]], &apos;pred_id&apos;: 3, &apos;pred&apos;: &apos;ANSWER&apos;&#125;, &#123;&apos;transcription&apos;: &apos;2023-03-29&apos;, &apos;bbox&apos;: [71, 342, 173, 362], &apos;points&apos;: [[72.0, 342.0], [173.0, 345.0], [173.0, 362.0], [71.0, 360.0]], &apos;pred_id&apos;: 3, &apos;pred&apos;: &apos;ANSWER&apos;&#125;, &#123;&apos;transcription&apos;: &apos;初领日期&apos;, &apos;bbox&apos;: [73, 325, 128, 341], &apos;points&apos;: [[73.0, 325.0], [128.0, 325.0], [128.0, 341.0], [73.0, 341.0]], &apos;pred_id&apos;: 1, &apos;pred&apos;: &apos;QUESTION&apos;&#125;, &#123;&apos;transcription&apos;: &apos;有效期限&apos;, &apos;bbox&apos;: [226, 327, 278, 342], &apos;points&apos;: [[226.0, 327.0], [278.0, 327.0], [278.0, 342.0], [226.0, 342.0]], &apos;pred_id&apos;: 1, &apos;pred&apos;: &apos;QUESTION&apos;&#125;, &#123;&apos;transcription&apos;: &apos;2023-03-29至2029-03-28&apos;, &apos;bbox&apos;: [226, 345, 448, 362], &apos;points&apos;: [[226.0, 345.0], [448.0, 345.0], [448.0, 362.0], [226.0, 362.0]], &apos;pred_id&apos;: 3, &apos;pred&apos;: &apos;ANSWER&apos;&#125;, &#123;&apos;transcription&apos;: &apos;2026-03-28前&apos;, &apos;bbox&apos;: [71, 389, 192, 413], &apos;points&apos;: [[72.0, 389.0], [192.0, 392.0], [192.0, 413.0], [71.0, 410.0]], &apos;pred_id&apos;: 3, &apos;pred&apos;: &apos;ANSWER&apos;&#125;, &#123;&apos;transcription&apos;: &apos;应复审日期&apos;, &apos;bbox&apos;: [74, 373, 140, 390], &apos;points&apos;: [[74.0, 373.0], [140.0, 373.0], [140.0, 390.0], [74.0, 390.0]], &apos;pred_id&apos;: 0, &apos;pred&apos;: &apos;O&apos;&#125;, &#123;&apos;transcription&apos;: &apos;签发机关&apos;, &apos;bbox&apos;: [223, 374, 278, 393], &apos;points&apos;: [[223.0, 375.0], [277.0, 374.0], [278.0, 392.0], [224.0, 393.0]], &apos;pred_id&apos;: 1, &apos;pred&apos;: &apos;QUESTION&apos;&#125;, &#123;&apos;transcription&apos;: &apos;河南省应急管理厅&apos;, &apos;bbox&apos;: [224, 394, 378, 414], &apos;points&apos;: [[224.0, 394.0], [378.0, 394.0], [378.0, 414.0], [224.0, 414.0]], &apos;pred_id&apos;: 3, &apos;pred&apos;: &apos;ANSWER&apos;&#125;]] [[(&#123;&apos;transcription&apos;: &apos;姓名&apos;, &apos;bbox&apos;: [239, 162, 266, 178], &apos;points&apos;: [[239.0, 162.0], [266.0, 162.0], [266.0, 178.0], [239.0, 178.0]], &apos;pred_id&apos;: 1, &apos;pred&apos;: &apos;QUESTION&apos;&#125;, &#123;&apos;transcription&apos;: &apos;毕宏&apos;, &apos;bbox&apos;: [237, 182, 296, 203], &apos;points&apos;: [[237.0, 182.0], [296.0, 182.0], [296.0, 203.0], [237.0, 203.0]], &apos;pred_id&apos;: 3, &apos;pred&apos;: &apos;ANSWER&apos;&#125;), (&#123;&apos;transcription&apos;: &apos;作业类别&apos;, &apos;bbox&apos;: [431, 162, 486, 179], &apos;points&apos;: [[431.0, 162.0], [486.0, 162.0], [486.0, 179.0], [431.0, 179.0]], &apos;pred_id&apos;: 1, &apos;pred&apos;: &apos;QUESTION&apos;&#125;, &#123;&apos;transcription&apos;: &apos;焊接与热切割作业&apos;, &apos;bbox&apos;: [432, 185, 586, 204], &apos;points&apos;: [[432.0, 185.0], [586.0, 185.0], [586.0, 204.0], [432.0, 204.0]], &apos;pred_id&apos;: 3, &apos;pred&apos;: &apos;ANSWER&apos;&#125;), (&#123;&apos;transcription&apos;: &apos;作业类别&apos;, &apos;bbox&apos;: [431, 162, 486, 179], &apos;points&apos;: [[431.0, 162.0], [486.0, 162.0], [486.0, 179.0], [431.0, 179.0]], &apos;pred_id&apos;: 1, &apos;pred&apos;: &apos;QUESTION&apos;&#125;, &#123;&apos;transcription&apos;: &apos;熔化焊接与热切割作业&apos;, &apos;bbox&apos;: [432, 251, 622, 268], &apos;points&apos;: [[432.0, 251.0], [622.0, 251.0], [622.0, 268.0], [432.0, 268.0]], &apos;pred_id&apos;: 3, &apos;pred&apos;: &apos;ANSWER&apos;&#125;), (&#123;&apos;transcription&apos;: &apos;性别&apos;, &apos;bbox&apos;: [238, 227, 269, 246], &apos;points&apos;: [[238.0, 227.0], [269.0, 227.0], [269.0, 246.0], [238.0, 246.0]], &apos;pred_id&apos;: 1, &apos;pred&apos;: &apos;QUESTION&apos;&#125;, &#123;&apos;transcription&apos;: &apos;男&apos;, &apos;bbox&apos;: [237, 246, 259, 269], &apos;points&apos;: [[237.0, 246.0], [259.0, 246.0], [259.0, 269.0], [237.0, 269.0]], &apos;pred_id&apos;: 3, &apos;pred&apos;: &apos;ANSWER&apos;&#125;), (&#123;&apos;transcription&apos;: &apos;初领日期&apos;, &apos;bbox&apos;: [73, 325, 128, 341], &apos;points&apos;: [[73.0, 325.0], [128.0, 325.0], [128.0, 341.0], [73.0, 341.0]], &apos;pred_id&apos;: 1, &apos;pred&apos;: &apos;QUESTION&apos;&#125;, &#123;&apos;transcription&apos;: &apos;2023-03-29&apos;, &apos;bbox&apos;: [71, 342, 173, 362], &apos;points&apos;: [[72.0, 342.0], [173.0, 345.0], [173.0, 362.0], [71.0, 360.0]], &apos;pred_id&apos;: 3, &apos;pred&apos;: &apos;ANSWER&apos;&#125;), (&#123;&apos;transcription&apos;: &apos;有效期限&apos;, &apos;bbox&apos;: [226, 327, 278, 342], &apos;points&apos;: [[226.0, 327.0], [278.0, 327.0], [278.0, 342.0], [226.0, 342.0]], &apos;pred_id&apos;: 1, &apos;pred&apos;: &apos;QUESTION&apos;&#125;, &#123;&apos;transcription&apos;: &apos;2023-03-29至2029-03-28&apos;, &apos;bbox&apos;: [226, 345, 448, 362], &apos;points&apos;: [[226.0, 345.0], [448.0, 345.0], [448.0, 362.0], [226.0, 362.0]], &apos;pred_id&apos;: 3, &apos;pred&apos;: &apos;ANSWER&apos;&#125;), (&#123;&apos;transcription&apos;: &apos;签发机关&apos;, &apos;bbox&apos;: [223, 374, 278, 393], &apos;points&apos;: [[223.0, 375.0], [277.0, 374.0], [278.0, 392.0], [224.0, 393.0]], &apos;pred_id&apos;: 1, &apos;pred&apos;: &apos;QUESTION&apos;&#125;, &#123;&apos;transcription&apos;: &apos;河南省应急管理厅&apos;, &apos;bbox&apos;: [224, 394, 378, 414], &apos;points&apos;: [[224.0, 394.0], [378.0, 394.0], [378.0, 414.0], [224.0, 414.0]], &apos;pred_id&apos;: 3, &apos;pred&apos;: &apos;ANSWER&apos;&#125;)]] 0.959315299987793 训练（基于XFUND数据集）参考：https://paddlepaddle.github.io/PaddleOCR/latest/ppocr/model_train/kie.html 数据下载XFUND github主页：https://github.com/doc-analysis/XFUND 数据集下载地址：https://github.com/doc-analysis/XFUND/releases/tag/v1.0 选择中文数据集zh.train.json、zh.train.zip、zh.val.json、zh.val.zip并下载 数据格式转换创建XFUND格式转Paddle训练格式代码demo/trans_xfun_data.py（源码：paddleocr/ppstructure/kie/tools/trans_xfun_data.py） 123456789101112131415161718192021222324252627282930313233343536373839import jsondef transfer_xfun_data(json_path=None, output_file=None): with open(json_path, \"r\", encoding=\"utf-8\") as fin: lines = fin.readlines() json_info = json.loads(lines[0]) documents = json_info[\"documents\"] with open(output_file, \"w\", encoding=\"utf-8\") as fout: for idx, document in enumerate(documents): label_info = [] img_info = document[\"img\"] document = document[\"document\"] image_path = img_info[\"fname\"] for doc in document: x1, y1, x2, y2 = doc[\"box\"] points = [[x1, y1], [x2, y1], [x2, y2], [x1, y2]] label_info.append( &#123; \"transcription\": doc[\"text\"], \"label\": doc[\"label\"], \"points\": points, \"id\": doc[\"id\"], \"linking\": doc[\"linking\"], &#125; ) fout.write( image_path + \"\\t\" + json.dumps(label_info, ensure_ascii=False) + \"\\n\" ) print(\"===ok====\")ori_gt_path = r'D:\\pycharmproject_2\\kie\\demo\\dataset\\zh.val.json'output_path = r'D:\\pycharmproject_2\\kie\\demo\\dataset\\xfun_val.json'transfer_xfun_data(ori_gt_path, output_path) 原始格式： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&#123; \"lang\": \"zh\", \"version\": \"0.1\", \"split\": \"train\", \"documents\": [ &#123; \"id\": \"zh_train_0\", \"uid\": \"640a0301a1cb24331748b579405502b44d6791883b25ea0eafc8a68126ccdadd\", \"document\": [ &#123; \"box\": [ 104, 114, 530, 175 ], \"text\": \"汇丰晋信\", \"label\": \"other\", \"words\": [ &#123; \"box\": [ 110, 117, 152, 175 ], \"text\": \"汇\" &#125;, &#123; \"box\": [ 189, 117, 229, 177 ], \"text\": \"丰\" &#125;, &#123; \"box\": [ 385, 117, 426, 177 ], \"text\": \"晋\" &#125;, &#123; \"box\": [ 466, 116, 508, 177 ], \"text\": \"信\" &#125; ], \"linking\": [], \"id\": 1 &#125; ... ] &#125; ... ]&#125; 转换： 12zh_train_0.jpg [&#123;\"transcription\": \"汇丰晋信\", \"label\": \"other\", \"points\": [[104, 114], [530, 114], [530, 175], [104, 175]], \"id\": 1, \"linking\": []&#125;, &#123;\"transcription\": \"受理时间:\", \"label\": \"question\", \"points\": [[126, 267], [266, 267], [266, 305], [126, 305]], \"id\": 7, \"linking\": [[7, 13]]&#125;, &#123;\"transcription\": \"2020.6.15\", \"label\": \"answer\", \"points\": [[321, 239], [537, 239], [537, 285], [321, 285]], \"id\": 13, \"linking\": [[7, 13]]&#125;...]zh_train_1.jpg [...] 配置字典文件用于存储各段文本的标签类别，这里使用models/xfund_class_list.txt字典文件 1234OTHERQUESTIONANSWERHEADER 注：字典中的标签和json文件中的标签不区分大小写 配置训练文件SER模型训练文件创建并修改SER模型训练yaml文件demo/ser_vi_layoutxlm_xfund_zh.yml，源文件路径：configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138Global: use_gpu: True epoch_num: &amp;epoch_num 200 log_smooth_window: 10 print_batch_step: 10 save_model_dir: ./output/ser_vi_layoutxlm_xfund_zh save_epoch_step: 2000 # evaluation is run every 10 iterations after the 0th iteration eval_batch_step: [ 0, 19 ] cal_metric_during_train: False save_inference_dir: use_visualdl: False seed: 2022 infer_img: dataset/xfund_train/zh_train/zh_val_42.jpg d2s_train_image_shape: [3, 224, 224] # if you want to predict using the groundtruth ocr info, # you can use the following config # infer_img: train_data/XFUND/zh_val/val.json # infer_mode: False save_res_path: ./output/ser/xfund_zh/res kie_rec_model_dir: kie_det_model_dir: amp_custom_white_list: ['scale', 'concat', 'elementwise_add']Architecture: model_type: kie algorithm: &amp;algorithm \"LayoutXLM\" Transform: Backbone: name: LayoutXLMForSer pretrained: True checkpoints: # one of base or vi mode: vi num_classes: &amp;num_classes 7 # &lt;--------------------------------------当标签数为n时，存在OTHER标签，取2n-1；不存在OTHER标签，取2n+1Loss: name: VQASerTokenLayoutLMLoss num_classes: *num_classes key: \"backbone_out\"Optimizer: name: AdamW beta1: 0.9 beta2: 0.999 lr: name: Linear learning_rate: 0.00005 epochs: *epoch_num warmup_epoch: 2 regularizer: name: L2 factor: 0.00000 PostProcess: name: VQASerTokenLayoutLMPostProcess class_path: &amp;class_path models/xfund_class_list.txt # &lt;--------------------------------------字典文件路径Metric: name: VQASerTokenMetric main_indicator: hmeanTrain: dataset: name: SimpleDataSet data_dir: dataset/xfund_train/zh_train # &lt;--------------------------------------训练集图像路径 label_file_list: - dataset/xfund_train/xfun_train.json # &lt;--------------------------------------训练集json文件路径 ratio_list: [ 1.0 ] transforms: - DecodeImage: # load image img_mode: RGB channel_first: False - VQATokenLabelEncode: # Class handling label contains_re: False algorithm: *algorithm class_path: *class_path use_textline_bbox_info: &amp;use_textline_bbox_info True # one of [None, \"tb-yx\"] order_method: &amp;order_method \"tb-yx\" - VQATokenPad: max_seq_len: &amp;max_seq_len 512 return_attention_mask: True - VQASerTokenChunk: max_seq_len: *max_seq_len - Resize: size: [224,224] - NormalizeImage: scale: 1 mean: [ 123.675, 116.28, 103.53 ] std: [ 58.395, 57.12, 57.375 ] order: 'hwc' - ToCHWImage: - KeepKeys: keep_keys: [ 'input_ids', 'bbox', 'attention_mask', 'token_type_ids', 'image', 'labels'] # dataloader will return list in this order loader: shuffle: True drop_last: False batch_size_per_card: 8 num_workers: 4Eval: dataset: name: SimpleDataSet data_dir: dataset/xfund_train/zh_val # &lt;--------------------------------------验证集图片路径 label_file_list: - dataset/xfund_train/xfun_val.json # &lt;--------------------------------------验证集json文件路径 transforms: - DecodeImage: # load image img_mode: RGB channel_first: False - VQATokenLabelEncode: # Class handling label contains_re: False algorithm: *algorithm class_path: *class_path use_textline_bbox_info: *use_textline_bbox_info order_method: *order_method - VQATokenPad: max_seq_len: *max_seq_len return_attention_mask: True - VQASerTokenChunk: max_seq_len: *max_seq_len - Resize: size: [224,224] - NormalizeImage: scale: 1 mean: [ 123.675, 116.28, 103.53 ] std: [ 58.395, 57.12, 57.375 ] order: 'hwc' - ToCHWImage: - KeepKeys: keep_keys: [ 'input_ids', 'bbox', 'attention_mask', 'token_type_ids', 'image', 'labels'] # dataloader will return list in this order loader: shuffle: False drop_last: False batch_size_per_card: 8 num_workers: 4 RE模型训练文件创建并修改RE模型训练yaml文件demo/re_vi_layoutxlm_xfund_zh.yml，源文件路径：configs/kie/vi_layoutxlm/re_vi_layoutxlm_xfund_zh.yml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130Global: use_gpu: True epoch_num: &amp;epoch_num 130 log_smooth_window: 10 print_batch_step: 10 save_model_dir: ./output/re_vi_layoutxlm_xfund_zh save_epoch_step: 2000 # evaluation is run every 10 iterations after the 0th iteration eval_batch_step: [ 0, 19 ] cal_metric_during_train: False save_inference_dir: use_visualdl: False seed: 2022 infer_img: dataset/xfund_train/zh_train/zh_val_21.jpg save_res_path: ./output/re/xfund_zh/with_gt kie_rec_model_dir: kie_det_model_dir:Architecture: model_type: kie algorithm: &amp;algorithm \"LayoutXLM\" Transform: Backbone: name: LayoutXLMForRe pretrained: dataset/re_vi_layoutxlm_xfund_pretrained mode: vi checkpoints:Loss: name: LossFromOutput key: loss reduction: meanOptimizer: name: AdamW beta1: 0.9 beta2: 0.999 clip_norm: 10 lr: learning_rate: 0.00005 warmup_epoch: 10 regularizer: name: L2 factor: 0.00000 PostProcess: name: VQAReTokenLayoutLMPostProcessMetric: name: VQAReTokenMetric main_indicator: hmeanTrain: dataset: name: SimpleDataSet data_dir: dataset/xfund_train/zh_train # &lt;--------------------------------------训练集图片路径 label_file_list: - dataset/xfund_train/xfun_train.json # &lt;--------------------------------------训练集json文件路径 ratio_list: [ 1.0 ] transforms: - DecodeImage: # load image img_mode: RGB channel_first: False - VQATokenLabelEncode: # Class handling label contains_re: True algorithm: *algorithm class_path: &amp;class_path models/xfund_class_list.txt # &lt;--------------------------------------字典文件路径 use_textline_bbox_info: &amp;use_textline_bbox_info True order_method: &amp;order_method \"tb-yx\" - VQATokenPad: max_seq_len: &amp;max_seq_len 512 return_attention_mask: True - VQAReTokenRelation: - VQAReTokenChunk: max_seq_len: *max_seq_len - TensorizeEntitiesRelations: - Resize: size: [224,224] - NormalizeImage: scale: 1 mean: [ 123.675, 116.28, 103.53 ] std: [ 58.395, 57.12, 57.375 ] order: 'hwc' - ToCHWImage: - KeepKeys: keep_keys: [ 'input_ids', 'bbox','attention_mask', 'token_type_ids', 'entities', 'relations'] # dataloader will return list in this order loader: shuffle: True drop_last: False batch_size_per_card: 2 num_workers: 4Eval: dataset: name: SimpleDataSet data_dir: dataset/xfund_train/zh_val # &lt;--------------------------------------验证集图片路径 label_file_list: - dataset/xfund_train/xfun_val.json # &lt;--------------------------------------验证集json文件路径 transforms: - DecodeImage: # load image img_mode: RGB channel_first: False - VQATokenLabelEncode: # Class handling label contains_re: True algorithm: *algorithm class_path: *class_path use_textline_bbox_info: *use_textline_bbox_info order_method: *order_method - VQATokenPad: max_seq_len: *max_seq_len return_attention_mask: True - VQAReTokenRelation: - VQAReTokenChunk: max_seq_len: *max_seq_len - TensorizeEntitiesRelations: - Resize: size: [224,224] - NormalizeImage: scale: 1 mean: [ 123.675, 116.28, 103.53 ] std: [ 58.395, 57.12, 57.375 ] order: 'hwc' - ToCHWImage: - KeepKeys: keep_keys: [ 'input_ids', 'bbox', 'attention_mask', 'token_type_ids', 'entities', 'relations'] # dataloader will return list in this order loader: shuffle: False drop_last: False batch_size_per_card: 8 num_workers: 8 开始训练创建训练代码demo/train.py，训练源码位置：paddleocr/tools/train.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.## Licensed under the Apache License, Version 2.0 (the \"License\");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.from __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_functionimport osimport sys__dir__ = os.path.dirname(os.path.abspath(__file__))sys.path.append(__dir__)sys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"..\")))import yamlimport paddleimport paddle.distributed as distfrom paddleocr.ppocr.data import build_dataloader, set_signal_handlersfrom paddleocr.ppocr.modeling.architectures import build_modelfrom paddleocr.ppocr.losses import build_lossfrom paddleocr.ppocr.optimizer import build_optimizerfrom paddleocr.ppocr.postprocess import build_post_processfrom paddleocr.ppocr.metrics import build_metricfrom paddleocr.ppocr.utils.save_load import load_modelfrom paddleocr.ppocr.utils.utility import set_seedfrom paddleocr.ppocr.modeling.architectures import apply_to_staticimport paddleocr.tools.program as programimport paddleocr.tools.naive_sync_bn as naive_sync_bndist.get_world_size()def main(config, device, logger, vdl_writer, seed): # init dist environment if config[\"Global\"][\"distributed\"]: dist.init_parallel_env() global_config = config[\"Global\"] # build dataloader set_signal_handlers() train_dataloader = build_dataloader(config, \"Train\", device, logger, seed) if len(train_dataloader) == 0: logger.error( \"No Images in train dataset, please ensure\\n\" + \"\\t1. The images num in the train label_file_list should be larger than or equal with batch size.\\n\" + \"\\t2. The annotation file and path in the configuration file are provided normally.\" ) return if config[\"Eval\"]: valid_dataloader = build_dataloader(config, \"Eval\", device, logger, seed) else: valid_dataloader = None step_pre_epoch = len(train_dataloader) # build post process post_process_class = build_post_process(config[\"PostProcess\"], global_config) # build model # for rec algorithm if hasattr(post_process_class, \"character\"): char_num = len(getattr(post_process_class, \"character\")) if config[\"Architecture\"][\"algorithm\"] in [ \"Distillation\", ]: # distillation model for key in config[\"Architecture\"][\"Models\"]: if ( config[\"Architecture\"][\"Models\"][key][\"Head\"][\"name\"] == \"MultiHead\" ): # for multi head if config[\"PostProcess\"][\"name\"] == \"DistillationSARLabelDecode\": char_num = char_num - 2 if config[\"PostProcess\"][\"name\"] == \"DistillationNRTRLabelDecode\": char_num = char_num - 3 out_channels_list = &#123;&#125; out_channels_list[\"CTCLabelDecode\"] = char_num # update SARLoss params if ( list(config[\"Loss\"][\"loss_config_list\"][-1].keys())[0] == \"DistillationSARLoss\" ): config[\"Loss\"][\"loss_config_list\"][-1][\"DistillationSARLoss\"][ \"ignore_index\" ] = (char_num + 1) out_channels_list[\"SARLabelDecode\"] = char_num + 2 elif any( \"DistillationNRTRLoss\" in d for d in config[\"Loss\"][\"loss_config_list\"] ): out_channels_list[\"NRTRLabelDecode\"] = char_num + 3 config[\"Architecture\"][\"Models\"][key][\"Head\"][ \"out_channels_list\" ] = out_channels_list else: config[\"Architecture\"][\"Models\"][key][\"Head\"][ \"out_channels\" ] = char_num elif config[\"Architecture\"][\"Head\"][\"name\"] == \"MultiHead\": # for multi head if config[\"PostProcess\"][\"name\"] == \"SARLabelDecode\": char_num = char_num - 2 if config[\"PostProcess\"][\"name\"] == \"NRTRLabelDecode\": char_num = char_num - 3 out_channels_list = &#123;&#125; out_channels_list[\"CTCLabelDecode\"] = char_num # update SARLoss params if list(config[\"Loss\"][\"loss_config_list\"][1].keys())[0] == \"SARLoss\": if config[\"Loss\"][\"loss_config_list\"][1][\"SARLoss\"] is None: config[\"Loss\"][\"loss_config_list\"][1][\"SARLoss\"] = &#123; \"ignore_index\": char_num + 1 &#125; else: config[\"Loss\"][\"loss_config_list\"][1][\"SARLoss\"][\"ignore_index\"] = ( char_num + 1 ) out_channels_list[\"SARLabelDecode\"] = char_num + 2 elif list(config[\"Loss\"][\"loss_config_list\"][1].keys())[0] == \"NRTRLoss\": out_channels_list[\"NRTRLabelDecode\"] = char_num + 3 config[\"Architecture\"][\"Head\"][\"out_channels_list\"] = out_channels_list else: # base rec model config[\"Architecture\"][\"Head\"][\"out_channels\"] = char_num if config[\"PostProcess\"][\"name\"] == \"SARLabelDecode\": # for SAR model config[\"Loss\"][\"ignore_index\"] = char_num - 1 model = build_model(config[\"Architecture\"]) use_sync_bn = config[\"Global\"].get(\"use_sync_bn\", False) if use_sync_bn: if config[\"Global\"].get(\"use_npu\", False): naive_sync_bn.convert_syncbn(model) else: model = paddle.nn.SyncBatchNorm.convert_sync_batchnorm(model) logger.info(\"convert_sync_batchnorm\") model = apply_to_static(model, config, logger) # build loss loss_class = build_loss(config[\"Loss\"]) # build optim optimizer, lr_scheduler = build_optimizer( config[\"Optimizer\"], epochs=config[\"Global\"][\"epoch_num\"], step_each_epoch=len(train_dataloader), model=model, ) # build metric eval_class = build_metric(config[\"Metric\"]) logger.info(\"train dataloader has &#123;&#125; iters\".format(len(train_dataloader))) if valid_dataloader is not None: logger.info(\"valid dataloader has &#123;&#125; iters\".format(len(valid_dataloader))) use_amp = config[\"Global\"].get(\"use_amp\", False) amp_level = config[\"Global\"].get(\"amp_level\", \"O2\") amp_dtype = config[\"Global\"].get(\"amp_dtype\", \"float16\") amp_custom_black_list = config[\"Global\"].get(\"amp_custom_black_list\", []) amp_custom_white_list = config[\"Global\"].get(\"amp_custom_white_list\", []) if os.path.exists( os.path.join(config[\"Global\"][\"save_model_dir\"], \"train_result.json\") ): try: os.remove( os.path.join(config[\"Global\"][\"save_model_dir\"], \"train_result.json\") ) except: pass if use_amp: AMP_RELATED_FLAGS_SETTING = &#123; \"FLAGS_max_inplace_grad_add\": 8, &#125; if paddle.is_compiled_with_cuda(): AMP_RELATED_FLAGS_SETTING.update( &#123; \"FLAGS_cudnn_batchnorm_spatial_persistent\": 1, \"FLAGS_gemm_use_half_precision_compute_type\": 0, &#125; ) paddle.set_flags(AMP_RELATED_FLAGS_SETTING) scale_loss = config[\"Global\"].get(\"scale_loss\", 1.0) use_dynamic_loss_scaling = config[\"Global\"].get( \"use_dynamic_loss_scaling\", False ) scaler = paddle.amp.GradScaler( init_loss_scaling=scale_loss, use_dynamic_loss_scaling=use_dynamic_loss_scaling, ) if amp_level == \"O2\": model, optimizer = paddle.amp.decorate( models=model, optimizers=optimizer, level=amp_level, master_weight=True, dtype=amp_dtype, ) else: scaler = None # load pretrain model pre_best_model_dict = load_model( config, model, optimizer, config[\"Architecture\"][\"model_type\"] ) if config[\"Global\"][\"distributed\"]: model = paddle.DataParallel(model) # start train program.train( config, train_dataloader, valid_dataloader, device, model, loss_class, optimizer, lr_scheduler, post_process_class, eval_class, pre_best_model_dict, logger, step_pre_epoch, vdl_writer, scaler, amp_level, amp_custom_black_list, amp_custom_white_list, amp_dtype, )def test_reader(config, device, logger): loader = build_dataloader(config, \"Train\", device, logger) import time starttime = time.time() count = 0 try: for data in loader(): count += 1 if count % 1 == 0: batch_time = time.time() - starttime starttime = time.time() logger.info( \"reader: &#123;&#125;, &#123;&#125;, &#123;&#125;\".format(count, len(data[0]), batch_time) ) except Exception as e: logger.info(e) logger.info(\"finish reader: &#123;&#125;, Success!\".format(count))if __name__ == \"__main__\": config, device, logger, vdl_writer = program.preprocess(is_train=True) seed = config[\"Global\"][\"seed\"] if \"seed\" in config[\"Global\"] else 1024 set_seed(seed) main(config, device, logger, vdl_writer, seed) # test_reader(config, device, logger) 根据训练目标的配置文件，分发显卡和训练任务： SER: CUDA_VISIBLE_DEVICES=0 python train.py -c ser_vi_layoutxlm_xfund_zh.yml RE: CUDA_VISIBLE_DEVICES=1 python train.py -c re_vi_layoutxlm_xfund_zh.yml 首次启动将自动下载预训练模型。日志输出样例如下： 1234567[2024/11/26 17:35:39] ppocr INFO: epoch: [1/200], global_step: 10, lr: 0.000006, loss: 1.838789, avg_reader_cost: 0.39165 s, avg_batch_cost: 0.54732 s, avg_samples: 8.0, ips: 14.61671 samples/s, eta: 0:34:34, max_mem_reserved: 11694 MB, max_mem_allocated: 10523 MB[2024/11/26 17:35:43] ppocr INFO: epoch: [1/200], global_step: 19, lr: 0.000018, loss: 1.446505, avg_reader_cost: 0.22235 s, avg_batch_cost: 0.32516 s, avg_samples: 6.9, ips: 21.22014 samples/s, eta: 0:28:56, max_mem_reserved: 11694 MB, max_mem_allocated: 10523 MBeval model:: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:04&lt;00:00, 1.61it/s][2024/11/26 17:35:47] ppocr INFO: cur metric, precision: 0.05993363749481543, recall: 0.10410662824207492, hmean: 0.07607265069755198, fps: 101.1334614167882[2024/11/26 17:36:22] ppocr INFO: save best model is to ./output/ser_vi_layoutxlm_xfund_zh/best_accuracy[2024/11/26 17:36:22] ppocr INFO: best metric, hmean: 0.07607265069755198, precision: 0.05993363749481543, recall: 0.10410662824207492, fps: 101.1334614167882, best_epoch: 1[2024/11/26 17:36:39] ppocr INFO: save model in ./output/ser_vi_layoutxlm_xfund_zh/latest 模型导出训练结束后，进入最佳模型目录demo/output/re_vi_layoutxlm_xfund_zh/best_accuracy，此时模型不具备.pdmodel文件，不能直接用于预测，需要导出为inference模型（本文适当略过模型评估步骤） 创建模型导出代码demo/export_model.py，导出源码位置：paddleocr/tools/export_model.py 12345678910111213141516171819202122232425262728293031323334353637# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.## Licensed under the Apache License, Version 2.0 (the \"License\");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.import osimport sys__dir__ = os.path.dirname(os.path.abspath(__file__))sys.path.append(__dir__)sys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"..\")))import argparsefrom paddleocr.tools.program import load_config, merge_config, ArgsParserfrom paddleocr.ppocr.utils.export_model import exportdef main(): FLAGS = ArgsParser().parse_args() config = load_config(FLAGS.config) config = merge_config(config, FLAGS.opt) # export model export(config)if __name__ == \"__main__\": main() 导出命令： SER: 12python export_model.py -c ser_vi_layoutxlm_xfund_zh.yml -o Architecture.Backbone.checkpoints=./output/ser_vi_layoutxlm_xfund_zh/best_accuracy Global.save_inference_dir=./inference/ser_vi_layoutxlm Global.save_inference_dir=./inference/ser_vi_layoutxlm RE: 12python export_model.py -c re_vi_layoutxlm_xfund_zh.yml -o Architecture.Backbone.checkpoints=./output/re_vi_layoutxlm_xfund_zh/best_accuracy Global.save_inference_dir=./inference/re_vi_layoutxlm Global.save_inference_dir=./inference/re_vi_layoutxlm 将在demo/inference目录下生成导出的推理模型，可修改args参数中模型路径进行验证。 客制化标注工具：PPOCRLabel操作系统回到windows，并创建demo/dataset/temp文件夹存放训练图像 安装：pip install PPOCRLabel==2.1.3 -i https://pypi.tuna.tsinghua.edu.cn/simple 启动：PPOCRLabel --lang ch --kie True 此时标注工具左下角显示关键词列表项 注： 最新版可能存在bug，建议使用2.1.3版本。且截至目前（24.11月 v2.1.12版本），工具仍不具备RE任务关系标注功能，需制作后处理脚本 *难点 后处理：SER 创建SER任务处理代码demo/trans+ppocrlabel.py 1234567891011121314151617181920212223import jsonwith open('./dataset/temp/Label.txt', 'r', encoding='utf-8') as l: content = l.readlines() for line in content: image = line.split('\\t')[0] label = json.loads(line.split('\\t')[1]) label_set = [] for i, lab in enumerate(label): new_lab = &#123; 'transcription': lab.get('transcription'), 'points': lab.get('points'), 'label': lab.get('key_cls'), 'id': i, 'linking': [] &#125; label_set.append(new_lab) label_set_json = json.dumps(label_set, ensure_ascii=False) with open('./dataset/project/train.json', 'a+', encoding='utf-8') as f: f.write(image) f.write('\\t') f.write(label_set_json) f.write('\\n') RE 测试中，未完待续","categories":[],"tags":[{"name":"OCR","slug":"OCR","permalink":"http://yoursite.com/tags/OCR/"},{"name":"paddle","slug":"paddle","permalink":"http://yoursite.com/tags/paddle/"},{"name":"KIE","slug":"KIE","permalink":"http://yoursite.com/tags/KIE/"}]},{"title":"间隙树排序算法的可视化","slug":"间隙树排序算法的可视化","date":"2024-08-06T07:10:43.000Z","updated":"2024-08-06T08:41:44.334Z","comments":true,"path":"2024/08/06/间隙树排序算法的可视化/","link":"","permalink":"http://yoursite.com/2024/08/06/间隙树排序算法的可视化/","excerpt":"做文档翻译的OCR程序时，会遇到这样一个场景，因为通常OCR模型的输出都是按文本块逐行返回，当结果进入翻译模型时会丢失行与行之间的信息。为了解决这个问题，需要对OCR结果进行进一步的版面分析，将文本块合并成段落，再输入到翻译模型中去。 算法间隙·树·排序算法 参考链接：https://github.com/hiroi-sora/GapTree_Sort_Algorithm 算法主要对文本块按间隙进行划分，再经过树形排序，将底层OCR的输出结果从子文本块转化成段落文本块。 算法重构为了使文本块的定位更加准确，使用原算法输出段落块（content_blocks）的上下左右四个边界点构成新段落块（new_blocks） 源码： 1234567891011121314151617181920212223242526272829def structure_ocr(self): for line, tb in enumerate(self.blocks_data): tb[\"bbox\"] = self.bboxes[line] gtree = GapTree(lambda tb: tb[\"bbox\"]) sorted_text_blocks = gtree.sort(self.blocks_data) # 文本块排序 # print(sorted_text_blocks) pp = ParagraphParse(self.get_info, self.set_end) # 获取所有区块的文本块 nodes_text_blocks = gtree.get_nodes_text_blocks() content_blocks = [] for tbs in nodes_text_blocks: content = \"\" tbs = pp.run(tbs) # 预测结尾分隔符 for tb in tbs: # 输出文本和结尾分隔符 content += tb[\"text\"] + tb[\"end\"] content_blocks.append(content) node_tbs = [] for node in gtree.current_nodes: if not node[\"units\"]: continue # 跳过没有块的根节点 x0 = node[\"x_left\"] x1 = node[\"x_right\"] y0 = gtree.current_rows[node[\"r_top\"]][0][0][1] y1 = gtree.current_rows[node[\"r_bottom\"]][0][0][3] node_tbs.append([[x0, y0], [x1, y0], [x1, y1], [x0, y1]]) return node_tbs, content_blocks","text":"做文档翻译的OCR程序时，会遇到这样一个场景，因为通常OCR模型的输出都是按文本块逐行返回，当结果进入翻译模型时会丢失行与行之间的信息。为了解决这个问题，需要对OCR结果进行进一步的版面分析，将文本块合并成段落，再输入到翻译模型中去。 算法间隙·树·排序算法 参考链接：https://github.com/hiroi-sora/GapTree_Sort_Algorithm 算法主要对文本块按间隙进行划分，再经过树形排序，将底层OCR的输出结果从子文本块转化成段落文本块。 算法重构为了使文本块的定位更加准确，使用原算法输出段落块（content_blocks）的上下左右四个边界点构成新段落块（new_blocks） 源码： 1234567891011121314151617181920212223242526272829def structure_ocr(self): for line, tb in enumerate(self.blocks_data): tb[\"bbox\"] = self.bboxes[line] gtree = GapTree(lambda tb: tb[\"bbox\"]) sorted_text_blocks = gtree.sort(self.blocks_data) # 文本块排序 # print(sorted_text_blocks) pp = ParagraphParse(self.get_info, self.set_end) # 获取所有区块的文本块 nodes_text_blocks = gtree.get_nodes_text_blocks() content_blocks = [] for tbs in nodes_text_blocks: content = \"\" tbs = pp.run(tbs) # 预测结尾分隔符 for tb in tbs: # 输出文本和结尾分隔符 content += tb[\"text\"] + tb[\"end\"] content_blocks.append(content) node_tbs = [] for node in gtree.current_nodes: if not node[\"units\"]: continue # 跳过没有块的根节点 x0 = node[\"x_left\"] x1 = node[\"x_right\"] y0 = gtree.current_rows[node[\"r_top\"]][0][0][1] y1 = gtree.current_rows[node[\"r_bottom\"]][0][0][3] node_tbs.append([[x0, y0], [x1, y0], [x1, y1], [x0, y1]]) return node_tbs, content_blocks 修改后： 12345678910111213141516171819202122232425262728293031323334353637def structure_ocr(self): for line, tb in enumerate(self.blocks_data): tb[\"bbox\"] = self.bboxes[line] gtree = GapTree(lambda tb: tb[\"bbox\"]) sorted_text_blocks = gtree.sort(self.blocks_data) # 文本块排序 # print(sorted_text_blocks) pp = ParagraphParse(self.get_info, self.set_end) # 获取所有区块的文本块 nodes_text_blocks = gtree.get_nodes_text_blocks() content_blocks = [] for tbs in nodes_text_blocks: content = \"\" tbs = pp.run(tbs) # 预测结尾分隔符 for tb in tbs: # 输出文本和结尾分隔符 content += tb[\"text\"] + tb[\"end\"] content_blocks.append(content) new_blocks = [] for m in nodes_text_blocks: x1_list = [] y1_list = [] x2_list = [] y2_list = [] for n in m: box_ = n.get('box') x1_list.append(box_[0][0]) y1_list.append(box_[0][1]) x2_list.append(box_[2][0]) y2_list.append(box_[2][1]) x1_ = min(x1_list) y1_ = min(y1_list) x2_ = max(x2_list) y2_ = max(y2_list) new_blocks.append([[x1_, y1_], [x2_, y1_], [x2_, y2_], [x1_, y2_]]) return new_blocks, content_blocks 封装1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162class StructureOCR: def __init__(self, blocks_data): self.blocks_data = blocks_data self.bboxes = linePreprocessing(self.blocks_data) @staticmethod def get_info(tb): # 返回信息 b = tb[\"box\"] return (b[0][0], b[0][1], b[2][0], b[2][1]), tb[\"text\"] @staticmethod def set_end(tb, end): # 获取预测的块尾分隔符 tb[\"end\"] = end # also： tb[\"text\"] += end def structure_ocr(self): for line, tb in enumerate(self.blocks_data): tb[\"bbox\"] = self.bboxes[line] gtree = GapTree(lambda tb: tb[\"bbox\"]) sorted_text_blocks = gtree.sort(self.blocks_data) # 文本块排序 # print(sorted_text_blocks) pp = ParagraphParse(self.get_info, self.set_end) # 获取所有区块的文本块 nodes_text_blocks = gtree.get_nodes_text_blocks() content_blocks = [] for tbs in nodes_text_blocks: content = \"\" tbs = pp.run(tbs) # 预测结尾分隔符 for tb in tbs: # 输出文本和结尾分隔符 content += tb[\"text\"] + tb[\"end\"] content_blocks.append(content) node_tbs = [] for node in gtree.current_nodes: if not node[\"units\"]: continue # 跳过没有块的根节点 x0 = node[\"x_left\"] x1 = node[\"x_right\"] y0 = gtree.current_rows[node[\"r_top\"]][0][0][1] y1 = gtree.current_rows[node[\"r_bottom\"]][0][0][3] node_tbs.append([[x0, y0], [x1, y0], [x1, y1], [x0, y1]]) new_blocks = [] for m in nodes_text_blocks: x1_list = [] y1_list = [] x2_list = [] y2_list = [] for n in m: box_ = n.get('box') x1_list.append(box_[0][0]) y1_list.append(box_[0][1]) x2_list.append(box_[2][0]) y2_list.append(box_[2][1]) x1_ = min(x1_list) y1_ = min(y1_list) x2_ = max(x2_list) y2_ = max(y2_list) new_blocks.append([[x1_, y1_], [x2_, y1_], [x2_, y2_], [x1_, y2_]]) return new_blocks, content_blocks 调用使用paddleOCR，将OCR结果转成算法输入的json_data 1234567891011121314151617181920212223242526272829if __name__ == '__main__': import base64 import requests test_image = './test/t2.png' origin_image = cv2.imread(test_image) encoded = cv2_base64(origin_image) json_data = &#123; \"img_b64\": encoded, \"lang\": \"cn\" &#125; response = requests.post('localhost:port/ai/ppocr/ai/ppocr', json=json_data).json() ocr_result = response.get('data') ocr_boxes = [line[0] for line in ocr_result[0]] ocr_txts = [line[1][0] for line in ocr_result[0]] ocr_scores = [line[1][1] for line in ocr_result[0]] json_data = [] for i in range(len(ocr_result[0])): json_data.append(&#123; \"box\": [[int(i[0]), int(i[1])] for i in ocr_boxes[i]], \"score\": ocr_scores[i], \"text\": ocr_txts[i] &#125;) so = StructureOCR(json_data) blocks, paragraphs = so.structure_ocr() 可视化当文本成段后，不能直接通过draw.text等方法将文本写作一行，而是要设置每行文本不得超过段落块的长度，并且总行数不能超出段落块总长度。需要设计算法，当每行文本超出长度限制时，自动添加换行符。 段落换行算法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162class VisualizeOCR: def __init__(self, im, boxes, texts): self.boxes = boxes self.texts = texts if isinstance(im, str): self.im = Image.open(im) self.im = np.ascontiguousarray(np.copy(im)) self.im = cv2.cvtColor(im, cv2.COLOR_RGB2BGR) else: self.im = np.ascontiguousarray(np.copy(im)) self.im = Image.fromarray(self.im) self.im = self.im.convert('RGBA') self.size = (int(self.im.size[0]), int(self.im.size[1])) def split_text(self, width, sentence, font, text_scale): # 按规定宽度分组 max_line_height, total_lines = 0, 0 allText = [] for sen in sentence.split('\\n'): paragraph, line_height, line_count = self.get_paragraph(sen, width, font, text_scale) max_line_height = max(line_height, max_line_height) total_lines += line_count allText.append((paragraph, line_count)) line_height = max_line_height total_height = total_lines * line_height return allText, total_height, line_height @staticmethod def get_paragraph(text, width, font, text_scale): # 字体像素较小时，换行效果不佳，每5个像素换行下移0.05个像素 text_size = 0 if text_scale &lt;= 15: text_size = 0.1 elif text_scale &lt;= 10: text_size = 0.15 elif text_scale &lt;= 5: text_size = 0.2 txt = Image.new('RGBA', (1033, 737), (255, 255, 255, 0)) draw = ImageDraw.Draw(txt) # 所有文字的段落 paragraph = \"\" # 宽度总和 sum_width = 0 # 行数 line_count = 1 # 行高 line_height = 0 for char in text: _, _, w, h = draw.textbbox((0, 0), char, font=font) sum_width += w if sum_width &gt; width: # 超过预设宽度就修改段落 以及当前行数 line_count += 1 line_count += text_size + 0.1 sum_width = 0 paragraph += '\\n' paragraph += char line_height = max(h, line_height) if not paragraph.endswith('\\n'): paragraph += '\\n' return paragraph, line_height, line_count 实现换行后，还需要考虑字体的大小，将文字锁定在文本框内。在pillow库中draw.text方法中中文字符的大小近似于其正方矩形像素块的边长。接下来需要考虑的参数变量有三个： 1.每行最多的字符x（x_num）; 2.总行数y（y_num）; 3.字体像素s（text_scale） 同时考虑到并不是每行都能够写满段落块的长度，可能存在提前换行、空行、结束等情况。可以计算段落的总换行符数量（count(‘\\n’)），将换行符出现的地方视作空行，并×2算作换行+留白部分产生的像素面积。那么已知常量包括： 1.换行次数b（blank_scale）； 2.总字数l（len(text)）； 3.段落框长度w（width_p，值取width*0.97，减少出界概率） 3.段落框宽度h（height） 设立方程组：{ ① (y-b)*x=l ② x*s = w ③ y*s = h { 转换到代码中：123(y_num - blank_scale) * x_num = len(text)x_num * text_scale = width_py_num * text_scale = height 简化方程组：123456789(y_num - blank_scale) * x_num = len(text)x_num * text_scale = width_py_num * text_scale = height===&gt;x_num = len(text) / (y_num - blank_scale)x_num = width_p / text_scalewidth_p / text_scale = len(text) / ((height / text_scale) - blank_scale)===&gt;len(text) * text_scale * text_scale + blank_scale * len(text) * text_scale - width_p * height = 0 解方程123456789def quadratic(a, b, c): n = b * b - 4 * a * c import math if n &gt;= 0: x1 = (-b + math.sqrt(n)) / (2 * a) x2 = (-b - math.sqrt(n)) / (2 * a) return x1 if x1 &gt; 0 else x2 else: raise 获取最佳像素值经过实际观察，对于中文字符，将计算得到的像素大小-2后视觉效果更佳。同时可以根据图像大小、单文本行不换行等因素，选择最佳字体大小 12345text_scale = min( int(quadratic(len(text), blank_scale * len(text), -width_p * height)) - 2, int(self.size[0] / 66), int(height/2)) 封装123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144class VisualizeOCR: def __init__(self, im, boxes, texts): self.boxes = boxes self.texts = texts if isinstance(im, str): self.im = Image.open(im) self.im = np.ascontiguousarray(np.copy(im)) self.im = cv2.cvtColor(im, cv2.COLOR_RGB2BGR) else: self.im = np.ascontiguousarray(np.copy(im)) self.im = Image.fromarray(self.im) self.im = self.im.convert('RGBA') self.size = (int(self.im.size[0]), int(self.im.size[1])) def split_text(self, width, sentence, font, text_scale): # 按规定宽度分组 max_line_height, total_lines = 0, 0 allText = [] for sen in sentence.split('\\n'): paragraph, line_height, line_count = self.get_paragraph(sen, width, font, text_scale) max_line_height = max(line_height, max_line_height) total_lines += line_count allText.append((paragraph, line_count)) line_height = max_line_height total_height = total_lines * line_height return allText, total_height, line_height @staticmethod def get_paragraph(text, width, font, text_scale): text_size = 0 if text_scale &lt;= 15: text_size = 0.1 elif text_scale &lt;= 10: text_size = 0.15 elif text_scale &lt;= 5: text_size = 0.2 txt = Image.new('RGBA', (1033, 737), (255, 255, 255, 0)) draw = ImageDraw.Draw(txt) # 所有文字的段落 paragraph = \"\" # 宽度总和 sum_width = 0 # 行数 line_count = 1 # 行高 line_height = 0 for char in text: _, _, w, h = draw.textbbox((0, 0), char, font=font) sum_width += w if sum_width &gt; width: # 超过预设宽度就修改段落 以及当前行数 line_count += 1 line_count += text_size + 0.1 sum_width = 0 paragraph += '\\n' paragraph += char line_height = max(h, line_height) if not paragraph.endswith('\\n'): paragraph += '\\n' return paragraph, line_height, line_count def visualize_ocr(self): im_canvas = Image.new('RGBA', self.size, (255, 255, 255, 1000)) for i, res in enumerate(self.texts): if self.boxes is not None: box = self.boxes[i] x, y = box[0][0], box[0][1] width = int(box[1][0] - box[0][0]) height = int(box[2][1] - box[1][1]) width_p = int((box[1][0] - box[0][0]) * 0.97) text = res if text == \"\": continue blank_line_count = text.count('\\n') blank_scale = blank_line_count * 2 if blank_line_count &gt; 1 else blank_line_count \"\"\" 方程组，求字体最大像素 x_num: x轴个数 y_num: y轴个数 text_scale: 文本像素 (y_num - blank_scale) * x_num = len(text) x_num * text_scale = width_p y_num * text_scale = height ===&gt; x_num = len(text) / (y_num - blank_scale) x_num = width_p / text_scale width_p / text_scale = len(text) / ((height / text_scale) - blank_scale) ===&gt; len(text) * text_scale * text_scale + blank_scale * len(text) * text_scale - width_p * height = 0 \"\"\" text_scale = min( int(quadratic(len(text), blank_scale * len(text), -width_p * height)) - 2, int(self.size[0] / 66), int(height/2) ) font = ImageFont.truetype(\"SourceHanSansCN-Medium.otf\", text_scale) draw = ImageDraw.Draw(im_canvas) paragraph, note_height, line_height = self.split_text(width_p, text, font, text_scale) for sen, line_count in paragraph: draw.text((x, y), sen, fill=(255, 0, 0), font=font) y += line_height * line_count draw.rectangle( ((box[0][0], box[0][1]), (box[2][0], box[2][1])), fill=None, outline=(139, 0, 139), width=1) im = image_join(self.im, im_canvas, 'x') im = im.convert('RGB') # 还原连续存储数组 im = np.ascontiguousarray(np.copy(im)) return imdef quadratic(a, b, c): n = b * b - 4 * a * c import math if n &gt;= 0: x1 = (-b + math.sqrt(n)) / (2 * a) x2 = (-b - math.sqrt(n)) / (2 * a) return x1 if x1 &gt; 0 else x2 else: return '该一元二次方程无解'def image_join(img1, img2, flag='y'): size1, size2 = img1.size, img2.size if flag == 'x': im = Image.new(\"RGB\", (size1[0] + size2[0], size1[1])) loc1, loc2 = (0, 0), (size1[0], 0) else: im = Image.new(\"RGB\", (size1[0], size2[1] + size1[1])) loc1, loc2 = (0, 0), (0, size1[1]) im.paste(img1, loc1) im.paste(img2, loc2) return im 调用12345678910111213141516171819202122232425262728293031323334353637383940414243444546if __name__ == '__main__': import base64 import requests test_image = './test/t2.png' origin_image = cv2.imread(test_image) encoded = cv2_base64(origin_image) json_data = &#123; \"img_b64\": encoded, \"lang\": \"cn\" &#125; response = requests.post('localhost:port/ai/ppocr', json=json_data).json() ocr_result = response.get('data') ocr_boxes = [line[0] for line in ocr_result[0]] ocr_txts = [line[1][0] for line in ocr_result[0]] ocr_scores = [line[1][1] for line in ocr_result[0]] json_data = [] for i in range(len(ocr_result[0])): json_data.append(&#123; \"box\": [[int(i[0]), int(i[1])] for i in ocr_boxes[i]], \"score\": ocr_scores[i], \"text\": ocr_txts[i] &#125;) so = StructureOCR(json_data) blocks, paragraphs = so.structure_ocr() vo = VisualizeOCR(origin_image, blocks, paragraphs) image = vo.visualize_ocr() \"\"\" 显示图像 \"\"\" # 适配显示器 if image.shape[0] &gt; 1080: mag = int(image.shape[0] / 1080) image = cv2.resize(image, (int(image.shape[1] / mag), int(image.shape[0] / mag))) if image.shape[1] &gt; 1920: mag = image.shape[1] / 1920 image = cv2.resize(image, (int(image.shape[1] / mag), int(image.shape[0] / mag))) cv2.imshow(\"image\", image) cv2.waitKey(-1) 效果演示：","categories":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/算法/"}],"tags":[{"name":"OCR","slug":"OCR","permalink":"http://yoursite.com/tags/OCR/"}]},{"title":"Surya OCR","slug":"Surya-OCR","date":"2024-07-08T09:26:27.000Z","updated":"2024-12-06T09:44:51.112Z","comments":true,"path":"2024/07/08/Surya-OCR/","link":"","permalink":"http://yoursite.com/2024/07/08/Surya-OCR/","excerpt":"环境准备版本：python3.9 + surya-ocr 0.4.15 模型准备：检测模型：surya_det3 识别模型：surya_rec 版面模型：surya_layout3 源码修改因首次使用下载模型被墙，提前将模型收录至模型文件夹并修改源码导入部分： (源码位置：...Python39/Lib/site-packages/surya/settings.py) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586from typing import Dict, Optionalfrom dotenv import find_dotenvfrom pydantic import computed_fieldfrom pydantic_settings import BaseSettingsimport torchimport osclass Settings(BaseSettings): # General TORCH_DEVICE: Optional[str] = None IMAGE_DPI: int = 96 IN_STREAMLIT: bool = False # Whether we're running in streamlit # Paths DATA_DIR: str = \"data\" RESULT_DIR: str = \"results\" BASE_DIR: str = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) FONT_DIR: str = os.path.join(BASE_DIR, \"static\", \"fonts\") @computed_field def TORCH_DEVICE_MODEL(self) -&gt; str: if self.TORCH_DEVICE is not None: return self.TORCH_DEVICE if torch.cuda.is_available(): return \"cuda\" if torch.backends.mps.is_available(): return \"mps\" return \"cpu\" # Text detection DETECTOR_BATCH_SIZE: Optional[int] = None # Defaults to 2 for CPU/MPS, 32 otherwise DETECTOR_MODEL_CHECKPOINT: str = r\"D:\\pycharmproject_2\\translate_plat\\surya_ocr\\models\\surya_det3\" DETECTOR_BENCH_DATASET_NAME: str = \"vikp/doclaynet_bench\" DETECTOR_IMAGE_CHUNK_HEIGHT: int = 1400 # Height at which to slice images vertically DETECTOR_TEXT_THRESHOLD: float = 0.6 # Threshold for text detection (above this is considered text) DETECTOR_BLANK_THRESHOLD: float = 0.35 # Threshold for blank space (below this is considered blank) DETECTOR_POSTPROCESSING_CPU_WORKERS: int = min(8, os.cpu_count()) # Number of workers for postprocessing DETECTOR_MIN_PARALLEL_THRESH: int = 3 # Minimum number of images before we parallelize # Text recognition RECOGNITION_MODEL_CHECKPOINT: str = r\"D:\\pycharmproject_2\\translate_plat\\surya_ocr\\models\\surya_rec\" RECOGNITION_MAX_TOKENS: int = 175 RECOGNITION_BATCH_SIZE: Optional[int] = None # Defaults to 8 for CPU/MPS, 256 otherwise RECOGNITION_IMAGE_SIZE: Dict = &#123;\"height\": 196, \"width\": 896&#125; RECOGNITION_RENDER_FONTS: Dict[str, str] = &#123; \"all\": os.path.join(FONT_DIR, \"GoNotoCurrent-Regular.ttf\"), \"zh\": os.path.join(FONT_DIR, \"GoNotoCJKCore.ttf\"), \"ja\": os.path.join(FONT_DIR, \"GoNotoCJKCore.ttf\"), \"ko\": os.path.join(FONT_DIR, \"GoNotoCJKCore.ttf\"), &#125; RECOGNITION_FONT_DL_BASE: str = \"https://github.com/satbyy/go-noto-universal/releases/download/v7.0\" RECOGNITION_BENCH_DATASET_NAME: str = \"vikp/rec_bench\" RECOGNITION_PAD_VALUE: int = 255 # Should be 0 or 255 RECOGNITION_STATIC_CACHE: bool = False # Static cache for torch compile RECOGNITION_MAX_LANGS: int = 4 # Layout LAYOUT_MODEL_CHECKPOINT: str = r\"D:\\pycharmproject_2\\translate_plat\\surya_ocr\\models\\surya_layout3\" LAYOUT_BENCH_DATASET_NAME: str = \"vikp/publaynet_bench\" # Ordering ORDER_MODEL_CHECKPOINT: str = \"vikp/surya_order\" ORDER_IMAGE_SIZE: Dict = &#123;\"height\": 1024, \"width\": 1024&#125; ORDER_MAX_BOXES: int = 256 ORDER_BATCH_SIZE: Optional[int] = None # Defaults to 4 for CPU/MPS, 32 otherwise ORDER_BENCH_DATASET_NAME: str = \"vikp/order_bench\" # Tesseract (for benchmarks only) TESSDATA_PREFIX: Optional[str] = None @computed_field @property def MODEL_DTYPE(self) -&gt; torch.dtype: return torch.float32 if self.TORCH_DEVICE_MODEL == \"cpu\" else torch.float16 class Config: env_file = find_dotenv(\"local.env\") extra = \"ignore\"settings = Settings()","text":"环境准备版本：python3.9 + surya-ocr 0.4.15 模型准备：检测模型：surya_det3 识别模型：surya_rec 版面模型：surya_layout3 源码修改因首次使用下载模型被墙，提前将模型收录至模型文件夹并修改源码导入部分： (源码位置：...Python39/Lib/site-packages/surya/settings.py) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586from typing import Dict, Optionalfrom dotenv import find_dotenvfrom pydantic import computed_fieldfrom pydantic_settings import BaseSettingsimport torchimport osclass Settings(BaseSettings): # General TORCH_DEVICE: Optional[str] = None IMAGE_DPI: int = 96 IN_STREAMLIT: bool = False # Whether we're running in streamlit # Paths DATA_DIR: str = \"data\" RESULT_DIR: str = \"results\" BASE_DIR: str = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) FONT_DIR: str = os.path.join(BASE_DIR, \"static\", \"fonts\") @computed_field def TORCH_DEVICE_MODEL(self) -&gt; str: if self.TORCH_DEVICE is not None: return self.TORCH_DEVICE if torch.cuda.is_available(): return \"cuda\" if torch.backends.mps.is_available(): return \"mps\" return \"cpu\" # Text detection DETECTOR_BATCH_SIZE: Optional[int] = None # Defaults to 2 for CPU/MPS, 32 otherwise DETECTOR_MODEL_CHECKPOINT: str = r\"D:\\pycharmproject_2\\translate_plat\\surya_ocr\\models\\surya_det3\" DETECTOR_BENCH_DATASET_NAME: str = \"vikp/doclaynet_bench\" DETECTOR_IMAGE_CHUNK_HEIGHT: int = 1400 # Height at which to slice images vertically DETECTOR_TEXT_THRESHOLD: float = 0.6 # Threshold for text detection (above this is considered text) DETECTOR_BLANK_THRESHOLD: float = 0.35 # Threshold for blank space (below this is considered blank) DETECTOR_POSTPROCESSING_CPU_WORKERS: int = min(8, os.cpu_count()) # Number of workers for postprocessing DETECTOR_MIN_PARALLEL_THRESH: int = 3 # Minimum number of images before we parallelize # Text recognition RECOGNITION_MODEL_CHECKPOINT: str = r\"D:\\pycharmproject_2\\translate_plat\\surya_ocr\\models\\surya_rec\" RECOGNITION_MAX_TOKENS: int = 175 RECOGNITION_BATCH_SIZE: Optional[int] = None # Defaults to 8 for CPU/MPS, 256 otherwise RECOGNITION_IMAGE_SIZE: Dict = &#123;\"height\": 196, \"width\": 896&#125; RECOGNITION_RENDER_FONTS: Dict[str, str] = &#123; \"all\": os.path.join(FONT_DIR, \"GoNotoCurrent-Regular.ttf\"), \"zh\": os.path.join(FONT_DIR, \"GoNotoCJKCore.ttf\"), \"ja\": os.path.join(FONT_DIR, \"GoNotoCJKCore.ttf\"), \"ko\": os.path.join(FONT_DIR, \"GoNotoCJKCore.ttf\"), &#125; RECOGNITION_FONT_DL_BASE: str = \"https://github.com/satbyy/go-noto-universal/releases/download/v7.0\" RECOGNITION_BENCH_DATASET_NAME: str = \"vikp/rec_bench\" RECOGNITION_PAD_VALUE: int = 255 # Should be 0 or 255 RECOGNITION_STATIC_CACHE: bool = False # Static cache for torch compile RECOGNITION_MAX_LANGS: int = 4 # Layout LAYOUT_MODEL_CHECKPOINT: str = r\"D:\\pycharmproject_2\\translate_plat\\surya_ocr\\models\\surya_layout3\" LAYOUT_BENCH_DATASET_NAME: str = \"vikp/publaynet_bench\" # Ordering ORDER_MODEL_CHECKPOINT: str = \"vikp/surya_order\" ORDER_IMAGE_SIZE: Dict = &#123;\"height\": 1024, \"width\": 1024&#125; ORDER_MAX_BOXES: int = 256 ORDER_BATCH_SIZE: Optional[int] = None # Defaults to 4 for CPU/MPS, 32 otherwise ORDER_BENCH_DATASET_NAME: str = \"vikp/order_bench\" # Tesseract (for benchmarks only) TESSDATA_PREFIX: Optional[str] = None @computed_field @property def MODEL_DTYPE(self) -&gt; torch.dtype: return torch.float32 if self.TORCH_DEVICE_MODEL == \"cpu\" else torch.float16 class Config: env_file = find_dotenv(\"local.env\") extra = \"ignore\"settings = Settings() 使用小语种OCR识别 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import cv2from PIL import Imagefrom surya.detection import batch_text_detectionfrom surya.layout import batch_layout_detectionfrom surya.model.detection.model import load_model, load_processorfrom surya.settings import settingsIMAGE_PATH = './test/7.png' ## 需检测图片地址DET_MODEL_PATH = './models/surya_det3' ## 模型参数保存地址LAYOUT_MODEL_PATH = './models/surya_layout3' ## 模型参数保存地址image = Image.open(IMAGE_PATH)surya_layout_model = load_model(checkpoint=settings.LAYOUT_MODEL_CHECKPOINT)surya_processor = load_processor(checkpoint=settings.LAYOUT_MODEL_CHECKPOINT)surya_det_model = load_model()surya_det_processor = load_processor()# layout_predictions is a list of dicts, one per imageline_predictions = batch_text_detection([image], surya_det_model, surya_det_processor)layout_predictions = batch_layout_detection([image], surya_layout_model, surya_processor, line_predictions)image = cv2.imread(IMAGE_PATH)def surya_layout2paddle_structure(surya_layout_res, im): paddle_structure_res = [] for item in surya_layout_res[0].bboxes: surya_layout_dict = dict(item) paddle_structure_res.append(&#123; 'bbox': [ surya_layout_dict['polygon'][0][0], surya_layout_dict['polygon'][0][1], surya_layout_dict['polygon'][2][0], surya_layout_dict['polygon'][2][1] ], 'type': surya_layout_dict['label'], 'img': im[ surya_layout_dict['polygon'][0][1]: surya_layout_dict['polygon'][2][1], surya_layout_dict['polygon'][0][0]: surya_layout_dict['polygon'][2][0] ] &#125;) return paddle_structure_resprint(surya_layout2paddle_structure(layout_predictions, image))res = []for i in layout_predictions[0].bboxes: res.append(dict(i))print(res) 识别结果： 12345678910Loaded detection model ./models/surya_det3 on device cpu with dtype torch.float32Loaded recognition model ./models/surya_rec on device cpu with dtype torch.float32okDetecting bboxes: 100%|██████████| 1/1 [00:05&lt;00:00, 5.15s/it]Recognizing Text: 100%|██████████| 1/1 [00:18&lt;00:00, 18.12s/it][OCRResult(text_lines=[TextLine(polygon=[[65.0, 12.0], [424.0, 11.0], [425.0, 49.0], [66.0, 50.0]], confidence=0.8231586813926697, text=&apos;Đ Phong Cách&apos;, bbox=[65.0, 12.0, 424.0, 49.0]), TextLine(polygon=[[146.0, 60.0], [345.0, 60.0], [345.0, 95.0], [146.0, 95.0]], confidence=0.9834306836128235, text=&apos;Khác Biêt&apos;, bbox=[146.0, 60.0, 345.0, 95.0]), TextLine(polygon=[[9.0, 114.0], [482.0, 114.0], [482.0, 126.0], [9.0, 126.0]], confidence=0.9684180021286011, text=&apos;Trên tay ché tác nquyên khői đẫn đầu xu hướng với thiết kế thân máy lièn mach, đô mòng ấn tương 8.5mm cùng&apos;, bbox=[9.0, 114.0, 482.0, 126.0]), TextLine(polygon=[[0.0, 132.0], [490.0, 132.0], [490.0, 143.0], [0.0, 143.0]], confidence=0.9850525856018066, text=&apos;kiểu dáng măt kính bóng mươt, sang trong từ Galaxy M30. Vừa văn hoàn hảo trong lòng bàn tay, thoả thích thể hiện&apos;, bbox=[0.0, 132.0, 490.0, 143.0]), TextLine(polygon=[[95.0, 149.0], [393.0, 149.0], [394.0, 160.0], [95.0, 160.0]], confidence=0.9769484400749207, text=&apos;phong cach thời thượng với hai phiên bản mảu Đen hoāc Xanh cá tính.&apos;, bbox=[95.0, 149.0, 393.0, 160.0])], languages=[&apos;en&apos;], image_bbox=[0.0, 0.0, 494.0, 182.0])][&apos;Đ Phong Cách&apos;, &apos;Khác Biêt&apos;, &apos;Trên tay ché tác nquyên khői đẫn đầu xu hướng với thiết kế thân máy lièn mach, đô mòng ấn tương 8.5mm cùng&apos;, &apos;kiểu dáng măt kính bóng mươt, sang trong từ Galaxy M30. Vừa văn hoàn hảo trong lòng bàn tay, thoả thích thể hiện&apos;, &apos;phong cach thời thượng với hai phiên bản mảu Đen hoāc Xanh cá tính.&apos;] 5[[[65.0, 12.0], [424.0, 11.0], [425.0, 49.0], [66.0, 50.0]], [[146.0, 60.0], [345.0, 60.0], [345.0, 95.0], [146.0, 95.0]], [[9.0, 114.0], [482.0, 114.0], [482.0, 126.0], [9.0, 126.0]], [[0.0, 132.0], [490.0, 132.0], [490.0, 143.0], [0.0, 143.0]], [[95.0, 149.0], [393.0, 149.0], [394.0, 160.0], [95.0, 160.0]]] 5[0.8231586813926697, 0.9834306836128235, 0.9684180021286011, 0.9850525856018066, 0.9769484400749207] 5 OCR版面分析未完待续","categories":[],"tags":[{"name":"OCR","slug":"OCR","permalink":"http://yoursite.com/tags/OCR/"}]},{"title":"doccano","slug":"doccano","date":"2024-06-28T02:42:26.000Z","updated":"2024-06-28T09:28:33.660Z","comments":true,"path":"2024/06/28/doccano/","link":"","permalink":"http://yoursite.com/2024/06/28/doccano/","excerpt":"Doccano是一种用于文本标注的开源工具，旨在简化和加速标注任务的进行。它提供了一个直观的用户界面，使标注人员能够轻松地对文本数据进行标注，并创建高质量的训练数据集用于机器学习和自然语言处理任务。 链接：https://github.com/doccano/doccano 一、安装部署环境操作系统：Centos7.9 python：3.10 doccano：1.6.2 pip安装注：百度源没有相应安装包 pip install doccano==1.6.2 -i https://pypi.tuna.tsinghua.edu.cn/simple 初始化doccano init 设置超级管理员账号密码doccano createuser --username admin --password 123456 启动服务doccano webserver --port 8000","text":"Doccano是一种用于文本标注的开源工具，旨在简化和加速标注任务的进行。它提供了一个直观的用户界面，使标注人员能够轻松地对文本数据进行标注，并创建高质量的训练数据集用于机器学习和自然语言处理任务。 链接：https://github.com/doccano/doccano 一、安装部署环境操作系统：Centos7.9 python：3.10 doccano：1.6.2 pip安装注：百度源没有相应安装包 pip install doccano==1.6.2 -i https://pypi.tuna.tsinghua.edu.cn/simple 初始化doccano init 设置超级管理员账号密码doccano createuser --username admin --password 123456 启动服务doccano webserver --port 8000 ERNIE-UIE 关系抽取微调数据标注创建序列标注任务 导入增量训练数据集 注：如果导入不成功，长时间转圈，需要去控制台执行doccano task 创建实体标签和关系标签 数据标注 数据集导出 12345&#123;\"id\": 11, \"text\": \"钢筋调直宜采用机械方法,也可以采用冷拉方法\", \"relations\": [&#123;\"id\": 1, \"from_id\": 45, \"to_id\": 44, \"type\": \"材料\"&#125;, &#123;\"id\": 2, \"from_id\": 46, \"to_id\": 44, \"type\": \"材料\"&#125;], \"entities\": [&#123;\"id\": 44, \"start_offset\": 0, \"end_offset\": 4, \"label\": \"工程\"&#125;, &#123;\"id\": 45, \"start_offset\": 7, \"end_offset\": 11, \"label\": \"工艺\"&#125;, &#123;\"id\": 46, \"start_offset\": 17, \"end_offset\": 21, \"label\": \"工艺\"&#125;]&#125;&#123;\"id\": 12, \"text\": \"受力钢筋的接头形式应按设计要求采用,若设计无要求时,钢筋宜采用焊接接头和机械连接接头,也可采用绑扎接头。\", \"relations\": [&#123;\"id\": 3, \"from_id\": 53, \"to_id\": 22, \"type\": \"材料\"&#125;, &#123;\"id\": 4, \"from_id\": 54, \"to_id\": 22, \"type\": \"材料\"&#125;, &#123;\"id\": 5, \"from_id\": 55, \"to_id\": 22, \"type\": \"材料\"&#125;], \"entities\": [&#123;\"id\": 22, \"start_offset\": 0, \"end_offset\": 9, \"label\": \"工程\"&#125;, &#123;\"id\": 53, \"start_offset\": 31, \"end_offset\": 35, \"label\": \"工艺\"&#125;, &#123;\"id\": 54, \"start_offset\": 36, \"end_offset\": 42, \"label\": \"工艺\"&#125;, &#123;\"id\": 55, \"start_offset\": 47, \"end_offset\": 51, \"label\": \"工艺\"&#125;]&#125;&#123;\"id\": 13, \"text\": \"多层非焊接钢筋骨架的各层钢筋之间,应保持层距准确,宜采用短钢筋支垫。\", \"relations\": [&#123;\"id\": 6, \"from_id\": 60, \"to_id\": 59, \"type\": \"工艺\"&#125;], \"entities\": [&#123;\"id\": 59, \"start_offset\": 0, \"end_offset\": 9, \"label\": \"工程\"&#125;, &#123;\"id\": 60, \"start_offset\": 28, \"end_offset\": 33, \"label\": \"工艺\"&#125;]&#125;&#123;\"id\": 14, \"text\": \"预制桩的修筑工艺包括一体化成孔、自灌注。\", \"relations\": [&#123;\"id\": 7, \"from_id\": 62, \"to_id\": 28, \"type\": \"工艺\"&#125;, &#123;\"id\": 8, \"from_id\": 61, \"to_id\": 28, \"type\": \"工艺\"&#125;], \"entities\": [&#123;\"id\": 28, \"start_offset\": 0, \"end_offset\": 3, \"label\": \"工程\"&#125;, &#123;\"id\": 61, \"start_offset\": 10, \"end_offset\": 15, \"label\": \"工艺\"&#125;, &#123;\"id\": 62, \"start_offset\": 16, \"end_offset\": 19, \"label\": \"工艺\"&#125;]&#125;&#123;\"id\": 15, \"text\": \"目前我国水运工程的模板用材已向多样化发展,除钢材和木材外,胶木板、竹胶板、塑料等已得到广泛运用,并取得了较好的技术经济效益。\", \"relations\": [&#123;\"id\": 9, \"from_id\": 63, \"to_id\": 52, \"type\": \"材料\"&#125;, &#123;\"id\": 10, \"from_id\": 64, \"to_id\": 52, \"type\": \"材料\"&#125;, &#123;\"id\": 11, \"from_id\": 65, \"to_id\": 52, \"type\": \"材料\"&#125;, &#123;\"id\": 12, \"from_id\": 67, \"to_id\": 52, \"type\": \"材料\"&#125;], \"entities\": [&#123;\"id\": 52, \"start_offset\": 4, \"end_offset\": 8, \"label\": \"工程\"&#125;, &#123;\"id\": 63, \"start_offset\": 22, \"end_offset\": 24, \"label\": \"材料\"&#125;, &#123;\"id\": 64, \"start_offset\": 25, \"end_offset\": 27, \"label\": \"材料\"&#125;, &#123;\"id\": 65, \"start_offset\": 29, \"end_offset\": 32, \"label\": \"材料\"&#125;, &#123;\"id\": 66, \"start_offset\": 33, \"end_offset\": 36, \"label\": \"材料\"&#125;, &#123;\"id\": 67, \"start_offset\": 37, \"end_offset\": 39, \"label\": \"材料\"&#125;]&#125; 数据集格式转换参考https://github.com/PaddlePaddle/PaddleNLP/tree/develop/legacy/model_zoo/uie 进入PaddleNLP-UIE路径，将json文件放置在路径下，创建data文件夹用于存储数据集，执行： 123456python doccano.py \\ --doccano_file ./data/doccano_ext.json \\ --task_type ext \\ --save_dir ./data \\ --splits 0.8 0.2 0 \\ --schema_lang ch 由于测试样本较少（5条），未自动化分验证集dev，手动将测试集内容复制到验证集当中。 开启训练在UIE路径下创建checkpoint/model_best用于存放模型 执行（GPU）： 123456789101112131415161718192021222324252627export finetuned_model=./checkpoint/model_bestpython -u -m paddle.distributed.launch --gpus \"0,1\" finetune.py \\ --device gpu \\ --logging_steps 10 \\ --save_steps 100 \\ --eval_steps 100 \\ --seed 42 \\ --model_name_or_path uie-base \\ --output_dir $finetuned_model \\ --train_path data/train.txt \\ --dev_path data/dev.txt \\ --max_seq_length 512 \\ --per_device_eval_batch_size 16 \\ --per_device_train_batch_size 16 \\ --num_train_epochs 100 \\ --learning_rate 1e-5 \\ --do_train \\ --do_eval \\ --do_export \\ --export_model_dir $finetuned_model \\ --label_names \"start_positions\" \"end_positions\" \\ --overwrite_output_dir \\ --disable_tqdm True \\ --metric_for_best_model eval_f1 \\ --load_best_model_at_end True \\ --save_total_limit 1 \\ 训练结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379(py39_ppner_2_7_2) [root@jdz uie]# python -u -m paddle.distributed.launch --gpus \"1\" finetune.py --device gpu --logging_steps 10 --save_steps 100 --eval_steps 100 --seed 42 --model_name_or_path uie-base --output_dir $finetuned_model --train_path data/train.txt --dev_path data/dev.txt --max_seq_length 512 --per_device_eval_batch_size 16 --per_device_train_batch_size 16 --num_train_epochs 100 --learning_rate 1e-5 --do_train --do_eval --do_export --export_model_dir $finetuned_model --label_names \"start_positions\" \"end_positions\" --overwrite_output_dir --disable_tqdm True --metric_for_best_model eval_f1 --load_best_model_at_end True --save_total_limit 1 LAUNCH INFO 2024-06-26 18:05:23,778 ----------- Configuration ----------------------LAUNCH INFO 2024-06-26 18:05:23,779 auto_parallel_config: NoneLAUNCH INFO 2024-06-26 18:05:23,779 auto_tuner_json: NoneLAUNCH INFO 2024-06-26 18:05:23,779 devices: 1LAUNCH INFO 2024-06-26 18:05:23,779 elastic_level: -1LAUNCH INFO 2024-06-26 18:05:23,779 elastic_timeout: 30LAUNCH INFO 2024-06-26 18:05:23,779 enable_gpu_log: TrueLAUNCH INFO 2024-06-26 18:05:23,779 gloo_port: 6767LAUNCH INFO 2024-06-26 18:05:23,779 host: NoneLAUNCH INFO 2024-06-26 18:05:23,779 ips: NoneLAUNCH INFO 2024-06-26 18:05:23,779 job_id: defaultLAUNCH INFO 2024-06-26 18:05:23,779 legacy: FalseLAUNCH INFO 2024-06-26 18:05:23,779 log_dir: logLAUNCH INFO 2024-06-26 18:05:23,780 log_level: INFOLAUNCH INFO 2024-06-26 18:05:23,780 log_overwrite: FalseLAUNCH INFO 2024-06-26 18:05:23,780 master: NoneLAUNCH INFO 2024-06-26 18:05:23,780 max_restart: 3LAUNCH INFO 2024-06-26 18:05:23,780 nnodes: 1LAUNCH INFO 2024-06-26 18:05:23,780 nproc_per_node: NoneLAUNCH INFO 2024-06-26 18:05:23,780 rank: -1LAUNCH INFO 2024-06-26 18:05:23,780 run_mode: collectiveLAUNCH INFO 2024-06-26 18:05:23,780 server_num: NoneLAUNCH INFO 2024-06-26 18:05:23,780 servers: LAUNCH INFO 2024-06-26 18:05:23,780 sort_ip: FalseLAUNCH INFO 2024-06-26 18:05:23,780 start_port: 6070LAUNCH INFO 2024-06-26 18:05:23,780 trainer_num: NoneLAUNCH INFO 2024-06-26 18:05:23,780 trainers: LAUNCH INFO 2024-06-26 18:05:23,780 training_script: finetune.pyLAUNCH INFO 2024-06-26 18:05:23,781 training_script_args: ['--device', 'gpu', '--logging_steps', '10', '--save_steps', '100', '--eval_steps', '100', '--seed', '42', '--model_name_or_path', 'uie-base', '--output_dir', './checkpoint/model_best', '--train_path', 'data/train.txt', '--dev_path', 'data/dev.txt', '--max_seq_length', '512', '--per_device_eval_batch_size', '16', '--per_device_train_batch_size', '16', '--num_train_epochs', '100', '--learning_rate', '1e-5', '--do_train', '--do_eval', '--do_export', '--export_model_dir', './checkpoint/model_best', '--label_names', 'start_positions', 'end_positions', '--overwrite_output_dir', '--disable_tqdm', 'True', '--metric_for_best_model', 'eval_f1', '--load_best_model_at_end', 'True', '--save_total_limit', '1']LAUNCH INFO 2024-06-26 18:05:23,781 with_gloo: 1LAUNCH INFO 2024-06-26 18:05:23,781 --------------------------------------------------LAUNCH INFO 2024-06-26 18:05:23,782 Job: default, mode collective, replicas 1[1:1], elastic FalseLAUNCH INFO 2024-06-26 18:05:23,797 Run Pod: pwbjet, replicas 1, status readyLAUNCH INFO 2024-06-26 18:05:23,824 Watching Pod: pwbjet, replicas 1, status running/root/anaconda3/envs/py39_ppner_2_7_2/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils. warnings.warn(\"Setuptools is replacing distutils.\")[2024-06-26 18:05:27,933] [ WARNING] - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[2024-06-26 18:05:27,933] [ INFO] - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[2024-06-26 18:05:27,934] [ INFO] - ============================================================[2024-06-26 18:05:27,934] [ INFO] - Model Configuration Arguments [2024-06-26 18:05:27,934] [ INFO] - paddle commit id :fbf852dd832bc0e63ae31cd4aa37defd829e4c03[2024-06-26 18:05:27,934] [ INFO] - export_model_dir :./checkpoint/model_best[2024-06-26 18:05:27,934] [ INFO] - model_name_or_path :uie-base[2024-06-26 18:05:27,934] [ INFO] - multilingual :False[2024-06-26 18:05:27,934] [ INFO] - [2024-06-26 18:05:27,934] [ INFO] - ============================================================[2024-06-26 18:05:27,934] [ INFO] - Data Configuration Arguments [2024-06-26 18:05:27,934] [ INFO] - paddle commit id :fbf852dd832bc0e63ae31cd4aa37defd829e4c03[2024-06-26 18:05:27,935] [ INFO] - dev_path :data/dev.txt[2024-06-26 18:05:27,935] [ INFO] - dynamic_max_length :None[2024-06-26 18:05:27,935] [ INFO] - max_seq_length :512[2024-06-26 18:05:27,935] [ INFO] - train_path :data/train.txt[2024-06-26 18:05:27,935] [ INFO] - [2024-06-26 18:05:27,935] [ WARNING] - Process rank: -1, device: gpu, world_size: 1, distributed training: False, 16-bits training: False[2024-06-26 18:05:27,935] [ INFO] - We are using (&lt;class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'&gt;, False) to load 'uie-base'.[2024-06-26 18:05:27,936] [ INFO] - Already cached /root/.paddlenlp/models/uie-base/ernie_3.0_base_zh_vocab.txt[2024-06-26 18:05:27,968] [ INFO] - tokenizer config file saved in /root/.paddlenlp/models/uie-base/tokenizer_config.json[2024-06-26 18:05:27,969] [ INFO] - Special tokens file saved in /root/.paddlenlp/models/uie-base/special_tokens_map.json[2024-06-26 18:05:27,970] [ INFO] - Already cached /root/.paddlenlp/models/uie-base/model_state.pdparams[2024-06-26 18:05:27,970] [ INFO] - Loading weights file model_state.pdparams from cache at /root/.paddlenlp/models/uie-base/model_state.pdparams[2024-06-26 18:05:28,898] [ INFO] - Loaded weights file from disk, setting weights to model.W0626 18:05:29.062965 285242 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.6, Driver API Version: 12.2, Runtime API Version: 12.0W0626 18:05:29.064332 285242 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.[2024-06-26 18:05:30,516] [ INFO] - All model checkpoint weights were used when initializing UIE.[2024-06-26 18:05:30,517] [ INFO] - All the weights of UIE were initialized from the model checkpoint at uie-base.If your task is similar to the task the model of the checkpoint was trained on, you can already use UIE for predictions without further training.[2024-06-26 18:05:30,562] [ INFO] - The global seed is set to 42, local seed is set to 43 and random seed is set to 42.[2024-06-26 18:05:30,655] [ DEBUG] - ============================================================[2024-06-26 18:05:30,655] [ DEBUG] - Training Configuration Arguments [2024-06-26 18:05:30,656] [ DEBUG] - paddle commit id : fbf852dd832bc0e63ae31cd4aa37defd829e4c03[2024-06-26 18:05:30,656] [ DEBUG] - paddlenlp commit id : b39e701e21d11ff66ac3abfc81d384b6af8f8240[2024-06-26 18:05:30,656] [ DEBUG] - _no_sync_in_gradient_accumulation: True[2024-06-26 18:05:30,656] [ DEBUG] - activation_quantize_type : None[2024-06-26 18:05:30,656] [ DEBUG] - adam_beta1 : 0.9[2024-06-26 18:05:30,656] [ DEBUG] - adam_beta2 : 0.999[2024-06-26 18:05:30,656] [ DEBUG] - adam_epsilon : 1e-08[2024-06-26 18:05:30,656] [ DEBUG] - algo_list : None[2024-06-26 18:05:30,656] [ DEBUG] - amp_custom_black_list : None[2024-06-26 18:05:30,656] [ DEBUG] - amp_custom_white_list : None[2024-06-26 18:05:30,656] [ DEBUG] - amp_master_grad : False[2024-06-26 18:05:30,656] [ DEBUG] - batch_num_list : None[2024-06-26 18:05:30,657] [ DEBUG] - batch_size_list : None[2024-06-26 18:05:30,657] [ DEBUG] - bf16 : False[2024-06-26 18:05:30,657] [ DEBUG] - bf16_full_eval : False[2024-06-26 18:05:30,657] [ DEBUG] - bias_correction : False[2024-06-26 18:05:30,657] [ DEBUG] - current_device : gpu:1[2024-06-26 18:05:30,657] [ DEBUG] - data_parallel_rank : 0[2024-06-26 18:05:30,657] [ DEBUG] - dataloader_drop_last : False[2024-06-26 18:05:30,657] [ DEBUG] - dataloader_num_workers : 0[2024-06-26 18:05:30,657] [ DEBUG] - dataset_rank : 0[2024-06-26 18:05:30,657] [ DEBUG] - dataset_world_size : 1[2024-06-26 18:05:30,657] [ DEBUG] - device : gpu[2024-06-26 18:05:30,657] [ DEBUG] - disable_tqdm : True[2024-06-26 18:05:30,657] [ DEBUG] - distributed_dataloader : False[2024-06-26 18:05:30,657] [ DEBUG] - do_compress : False[2024-06-26 18:05:30,658] [ DEBUG] - do_eval : True[2024-06-26 18:05:30,658] [ DEBUG] - do_export : True[2024-06-26 18:05:30,658] [ DEBUG] - do_predict : False[2024-06-26 18:05:30,658] [ DEBUG] - do_train : True[2024-06-26 18:05:30,658] [ DEBUG] - eval_accumulation_steps : None[2024-06-26 18:05:30,658] [ DEBUG] - eval_batch_size : 16[2024-06-26 18:05:30,658] [ DEBUG] - eval_steps : 100[2024-06-26 18:05:30,658] [ DEBUG] - evaluation_strategy : IntervalStrategy.STEPS[2024-06-26 18:05:30,658] [ DEBUG] - flatten_param_grads : False[2024-06-26 18:05:30,658] [ DEBUG] - force_reshard_pp : False[2024-06-26 18:05:30,658] [ DEBUG] - fp16 : False[2024-06-26 18:05:30,658] [ DEBUG] - fp16_full_eval : False[2024-06-26 18:05:30,658] [ DEBUG] - fp16_opt_level : O1[2024-06-26 18:05:30,658] [ DEBUG] - gradient_accumulation_steps : 1[2024-06-26 18:05:30,659] [ DEBUG] - greater_is_better : True[2024-06-26 18:05:30,659] [ DEBUG] - hybrid_parallel_topo_order : None[2024-06-26 18:05:30,659] [ DEBUG] - ignore_data_skip : False[2024-06-26 18:05:30,659] [ DEBUG] - ignore_load_lr_and_optim : False[2024-06-26 18:05:30,659] [ DEBUG] - input_dtype : int64[2024-06-26 18:05:30,659] [ DEBUG] - input_infer_model_path : None[2024-06-26 18:05:30,659] [ DEBUG] - label_names : ['start_positions', 'end_positions'][2024-06-26 18:05:30,659] [ DEBUG] - lazy_data_processing : True[2024-06-26 18:05:30,659] [ DEBUG] - learning_rate : 1e-05[2024-06-26 18:05:30,659] [ DEBUG] - load_best_model_at_end : True[2024-06-26 18:05:30,659] [ DEBUG] - load_sharded_model : False[2024-06-26 18:05:30,659] [ DEBUG] - local_process_index : 0[2024-06-26 18:05:30,659] [ DEBUG] - local_rank : -1[2024-06-26 18:05:30,659] [ DEBUG] - log_level : -1[2024-06-26 18:05:30,660] [ DEBUG] - log_level_replica : -1[2024-06-26 18:05:30,660] [ DEBUG] - log_on_each_node : True[2024-06-26 18:05:30,660] [ DEBUG] - logging_dir : ./checkpoint/model_best/runs/Jun26_18-05-27_jdz[2024-06-26 18:05:30,660] [ DEBUG] - logging_first_step : False[2024-06-26 18:05:30,660] [ DEBUG] - logging_steps : 10[2024-06-26 18:05:30,660] [ DEBUG] - logging_strategy : IntervalStrategy.STEPS[2024-06-26 18:05:30,660] [ DEBUG] - logical_process_index : 0[2024-06-26 18:05:30,660] [ DEBUG] - lr_end : 1e-07[2024-06-26 18:05:30,660] [ DEBUG] - lr_scheduler_type : SchedulerType.LINEAR[2024-06-26 18:05:30,660] [ DEBUG] - max_evaluate_steps : -1[2024-06-26 18:05:30,660] [ DEBUG] - max_grad_norm : 1.0[2024-06-26 18:05:30,660] [ DEBUG] - max_steps : -1[2024-06-26 18:05:30,660] [ DEBUG] - metric_for_best_model : eval_f1[2024-06-26 18:05:30,660] [ DEBUG] - minimum_eval_times : None[2024-06-26 18:05:30,660] [ DEBUG] - moving_rate : 0.9[2024-06-26 18:05:30,661] [ DEBUG] - no_cuda : False[2024-06-26 18:05:30,661] [ DEBUG] - num_cycles : 0.5[2024-06-26 18:05:30,661] [ DEBUG] - num_train_epochs : 100.0[2024-06-26 18:05:30,661] [ DEBUG] - onnx_format : True[2024-06-26 18:05:30,661] [ DEBUG] - optim : OptimizerNames.ADAMW[2024-06-26 18:05:30,661] [ DEBUG] - optimizer_name_suffix : None[2024-06-26 18:05:30,661] [ DEBUG] - output_dir : ./checkpoint/model_best[2024-06-26 18:05:30,661] [ DEBUG] - overwrite_output_dir : True[2024-06-26 18:05:30,661] [ DEBUG] - past_index : -1[2024-06-26 18:05:30,661] [ DEBUG] - per_device_eval_batch_size : 16[2024-06-26 18:05:30,661] [ DEBUG] - per_device_train_batch_size : 16[2024-06-26 18:05:30,661] [ DEBUG] - pipeline_parallel_config : [2024-06-26 18:05:30,661] [ DEBUG] - pipeline_parallel_degree : -1[2024-06-26 18:05:30,661] [ DEBUG] - pipeline_parallel_rank : 0[2024-06-26 18:05:30,662] [ DEBUG] - power : 1.0[2024-06-26 18:05:30,662] [ DEBUG] - prediction_loss_only : False[2024-06-26 18:05:30,662] [ DEBUG] - process_index : 0[2024-06-26 18:05:30,662] [ DEBUG] - prune_embeddings : False[2024-06-26 18:05:30,662] [ DEBUG] - recompute : False[2024-06-26 18:05:30,662] [ DEBUG] - remove_unused_columns : True[2024-06-26 18:05:30,662] [ DEBUG] - report_to : ['visualdl'][2024-06-26 18:05:30,662] [ DEBUG] - resume_from_checkpoint : None[2024-06-26 18:05:30,662] [ DEBUG] - round_type : round[2024-06-26 18:05:30,662] [ DEBUG] - run_name : ./checkpoint/model_best[2024-06-26 18:05:30,662] [ DEBUG] - save_on_each_node : False[2024-06-26 18:05:30,662] [ DEBUG] - save_sharded_model : False[2024-06-26 18:05:30,662] [ DEBUG] - save_steps : 100[2024-06-26 18:05:30,662] [ DEBUG] - save_strategy : IntervalStrategy.STEPS[2024-06-26 18:05:30,663] [ DEBUG] - save_total_limit : 1[2024-06-26 18:05:30,663] [ DEBUG] - scale_loss : 32768[2024-06-26 18:05:30,663] [ DEBUG] - seed : 42[2024-06-26 18:05:30,663] [ DEBUG] - sep_parallel_degree : -1[2024-06-26 18:05:30,663] [ DEBUG] - sharding : [][2024-06-26 18:05:30,663] [ DEBUG] - sharding_degree : -1[2024-06-26 18:05:30,663] [ DEBUG] - sharding_parallel_config : [2024-06-26 18:05:30,663] [ DEBUG] - sharding_parallel_degree : -1[2024-06-26 18:05:30,663] [ DEBUG] - sharding_parallel_rank : 0[2024-06-26 18:05:30,663] [ DEBUG] - should_load_dataset : True[2024-06-26 18:05:30,663] [ DEBUG] - should_load_sharding_stage1_model: False[2024-06-26 18:05:30,663] [ DEBUG] - should_log : True[2024-06-26 18:05:30,663] [ DEBUG] - should_save : True[2024-06-26 18:05:30,663] [ DEBUG] - should_save_model_state : True[2024-06-26 18:05:30,664] [ DEBUG] - should_save_sharding_stage1_model: False[2024-06-26 18:05:30,664] [ DEBUG] - skip_memory_metrics : True[2024-06-26 18:05:30,664] [ DEBUG] - skip_profile_timer : True[2024-06-26 18:05:30,664] [ DEBUG] - strategy : dynabert+ptq[2024-06-26 18:05:30,664] [ DEBUG] - tensor_parallel_config : [2024-06-26 18:05:30,664] [ DEBUG] - tensor_parallel_degree : -1[2024-06-26 18:05:30,664] [ DEBUG] - tensor_parallel_rank : 0[2024-06-26 18:05:30,664] [ DEBUG] - to_static : False[2024-06-26 18:05:30,664] [ DEBUG] - train_batch_size : 16[2024-06-26 18:05:30,664] [ DEBUG] - unified_checkpoint : False[2024-06-26 18:05:30,664] [ DEBUG] - unified_checkpoint_config : [2024-06-26 18:05:30,664] [ DEBUG] - use_auto_parallel : False[2024-06-26 18:05:30,664] [ DEBUG] - use_hybrid_parallel : False[2024-06-26 18:05:30,664] [ DEBUG] - use_pact : True[2024-06-26 18:05:30,665] [ DEBUG] - warmup_ratio : 0.1[2024-06-26 18:05:30,665] [ DEBUG] - warmup_steps : 0[2024-06-26 18:05:30,665] [ DEBUG] - weight_decay : 0.0[2024-06-26 18:05:30,665] [ DEBUG] - weight_name_suffix : None[2024-06-26 18:05:30,665] [ DEBUG] - weight_quantize_type : channel_wise_abs_max[2024-06-26 18:05:30,665] [ DEBUG] - width_mult_list : None[2024-06-26 18:05:30,665] [ DEBUG] - world_size : 1[2024-06-26 18:05:30,665] [ DEBUG] - [2024-06-26 18:05:30,666] [ INFO] - Starting training from resume_from_checkpoint : None/root/anaconda3/envs/py39_ppner_2_7_2/lib/python3.9/site-packages/paddle/distributed/parallel.py:410: UserWarning: The program will return to single-card operation. Please check 1, whether you use spawn or fleetrun to start the program. 2, Whether it is a multi-card program. 3, Is the current environment multi-card. warnings.warn([2024-06-26 18:05:30,667] [ INFO] - [timelog] checkpoint loading time: 0.00s (2024-06-26 18:05:30) [2024-06-26 18:05:30,667] [ INFO] - ***** Running training *****[2024-06-26 18:05:30,667] [ INFO] - Num examples = 68[2024-06-26 18:05:30,667] [ INFO] - Num Epochs = 100[2024-06-26 18:05:30,667] [ INFO] - Instantaneous batch size per device = 16[2024-06-26 18:05:30,668] [ INFO] - Total train batch size (w. parallel, distributed &amp; accumulation) = 16[2024-06-26 18:05:30,668] [ INFO] - Gradient Accumulation steps = 1[2024-06-26 18:05:30,668] [ INFO] - Total optimization steps = 500[2024-06-26 18:05:30,668] [ INFO] - Total num train samples = 6,800[2024-06-26 18:05:30,670] [ DEBUG] - Number of trainable parameters = 117,946,370 (per device)/root/anaconda3/envs/py39_ppner_2_7_2/lib/python3.9/site-packages/paddlenlp/transformers/tokenizer_utils_base.py:2538: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead. warnings.warn(/root/anaconda3/envs/py39_ppner_2_7_2/lib/python3.9/site-packages/paddlenlp/transformers/tokenizer_utils_base.py:1938: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert). warnings.warn([2024-06-26 18:05:34,727] [ INFO] - loss: 0.00208795, learning_rate: 1e-05, global_step: 10, interval_runtime: 4.0565, interval_samples_per_second: 39.44293710564109, interval_steps_per_second: 2.4651835691025683, progress_or_epoch: 2.0[2024-06-26 18:05:37,278] [ INFO] - loss: 0.00129266, learning_rate: 1e-05, global_step: 20, interval_runtime: 2.55, interval_samples_per_second: 62.74611413497253, interval_steps_per_second: 3.921632133435783, progress_or_epoch: 4.0[2024-06-26 18:05:39,833] [ INFO] - loss: 0.00092623, learning_rate: 1e-05, global_step: 30, interval_runtime: 2.5559, interval_samples_per_second: 62.599374107598685, interval_steps_per_second: 3.912460881724918, progress_or_epoch: 6.0[2024-06-26 18:05:42,413] [ INFO] - loss: 0.00058895, learning_rate: 1e-05, global_step: 40, interval_runtime: 2.5801, interval_samples_per_second: 62.012657597381434, interval_steps_per_second: 3.8757910998363396, progress_or_epoch: 8.0[2024-06-26 18:05:44,946] [ INFO] - loss: 0.00047016, learning_rate: 1e-05, global_step: 50, interval_runtime: 2.5327, interval_samples_per_second: 63.17332385701746, interval_steps_per_second: 3.948332741063591, progress_or_epoch: 10.0[2024-06-26 18:05:47,514] [ INFO] - loss: 0.00031726, learning_rate: 1e-05, global_step: 60, interval_runtime: 2.5675, interval_samples_per_second: 62.316911606640005, interval_steps_per_second: 3.8948069754150003, progress_or_epoch: 12.0[2024-06-26 18:05:50,081] [ INFO] - loss: 0.00024869, learning_rate: 1e-05, global_step: 70, interval_runtime: 2.5673, interval_samples_per_second: 62.32305770213569, interval_steps_per_second: 3.8951911063834808, progress_or_epoch: 14.0[2024-06-26 18:05:52,658] [ INFO] - loss: 0.00041933, learning_rate: 1e-05, global_step: 80, interval_runtime: 2.5765, interval_samples_per_second: 62.10074334302723, interval_steps_per_second: 3.881296458939202, progress_or_epoch: 16.0[2024-06-26 18:05:55,223] [ INFO] - loss: 0.00017784, learning_rate: 1e-05, global_step: 90, interval_runtime: 2.5647, interval_samples_per_second: 62.38556549647052, interval_steps_per_second: 3.8990978435294075, progress_or_epoch: 18.0[2024-06-26 18:05:57,816] [ INFO] - loss: 0.00019145, learning_rate: 1e-05, global_step: 100, interval_runtime: 2.5931, interval_samples_per_second: 61.70326891265709, interval_steps_per_second: 3.856454307041068, progress_or_epoch: 20.0[2024-06-26 18:05:57,817] [ INFO] - ***** Running Evaluation *****[2024-06-26 18:05:57,817] [ INFO] - Num examples = 4[2024-06-26 18:05:57,817] [ INFO] - Total prediction steps = 1[2024-06-26 18:05:57,817] [ INFO] - Pre device batch size = 16[2024-06-26 18:05:57,818] [ INFO] - Total Batch size = 16[2024-06-26 18:05:57,889] [ INFO] - eval_loss: 0.005224619060754776, eval_precision: 0.4, eval_recall: 0.4, eval_f1: 0.4000000000000001, eval_runtime: 0.0705, eval_samples_per_second: 56.74937846074747, eval_steps_per_second: 14.187344615186868, progress_or_epoch: 20.0[2024-06-26 18:05:57,889] [ INFO] - Saving model checkpoint to ./checkpoint/model_best/checkpoint-100[2024-06-26 18:05:57,890] [ INFO] - tokenizer config file saved in ./checkpoint/model_best/checkpoint-100/tokenizer_config.json[2024-06-26 18:05:57,890] [ INFO] - Special tokens file saved in ./checkpoint/model_best/checkpoint-100/special_tokens_map.json[2024-06-26 18:05:57,902] [ INFO] - Configuration saved in ./checkpoint/model_best/checkpoint-100/config.json[2024-06-26 18:06:00,646] [ INFO] - Model weights saved in ./checkpoint/model_best/checkpoint-100/model_state.pdparams[2024-06-26 18:06:00,648] [ INFO] - Saving optimizer files.[2024-06-26 18:06:07,375] [ INFO] - [timelog] checkpoint saving time: 9.48s (2024-06-26 18:06:07) [2024-06-26 18:06:09,924] [ INFO] - loss: 0.00022237, learning_rate: 1e-05, global_step: 110, interval_runtime: 12.1083, interval_samples_per_second: 13.21409688201894, interval_steps_per_second: 0.8258810551261837, progress_or_epoch: 22.0[2024-06-26 18:06:12,478] [ INFO] - loss: 0.00017666, learning_rate: 1e-05, global_step: 120, interval_runtime: 2.5542, interval_samples_per_second: 62.64218200480218, interval_steps_per_second: 3.9151363753001363, progress_or_epoch: 24.0[2024-06-26 18:06:15,065] [ INFO] - loss: 0.00024367, learning_rate: 1e-05, global_step: 130, interval_runtime: 2.587, interval_samples_per_second: 61.84880269252828, interval_steps_per_second: 3.8655501682830176, progress_or_epoch: 26.0[2024-06-26 18:06:17,695] [ INFO] - loss: 0.0001915, learning_rate: 1e-05, global_step: 140, interval_runtime: 2.6297, interval_samples_per_second: 60.84334418667863, interval_steps_per_second: 3.8027090116674143, progress_or_epoch: 28.0[2024-06-26 18:06:20,298] [ INFO] - loss: 0.00017253, learning_rate: 1e-05, global_step: 150, interval_runtime: 2.5998, interval_samples_per_second: 61.542086843031, interval_steps_per_second: 3.8463804276894376, progress_or_epoch: 30.0[2024-06-26 18:06:22,895] [ INFO] - loss: 0.00014841, learning_rate: 1e-05, global_step: 160, interval_runtime: 2.6008, interval_samples_per_second: 61.51999965531349, interval_steps_per_second: 3.844999978457093, progress_or_epoch: 32.0[2024-06-26 18:06:25,498] [ INFO] - loss: 0.00012293, learning_rate: 1e-05, global_step: 170, interval_runtime: 2.6024, interval_samples_per_second: 61.48274409158785, interval_steps_per_second: 3.8426715057242404, progress_or_epoch: 34.0[2024-06-26 18:06:28,099] [ INFO] - loss: 0.00010312, learning_rate: 1e-05, global_step: 180, interval_runtime: 2.601, interval_samples_per_second: 61.514997685471634, interval_steps_per_second: 3.844687355341977, progress_or_epoch: 36.0[2024-06-26 18:06:30,698] [ INFO] - loss: 9.851e-05, learning_rate: 1e-05, global_step: 190, interval_runtime: 2.5992, interval_samples_per_second: 61.5571084161852, interval_steps_per_second: 3.847319276011575, progress_or_epoch: 38.0[2024-06-26 18:06:33,295] [ INFO] - loss: 0.00013552, learning_rate: 1e-05, global_step: 200, interval_runtime: 2.5971, interval_samples_per_second: 61.60644154857953, interval_steps_per_second: 3.8504025967862208, progress_or_epoch: 40.0[2024-06-26 18:06:33,295] [ INFO] - ***** Running Evaluation *****[2024-06-26 18:06:33,295] [ INFO] - Num examples = 4[2024-06-26 18:06:33,295] [ INFO] - Total prediction steps = 1[2024-06-26 18:06:33,295] [ INFO] - Pre device batch size = 16[2024-06-26 18:06:33,295] [ INFO] - Total Batch size = 16[2024-06-26 18:06:33,358] [ INFO] - eval_loss: 0.006688571535050869, eval_precision: 0.4, eval_recall: 0.4, eval_f1: 0.4000000000000001, eval_runtime: 0.0621, eval_samples_per_second: 64.41700614712398, eval_steps_per_second: 16.104251536780996, progress_or_epoch: 40.0[2024-06-26 18:06:33,359] [ INFO] - Saving model checkpoint to ./checkpoint/model_best/checkpoint-200[2024-06-26 18:06:33,360] [ INFO] - tokenizer config file saved in ./checkpoint/model_best/checkpoint-200/tokenizer_config.json[2024-06-26 18:06:33,360] [ INFO] - Special tokens file saved in ./checkpoint/model_best/checkpoint-200/special_tokens_map.json[2024-06-26 18:06:33,370] [ INFO] - Configuration saved in ./checkpoint/model_best/checkpoint-200/config.json[2024-06-26 18:06:34,574] [ INFO] - Model weights saved in ./checkpoint/model_best/checkpoint-200/model_state.pdparams[2024-06-26 18:06:34,574] [ INFO] - Saving optimizer files.[2024-06-26 18:06:39,080] [ INFO] - [timelog] checkpoint saving time: 5.72s (2024-06-26 18:06:39) [2024-06-26 18:06:41,672] [ INFO] - loss: 0.00011128, learning_rate: 1e-05, global_step: 210, interval_runtime: 8.3772, interval_samples_per_second: 19.099448736351285, interval_steps_per_second: 1.1937155460219553, progress_or_epoch: 42.0[2024-06-26 18:06:44,288] [ INFO] - loss: 0.00011245, learning_rate: 1e-05, global_step: 220, interval_runtime: 2.6149, interval_samples_per_second: 61.18868160731556, interval_steps_per_second: 3.8242926004572224, progress_or_epoch: 44.0[2024-06-26 18:06:46,911] [ INFO] - loss: 0.00012268, learning_rate: 1e-05, global_step: 230, interval_runtime: 2.6234, interval_samples_per_second: 60.98920142963071, interval_steps_per_second: 3.8118250893519194, progress_or_epoch: 46.0[2024-06-26 18:06:49,537] [ INFO] - loss: 0.00013804, learning_rate: 1e-05, global_step: 240, interval_runtime: 2.6259, interval_samples_per_second: 60.93070388366043, interval_steps_per_second: 3.8081689927287767, progress_or_epoch: 48.0[2024-06-26 18:06:52,117] [ INFO] - loss: 0.00020081, learning_rate: 1e-05, global_step: 250, interval_runtime: 2.5809, interval_samples_per_second: 61.99492724918558, interval_steps_per_second: 3.8746829530740987, progress_or_epoch: 50.0[2024-06-26 18:06:54,753] [ INFO] - loss: 0.00014258, learning_rate: 1e-05, global_step: 260, interval_runtime: 2.6358, interval_samples_per_second: 60.702937625070206, interval_steps_per_second: 3.793933601566888, progress_or_epoch: 52.0[2024-06-26 18:06:57,369] [ INFO] - loss: 0.00012738, learning_rate: 1e-05, global_step: 270, interval_runtime: 2.6164, interval_samples_per_second: 61.153369707036596, interval_steps_per_second: 3.8220856066897873, progress_or_epoch: 54.0[2024-06-26 18:07:00,006] [ INFO] - loss: 0.00010497, learning_rate: 1e-05, global_step: 280, interval_runtime: 2.6373, interval_samples_per_second: 60.669083466377856, interval_steps_per_second: 3.791817716648616, progress_or_epoch: 56.0[2024-06-26 18:07:02,657] [ INFO] - loss: 0.00010167, learning_rate: 1e-05, global_step: 290, interval_runtime: 2.6499, interval_samples_per_second: 60.38069724825452, interval_steps_per_second: 3.7737935780159075, progress_or_epoch: 58.0[2024-06-26 18:07:05,297] [ INFO] - loss: 0.000171, learning_rate: 1e-05, global_step: 300, interval_runtime: 2.6399, interval_samples_per_second: 60.60732256501828, interval_steps_per_second: 3.7879576603136424, progress_or_epoch: 60.0[2024-06-26 18:07:05,298] [ INFO] - ***** Running Evaluation *****[2024-06-26 18:07:05,298] [ INFO] - Num examples = 4[2024-06-26 18:07:05,298] [ INFO] - Total prediction steps = 1[2024-06-26 18:07:05,298] [ INFO] - Pre device batch size = 16[2024-06-26 18:07:05,298] [ INFO] - Total Batch size = 16[2024-06-26 18:07:05,357] [ INFO] - eval_loss: 0.007026550825685263, eval_precision: 0.4, eval_recall: 0.4, eval_f1: 0.4000000000000001, eval_runtime: 0.0582, eval_samples_per_second: 68.67042956838507, eval_steps_per_second: 17.16760739209627, progress_or_epoch: 60.0[2024-06-26 18:07:05,358] [ INFO] - Saving model checkpoint to ./checkpoint/model_best/checkpoint-300[2024-06-26 18:07:05,358] [ INFO] - tokenizer config file saved in ./checkpoint/model_best/checkpoint-300/tokenizer_config.json[2024-06-26 18:07:05,358] [ INFO] - Special tokens file saved in ./checkpoint/model_best/checkpoint-300/special_tokens_map.json[2024-06-26 18:07:05,364] [ INFO] - Configuration saved in ./checkpoint/model_best/checkpoint-300/config.json[2024-06-26 18:07:07,995] [ INFO] - Model weights saved in ./checkpoint/model_best/checkpoint-300/model_state.pdparams[2024-06-26 18:07:07,996] [ INFO] - Saving optimizer files.[2024-06-26 18:07:11,005] [ INFO] - [timelog] checkpoint saving time: 5.64s (2024-06-26 18:07:11) [2024-06-26 18:07:13,640] [ INFO] - loss: 9.265e-05, learning_rate: 1e-05, global_step: 310, interval_runtime: 8.3438, interval_samples_per_second: 19.17600366118997, interval_steps_per_second: 1.1985002288243731, progress_or_epoch: 62.0[2024-06-26 18:07:16,285] [ INFO] - loss: 0.00012613, learning_rate: 1e-05, global_step: 320, interval_runtime: 2.6441, interval_samples_per_second: 60.512013700058034, interval_steps_per_second: 3.782000856253627, progress_or_epoch: 64.0[2024-06-26 18:07:18,887] [ INFO] - loss: 7.656e-05, learning_rate: 1e-05, global_step: 330, interval_runtime: 2.603, interval_samples_per_second: 61.4676855948393, interval_steps_per_second: 3.8417303496774564, progress_or_epoch: 66.0[2024-06-26 18:07:21,539] [ INFO] - loss: 9.599e-05, learning_rate: 1e-05, global_step: 340, interval_runtime: 2.6517, interval_samples_per_second: 60.33811856609532, interval_steps_per_second: 3.7711324103809574, progress_or_epoch: 68.0[2024-06-26 18:07:24,185] [ INFO] - loss: 9.96e-05, learning_rate: 1e-05, global_step: 350, interval_runtime: 2.6462, interval_samples_per_second: 60.463986738962795, interval_steps_per_second: 3.7789991711851747, progress_or_epoch: 70.0[2024-06-26 18:07:26,830] [ INFO] - loss: 9.502e-05, learning_rate: 1e-05, global_step: 360, interval_runtime: 2.6438, interval_samples_per_second: 60.51813635974227, interval_steps_per_second: 3.782383522483892, progress_or_epoch: 72.0[2024-06-26 18:07:29,486] [ INFO] - loss: 8.396e-05, learning_rate: 1e-05, global_step: 370, interval_runtime: 2.6568, interval_samples_per_second: 60.22368303445632, interval_steps_per_second: 3.76398018965352, progress_or_epoch: 74.0[2024-06-26 18:07:32,128] [ INFO] - loss: 0.000107, learning_rate: 1e-05, global_step: 380, interval_runtime: 2.6414, interval_samples_per_second: 60.57406701856363, interval_steps_per_second: 3.785879188660227, progress_or_epoch: 76.0[2024-06-26 18:07:34,774] [ INFO] - loss: 0.00015859, learning_rate: 1e-05, global_step: 390, interval_runtime: 2.646, interval_samples_per_second: 60.46891186644317, interval_steps_per_second: 3.7793069916526982, progress_or_epoch: 78.0[2024-06-26 18:07:37,422] [ INFO] - loss: 8.44e-05, learning_rate: 1e-05, global_step: 400, interval_runtime: 2.6486, interval_samples_per_second: 60.40979254722639, interval_steps_per_second: 3.7756120342016493, progress_or_epoch: 80.0[2024-06-26 18:07:37,423] [ INFO] - ***** Running Evaluation *****[2024-06-26 18:07:37,423] [ INFO] - Num examples = 4[2024-06-26 18:07:37,423] [ INFO] - Total prediction steps = 1[2024-06-26 18:07:37,423] [ INFO] - Pre device batch size = 16[2024-06-26 18:07:37,423] [ INFO] - Total Batch size = 16[2024-06-26 18:07:37,482] [ INFO] - eval_loss: 0.00755023630335927, eval_precision: 0.4, eval_recall: 0.4, eval_f1: 0.4000000000000001, eval_runtime: 0.0578, eval_samples_per_second: 69.19010227647641, eval_steps_per_second: 17.297525569119102, progress_or_epoch: 80.0[2024-06-26 18:07:37,482] [ INFO] - Saving model checkpoint to ./checkpoint/model_best/checkpoint-400[2024-06-26 18:07:37,483] [ INFO] - tokenizer config file saved in ./checkpoint/model_best/checkpoint-400/tokenizer_config.json[2024-06-26 18:07:37,483] [ INFO] - Special tokens file saved in ./checkpoint/model_best/checkpoint-400/special_tokens_map.json[2024-06-26 18:07:37,489] [ INFO] - Configuration saved in ./checkpoint/model_best/checkpoint-400/config.json[2024-06-26 18:07:38,671] [ INFO] - Model weights saved in ./checkpoint/model_best/checkpoint-400/model_state.pdparams[2024-06-26 18:07:38,671] [ INFO] - Saving optimizer files.[2024-06-26 18:07:41,730] [ INFO] - [timelog] checkpoint saving time: 4.24s (2024-06-26 18:07:41) [2024-06-26 18:07:44,394] [ INFO] - loss: 8.615e-05, learning_rate: 1e-05, global_step: 410, interval_runtime: 6.9719, interval_samples_per_second: 22.949178353528108, interval_steps_per_second: 1.4343236470955067, progress_or_epoch: 82.0[2024-06-26 18:07:47,030] [ INFO] - loss: 8.146e-05, learning_rate: 1e-05, global_step: 420, interval_runtime: 2.6363, interval_samples_per_second: 60.691782269542095, interval_steps_per_second: 3.793236391846381, progress_or_epoch: 84.0[2024-06-26 18:07:49,660] [ INFO] - loss: 0.0001024, learning_rate: 1e-05, global_step: 430, interval_runtime: 2.6297, interval_samples_per_second: 60.84334970295866, interval_steps_per_second: 3.8027093564349164, progress_or_epoch: 86.0[2024-06-26 18:07:52,296] [ INFO] - loss: 0.00011797, learning_rate: 1e-05, global_step: 440, interval_runtime: 2.6357, interval_samples_per_second: 60.70518895680559, interval_steps_per_second: 3.7940743098003495, progress_or_epoch: 88.0[2024-06-26 18:07:54,939] [ INFO] - loss: 0.00014716, learning_rate: 1e-05, global_step: 450, interval_runtime: 2.6435, interval_samples_per_second: 60.52505176192572, interval_steps_per_second: 3.7828157351203573, progress_or_epoch: 90.0[2024-06-26 18:07:57,583] [ INFO] - loss: 7.592e-05, learning_rate: 1e-05, global_step: 460, interval_runtime: 2.6437, interval_samples_per_second: 60.520177521644605, interval_steps_per_second: 3.782511095102788, progress_or_epoch: 92.0[2024-06-26 18:08:00,303] [ INFO] - loss: 8.616e-05, learning_rate: 1e-05, global_step: 470, interval_runtime: 2.7201, interval_samples_per_second: 58.82132721602211, interval_steps_per_second: 3.676332951001382, progress_or_epoch: 94.0[2024-06-26 18:08:02,968] [ INFO] - loss: 7.984e-05, learning_rate: 1e-05, global_step: 480, interval_runtime: 2.6645, interval_samples_per_second: 60.048504909189305, interval_steps_per_second: 3.7530315568243315, progress_or_epoch: 96.0[2024-06-26 18:08:05,618] [ INFO] - loss: 7.743e-05, learning_rate: 1e-05, global_step: 490, interval_runtime: 2.6507, interval_samples_per_second: 60.3610372488141, interval_steps_per_second: 3.7725648280508812, progress_or_epoch: 98.0[2024-06-26 18:08:08,285] [ INFO] - loss: 7.793e-05, learning_rate: 1e-05, global_step: 500, interval_runtime: 2.6661, interval_samples_per_second: 60.01194714924565, interval_steps_per_second: 3.750746696827853, progress_or_epoch: 100.0[2024-06-26 18:08:08,285] [ INFO] - ***** Running Evaluation *****[2024-06-26 18:08:08,285] [ INFO] - Num examples = 4[2024-06-26 18:08:08,286] [ INFO] - Total prediction steps = 1[2024-06-26 18:08:08,286] [ INFO] - Pre device batch size = 16[2024-06-26 18:08:08,286] [ INFO] - Total Batch size = 16[2024-06-26 18:08:08,344] [ INFO] - eval_loss: 0.007735834456980228, eval_precision: 0.4, eval_recall: 0.4, eval_f1: 0.4000000000000001, eval_runtime: 0.0574, eval_samples_per_second: 69.71943866123114, eval_steps_per_second: 17.429859665307784, progress_or_epoch: 100.0[2024-06-26 18:08:08,344] [ INFO] - Saving model checkpoint to ./checkpoint/model_best/checkpoint-500[2024-06-26 18:08:08,345] [ INFO] - tokenizer config file saved in ./checkpoint/model_best/checkpoint-500/tokenizer_config.json[2024-06-26 18:08:08,345] [ INFO] - Special tokens file saved in ./checkpoint/model_best/checkpoint-500/special_tokens_map.json[2024-06-26 18:08:08,351] [ INFO] - Configuration saved in ./checkpoint/model_best/checkpoint-500/config.json[2024-06-26 18:08:09,518] [ INFO] - Model weights saved in ./checkpoint/model_best/checkpoint-500/model_state.pdparams[2024-06-26 18:08:09,518] [ INFO] - Saving optimizer files.[2024-06-26 18:08:11,773] [ INFO] - [timelog] checkpoint saving time: 3.42s (2024-06-26 18:08:11) [2024-06-26 18:08:11,774] [ INFO] - Training completed. [2024-06-26 18:08:11,774] [ INFO] - Loading best model from ./checkpoint/model_best/checkpoint-100 (score: 0.4000000000000001).[2024-06-26 18:08:12,116] [ INFO] - set state-dict :([], [])[2024-06-26 18:08:12,118] [ INFO] - train_runtime: 161.4472, train_samples_per_second: 42.1190341985795, train_steps_per_second: 3.0969878087190805, train_loss: 0.00023241847997996956, progress_or_epoch: 100.0[2024-06-26 18:08:12,134] [ INFO] - Saving model checkpoint to ./checkpoint/model_best[2024-06-26 18:08:12,135] [ INFO] - tokenizer config file saved in ./checkpoint/model_best/tokenizer_config.json[2024-06-26 18:08:12,135] [ INFO] - Special tokens file saved in ./checkpoint/model_best/special_tokens_map.json[2024-06-26 18:08:12,142] [ INFO] - Configuration saved in ./checkpoint/model_best/config.json[2024-06-26 18:08:13,814] [ INFO] - Model weights saved in ./checkpoint/model_best/model_state.pdparams[2024-06-26 18:08:13,815] [ INFO] - ***** train metrics *****[2024-06-26 18:08:13,815] [ INFO] - progress_or_epoch = 100.0[2024-06-26 18:08:13,815] [ INFO] - train_loss = 0.0002[2024-06-26 18:08:13,815] [ INFO] - train_runtime = 0:02:41.44[2024-06-26 18:08:13,815] [ INFO] - train_samples_per_second = 42.119[2024-06-26 18:08:13,815] [ INFO] - train_steps_per_second = 3.097[2024-06-26 18:08:13,820] [ INFO] - ***** Running Evaluation *****[2024-06-26 18:08:13,820] [ INFO] - Num examples = 4[2024-06-26 18:08:13,820] [ INFO] - Total prediction steps = 1[2024-06-26 18:08:13,820] [ INFO] - Pre device batch size = 16[2024-06-26 18:08:13,820] [ INFO] - Total Batch size = 16[2024-06-26 18:08:13,888] [ INFO] - eval_loss: 0.005224619060754776, eval_precision: 0.4, eval_recall: 0.4, eval_f1: 0.4000000000000001, eval_runtime: 0.0687, eval_samples_per_second: 58.19804494272889, eval_steps_per_second: 14.549511235682223, progress_or_epoch: 100.0[2024-06-26 18:08:13,889] [ INFO] - ***** eval metrics *****[2024-06-26 18:08:13,889] [ INFO] - eval_f1 = 0.4[2024-06-26 18:08:13,889] [ INFO] - eval_loss = 0.0052[2024-06-26 18:08:13,889] [ INFO] - eval_precision = 0.4[2024-06-26 18:08:13,889] [ INFO] - eval_recall = 0.4[2024-06-26 18:08:13,889] [ INFO] - eval_runtime = 0:00:00.06[2024-06-26 18:08:13,890] [ INFO] - eval_samples_per_second = 58.198[2024-06-26 18:08:13,890] [ INFO] - eval_steps_per_second = 14.5495[2024-06-26 18:08:13,890] [ INFO] - progress_or_epoch = 100.0/root/anaconda3/envs/py39_ppner_2_7_2/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py:712: UserWarning: full_graph=False don't support input_spec arguments. It will not produce any effect.You can set full_graph=True, then you can assign input spec. warnings.warn([2024-06-26 18:08:13,895] [ INFO] - Exporting inference model to ./checkpoint/model_best/modelI0626 18:08:15.824677 285242 program_interpreter.cc:212] New Executor is Running.[2024-06-26 18:08:18,295] [ INFO] - Inference model exported.[2024-06-26 18:08:18,297] [ INFO] - tokenizer config file saved in ./checkpoint/model_best/tokenizer_config.json[2024-06-26 18:08:18,297] [ INFO] - Special tokens file saved in ./checkpoint/model_best/special_tokens_map.jsonLAUNCH INFO 2024-06-26 18:08:20,044 Pod completedLAUNCH INFO 2024-06-26 18:08:20,045 Exit code 0 模型文件 12345678910111213141516171819202122(py39_ppner_2_7_2) [root@jdz uie]# ll checkpoint/model_best/total 919580-rw-r--r-- 1 root root 207 Jun 26 18:08 all_results.jsondrwxr-xr-x 2 root root 251 Jun 26 18:06 checkpoint-100drwxr-xr-x 2 root root 251 Jun 26 18:06 checkpoint-200drwxr-xr-x 2 root root 251 Jun 26 18:07 checkpoint-300drwxr-xr-x 2 root root 251 Jun 26 18:07 checkpoint-400drwxr-xr-x 2 root root 251 Jun 26 18:08 checkpoint-500-rw-r--r-- 1 root root 559 Jun 26 18:08 config.json-rw-r--r-- 1 root root 469428391 Jun 26 18:08 model.pdiparams-rw-r--r-- 1 root root 17581 Jun 26 18:08 model.pdiparams.info-rw-r--r-- 1 root root 153135 Jun 26 18:08 model.pdmodel-rw-r--r-- 1 root root 471806543 Jun 26 18:12 model_state.pdparamsdrwxr-xr-x 7 root root 136 Jun 26 18:05 runs-rw-r--r-- 1 root root 112 Jun 26 18:08 special_tokens_map.jsondrwxr-xr-x 2 root root 90 Jun 26 18:12 static-rw-r--r-- 1 root root 197 Jun 26 18:08 tokenizer_config.json-rw-r--r-- 1 root root 16736 Jun 26 18:08 trainer_state.json-rw-r--r-- 1 root root 2598 Jun 26 18:08 training_args.bin-rw-r--r-- 1 root root 207 Jun 26 18:08 train_results.json-rw-r--r-- 1 root root 186807 Jun 26 18:08 vocab.txt(py39_ppner_2_7_2) [root@jdz uie]# 调用api测试 123456789from pprint import pprintfrom paddlenlp import Taskflowschema = [&#123;'工程': ['工艺']&#125;]ie = Taskflow('information_extraction', schema=schema, task_path='./checkpoint/model_best')ie.set_schema(schema) # Reset schemapprint(ie(\"\"\"受力钢筋的接头形式应按设计要求采用,若设计无要求时,钢筋宜采用焊接接头和机械连接接头,也可采用绑扎接头。\"\"\")) 测试结果 12345678[&#123;'工程': [&#123;'end': 9, 'probability': 0.9548929987860788, 'relations': &#123;'工艺': [&#123;'end': 42, 'probability': 0.2548182944884658, 'start': 36, 'text': '机械连接接头'&#125;]&#125;, 'start': 0, 'text': '受力钢筋的接头形式'&#125;]&#125;]","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}],"tags":[{"name":"doccano","slug":"doccano","permalink":"http://yoursite.com/tags/doccano/"},{"name":"paddle","slug":"paddle","permalink":"http://yoursite.com/tags/paddle/"},{"name":"ERNIE-UIE","slug":"ERNIE-UIE","permalink":"http://yoursite.com/tags/ERNIE-UIE/"}]},{"title":"yolov8-seg：皮带机偏移实时检测","slug":"yolov8-seg：皮带机偏移实时检测","date":"2024-05-11T01:58:37.000Z","updated":"2024-06-28T08:16:23.152Z","comments":true,"path":"2024/05/11/yolov8-seg：皮带机偏移实时检测/","link":"","permalink":"http://yoursite.com/2024/05/11/yolov8-seg：皮带机偏移实时检测/","excerpt":"环境&amp;安装同上文yolov8：火灾检测 模型使用yolov8n-seg 数据标注标注工具：labelme对分割目标进行多边矩形标注","text":"环境&amp;安装同上文yolov8：火灾检测 模型使用yolov8n-seg 数据标注标注工具：labelme对分割目标进行多边矩形标注 数据格式转换将labelme多边矩形数据格式转为yolo-seg数据格式，通用转换代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import osimport jsondef convert_labelme_to_yolo(json_file, output_txt_file, label_to_class_id): with open(json_file, 'r') as file: data = json.load(file) image_width = data['imageWidth'] image_height = data['imageHeight'] with open(output_txt_file, 'w') as file: for shape in data['shapes']: points = shape['points'] class_id = label_to_class_id.get(shape['label'], -1) normalized_points = [] for x, y in points: nx = round(x / image_width, 6) ny = round(y / image_height, 6) normalized_points.append(f\"&#123;nx&#125; &#123;ny&#125;\") file.write(f\"&#123;class_id&#125; \" + \" \".join(normalized_points) + \"\\n\")def batch_convert(json_folder, output_folder, label_to_class_id): # 确保输出文件夹存在 os.makedirs(output_folder, exist_ok=True) # 遍历文件夹中的所有JSON文件 for filename in os.listdir(json_folder): if filename.endswith('.json'): json_file = os.path.join(json_folder, filename) output_txt_file = os.path.join(output_folder, filename.replace('.json', '.txt')) convert_labelme_to_yolo(json_file, output_txt_file, label_to_class_id)def create_label_file(label_dict, output): sorted_labels = [label for label, _ in sorted(label_dict.items(), key=lambda item: item[1])] with open(output, 'w') as file: for label in sorted_labels: file.write(f\"&#123;label&#125;\\n\")if __name__ == '__main__': # 使用示例 label_map = &#123; 'belt': 0, &#125; json_path = r'D:\\pycharmproject_2\\yolo\\ultralytics\\datasets\\belt_seg\\json' output_path = r'D:\\pycharmproject_2\\yolo\\ultralytics\\datasets\\belt_seg\\images' batch_convert(json_path, output_path, label_map) # 生成labelme.txt文件，按照label_dict的值从小到大排列，一个类别1行 create_label_file(label_map, r'D:\\pycharmproject_2\\yolo\\ultralytics\\datasets\\belt_seg\\yolo-label.txt') 完成后稍作整理，将数据集按5:1:1比例划分成train/test/val 创建训练yaml文件参考yolov8n-seg.yaml 123456train: /exp/work/video/ultralytics/datasets/belt_seg/data/train/imagesval: /exp/work/video/ultralytics/datasets/belt_seg/data/val/imagestest: /exp/work/video/ultralytics/datasets/belt_seg/data/test/imagesnc: 1names: [belt] 训练代码12345678910111213from ultralytics import YOLO# 加载预训练模型model = YOLO('yolov8n-seg.pt') # load a pretrained model (recommended for training)# 启用双卡训练model.train( data='datasets/belt_seg/belt_seg.yaml', epochs=300, device=[0, 1], save_dir='runs/segment/belt')# 启用模型验证model.val() 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125(py39_yolov8) [root@jdz ultralytics]# python yolov8_train.py Ultralytics YOLOv8.2.41 🚀 Python-3.9.18 torch-2.1.2+cu121 CUDA:0 (NVIDIA GeForce RTX 3090, 24260MiB) CUDA:1 (NVIDIA GeForce RTX 3090, 24260MiB)engine/trainer: task=segment, mode=train, model=yolov8n-seg.pt, data=datasets/belt_seg/belt_seg.yaml, epochs=300, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=[0, 1], workers=8, project=None, name=train6, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/segment/train6Overriding model.yaml nc=80 with nc=1 from n params module arguments 0 -1 1 464 ultralytics.nn.modules.conv.Conv [3, 16, 3, 2] 1 -1 1 4672 ultralytics.nn.modules.conv.Conv [16, 32, 3, 2] 2 -1 1 7360 ultralytics.nn.modules.block.C2f [32, 32, 1, True] 3 -1 1 18560 ultralytics.nn.modules.conv.Conv [32, 64, 3, 2] 4 -1 2 49664 ultralytics.nn.modules.block.C2f [64, 64, 2, True] 5 -1 1 73984 ultralytics.nn.modules.conv.Conv [64, 128, 3, 2] 6 -1 2 197632 ultralytics.nn.modules.block.C2f [128, 128, 2, True] 7 -1 1 295424 ultralytics.nn.modules.conv.Conv [128, 256, 3, 2] 8 -1 1 460288 ultralytics.nn.modules.block.C2f [256, 256, 1, True] 9 -1 1 164608 ultralytics.nn.modules.block.SPPF [256, 256, 5] 10 -1 1 0 torch.nn.modules.upsampling.Upsample [None, 2, 'nearest'] 11 [-1, 6] 1 0 ultralytics.nn.modules.conv.Concat [1] 12 -1 1 148224 ultralytics.nn.modules.block.C2f [384, 128, 1] 13 -1 1 0 torch.nn.modules.upsampling.Upsample [None, 2, 'nearest'] 14 [-1, 4] 1 0 ultralytics.nn.modules.conv.Concat [1] 15 -1 1 37248 ultralytics.nn.modules.block.C2f [192, 64, 1] 16 -1 1 36992 ultralytics.nn.modules.conv.Conv [64, 64, 3, 2] 17 [-1, 12] 1 0 ultralytics.nn.modules.conv.Concat [1] 18 -1 1 123648 ultralytics.nn.modules.block.C2f [192, 128, 1] 19 -1 1 147712 ultralytics.nn.modules.conv.Conv [128, 128, 3, 2] 20 [-1, 9] 1 0 ultralytics.nn.modules.conv.Concat [1] 21 -1 1 493056 ultralytics.nn.modules.block.C2f [384, 256, 1] 22 [15, 18, 21] 1 1004275 ultralytics.nn.modules.head.Segment [1, 32, 64, [64, 128, 256]] YOLOv8n-seg summary: 261 layers, 3263811 parameters, 3263795 gradients, 12.1 GFLOPsTransferred 381/417 items from pretrained weightsDDP: debug command /root/anaconda3/envs/py39_yolov8/bin/python -m torch.distributed.run --nproc_per_node 2 --master_port 48598 /root/.config/Ultralytics/DDP/_temp_p1zi2y0s140608071072736.pyUltralytics YOLOv8.2.41 🚀 Python-3.9.18 torch-2.1.2+cu121 CUDA:0 (NVIDIA GeForce RTX 3090, 24260MiB) CUDA:1 (NVIDIA GeForce RTX 3090, 24260MiB)Overriding model.yaml nc=80 with nc=1Transferred 381/417 items from pretrained weightsFreezing layer 'model.22.dfl.conv.weight'AMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...AMP: checks passed ✅train: Scanning /exp/work/video/ultralytics/datasets/belt_seg/data/train/labels... 100 images, 0 backgrounds, 0 corrupt: 100%|██████████| 100/100 [00:00&lt;00:00, 681.30it/s]train: WARNING ⚠️ /exp/work/video/ultralytics/datasets/belt_seg/data/train/images/04221812151.jpg: corrupt JPEG restored and savedtrain: WARNING ⚠️ /exp/work/video/ultralytics/datasets/belt_seg/data/train/images/1549055475952230.jpg: corrupt JPEG restored and savedtrain: New cache created: /exp/work/video/ultralytics/datasets/belt_seg/data/train/labels.cacheval: Scanning /exp/work/video/ultralytics/datasets/belt_seg/data/val/labels... 20 images, 0 backgrounds, 0 corrupt: 100%|██████████| 20/20 [00:00&lt;00:00, 557.79it/s]val: New cache created: /exp/work/video/ultralytics/datasets/belt_seg/data/val/labels.cachePlotting labels to runs/segment/train6/labels.jpg... optimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... optimizer: AdamW(lr=0.000714, momentum=0.9) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.0005), 76 bias(decay=0.0)Image sizes 640 train, 640 valUsing 16 dataloader workersLogging results to runs/segment/train6Starting training for 300 epochs... Epoch GPU_mem box_loss seg_loss cls_loss dfl_loss Instances Size 1/300 1.57G 0.9085 3.003 2.743 1.368 6 640: 100%|██████████| 7/7 [00:05&lt;00:00, 1.35it/s] Class Images Instances Box(P R mAP50 mAP50-95) Mask(P R mAP50 mAP50-95): 100%|██████████| 2/2 [00:00&lt;00:00, 2.54it/s] all 20 21 0.0035 1 0.179 0.087 0.0035 1 0.113 0.0414 Epoch GPU_mem box_loss seg_loss cls_loss dfl_loss Instances Size 2/300 1.6G 0.7924 2.131 2.561 1.221 6 640: 100%|██████████| 7/7 [00:01&lt;00:00, 4.83it/s] Class Images Instances Box(P R mAP50 mAP50-95) Mask(P R mAP50 mAP50-95): 100%|██████████| 2/2 [00:00&lt;00:00, 8.52it/s] all 20 21 0.0035 1 0.753 0.548 0.0035 1 0.753 0.499 Epoch GPU_mem box_loss seg_loss cls_loss dfl_loss Instances Size 3/300 1.63G 0.5945 1.036 1.887 1.12 3 640: 100%|██████████| 7/7 [00:01&lt;00:00, 4.93it/s] Class Images Instances Box(P R mAP50 mAP50-95) Mask(P R mAP50 mAP50-95): 100%|██████████| 2/2 [00:00&lt;00:00, 8.46it/s] all 20 21 0.0035 1 0.886 0.581 0.0035 1 0.886 0.616 Epoch GPU_mem box_loss seg_loss cls_loss dfl_loss Instances Size 4/300 1.62G 0.5529 0.7711 1.401 1.065 7 640: 100%|██████████| 7/7 [00:01&lt;00:00, 5.25it/s] Class Images Instances Box(P R mAP50 mAP50-95) Mask(P R mAP50 mAP50-95): 100%|██████████| 2/2 [00:00&lt;00:00, 7.88it/s] all 20 21 0.606 0.952 0.819 0.582 0.606 0.952 0.819 0.641... Epoch GPU_mem box_loss seg_loss cls_loss dfl_loss Instances Size 295/300 1.61G 0.1653 0.1045 0.2493 0.8894 3 640: 100%|██████████| 7/7 [00:01&lt;00:00, 5.45it/s] Class Images Instances Box(P R mAP50 mAP50-95) Mask(P R mAP50 mAP50-95): 100%|██████████| 2/2 [00:00&lt;00:00, 9.24it/s] all 20 21 0.982 1 0.995 0.926 0.982 1 0.995 0.912 Epoch GPU_mem box_loss seg_loss cls_loss dfl_loss Instances Size 296/300 1.63G 0.2021 0.1132 0.2765 0.895 2 640: 100%|██████████| 7/7 [00:01&lt;00:00, 5.70it/s] Class Images Instances Box(P R mAP50 mAP50-95) Mask(P R mAP50 mAP50-95): 100%|██████████| 2/2 [00:00&lt;00:00, 9.00it/s] all 20 21 0.974 1 0.995 0.933 0.974 1 0.995 0.917 Epoch GPU_mem box_loss seg_loss cls_loss dfl_loss Instances Size 297/300 1.63G 0.1777 0.1075 0.2616 0.8351 2 640: 100%|██████████| 7/7 [00:01&lt;00:00, 5.51it/s] Class Images Instances Box(P R mAP50 mAP50-95) Mask(P R mAP50 mAP50-95): 100%|██████████| 2/2 [00:00&lt;00:00, 7.61it/s] all 20 21 0.969 1 0.995 0.927 0.969 1 0.995 0.915 Epoch GPU_mem box_loss seg_loss cls_loss dfl_loss Instances Size 298/300 1.63G 0.1736 0.1195 0.2461 0.8778 2 640: 100%|██████████| 7/7 [00:01&lt;00:00, 5.64it/s] Class Images Instances Box(P R mAP50 mAP50-95) Mask(P R mAP50 mAP50-95): 100%|██████████| 2/2 [00:00&lt;00:00, 8.73it/s] all 20 21 0.968 1 0.995 0.935 0.968 1 0.995 0.916 Epoch GPU_mem box_loss seg_loss cls_loss dfl_loss Instances Size 299/300 1.61G 0.2036 0.6821 0.2815 0.8217 2 640: 100%|██████████| 7/7 [00:01&lt;00:00, 5.33it/s] Class Images Instances Box(P R mAP50 mAP50-95) Mask(P R mAP50 mAP50-95): 100%|██████████| 2/2 [00:00&lt;00:00, 8.39it/s] all 20 21 0.962 1 0.995 0.933 0.962 1 0.995 0.905 Epoch GPU_mem box_loss seg_loss cls_loss dfl_loss Instances Size 300/300 1.63G 0.155 0.1236 0.2409 0.8592 2 640: 100%|██████████| 7/7 [00:01&lt;00:00, 5.38it/s] Class Images Instances Box(P R mAP50 mAP50-95) Mask(P R mAP50 mAP50-95): 100%|██████████| 2/2 [00:00&lt;00:00, 8.70it/s] all 20 21 0.977 1 0.995 0.927 0.977 1 0.995 0.917300 epochs completed in 0.181 hours.Optimizer stripped from runs/segment/train6/weights/last.pt, 6.8MBOptimizer stripped from runs/segment/train6/weights/best.pt, 6.8MBValidating runs/segment/train6/weights/best.pt...Ultralytics YOLOv8.2.41 🚀 Python-3.9.18 torch-2.1.2+cu121 CUDA:0 (NVIDIA GeForce RTX 3090, 24260MiB) CUDA:1 (NVIDIA GeForce RTX 3090, 24260MiB)YOLOv8n-seg summary (fused): 195 layers, 3258259 parameters, 0 gradients, 12.0 GFLOPs Class Images Instances Box(P R mAP50 mAP50-95) Mask(P R mAP50 mAP50-95): 100%|██████████| 2/2 [00:00&lt;00:00, 9.32it/s] all 20 21 0.995 1 0.995 0.948 0.995 1 0.995 0.944Speed: 0.2ms preprocess, 1.5ms inference, 0.0ms loss, 0.9ms postprocess per imageResults saved to runs/segment/train6Ultralytics YOLOv8.2.41 🚀 Python-3.9.18 torch-2.1.2+cu121 CUDA:0 (NVIDIA GeForce RTX 3090, 24260MiB) CUDA:1 (NVIDIA GeForce RTX 3090, 24260MiB)YOLOv8n-seg summary (fused): 195 layers, 3258259 parameters, 0 gradients, 12.0 GFLOPsval: Scanning /exp/work/video/ultralytics/datasets/belt_seg/data/val/labels.cache... 20 images, 0 backgrounds, 0 corrupt: 100%|██████████| 20/20 [00:00&lt;?, ?it/s] Class Images Instances Box(P R mAP50 mAP50-95) Mask(P R mAP50 mAP50-95): 100%|██████████| 2/2 [00:01&lt;00:00, 1.42it/s] all 20 21 0.995 1 0.995 0.948 0.995 1 0.995 0.944Speed: 0.3ms preprocess, 24.2ms inference, 0.0ms loss, 32.2ms postprocess per imageResults saved to runs/segment/train62 预测导包、模型加载&amp;GPU加载123456789101112import cv2import timefrom ultralytics import YOLOmodel2 = YOLO('runs/segment/train6/weights/best.pt')t = time.time()results = model2.predict('video/belt.mp4', stream=False, save=False, device=[0])for res in results: print(res)print(time.time() - t) 多媒体推拉流主函数（多进程）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122import cv2import timeimport threadingimport multiprocessingimport subprocess as spfrom ultralytics import YOLOfrom visualize.segment import *from settings import *class StreamingSegmentServer: def __init__(self, stream_src, stream_dst, cordon): self.pipe = None self.model = None self.capture = None self.fps = 0 self.width = 0 self.height = 0 self.frame_id = 0 self.solve_id = 0 self.cordon = cordon self.stream_src = stream_src self.stream_dst = stream_dst self.center = self.calculate_geometric_center() self.avg_width = (self.cordon[3][0]-self.cordon[0][0] + self.cordon[2][0]-self.cordon[1][0]) / 2 def calculate_geometric_center(self): if len(self.cordon) == 0: return None sum_x = sum([point[0] for point in self.cordon]) sum_y = sum([point[1] for point in self.cordon]) center_x = sum_x / len(self.cordon) center_y = sum_y / len(self.cordon) return center_x, center_y def run(self): framequeue = multiprocessing.Queue(10) solvequeue = multiprocessing.Queue(10) solvequeue.put(1) self.capture = cv2.VideoCapture(self.stream_src) self.fps = int(self.capture.get(cv2.CAP_PROP_FPS)) self.width = int(self.capture.get(cv2.CAP_PROP_FRAME_WIDTH)) self.height = int(self.capture.get(cv2.CAP_PROP_FRAME_HEIGHT)) command = ['ffmpeg', '-y', '-f', 'rawvideo', '-vcodec', 'rawvideo', '-pix_fmt', 'bgr24', '-s', \"&#123;&#125;x&#123;&#125;\".format(self.width, self.height), '-r', str(self.fps), '-i', '-', '-c:v', 'libx264', '-pix_fmt', 'yuv420p', '-preset', 'ultrafast', '-f', 'rtsp', self.stream_dst] pipe = sp.Popen(command, stdin=sp.PIPE) capture_thread = threading.Thread(target=self.capture_video, args=(framequeue,)) capture_thread.start() solve_id = multiprocessing.Value('i', 1) for _ in range(worker.get('worker_threads')): process_thread = multiprocessing.Process(target=self.process_video, args=(pipe, framequeue, solve_id)) process_thread.start() def capture_video(self, framequeue): while True: if framequeue.full(): time.sleep(0.05) else: ret, frame = self.capture.read() if not ret: continue image_bgr = frame self.frame_id += 1 framequeue.put((image_bgr, self.frame_id)) def process_video(self, pipe, framequeue, solve_id): model = YOLO('runs/segment/train6/weights/best.pt') while True: if not framequeue.empty(): frame_bgr, frame_id = framequeue.get() results = model.predict( frame_bgr, stream=False, save=False, device=[0], verbose=False ) names = results[0].names clses = [int(i) for i in results[0].boxes.cls.tolist()] masks = [i.tolist() for i in results[0].masks.xy] name_set = [names.get(i) for i in clses] frame_bgr = visualize_seg( frame_bgr, masks, name_set, self.cordon, self.center, self.avg_width) while True: time.sleep(0.0001) if solve_id.value == frame_id: pipe.stdin.write(frame_bgr.tobytes()) solve_id.value += 1 breakif __name__ == '__main__': rtsp_url = 'rtsp://192.168.9.164:8554/video' rtsp_stream = 'rtsp://192.168.9.164:8554/live' outline = [[268, 4], [145, 311],..., [482, 311], [358, 4]] # 自定义安全区域 sss = StreamingSegmentServer(rtsp_url, rtsp_stream, outline) sss.run() 可视化 &amp; 偏移算法（计算分割掩码与安全区域中心点x轴偏移量） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import osimport cv2import mathimport numpy as npfrom PIL import Image, ImageDraw, ImageFile, ImageFontfrom settings import PRfont_file = os.path.join(PR, 'visualize/SourceHanSansCN-Medium.otf')ImageFile.LOAD_TRUNCATED_IMAGES = Truedef calculate_geometric_center(points): if len(points) == 0: return None sum_x = sum([point[0] for point in points]) sum_y = sum([point[1] for point in points]) center_x = sum_x / len(points) center_y = sum_y / len(points) return center_x, center_ydef visualize_seg(im, masks, names, cordon, center, avg_width): if isinstance(im, str): im = Image.open(im) im = np.ascontiguousarray(np.copy(im)) im = cv2.cvtColor(im, cv2.COLOR_RGB2BGR) else: im = np.ascontiguousarray(np.copy(im)) im = Image.fromarray(im) im = im.convert('RGBA') size = im.size layer = Image.new('RGBA', size) draw = ImageDraw.Draw(im) draw2 = ImageDraw.Draw(layer) for i, name in enumerate(names): mask = masks[i] mask = [tuple(j) for j in mask] text = name center_rt = calculate_geometric_center(mask) # offset = math.sqrt(pow(center_rt[0] - center[0], 2) + pow(center_rt[1] - center[1], 2)) / avg_width 计算距离偏移 offset = (center_rt[0] - center[0]) / avg_width draw2.polygon(mask, fill=(128, 0, 128, 160), outline=(255, 255, 255, 255)) im.paste(layer, mask=layer) draw.text( (mask[0][0], mask[0][1]), text, font=ImageFont.truetype(font_file, size=13), fill=(0, 0, 0, 1000)) draw.text( (mask[0][0], mask[0][1]+20), 'offset: ' + str(offset), font=ImageFont.truetype(font_file, size=13), fill=(0, 0, 0, 1000)) draw.polygon([tuple(i) for i in cordon], fill=None, outline=(255, 255, 0, 255)) im = im.convert('RGB') im = np.ascontiguousarray(np.copy(im)) return im 配置项 12345678import osPR = os.path.dirname(__file__)worker = &#123; 'worker_threads': 12, 'worker_queue_len': 10&#125;","categories":[{"name":"实例分割","slug":"实例分割","permalink":"http://yoursite.com/categories/实例分割/"}],"tags":[{"name":"yolov8","slug":"yolov8","permalink":"http://yoursite.com/tags/yolov8/"},{"name":"opencv","slug":"opencv","permalink":"http://yoursite.com/tags/opencv/"}]},{"title":"Bert+GRU地址归一算法","slug":"Bert-GRU地址归一算法","date":"2024-04-05T08:23:15.000Z","updated":"2024-06-28T08:40:03.114Z","comments":true,"path":"2024/04/05/Bert-GRU地址归一算法/","link":"","permalink":"http://yoursite.com/2024/04/05/Bert-GRU地址归一算法/","excerpt":"一、 算法简介本地址归一算法（已经下简称算法）旨在对输入文本中出现的地址信息或一般地址信息做地址结构化抽取，并输出该地址映射到数据库中的标准化地址。 二、算法模块算法由以下不同模块共同组成，各模块在算法的各个生命周期起到重要作用： 1、建立原始地址库使用postgres数据库（以下简pg）创建存储全国各级原始地址的原始地址库。通过网络爬虫不断采集和更新地址数据（主要来源为高德地图），并存储到原始地址库，地址库包括地址的名称信息、poi信息、类别信息、经纬度信息等原始内容，为后续工作的开展做数据支撑。 2、文本地址抽取使用UIE(Universal Information Extraction)框架，结合ERNIE3.0模型，使模型具备从无结构或半结构的文本中抽取地址信息的能力。 3、地址分级算法i)分级标准首先需要确定一套地址的分级标准细节，本算法采用的分级标准基于阿里《文地址要素解析标注规范》，并做一定程度范围的修改，将地址分为18个不同级别： ① Prov：省级行政区划，省、自治区、直辖市 ② City：地级行政区划，地级市、地区、自治州等 ③ District：县级行政区划，市辖区、县级市、县等 ④ Devzone：广义的上的开发区，包含一般性产业 园区、度假区 ⑤ Town：乡级行政区划，镇、街道、乡等 ⑥ Community：包含社区、行政村（生产大队、村委会），自然村 ⑦ Village Group：限定 xx 组、xx 队、xx 社 ⑧ Road：有正式名称的道路，包括隧道、高架、街、弄、巷等。 步行街、商业街 ⑨ Roadno：路牌号 ⑩ Poi：目标兴趣点 ⑪ Subpoi：目标兴趣点的子兴趣点 ⑫ Houseno：楼栋号，农村地址的门牌号(包括类似南楼、北楼一类的描述) ⑬ Cellno：单元号，包括甲乙丙丁等 ⑭ Floorno：楼层号 ⑮ Roomno：房间号 ⑯ Assist：定位词，包括方位、解释性名词 ⑰ Intersection：路桥交叉口、交汇处、十字路口等 ⑱ Distance：距离 ii)算法细节从爬虫获取的原始地址库构建地址结构化数据集，并对数据集划分为训练集和验证集进行数据标注。标注方式采用B-I-E-O-S五位序列标注法，该标注法将尽可能的保留被标注地址的分级信息。 构建地址分级模型。使用基于多头注意力机制Transformers架构的BERT大模型作为预训练模型，并将模型结合CRF、GRU算法。CRF：全称为条件随机场（Conditional Random Fields），结合了最大熵模型和隐马尔可夫模型的特点，是一种无向图模型。它在序列标注任务如分词、词性标注和命名实体识别等方面取得了很好的效果；GRU：全称为门控循环单元（Gated Recurrent Unit），是一种常用于序列数据建模的神经网络模型，能够很好解决BERT循环神经网络中的长期依赖问题，捕获序列中的长期特征，避免训练过程中的梯度消失和梯度爆炸，使完成的模型结构具有更优秀的泛化能力、更好地拟合真实地址数据。","text":"一、 算法简介本地址归一算法（已经下简称算法）旨在对输入文本中出现的地址信息或一般地址信息做地址结构化抽取，并输出该地址映射到数据库中的标准化地址。 二、算法模块算法由以下不同模块共同组成，各模块在算法的各个生命周期起到重要作用： 1、建立原始地址库使用postgres数据库（以下简pg）创建存储全国各级原始地址的原始地址库。通过网络爬虫不断采集和更新地址数据（主要来源为高德地图），并存储到原始地址库，地址库包括地址的名称信息、poi信息、类别信息、经纬度信息等原始内容，为后续工作的开展做数据支撑。 2、文本地址抽取使用UIE(Universal Information Extraction)框架，结合ERNIE3.0模型，使模型具备从无结构或半结构的文本中抽取地址信息的能力。 3、地址分级算法i)分级标准首先需要确定一套地址的分级标准细节，本算法采用的分级标准基于阿里《文地址要素解析标注规范》，并做一定程度范围的修改，将地址分为18个不同级别： ① Prov：省级行政区划，省、自治区、直辖市 ② City：地级行政区划，地级市、地区、自治州等 ③ District：县级行政区划，市辖区、县级市、县等 ④ Devzone：广义的上的开发区，包含一般性产业 园区、度假区 ⑤ Town：乡级行政区划，镇、街道、乡等 ⑥ Community：包含社区、行政村（生产大队、村委会），自然村 ⑦ Village Group：限定 xx 组、xx 队、xx 社 ⑧ Road：有正式名称的道路，包括隧道、高架、街、弄、巷等。 步行街、商业街 ⑨ Roadno：路牌号 ⑩ Poi：目标兴趣点 ⑪ Subpoi：目标兴趣点的子兴趣点 ⑫ Houseno：楼栋号，农村地址的门牌号(包括类似南楼、北楼一类的描述) ⑬ Cellno：单元号，包括甲乙丙丁等 ⑭ Floorno：楼层号 ⑮ Roomno：房间号 ⑯ Assist：定位词，包括方位、解释性名词 ⑰ Intersection：路桥交叉口、交汇处、十字路口等 ⑱ Distance：距离 ii)算法细节从爬虫获取的原始地址库构建地址结构化数据集，并对数据集划分为训练集和验证集进行数据标注。标注方式采用B-I-E-O-S五位序列标注法，该标注法将尽可能的保留被标注地址的分级信息。 构建地址分级模型。使用基于多头注意力机制Transformers架构的BERT大模型作为预训练模型，并将模型结合CRF、GRU算法。CRF：全称为条件随机场（Conditional Random Fields），结合了最大熵模型和隐马尔可夫模型的特点，是一种无向图模型。它在序列标注任务如分词、词性标注和命名实体识别等方面取得了很好的效果；GRU：全称为门控循环单元（Gated Recurrent Unit），是一种常用于序列数据建模的神经网络模型，能够很好解决BERT循环神经网络中的长期依赖问题，捕获序列中的长期特征，避免训练过程中的梯度消失和梯度爆炸，使完成的模型结构具有更优秀的泛化能力、更好地拟合真实地址数据。 4、建立标准地址库使用pg数据库建立标准地址库，地址库将包括地址的原始id、名称信息、poi信息、类别信息、经纬度信息以及18级分级信息。 通过上一步骤训练好地址分级模型，并封装进数据计算中心，使用GPU并发计算原始地址库的数据，并将结果输出保存进标准地址库。 5、地址排序算法主要是基于希尔排序算法思想将模块二抽取到的地址信息进行排列组合，去除噪声、过滤冗余结构，最后拟合得到数据库中最接近当前地址信息的查询条件。 6、建立地址分级匹配系统（检索）通过pg数据库的高效检索功能，实现对地址数据的实时结构化查询和数据推送，实现算法完整功能。 三、接口服务算法目前可提供5类接口： 1、文本地址自动归一接口：提取文本中的所有地址，并标准化； 2、地址检索接口：自动拆分、检索相关地址； 3、地址提取接口：仅提取文本中所有地址，不做标准化处理； 4、地点检索接口：检索相关地点的标准地址； 5、地址分级接口：对输入地址进行层级划分。 四、模型更新 &amp; 数据运维当前版本模型的更新包括分级模型的更新（再训练）和排序算法的更新。数据的维护主要从三个方面进行： 1、爬虫脚本的定期维护，更新原始地址pg数据库； 2、维持标准地址库的数据计算中心与爬虫同步运转； 3、对标准地址库中的错误信息做提取，添加进训练集。 五、机器配置算法和模型的各个模块使用python语言编写，在linux服务器上进行训练和测试，并使用基于NVIDIA CUDA生态的Torch框架进行科学计算。 目前满足模型各功能正常快速运转占用的显存理论上不低于5GB，显卡显存不低于8GB。 六、问题目前模型版本可能存在的问题： 1、B-I-E-O-S序列标注法数据标注过程复杂，且对数据集的精度要求较高； 2、部分复杂结构地址样本暂不充足； 3、爬虫模块需要账号支撑正常运行，爬虫目前递归算法（DFS），可能存在遗漏情况； 4、全国地址更新较为频繁，需要从爬虫和数据计算中心两个层面分别解决数据更新的问题； 5、目前使用pg模糊检索不支持字段索引，随数据量指数上升可能存在性能瓶颈，暂未验证。","categories":[{"name":"地址归一","slug":"地址归一","permalink":"http://yoursite.com/categories/地址归一/"}],"tags":[{"name":"BERT","slug":"BERT","permalink":"http://yoursite.com/tags/BERT/"},{"name":"GRU","slug":"GRU","permalink":"http://yoursite.com/tags/GRU/"}]},{"title":"RTSP推拉流服务搭建","slug":"RTSP推拉流服务搭建","date":"2024-03-07T02:02:13.000Z","updated":"2024-07-08T07:14:12.066Z","comments":true,"path":"2024/03/07/RTSP推拉流服务搭建/","link":"","permalink":"http://yoursite.com/2024/03/07/RTSP推拉流服务搭建/","excerpt":"一、基础服务搭建（windows）1.下载RTSP服务器下载链接：https://github.com/aler9/rtsp-simple-server/releases 2.下载FFmpeg工具下载链接：https://github.com/BtbN/FFmpeg-Builds/releases 3.启动服务器进入RTSP服务器路径，控制台执行.\\mediamtx.exe","text":"一、基础服务搭建（windows）1.下载RTSP服务器下载链接：https://github.com/aler9/rtsp-simple-server/releases 2.下载FFmpeg工具下载链接：https://github.com/BtbN/FFmpeg-Builds/releases 3.启动服务器进入RTSP服务器路径，控制台执行.\\mediamtx.exe 4.启动推流服务进入FFmpeg服务器路径，控制台执行ffmpeg -re -stream_loop -1 -i belt.mp4 -c copy -f rtsp rtsp://127.0.0.1:8554/video，循环播放视频文件并进行推流 5.播放流媒体启动VLC播放器 二、opencv+python 实现RTSP推拉流1.拉流12345678910111213141516171819202122232425262728293031323334353637383940import cv2import timeimport queueimport threadingdef capturevideo(capture, framequeue): while True: if framequeue.full(): time.sleep(0.1) else: ret, frame = capture.read() if not ret: return image_bgr = frame framequeue.put(image_bgr)def execute_rtsp(rtsp_src): capture = cv2.VideoCapture(rtsp_src) framequeue = queue.Queue(10) thread = threading.Thread( target=capturevideo, args=(capture, framequeue)) thread.start() time.sleep(1) width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH)) height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT)) fps = int(capture.get(cv2.CAP_PROP_FPS)) while True: if not framequeue.empty(): frame_bgr = framequeue.get() cv2.imshow(\"image\", frame_bgr) # 显示图像 cv2.waitKey(-1)if __name__ == '__main__': rtsp_url = 'rtsp://192.168.9.164:8554/video' execute_rtsp(rtsp_url) 2.拉流+推流123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import cv2import timeimport queueimport threadingimport subprocess as spdef capturevideo(capture, framequeue): while True: if framequeue.full(): time.sleep(0.1) else: ret, frame = capture.read() if not ret: return image_bgr = frame framequeue.put(image_bgr)def execute_rtsp(rtsp_src, rtsp_dst): capture = cv2.VideoCapture(rtsp_src) framequeue = queue.Queue(10) thread = threading.Thread( target=capturevideo, args=(capture, framequeue)) thread.start() time.sleep(1) width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH)) height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT)) fps = int(capture.get(cv2.CAP_PROP_FPS)) command = ['ffmpeg', '-y', '-f', 'rawvideo', '-vcodec', 'rawvideo', '-pix_fmt', 'bgr24', '-s', \"&#123;&#125;x&#123;&#125;\".format(width, height), '-r', str(fps), '-i', '-', '-c:v', 'libx264', '-pix_fmt', 'yuv420p', '-preset', 'ultrafast', '-f', 'rtsp', rtsp_dst] pipe = sp.Popen(command, stdin=sp.PIPE) while True: if not framequeue.empty(): frame_bgr = framequeue.get() pipe.stdin.write(frame_bgr.tostring())if __name__ == '__main__': rtsp_url = 'rtsp://192.168.9.164:8554/video' rtsp_stream = 'rtsp://192.168.9.164:8554/live' execute_rtsp(rtsp_url, rtsp_stream) 可以看到，视频码流已推送到8554/live地址下 三、视频处理，推流1.多线程处理视频帧主函数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293import cv2import timeimport queueimport threadingimport multiprocessingimport subprocess as spfrom ultralytics import YOLOfrom visualize.detection import *def capturevideo(capture, framequeue): global frame_id while True: if framequeue.full(): time.sleep(0.1) else: ret, frame = capture.read() if not ret: continue image_bgr = frame frame_id += 1 framequeue.put((image_bgr, frame_id))def execute_rtsp(rtsp_src, rtsp_dst): capture = cv2.VideoCapture(rtsp_src) framequeue = queue.Queue(10) outputqueue = queue.Queue(10) thread = threading.Thread( target=capturevideo, args=(capture, framequeue)) thread.start() time.sleep(1) width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH)) height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT)) fps = int(capture.get(cv2.CAP_PROP_FPS)) command = ['ffmpeg', '-y', '-f', 'rawvideo', '-vcodec', 'rawvideo', '-pix_fmt', 'bgr24', '-s', \"&#123;&#125;x&#123;&#125;\".format(width, height), '-r', str(fps), '-i', '-', '-c:v', 'libx264', '-pix_fmt', 'yuv420p', '-preset', 'ultrafast', '-f', 'rtsp', rtsp_dst] pipe = sp.Popen(command, stdin=sp.PIPE) for _ in range(5): t = threading.Thread(target=process, args=(framequeue, outputqueue, pipe)) t.start()def process(framequeue, outputqueue, pipe): global queue_id model = YOLO('runs/detect/helmet_model/weights/best.pt') while True: if not framequeue.empty(): frame_bgr, frame_uid = framequeue.get() results = model.predict( frame_bgr, stream=False, save=False, classes=[0, 2, 4, 7], device=[0, 1], verbose=False, ) names = results[0].names clses = [int(i) for i in results[0].boxes.cls.tolist()] boxes = results[0].boxes.xyxy.tolist() name_set = [names.get(i) for i in clses] frame_bgr = visualize_det(frame_bgr, boxes, name_set) while True: time.sleep(0.0001) if queue_id+1 == frame_uid: outputqueue.put(frame_bgr) break frame = outputqueue.get() pipe.stdin.write(frame.tostring()) queue_id += 1if __name__ == '__main__': frame_id = 0 queue_id = 0 rtsp_url = 'rtsp://192.168.9.164:8554/video' rtsp_stream = 'rtsp://192.168.9.164:8554/live' execute_rtsp(rtsp_url, rtsp_stream) 可视化： 123456789101112131415161718192021222324252627282930313233343536373839404142import osimport cv2import numpy as npfrom PIL import Image, ImageDraw, ImageFile, ImageFontfrom settings import PRfont_file = os.path.join(PR, 'visualize/SourceHanSansCN-Medium.otf')ImageFile.LOAD_TRUNCATED_IMAGES = Truedef visualize_det(im, boxes, names): if isinstance(im, str): im = Image.open(im) im = np.ascontiguousarray(np.copy(im)) im = cv2.cvtColor(im, cv2.COLOR_RGB2BGR) else: im = np.ascontiguousarray(np.copy(im)) im = Image.fromarray(im) im = im.convert('RGBA') for i, name in enumerate(names): box = boxes[i] box = [int(j) for j in box] text = name draw = ImageDraw.Draw(im) draw.text( (box[0], box[1]), text, font=ImageFont.truetype(font_file, size=20), fill=(0, 0, 0, 1000)) draw.rectangle( ((box[0], box[1]), (box[2], box[3])), fill=None, outline=(139, 0, 139), width=1) im = im.convert('RGB') im = np.ascontiguousarray(np.copy(im)) return im 2.多线程封装123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118import cv2import timeimport queueimport threadingimport subprocess as spimport concurrent.futuresfrom ultralytics import YOLOfrom visualize.detection import *from settings import *lock = threading.Lock()class StreamingDetectionServer: def __init__(self, stream_src, stream_dst): self.pipe = None self.model = None self.capture = None self.fps = 0 self.count = 0 self.width = 0 self.height = 0 self.frame_id = 0 self.solve_id = 1 self.stream_src = stream_src self.stream_dst = stream_dst self.framequeue = queue.PriorityQueue(10) self.outputqueue = queue.PriorityQueue() def run(self): self.capture = cv2.VideoCapture(self.stream_src) self.fps = int(self.capture.get(cv2.CAP_PROP_FPS)) self.width = int(self.capture.get(cv2.CAP_PROP_FRAME_WIDTH)) self.height = int(self.capture.get(cv2.CAP_PROP_FRAME_HEIGHT)) command = ['ffmpeg', # '-y', '-re', '-f', 'rawvideo', '-vcodec', 'rawvideo', '-pix_fmt', 'bgr24', '-s', \"&#123;&#125;x&#123;&#125;\".format(self.width, self.height), '-r', str(self.fps), '-i', '-', '-c:v', 'libx264', '-pix_fmt', 'yuv420p', '-preset', 'ultrafast', '-f', 'rtsp', self.stream_dst] pipe = sp.Popen(command, stdin=sp.PIPE) capture_thread = threading.Thread(target=self.capture_video) capture_thread.start() for _ in range(5): process_thread = threading.Thread(target=self.process_video) process_thread.start() output_thread = threading.Thread(target=self.output_video, args=(pipe,)) output_thread.start() def capture_video(self): while True: if self.framequeue.full(): time.sleep(0.01) else: ret, frame = self.capture.read() if not ret: continue image_bgr = frame self.frame_id += 1 self.framequeue.put((self.frame_id, image_bgr)) def output_video(self, pipe): while True: if not self.outputqueue.empty(): frame_id, frame_bgr = self.outputqueue.get() pipe.stdin.write(frame_bgr.tobytes()) else: time.sleep(0.01) def process_video(self): model = YOLO('runs/detect/helmet_model/weights/best.pt') while True: if not self.framequeue.empty(): with lock: frame_id, frame_bgr = self.framequeue.get() results = model.track( frame_bgr, stream=False, save=False, classes=[0, 2, 4, 7], device=\"0\", verbose=False, tracker=\"bytetrack.yaml\" ) names = results[0].names clses = [int(i) for i in results[0].boxes.cls.tolist()] boxes = results[0].boxes.xyxy.tolist() name_set = [names.get(i) for i in clses] frame_bgr = visualize_det_cv2(frame_bgr, boxes, name_set) while True: time.sleep(0.001) if self.solve_id == frame_id: self.outputqueue.put((frame_id, frame_bgr)) self.solve_id += 1 breakif __name__ == '__main__': rtsp_url = input('码流链接：') rtsp_stream = 'rtsp://192.168.9.164:8554/live2' sds = StreamingDetectionServer(rtsp_url, rtsp_stream) sds.run() 3.多进程处理视频帧（优化到极致，进一步提升算法处理速度，后续将单独开板块做出说明）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156import cv2import timeimport queueimport torchimport threadingimport multiprocessingimport subprocess as spimport concurrent.futuresfrom ultralytics import YOLOfrom visualize.detection import *from settings import *from queue import PriorityQueuefrom multiprocessing.managers import BaseManagerlock = multiprocessing.Lock()class Manager(BaseManager): passManager.register('get_priorityQueue', PriorityQueue)class StreamingDetectionServer: def __init__(self, stream_src, stream_dst): self.pipe = None self.model = None self.capture = None self.fps = 0 self.count = 0 self.width = 0 self.height = 0 self.frame_id = 0 self.stream_src = stream_src self.stream_dst = stream_dst def run(self): q = queue.PriorityQueue() # 用于将多进程队列中的数据同步到线程主线程中的优先级队列，实现按帧出队列 m = Manager() m.start() # framequeue = multiprocessing.Queue(10) framequeue = torch.multiprocessing.Queue(10) outputqueue = multiprocessing.Queue() # outputqueue = m.get_priorityQueue() # 继承多线程优先级队列的多进程队列，处理时间过长，放弃 self.capture = cv2.VideoCapture(self.stream_src) self.fps = int(self.capture.get(cv2.CAP_PROP_FPS)) self.width = int(self.capture.get(cv2.CAP_PROP_FRAME_WIDTH)) self.height = int(self.capture.get(cv2.CAP_PROP_FRAME_HEIGHT)) command = ['ffmpeg', # '-y', '-re', '-f', 'rawvideo', '-vcodec', 'rawvideo', '-pix_fmt', 'bgr24', '-s', \"&#123;&#125;x&#123;&#125;\".format(self.width, self.height), '-r', str(self.fps), '-i', '-', '-c:v', 'libx264', '-pix_fmt', 'yuv420p', '-preset', 'ultrafast', '-f', 'rtsp', self.stream_dst] pipe = sp.Popen(command, stdin=sp.PIPE) solve_id = multiprocessing.Value('i', 1) # 使用进程全局变量记录处理视频帧的顺序 capture_thread = threading.Thread(target=self.capture_video, args=(framequeue,)) capture_thread.start() for _ in range(5): process_thread = multiprocessing.Process(target=self.process_video, args=(framequeue, outputqueue, solve_id)) process_thread.start() queue_thread = threading.Thread(target=self.make_queue, args=(outputqueue, q)) queue_thread.start() output_thread = threading.Thread(target=self.output_video, args=(pipe, q)) output_thread.start() # 获取码流 def capture_video(self, framequeue): while True: if framequeue.full(): time.sleep(0.01) else: ret, frame = self.capture.read() if not ret: continue image_bgr = frame image_bgr = torch.from_numpy(image_bgr) image_bgr.share_memory_() self.frame_id += 1 framequeue.put((self.frame_id, image_bgr)) # 多进程队列数据同步至优先级队列 @staticmethod def make_queue(outputqueue, q): while True: if not outputqueue.empty(): frame_id, frame_bgr = outputqueue.get() q.put((frame_id, frame_bgr)) else: time.sleep(0.001) # 从优先级队列推流 @staticmethod def output_video(pipe, q): while True: if q.qsize() &gt;= 10: frame_id, frame_bgr = q.get() pipe.stdin.write(frame_bgr.tobytes()) else: time.sleep(0.001) # 多进程 @staticmethod def process_video(framequeue, outputqueue, solve_id): model = YOLO('runs/detect/helmet_model/weights/best.pt') while True: if not framequeue.empty(): with lock: frame_id, frame_bgr = framequeue.get() frame_bgr = frame_bgr.numpy() results = model.track( frame_bgr, stream=False, save=False, classes=[0, 2, 4, 7], device=\"0\", batch=16, verbose=False, tracker=\"bytetrack.yaml\" ) names = results[0].names clses = [int(i) for i in results[0].boxes.cls.tolist()] boxes = results[0].boxes.xyxy.tolist() name_set = [names.get(i) for i in clses] frame_bgr = visualize_det_cv2(frame_bgr, boxes, name_set) # 通过共享实现id实现优先级入队 while True: if solve_id.value == frame_id: outputqueue.put((frame_id, frame_bgr)) solve_id.value += 1 breakif __name__ == '__main__': rtsp_url = input('码流链接：') rtsp_stream = 'rtsp://192.168.9.164:8554/live' sds = StreamingDetectionServer(rtsp_url, rtsp_stream) sds.run()","categories":[{"name":"流媒体","slug":"流媒体","permalink":"http://yoursite.com/categories/流媒体/"}],"tags":[{"name":"yolov8","slug":"yolov8","permalink":"http://yoursite.com/tags/yolov8/"},{"name":"RTSP","slug":"RTSP","permalink":"http://yoursite.com/tags/RTSP/"}]},{"title":"yolov8-pose：关键点姿态检测","slug":"yolov8-pose","date":"2024-01-30T04:00:00.000Z","updated":"2024-06-28T07:31:50.162Z","comments":true,"path":"2024/01/30/yolov8-pose/","link":"","permalink":"http://yoursite.com/2024/01/30/yolov8-pose/","excerpt":"环境&amp;安装同上文yolov8：火灾检测 模型使用yolov8n-pose 数据标注标注工具：labelme对图像中的目标（人物）及其关键点进行标记，包括1个目标类别和17个关键点类别 数据格式转换将labelme数据格式转为yolo格式，通用转换代码： 123# TODO:# 参考yolov8-火灾检测，未完待续... 创建训练yaml文件参考yolov8n-pose.yaml 123456789101112train: /exp/work/video/yolov8/datasets/human-pose/images/train #训练集文件夹val: /exp/work/video/yolov8/datasets/human-pose/images/val # 验证集文件夹test: /exp/work/video/yolov8/datasets/human-pose/images/val # 测试集文件夹nc: 1 # 分类数# 关键点，每个关键点有 X Y 是否可见 三个参数# 可见性：2-可见不遮挡 1-遮挡 0-没有点kpt_shape: [17, 3]# 框的类别（对于关键点检测，只有一类）names: 0: people","text":"环境&amp;安装同上文yolov8：火灾检测 模型使用yolov8n-pose 数据标注标注工具：labelme对图像中的目标（人物）及其关键点进行标记，包括1个目标类别和17个关键点类别 数据格式转换将labelme数据格式转为yolo格式，通用转换代码： 123# TODO:# 参考yolov8-火灾检测，未完待续... 创建训练yaml文件参考yolov8n-pose.yaml 123456789101112train: /exp/work/video/yolov8/datasets/human-pose/images/train #训练集文件夹val: /exp/work/video/yolov8/datasets/human-pose/images/val # 验证集文件夹test: /exp/work/video/yolov8/datasets/human-pose/images/val # 测试集文件夹nc: 1 # 分类数# 关键点，每个关键点有 X Y 是否可见 三个参数# 可见性：2-可见不遮挡 1-遮挡 0-没有点kpt_shape: [17, 3]# 框的类别（对于关键点检测，只有一类）names: 0: people 训练代码12345678910111213from ultralytics import YOLO# 加载模型model = YOLO('yolov8n.pt') # 加载预训练模型# 双卡训练model.train( data='datasets/human-pose/data/human-pose.yaml', epochs=300, device=[0, 1])# 启动验证model.val() 预测导包、模型加载 &amp; GPU加载1234567891011121314151617import osimport cv2import numpy as npimport timeimport torchfrom tqdm import tqdmfrom ultralytics import YOLOimport matplotlib.pyplot as plt# GPUos.environ['CUDA_VISIBLE_DEVICES'] = '0,1'# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')# print('using device:', device)model = YOLO('yolov8n-pose.pt')# model.to(device) 设计样式参数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# 检测框（rectangle）可视化配置bbox_color = (150, 0, 0) # 框的 BGR 颜色bbox_thickness = 2 # 框的线宽# 检测框类别文字bbox_labelstr = &#123; 'font_size': 1, # 字体大小 'font_thickness': 2, # 字体粗细 'offset_x': 0, # X 方向，文字偏移距离，向右为正 'offset_y': -10, # Y 方向，文字偏移距离，向下为正&#125;# 关键点 BGR 配色kpt_color_map = &#123; 0: &#123;'name': 'Nose', 'color': [0, 0, 255], 'radius': 6&#125;, # 鼻尖 1: &#123;'name': 'Right Eye', 'color': [255, 0, 0], 'radius': 6&#125;, # 右边眼睛 2: &#123;'name': 'Left Eye', 'color': [255, 0, 0], 'radius': 6&#125;, # 左边眼睛 3: &#123;'name': 'Right Ear', 'color': [0, 255, 0], 'radius': 6&#125;, # 右边耳朵 4: &#123;'name': 'Left Ear', 'color': [0, 255, 0], 'radius': 6&#125;, # 左边耳朵 5: &#123;'name': 'Right Shoulder', 'color': [193, 182, 255], 'radius': 6&#125;, # 右边肩膀 6: &#123;'name': 'Left Shoulder', 'color': [193, 182, 255], 'radius': 6&#125;, # 左边肩膀 7: &#123;'name': 'Right Elbow', 'color': [16, 144, 247], 'radius': 6&#125;, # 右侧胳膊肘 8: &#123;'name': 'Left Elbow', 'color': [16, 144, 247], 'radius': 6&#125;, # 左侧胳膊肘 9: &#123;'name': 'Right Wrist', 'color': [1, 240, 255], 'radius': 6&#125;, # 右侧手腕 10: &#123;'name': 'Left Wrist', 'color': [1, 240, 255], 'radius': 6&#125;, # 左侧手腕 11: &#123;'name': 'Right Hip', 'color': [140, 47, 240], 'radius': 6&#125;, # 右侧胯 12: &#123;'name': 'Left Hip', 'color': [140, 47, 240], 'radius': 6&#125;, # 左侧胯 13: &#123;'name': 'Right Knee', 'color': [223, 155, 60], 'radius': 6&#125;, # 右侧膝盖 14: &#123;'name': 'Left Knee', 'color': [223, 155, 60], 'radius': 6&#125;, # 左侧膝盖 15: &#123;'name': 'Right Ankle', 'color': [139, 0, 0], 'radius': 6&#125;, # 右侧脚踝 16: &#123;'name': 'Left Ankle', 'color': [139, 0, 0], 'radius': 6&#125;, # 左侧脚踝&#125;# 点类别文字kpt_labelstr = &#123; 'font_size': 0.5, # 字体大小 'font_thickness': 1, # 字体粗细 'offset_x': 10, # X 方向，文字偏移距离，向右为正 'offset_y': 0, # Y 方向，文字偏移距离，向下为正&#125;# 骨架连接 BGR 配色skeleton_map = [ &#123;'srt_kpt_id': 15, 'dst_kpt_id': 13, 'color': [0, 100, 255], 'thickness': 2&#125;, # 右侧脚踝-右侧膝盖 &#123;'srt_kpt_id': 13, 'dst_kpt_id': 11, 'color': [0, 255, 0], 'thickness': 2&#125;, # 右侧膝盖-右侧胯 &#123;'srt_kpt_id': 16, 'dst_kpt_id': 14, 'color': [255, 0, 0], 'thickness': 2&#125;, # 左侧脚踝-左侧膝盖 &#123;'srt_kpt_id': 14, 'dst_kpt_id': 12, 'color': [0, 0, 255], 'thickness': 2&#125;, # 左侧膝盖-左侧胯 &#123;'srt_kpt_id': 11, 'dst_kpt_id': 12, 'color': [122, 160, 255], 'thickness': 2&#125;, # 右侧胯-左侧胯 &#123;'srt_kpt_id': 5, 'dst_kpt_id': 11, 'color': [139, 0, 139], 'thickness': 2&#125;, # 右边肩膀-右侧胯 &#123;'srt_kpt_id': 6, 'dst_kpt_id': 12, 'color': [237, 149, 100], 'thickness': 2&#125;, # 左边肩膀-左侧胯 &#123;'srt_kpt_id': 5, 'dst_kpt_id': 6, 'color': [152, 251, 152], 'thickness': 2&#125;, # 右边肩膀-左边肩膀 &#123;'srt_kpt_id': 5, 'dst_kpt_id': 7, 'color': [148, 0, 69], 'thickness': 2&#125;, # 右边肩膀-右侧胳膊肘 &#123;'srt_kpt_id': 6, 'dst_kpt_id': 8, 'color': [0, 75, 255], 'thickness': 2&#125;, # 左边肩膀-左侧胳膊肘 &#123;'srt_kpt_id': 7, 'dst_kpt_id': 9, 'color': [56, 230, 25], 'thickness': 2&#125;, # 右侧胳膊肘-右侧手腕 &#123;'srt_kpt_id': 8, 'dst_kpt_id': 10, 'color': [0, 240, 240], 'thickness': 2&#125;, # 左侧胳膊肘-左侧手腕 &#123;'srt_kpt_id': 1, 'dst_kpt_id': 2, 'color': [224, 255, 255], 'thickness': 2&#125;, # 右边眼睛-左边眼睛 &#123;'srt_kpt_id': 0, 'dst_kpt_id': 1, 'color': [47, 255, 173], 'thickness': 2&#125;, # 鼻尖-左边眼睛 &#123;'srt_kpt_id': 0, 'dst_kpt_id': 2, 'color': [203, 192, 255], 'thickness': 2&#125;, # 鼻尖-左边眼睛 &#123;'srt_kpt_id': 1, 'dst_kpt_id': 3, 'color': [196, 75, 255], 'thickness': 2&#125;, # 右边眼睛-右边耳朵 &#123;'srt_kpt_id': 2, 'dst_kpt_id': 4, 'color': [86, 0, 25], 'thickness': 2&#125;, # 左边眼睛-左边耳朵 &#123;'srt_kpt_id': 3, 'dst_kpt_id': 5, 'color': [255, 255, 0], 'thickness': 2&#125;, # 右边耳朵-右边肩膀 &#123;'srt_kpt_id': 4, 'dst_kpt_id': 6, 'color': [255, 18, 200], 'thickness': 2&#125; # 左边耳朵-左边肩膀] 视频处理函数1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162def process_frame(img_bgr): \"\"\" 输入摄像头画面 bgr-array，输出图像 bgr-array \"\"\" results = model(img_bgr, verbose=False) # verbose设置为False，不单独打印每一帧预测结果 # 预测框的个数 num_bbox = len(results[0].boxes.cls) # 预测框的 xyxy 坐标 bboxes_xyxy = results[0].boxes.xyxy.cpu().numpy().astype('uint32') # 关键点的 xy 坐标 bboxes_keypoints = results[0].keypoints.data.cpu().numpy() for idx in range(num_bbox): # 遍历每个框 # 获取该框坐标 bbox_xyxy = bboxes_xyxy[idx] # 获取框的预测类别(对于关键点检测，只有一个类别) bbox_label = results[0].names[0] # 画框 img_bgr = cv2.rectangle(img_bgr, (bbox_xyxy[0], bbox_xyxy[1]), (bbox_xyxy[2], bbox_xyxy[3]), bbox_color, bbox_thickness) # 写框类别文字：图片，文字字符串，文字左上角坐标，字体，字体大小，颜色，字体粗细 img_bgr = cv2.putText(img_bgr, bbox_label, (bbox_xyxy[0] + bbox_labelstr['offset_x'], bbox_xyxy[1] + bbox_labelstr['offset_y']), cv2.FONT_HERSHEY_SIMPLEX, bbox_labelstr['font_size'], bbox_color, bbox_labelstr['font_thickness']) bbox_keypoints = bboxes_keypoints[idx] # 该框所有关键点坐标和置信度 # 画该框的骨架连接 for skeleton in skeleton_map: # 获取起始点坐标 srt_kpt_id = skeleton['srt_kpt_id'] srt_kpt_x = round(bbox_keypoints[srt_kpt_id][0]) srt_kpt_y = round(bbox_keypoints[srt_kpt_id][1]) srt_kpt_conf = bbox_keypoints[srt_kpt_id][2] # 获取起始点置信度 # print(srt_kpt_conf) # 获取终止点坐标 dst_kpt_id = skeleton['dst_kpt_id'] dst_kpt_x = round(bbox_keypoints[dst_kpt_id][0]) dst_kpt_y = round(bbox_keypoints[dst_kpt_id][1]) dst_kpt_conf = bbox_keypoints[dst_kpt_id][2] # 获取终止点置信度 # print(dst_kpt_conf) # 获取骨架连接颜色 skeleton_color = skeleton['color'] # 获取骨架连接线宽 skeleton_thickness = skeleton['thickness'] # 如果起始点和终止点的置信度都高于阈值，才画骨架连接 if srt_kpt_conf &gt; 0.5 and dst_kpt_conf &gt; 0.5: # 画骨架连接 img_bgr = cv2.line(img_bgr, (srt_kpt_x, srt_kpt_y), (dst_kpt_x, dst_kpt_y), color=skeleton_color, thickness=skeleton_thickness) # 画该框的关键点 for kpt_id in kpt_color_map: # 获取该关键点的颜色、半径、XY坐标 kpt_color = kpt_color_map[kpt_id]['color'] kpt_radius = kpt_color_map[kpt_id]['radius'] kpt_x = round(bbox_keypoints[kpt_id][0]) kpt_y = round(bbox_keypoints[kpt_id][1]) kpt_conf = bbox_keypoints[kpt_id][2] # 获取该关键点置信度 if kpt_conf &gt; 0.5: # 画圆：图片、XY坐标、半径、颜色、线宽(-1为填充) img_bgr = cv2.circle(img_bgr, (kpt_x, kpt_y), kpt_radius, kpt_color, -1) return img_bgr CV2处理函数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061def generate_video(input_path='video/robot.mp4'): file_head = input_path.split('/')[-1] output_path = \"out-\" + file_head print('视频开始处理', input_path) # 获取视频总帧数 cap = cv2.VideoCapture(input_path) frame_count = 0 while cap.isOpened(): success, frame = cap.read() frame_count += 1 if not success: break cap.release() print('视频总帧数为', frame_count) # cv2.namedWindow('Crack Detection and Measurement Video Processing') cap = cv2.VideoCapture(input_path) frame_size = (cap.get(cv2.CAP_PROP_FRAME_WIDTH), cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) # fourcc = int(cap.get(cv2.CAP_PROP_FOURCC)) # fourcc = cv2.VideoWriter_fourcc(*'XVID') fourcc = cv2.VideoWriter_fourcc(*'mp4v') fps = cap.get(cv2.CAP_PROP_FPS) out = cv2.VideoWriter(output_path, fourcc, fps, (int(frame_size[0]), int(frame_size[1]))) # 进度条绑定视频总帧数 with tqdm(total=frame_count - 1) as pbar: # noinspection PyBroadException try: while cap.isOpened(): success, frame = cap.read() if not success: break # noinspection PyBroadException try: frame = process_frame(frame) except Exception: print('error') pass if success: # cv2.imshow('Video Processing', frame) out.write(frame) # 进度条更新一帧 pbar.update(1) # if cv2.waitKey(1) &amp; 0xFF == ord('q'): # break except Exception: print('中途中断') pass cv2.destroyAllWindows() out.release() cap.release() print('视频已保存', output_path) 预测将代码整合以后，执行： 12if __name__ == '__main__': generate_video(input_path='video/test.mp4')","categories":[{"name":"关键点检测","slug":"关键点检测","permalink":"http://yoursite.com/categories/关键点检测/"}],"tags":[{"name":"yolov8","slug":"yolov8","permalink":"http://yoursite.com/tags/yolov8/"},{"name":"opencv","slug":"opencv","permalink":"http://yoursite.com/tags/opencv/"}]},{"title":"yolov8-火灾检测","slug":"yolov8：火灾检测","date":"2024-01-25T09:23:20.000Z","updated":"2024-02-07T01:54:57.353Z","comments":true,"path":"2024/01/25/yolov8：火灾检测/","link":"","permalink":"http://yoursite.com/2024/01/25/yolov8：火灾检测/","excerpt":"环境GPU NVIDIA 3090*2 显卡驱动 535.104.05 CUDA版本 12.2 CUDAtoolkit (cuda_12.2.2_535.104.05_linux) cuDNN (v8.9.7) yolo版本 v8.1.5 (ultralytics yolov8) pytorch版本 v2.1.2 python环境 CentOS7.9 anaconda3 python3.9 安装源码主页：https://github.com/ultralytics/ultralytics 官方文档：https://docs.ultralytics.com/zh 克隆源码1git clone https://hub.nuaa.cf/ultralytics/ultralytics.git 安装依赖1pip install pip install ultralytics -i https://mirror.baidu.com/pypi/simple 环境验证python 12import ultralyticsultralytics.checks() cli 1yolo predict model=yolov8n.pt source=ultralytics/assets/zidane.jpg 执行完毕后得到输出的结果如下： 12345678(py39_yolov8) [root@jdz yolov8]# yolo predict model=yolov8n.pt source=ultralytics/assets/zidane.jpg Ultralytics YOLOv8.1.5 🚀 Python-3.9.18 torch-2.1.2+cu121 CUDA:0 (NVIDIA GeForce RTX 3090, 24260MiB)YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPsimage 1/1 /exp/work/video/yolov8/ultralytics/assets/zidane.jpg: 384x640 2 persons, 1 tie, 216.9msSpeed: 7.3ms preprocess, 216.9ms inference, 762.4ms postprocess per image at shape (1, 3, 384, 640)Results saved to runs/detect/predict💡 Learn more at https://docs.ultralytics.com/modes/predict 将在Results saved to runs/detect/predict目录下找到输出结果","text":"环境GPU NVIDIA 3090*2 显卡驱动 535.104.05 CUDA版本 12.2 CUDAtoolkit (cuda_12.2.2_535.104.05_linux) cuDNN (v8.9.7) yolo版本 v8.1.5 (ultralytics yolov8) pytorch版本 v2.1.2 python环境 CentOS7.9 anaconda3 python3.9 安装源码主页：https://github.com/ultralytics/ultralytics 官方文档：https://docs.ultralytics.com/zh 克隆源码1git clone https://hub.nuaa.cf/ultralytics/ultralytics.git 安装依赖1pip install pip install ultralytics -i https://mirror.baidu.com/pypi/simple 环境验证python 12import ultralyticsultralytics.checks() cli 1yolo predict model=yolov8n.pt source=ultralytics/assets/zidane.jpg 执行完毕后得到输出的结果如下： 12345678(py39_yolov8) [root@jdz yolov8]# yolo predict model=yolov8n.pt source=ultralytics/assets/zidane.jpg Ultralytics YOLOv8.1.5 🚀 Python-3.9.18 torch-2.1.2+cu121 CUDA:0 (NVIDIA GeForce RTX 3090, 24260MiB)YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPsimage 1/1 /exp/work/video/yolov8/ultralytics/assets/zidane.jpg: 384x640 2 persons, 1 tie, 216.9msSpeed: 7.3ms preprocess, 216.9ms inference, 762.4ms postprocess per image at shape (1, 3, 384, 640)Results saved to runs/detect/predict💡 Learn more at https://docs.ultralytics.com/modes/predict 将在Results saved to runs/detect/predict目录下找到输出结果 至此，已验证基础环境正常工作。 训练集前往kaggle官网寻找火焰训练集，这里使用Fire and Smoke Dataset数据集（https://www.kaggle.com/datasets/dataclusterlabs/fire-and-smoke-dataset） 数据集包含烟、火图像共100张 数据标注标注工具：labelme对图像中的火焰和烟雾进行矩形标记，将数据的label分为两类：0-火焰；1-烟雾 数据格式转换将labelme数据格式转为yolo格式 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138import osimport jsonimport shutilimport randomimport numpy as npfrom tqdm import tqdmfrom settings import PR# 数据集目录Dataset_root = os.path.join(PR, 'datasets/protective_clothing')os.chdir(fr'&#123;Dataset_root&#125;/images')test_frac = 0.2 # 测试集比例random.seed(123) # 随机数种子，便于复现img_paths = os.listdir(fr'&#123;Dataset_root&#125;/images')random.shuffle(img_paths) # 随机打乱val_number = int(len(img_paths) * test_frac) # 测试集文件个数train_files = img_paths[val_number:] # 训练集文件名列表val_files = img_paths[:val_number] # 测试集文件名列表print('数据集文件总数', len(img_paths))print('训练集文件个数', len(train_files))print('测试集文件个数', len(val_files))if not os.path.exists('train'): os.mkdir('train') for each in tqdm(train_files): shutil.move(each, 'train')if not os.path.exists('val'): os.mkdir('val') for each in tqdm(val_files): shutil.move(each, 'val')os.chdir(fr'&#123;Dataset_root&#125;/labelme_jsons')if not os.path.exists('train'): os.mkdir('train') for each in tqdm(train_files): srt_path = each.split('.')[0] + '.json' shutil.move(srt_path, 'train')if not os.path.exists('val'): os.mkdir('val') for each in tqdm(val_files): srt_path = each.split('.')[0] + '.json' shutil.move(srt_path, 'val')classes = &#123; 'unprotected': 0, 'protected': 1&#125;os.chdir(Dataset_root)with open('classes.txt', 'w', encoding='utf-8') as f: for each in list(classes.keys()): f.write(each + '\\n')if not os.path.exists('labels'): os.mkdir('labels')if not os.path.exists('labels/train'): os.mkdir('labels/train')if not os.path.exists('labels/val'): os.mkdir('labels/val')def process_single_json(labelme_path, save_folder=fr'&#123;Dataset_root&#125;/labels/train'): # 载入 labelme格式的 json 标注文件 with open(labelme_path, 'r', encoding='utf-8') as f: labelme = json.load(f) img_width = labelme['imageWidth'] # 图像宽度 img_height = labelme['imageHeight'] # 图像高度 # 生成 YOLO 格式的 txt 文件 suffix = labelme_path.split('.')[-2] yolo_txt_path = suffix + '.txt' with open(yolo_txt_path, 'w', encoding='utf-8') as f: for each_ann in labelme['shapes']: # 遍历每个框 if each_ann['shape_type'] == 'rectangle': # 筛选出框 # 获取类别 ID bbox_class_id = classes[each_ann['label']] # 左上角和右下角的 XY 像素坐标 bbox_top_left_x = int(min(each_ann['points'][0][0], each_ann['points'][1][0])) bbox_bottom_right_x = int(max(each_ann['points'][0][0], each_ann['points'][1][0])) bbox_top_left_y = int(min(each_ann['points'][0][1], each_ann['points'][1][1])) bbox_bottom_right_y = int(max(each_ann['points'][0][1], each_ann['points'][1][1])) # 框中心点的 XY 像素坐标 bbox_center_x = int((bbox_top_left_x + bbox_bottom_right_x) / 2) bbox_center_y = int((bbox_top_left_y + bbox_bottom_right_y) / 2) # 框宽度 bbox_width = bbox_bottom_right_x - bbox_top_left_x # 框高度 bbox_height = bbox_bottom_right_y - bbox_top_left_y # 框中心点归一化坐标 bbox_center_x_norm = bbox_center_x / img_width bbox_center_y_norm = bbox_center_y / img_height # 框归一化宽度 bbox_width_norm = bbox_width / img_width # 框归一化高度 bbox_height_norm = bbox_height / img_height # 生成 YOLO 格式的一行标注，指定保留小数点后几位 bbox_yolo_str = '&#123;&#125; &#123;:.4f&#125; &#123;:.4f&#125; &#123;:.4f&#125; &#123;:.4f&#125;'.format(bbox_class_id, bbox_center_x_norm, bbox_center_y_norm, bbox_width_norm, bbox_height_norm) # 写入 txt 文件中 f.write(bbox_yolo_str + '\\n') shutil.move(yolo_txt_path, save_folder) print('&#123;&#125; --&gt; &#123;&#125; 转换完成'.format(labelme_path, yolo_txt_path))os.chdir(fr'&#123;Dataset_root&#125;/labelme_jsons/train')save_folder = fr'&#123;Dataset_root&#125;/labels/train'for path in os.listdir(): process_single_json(path, save_folder=save_folder)print('YOLO格式的txt标注文件已保存至 ', save_folder)os.chdir(fr'&#123;Dataset_root&#125;/labelme_jsons/val')save_folder = fr'&#123;Dataset_root&#125;//labels/val'for path in os.listdir(): process_single_json(path, save_folder=save_folder)print('YOLO格式的txt标注文件已保存至 ', save_folder) 创建训练yaml文件在数据集的根目录下新建fire.yaml文件用于记录训练集信息 123456train: /exp/work/video/yolov8/datasets/fire/data/train/images # 训练集val: /exp/work/video/yolov8/datasets/fire/data/val/images # 验证集test: /exp/work/video/yolov8/datasets/fire/data/test/images # 测试集nc: 2 # 分类数量names: [fire, smoke] # 类别名称 训练在项目根目录下创建训练脚本yolov8_train.py，并设置双卡训练 12345678910111213from ultralytics import YOLO# 加载模型model = YOLO('yolov8n.pt') # 加载预训练模型# 双卡训练model.train( data='datasets/fire/data/fire.yaml', epochs=300, device=[0, 1])# 启动验证model.val() 启动训练脚本，开启训练 1python yolov8_train.py 打印配置信息 1234Ultralytics YOLOv8.1.5 🚀 Python-3.9.18 torch-2.1.2+cu121 CUDA:0 (NVIDIA GeForce RTX 3090, 24260MiB) CUDA:1 (NVIDIA GeForce RTX 3090, 24260MiB)engine/trainer: task=detect, mode=train, model=yolov8n.pt, data=datasets/fire/data/fire.yaml, epochs=300, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=[0, 1], workers=8, project=None, name=train9, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train9Overriding model.yaml nc=80 with nc=2 123456789101112131415161718192021222324252627282930313233343536373839 from n params module arguments 0 -1 1 464 ultralytics.nn.modules.conv.Conv [3, 16, 3, 2] 1 -1 1 4672 ultralytics.nn.modules.conv.Conv [16, 32, 3, 2] 2 -1 1 7360 ultralytics.nn.modules.block.C2f [32, 32, 1, True] 3 -1 1 18560 ultralytics.nn.modules.conv.Conv [32, 64, 3, 2] 4 -1 2 49664 ultralytics.nn.modules.block.C2f [64, 64, 2, True] 5 -1 1 73984 ultralytics.nn.modules.conv.Conv [64, 128, 3, 2] 6 -1 2 197632 ultralytics.nn.modules.block.C2f [128, 128, 2, True] 7 -1 1 295424 ultralytics.nn.modules.conv.Conv [128, 256, 3, 2] 8 -1 1 460288 ultralytics.nn.modules.block.C2f [256, 256, 1, True] 9 -1 1 164608 ultralytics.nn.modules.block.SPPF [256, 256, 5] 10 -1 1 0 torch.nn.modules.upsampling.Upsample [None, 2, 'nearest'] 11 [-1, 6] 1 0 ultralytics.nn.modules.conv.Concat [1] 12 -1 1 148224 ultralytics.nn.modules.block.C2f [384, 128, 1] 13 -1 1 0 torch.nn.modules.upsampling.Upsample [None, 2, 'nearest'] 14 [-1, 4] 1 0 ultralytics.nn.modules.conv.Concat [1] 15 -1 1 37248 ultralytics.nn.modules.block.C2f [192, 64, 1] 16 -1 1 36992 ultralytics.nn.modules.conv.Conv [64, 64, 3, 2] 17 [-1, 12] 1 0 ultralytics.nn.modules.conv.Concat [1] 18 -1 1 123648 ultralytics.nn.modules.block.C2f [192, 128, 1] 19 -1 1 147712 ultralytics.nn.modules.conv.Conv [128, 128, 3, 2] 20 [-1, 9] 1 0 ultralytics.nn.modules.conv.Concat [1] 21 -1 1 493056 ultralytics.nn.modules.block.C2f [384, 256, 1] 22 [15, 18, 21] 1 751702 ultralytics.nn.modules.head.Detect [2, [64, 128, 256]] Model summary: 225 layers, 3011238 parameters, 3011222 gradients, 8.2 GFLOPsTransferred 319/355 items from pretrained weightsDDP: debug command /root/anaconda3/envs/py39_yolov8/bin/python -m torch.distributed.run --nproc_per_node 2 --master_port 45027 /root/.config/Ultralytics/DDP/_temp_bfxihrb7140614302513712.pyWARNING:__main__:*****************************************Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. *****************************************Ultralytics YOLOv8.1.5 🚀 Python-3.9.18 torch-2.1.2+cu121 CUDA:0 (NVIDIA GeForce RTX 3090, 24260MiB) CUDA:1 (NVIDIA GeForce RTX 3090, 24260MiB)Overriding model.yaml nc=80 with nc=2Transferred 319/355 items from pretrained weightsFreezing layer 'model.22.dfl.conv.weight'AMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...AMP: checks passed ✅ 记录模型输出 123456Plotting labels to runs/detect/train9/labels.jpg... optimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... optimizer: AdamW(lr=0.000714, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)Image sizes 640 train, 640 valUsing 16 dataloader workersLogging results to runs/detect/train9 训练开始 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758Starting training for 300 epochs... Epoch GPU_mem box_loss cls_loss dfl_loss Instances Size 1/300 1.38G 2.132 4.599 2.13 6 640: 100%|██████████| 3/3 [00:04&lt;00:00, 1.53s/it] Class Images Instances Box(P R mAP50 mAP50-95): 100%|██████████| 3/3 [00:00&lt;00:00, 3.10it/s] all 41 62 0.00191 0.349 0.0223 0.0121 Epoch GPU_mem box_loss cls_loss dfl_loss Instances Size 2/300 1.44G 1.984 4.085 1.942 18 640: 100%|██████████| 3/3 [00:00&lt;00:00, 5.08it/s] Class Images Instances Box(P R mAP50 mAP50-95): 100%|██████████| 3/3 [00:00&lt;00:00, 11.57it/s] all 41 62 0.00209 0.373 0.0247 0.0119 Epoch GPU_mem box_loss cls_loss dfl_loss Instances Size 3/300 1.48G 2.421 4.407 2.348 14 640: 100%|██████████| 3/3 [00:00&lt;00:00, 5.78it/s] Class Images Instances Box(P R mAP50 mAP50-95): 100%|██████████| 3/3 [00:00&lt;00:00, 10.23it/s] all 41 62 0.00205 0.373 0.0335 0.0165 Epoch GPU_mem box_loss cls_loss dfl_loss Instances Size 4/300 1.48G 1.795 3.787 1.724 18 640: 100%|██████████| 3/3 [00:00&lt;00:00, 5.61it/s] Class Images Instances Box(P R mAP50 mAP50-95): 100%|██████████| 3/3 [00:00&lt;00:00, 9.82it/s] all 41 62 0.00239 0.434 0.0445 0.0219 Epoch GPU_mem box_loss cls_loss dfl_loss Instances Size 5/300 1.48G 1.707 3.619 1.785 17 640: 100%|██████████| 3/3 [00:00&lt;00:00, 5.49it/s] Class Images Instances Box(P R mAP50 mAP50-95): 100%|██████████| 3/3 [00:00&lt;00:00, 8.87it/s] all 41 62 0.003 0.495 0.0735 0.0283...... Epoch GPU_mem box_loss cls_loss dfl_loss Instances Size 295/300 1.5G 0.3742 0.5917 0.8247 7 640: 100%|██████████| 3/3 [00:00&lt;00:00, 7.21it/s] Class Images Instances Box(P R mAP50 mAP50-95): 100%|██████████| 3/3 [00:00&lt;00:00, 11.40it/s] all 41 62 0.996 1 0.995 0.946 Epoch GPU_mem box_loss cls_loss dfl_loss Instances Size 296/300 1.44G 0.4263 0.6716 0.8004 7 640: 100%|██████████| 3/3 [00:00&lt;00:00, 6.86it/s] Class Images Instances Box(P R mAP50 mAP50-95): 100%|██████████| 3/3 [00:00&lt;00:00, 11.97it/s] all 41 62 0.996 1 0.995 0.945 Epoch GPU_mem box_loss cls_loss dfl_loss Instances Size 297/300 1.5G 0.473 0.6775 0.8499 7 640: 100%|██████████| 3/3 [00:00&lt;00:00, 7.20it/s] Class Images Instances Box(P R mAP50 mAP50-95): 100%|██████████| 3/3 [00:00&lt;00:00, 11.12it/s] all 41 62 0.996 1 0.995 0.945 Epoch GPU_mem box_loss cls_loss dfl_loss Instances Size 298/300 1.51G 0.4877 0.6956 0.8936 11 640: 100%|██████████| 3/3 [00:00&lt;00:00, 6.73it/s] Class Images Instances Box(P R mAP50 mAP50-95): 100%|██████████| 3/3 [00:00&lt;00:00, 11.03it/s] all 41 62 0.996 1 0.995 0.942 Epoch GPU_mem box_loss cls_loss dfl_loss Instances Size 299/300 1.5G 0.4303 0.622 0.8264 6 640: 100%|██████████| 3/3 [00:00&lt;00:00, 6.89it/s] Class Images Instances Box(P R mAP50 mAP50-95): 100%|██████████| 3/3 [00:00&lt;00:00, 11.62it/s] all 41 62 0.996 1 0.995 0.941 Epoch GPU_mem box_loss cls_loss dfl_loss Instances Size 300/300 1.44G 0.4593 0.6413 0.8419 6 640: 100%|██████████| 3/3 [00:00&lt;00:00, 6.31it/s] Class Images Instances Box(P R mAP50 mAP50-95): 100%|██████████| 3/3 [00:00&lt;00:00, 11.83it/s] all 41 62 0.995 1 0.995 0.94 训练结束，验证 1234567891011121314151617181920212223300 epochs completed in 0.101 hours.Optimizer stripped from runs/detect/train9/weights/last.pt, 6.3MBOptimizer stripped from runs/detect/train9/weights/best.pt, 6.3MBValidating runs/detect/train9/weights/best.pt...Ultralytics YOLOv8.1.5 🚀 Python-3.9.18 torch-2.1.2+cu121 CUDA:0 (NVIDIA GeForce RTX 3090, 24260MiB) CUDA:1 (NVIDIA GeForce RTX 3090, 24260MiB)Model summary (fused): 168 layers, 3006038 parameters, 0 gradients, 8.1 GFLOPs Class Images Instances Box(P R mAP50 mAP50-95): 100%|██████████| 3/3 [00:00&lt;00:00, 10.62it/s] all 41 62 0.996 1 0.995 0.946 fire 41 41 0.999 1 0.995 0.938 smoke 41 21 0.992 1 0.995 0.955Speed: 0.1ms preprocess, 1.4ms inference, 0.0ms loss, 1.0ms postprocess per imageResults saved to runs/detect/train9Ultralytics YOLOv8.1.5 🚀 Python-3.9.18 torch-2.1.2+cu121 CUDA:0 (NVIDIA GeForce RTX 3090, 24260MiB) CUDA:1 (NVIDIA GeForce RTX 3090, 24260MiB)Model summary (fused): 168 layers, 3006038 parameters, 0 gradients, 8.1 GFLOPs Class Images Instances Box(P R mAP50 mAP50-95): 100%|██████████| 3/3 [00:04&lt;00:00, 1.41s/it] all 41 62 0.996 1 0.995 0.941 fire 41 41 0.999 1 0.995 0.939 smoke 41 21 0.992 1 0.995 0.942Speed: 0.2ms preprocess, 14.3ms inference, 0.0ms loss, 16.8ms postprocess per imageResults saved to runs/detect/train92 评估进入保存训练结果的文件夹内，其中weights文件夹包含了最佳训练模型和最近训练模型，文件夹外存储训练的各项记录和测试曲线等 result.csv文件中记录了300轮epoch的详细结果 预测在项目根目录下创建预测脚本yolov8_test.py 1234from ultralytics import YOLOmodel = YOLO('runs/detect/train9/weights/best.pt') # 指定最佳模型model.predict('video/fire.mp4', save=True, classes=[0, 1]) # 指定输出类别 执行 123456789101112131415161718192021222324252627282930313233video 1/1 (1/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 1 fire, 208.5msvideo 1/1 (2/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 (no detections), 9.6msvideo 1/1 (3/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 (no detections), 7.9msvideo 1/1 (4/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 (no detections), 7.9msvideo 1/1 (5/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 (no detections), 8.0msvideo 1/1 (6/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 (no detections), 7.8msvideo 1/1 (7/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 (no detections), 7.7msvideo 1/1 (8/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 (no detections), 7.6msvideo 1/1 (9/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 (no detections), 7.9msvideo 1/1 (10/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 (no detections), 7.8msvideo 1/1 (11/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 (no detections), 8.1msvideo 1/1 (12/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 (no detections), 8.1msvideo 1/1 (13/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 (no detections), 8.2msvideo 1/1 (14/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 (no detections), 8.0msvideo 1/1 (15/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 (no detections), 7.7msvideo 1/1 (16/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 (no detections), 7.8msvideo 1/1 (17/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 (no detections), 7.6msvideo 1/1 (18/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 1 fire, 7.6msvideo 1/1 (19/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 1 fire, 7.8msvideo 1/1 (20/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 1 fire, 7.6msvideo 1/1 (21/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 1 fire, 7.8msvideo 1/1 (22/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 2 fires, 7.7msvideo 1/1 (23/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 1 fire, 7.6msvideo 1/1 (24/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 1 fire, 7.5msvideo 1/1 (25/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 1 fire, 7.7msvideo 1/1 (26/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 1 fire, 7.5msvideo 1/1 (27/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 1 fire, 7.4msvideo 1/1 (28/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 1 fire, 7.6msvideo 1/1 (29/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 1 fire, 7.8msvideo 1/1 (30/306) /exp/work/video/yolov8/video/fire.mp4: 384x640 1 fire, 7.7ms...Speed: 2.2ms preprocess, 8.3ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)Results saved to runs/detect/predict13","categories":[{"name":"目标检测","slug":"目标检测","permalink":"http://yoursite.com/categories/目标检测/"}],"tags":[{"name":"yolov8","slug":"yolov8","permalink":"http://yoursite.com/tags/yolov8/"},{"name":"opencv","slug":"opencv","permalink":"http://yoursite.com/tags/opencv/"}]},{"title":"NetworkX: 图论算法应用","slug":"NetworkX-图论算法应用","date":"2024-01-08T07:48:56.000Z","updated":"2024-01-25T08:23:02.432Z","comments":true,"path":"2024/01/08/NetworkX-图论算法应用/","link":"","permalink":"http://yoursite.com/2024/01/08/NetworkX-图论算法应用/","excerpt":"NetworkXNetworkX是一款Python的软件包，用于创造、操作复杂网络，以及学习复杂网络的结构、动力学及其功能。有了NetworkX就可以用标准或者不标准的数据格式加载或者存储网络，它可以产生许多种类的随机网络或经典网络，也可以分析网络结构、建立网络模型、设计新的网络算法、绘制网络等 参考文献地址: https://www.osgeo.cn/networkx/reference/index.html 图计算应用方式比较1.nebula + spark依赖nebula-spark-connector包、nebula-algorithm包和spark集群的数据读取、图计算方式 2.clickhouse + NetworkX由于nebula-algorithm依赖spark集群，且nebula-console原生的数据读取能力不佳，在环境受限且计算量有限的情况下优先考虑跳过spark集群和nebula图库，采用clickhouse + NetworkX的图计算方式，其中clickhouse是存储了nebula源数据的列式分布式表，作用类似于方法1中将nebula集群数据通过nebula-spark-connector包导入为spark-DataFrame，仅用做数据读取，再通过将数据转化为NetworkX的图结构进行图计算","text":"NetworkXNetworkX是一款Python的软件包，用于创造、操作复杂网络，以及学习复杂网络的结构、动力学及其功能。有了NetworkX就可以用标准或者不标准的数据格式加载或者存储网络，它可以产生许多种类的随机网络或经典网络，也可以分析网络结构、建立网络模型、设计新的网络算法、绘制网络等 参考文献地址: https://www.osgeo.cn/networkx/reference/index.html 图计算应用方式比较1.nebula + spark依赖nebula-spark-connector包、nebula-algorithm包和spark集群的数据读取、图计算方式 2.clickhouse + NetworkX由于nebula-algorithm依赖spark集群，且nebula-console原生的数据读取能力不佳，在环境受限且计算量有限的情况下优先考虑跳过spark集群和nebula图库，采用clickhouse + NetworkX的图计算方式，其中clickhouse是存储了nebula源数据的列式分布式表，作用类似于方法1中将nebula集群数据通过nebula-spark-connector包导入为spark-DataFrame，仅用做数据读取，再通过将数据转化为NetworkX的图结构进行图计算 应用实例nebula集群存储以群聊和群成员、好友关系所组成的关系网，clickhouse集群存储节点源数据和对应关系 一、集合运算交、并、差集 直接使用clickhouse进行查询，适用于两个群组成员或账号所加群的交并差集 输入： · n个账号（n≥2） · n个群组（n≥2） 二、路径探寻直接使用nebula进行查询，适用于搜索图谱中任意两个节点的最短可达路径 输入： · 任意类型2个节点 三、中心性输入： · n个群组（n≥1），分析包括指定n个群组、群组群成员、群成员所加群组成关系网各节点的中心性 · n个账号（n≥1），分析包括指定n个账号、账号所加群、群成员组成关系网各节点的中心性 · 指定关系网，分析各节点中心性 ClickHouse SQL示例（关系图谱）： 123SELECT team_id,account_id FROM mqv3.ly_team_ship lts PREWHERE account_id GLOBAL IN (SELECT account_id FROM mqv3.ly_team_ship lts PREWHERE team_id='&#123;src_vid&#125;'); 1.度中心性衡量节点中心性的指标 123res = nx.degree_centrality(G)res = dict(sorted(res.items(), key=lambda x: x[1], reverse=True))print(res) e.g. 以群（-1151413367、-1541749047）为指定节点，计算三级关系网各节点度中心性（抛弃度&lt;5的节点，突出结构） 2.接近中心性反映在关系网络中某一节点与其他节点之间的接近程度 123res = nx.closeness_centrality(G)res = dict(sorted(res.items(), key=lambda x: x[1], reverse=True))print(res) e.g. 以群（-1151413367、-1541749047）为指定节点，计算三级关系网各节点接近中心性（抛弃度&lt;50的节点，突出结构） 3.中介中心性以经过某个节点的最短路径数目来刻画节点重要性的指标 123res = nx.betweenness_centrality(G, k=1000)res = dict(sorted(res.items(), key=lambda x: x[1], reverse=True))print(res) e.g. 以群（-1151413367、-1541749047）为指定节点，计算三级关系网各节点中介中心性（抛弃度&lt;5的节点，突出结构） 4.特征向量中心性关系网络中一个节点的重要性既取决于其邻居节点的数量（即该节点的度），也取决于其邻居节点的重要性 123res = nx.eigenvector_centrality(G)res = dict(sorted(res.items(), key=lambda x: x[1], reverse=True))print(res) e.g. 以群（-1151413367、-1541749047）为指定节点，计算三级关系网各节点特征向量中心性（抛弃度&lt;5的节点，突出结构） 四、重要性输入： · n个群组（n≥1），分析包括指定n个群组、群组群成员、群成员所加群组成关系网各节点的重要程度 · n个账号（n≥1），分析包括指定n个账号、账号所加群、群成员组成关系网各节点的重要程度 · 指定关系网，分析各节点的重要程度 1.k核用于在图中寻找符合一定紧密关系条件（K）的子图结构的算法，要求每个顶点至少与该子图的其他K个顶点关联 12345res1 = nx.core_number(G)res2 = nx.k_core(G, 6)res1 = dict(sorted(res1.items(), key=lambda x: x[1], reverse=True))print(res1)print(list(res2)) e.g. 以群（-1151413367、-1541749047）为指定节点，计算三级关系网各节点k-core值，结合louvain社区上色（全图） 2.PageRank衡量图中节点重要性的指标，值越高，图中访问该节点的概率越高 123res = nx.pagerank(G, alpha=0.85)res = dict(sorted(res.items(), key=lambda x: x[1], reverse=True))print(res) e.g.1. 以群（-1151413367、-1541749047）为指定节点，计算三级关系网各节点pagerank值（抛弃度&lt;5的节点，突出结构） e.g.2. 以群（-1151413367、-1541749047）为指定节点，计算三级关系网各节点pagerank值，结合louvain社区（全图） 五、聚类算法输入（社区发现）： · n个群组（n≥1），划分包括指定n个群组、群组群成员、群成员所加群组成关系网的各个社区 · n个账号（n≥1），划分包括指定n个账号、账号所加群、群成员组成关系网的各个社区 · 划分指定关系网各节点的社区 输入（三角形计数）： · n个群组（n≥1），计算包括指定n个群组、群组群成员、群成员好友、群成员所加群组、群成员所加群组成关系网的三角形数量，挖掘团体关系 · 计算指定关系网各节点的三角形数量，挖掘团体关系 用于计算图谱中三角关系数量，挖掘关系团体 ClickHouse SQL示例（三角形计数）： 12345678910111213141516WITH T AS(-- 指定群群成员SELECT DISTINCT team_id,account_id FROM mqv3.ly_team_ship PREWHERE team_id = '&#123;src_vid&#125;'),T1 AS(-- 指定群群成员的好友SELECT DISTINCT account_id,fri_account_id FROM mqv3.ly_account_ship PREWHERE account_id GLOBAL IN (SELECT account_id FROM T) UNION DISTINCT SELECT DISTINCT fri_account_id,account_id FROM mqv3.ly_account_ship PREWHERE fri_account_id GLOBAL IN (SELECT account_id FROM T)),T2 AS(-- 群成员所加群SELECT DISTINCT team_id,account_id FROM mqv3.ly_team_ship PREWHERE account_id GLOBAL IN (SELECT account_id FROM T)),T3 AS(-- 群成员所加群和群成员和群成员的好友SELECT DISTINCT team_id,account_id,fri_account_id FROM T2 RIGHT JOIN T1 ON T2.account_id = T1.account_id),T4 AS(-- 群成员好友所加群SELECT DISTINCT team_id,account_id AS fri_account_id FROM mqv3.ly_team_ship lts PREWHERE lts.account_id GLOBAL IN (SELECT fri_account_id FROM T1)),T5 AS(-- 共同所加群SELECT DISTINCT team_id,account_id,fri_account_id FROM T3 JOIN T4 ON (T3.team_id = T4.team_id AND T3.fri_account_id = T4.fri_account_id))select DISTINCT team_id,account_id,fri_account_id from T5 1.标签传播（LPA社区发现）基于lpa的社区划分 12res = nx.algorithms.community.label_propagation_communities(G)print(list(res)) e.g.以群（-1151413367、-1541749047）为指定节点，划分lpa标签传播社区（抛弃度&lt;5的节点，突出结构） 2.鲁汶（Louvain社区发现）基于louvain的社区划分 12res = nx.algorithms.community.louvain_communities(G, resolution=0.5)print(res) e.g.1.以群（-1151413367、-1541749047）为指定节点，划分louvain鲁汶社区（resolution=1，抛弃度&lt;5的节点，突出结构） e.g.2.以群（-1151413367、-1541749047）为指定节点，划分louvain鲁汶社区（resolution=0.3，抛弃度&lt;5的节点，突出结构） 3.三角形计数123res = nx.triangles(G)res = dict(sorted(res.items(), key=lambda x: x[1], reverse=True))print(res) e.g.以群（-1151413367、-1541749047）为指定节点，计算关系网中各个节点三角形数量（抛弃度&lt;5的节点，突出结构） 六、连通性输入： · 指定n个群组（n≥2），判断群组、群成员、群成员所加群所组关系网是否为连通图 · 指定n个账号（n≥2），判断账号、账号所加群、群成员所组关系网是否为连通图 · 指定关系网 1.连通性检测判断目标图谱是否为连通图 12res1 = nx.is_connected(G)print(res1) e.g.1.连通图 e.g.2.非连通图 2.联通组件数判断目标图谱中联通组件的数量（n） 12res2 = nx.number_connected_components(G)print(res2) 3.联通组件显示目标图谱中的全部联通组件 12res3 = nx.connected_components(G)print(res3) 4.指定节点联通组件显示目标图谱中指定节点所在的联通组件 12res4 = nx.node_connected_component(G, \"sample_node\")print(res4) 七、几何输入： · 指定n个群组（n≥2），计算群组、群成员、群成员所加群所组关系网的中心、重心 · 指定n个账号（n≥2），计算账号、账号所加群、群成员所组关系网的中心、重心 · 计算指定关系网几何中心、重心 1.几何重心图的重心节点 12res1 = nx.barycenter(G)print(res1) 2.几何中心图的中心节点 12res2 = nx.center(G)print(res2) 绘图e.g. 中心中心性 + LPA标签传播算法聚类 数据源：https://www.inetbio.org/wormnet/downloadnetwork.php · WormNet v.3-GS (https://www.inetbio.org/wormnet/download.php?type=2) 完整代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172from random import sampleimport networkx as nximport matplotlib.pyplot as plt# Gold standard data of positive gene functional associations# from https://www.inetbio.org/wormnet/downloadnetwork.phpG = nx.read_edgelist(\"WormNet.v3.benchmark.txt\")# remove randomly selected nodes (to make example fast)num_to_remove = int(len(G) / 1.5)nodes = sample(list(G.nodes), num_to_remove)G.remove_nodes_from(nodes)# remove low-degree nodeslow_degree = [n for n, d in G.degree() if d &lt; 10]G.remove_nodes_from(low_degree)# largest connected componentcomponents = nx.connected_components(G)largest_component = max(components, key=len)H = G.subgraph(largest_component)# compute centralitycentrality = nx.betweenness_centrality(H, k=10, endpoints=True)# compute community structurelpc = nx.community.label_propagation_communities(H)community_index = &#123;n: i for i, com in enumerate(lpc) for n in com&#125;#### draw graph ####fig, ax = plt.subplots(figsize=(20, 15))pos = nx.spring_layout(H, k=0.15, seed=4572321)node_color = [community_index[n] for n in H]node_size = [v * 20000 for v in centrality.values()]nx.draw_networkx( H, pos=pos, with_labels=False, node_color=node_color, node_size=node_size, edge_color=\"gainsboro\", alpha=0.4,)# Title/legendfont = &#123;\"color\": \"k\", \"fontweight\": \"bold\", \"fontsize\": 20&#125;ax.set_title(\"Gene functional association network (C. elegans)\", font)# Change font color for legendfont[\"color\"] = \"r\"ax.text( 0.80, 0.10, \"node color = community structure\", horizontalalignment=\"center\", transform=ax.transAxes, fontdict=font,)ax.text( 0.80, 0.06, \"node size = betweeness centrality\", horizontalalignment=\"center\", transform=ax.transAxes, fontdict=font,)# Resize figure for label readibilityax.margins(0.1, 0.05)fig.tight_layout()plt.axis(\"off\")plt.show()","categories":[{"name":"NetworkX","slug":"NetworkX","permalink":"http://yoursite.com/categories/NetworkX/"}],"tags":[{"name":"nebula","slug":"nebula","permalink":"http://yoursite.com/tags/nebula/"},{"name":"图算法","slug":"图算法","permalink":"http://yoursite.com/tags/图算法/"},{"name":"networkx","slug":"networkx","permalink":"http://yoursite.com/tags/networkx/"}]},{"title":"基于VGG16神经网络实现以图搜图","slug":"基于VGG神经网络实现以图搜图","date":"2024-01-02T11:11:22.000Z","updated":"2024-02-07T03:39:00.650Z","comments":true,"path":"2024/01/02/基于VGG神经网络实现以图搜图/","link":"","permalink":"http://yoursite.com/2024/01/02/基于VGG神经网络实现以图搜图/","excerpt":"思路 · 预先准备一份图片库，并对其中数据进行批处理操作，使用VGG16卷积神经网络提取图像的512维卷积特征，刷入数据库（ClickHouse）记录； · 上传目标图像进行识图，同样使用VGG16提取目标图像特征，使用CK数据库距离函数进行匹配，高于阈值即可返回识图结果 神经网络12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# -*- coding: utf-8 -*-# @Author : tianL.R# @Email : rtl1312@163.com# @Time : 2023.11.26import timeimport numpy as npfrom PIL import Imagefrom keras.applications.vgg16 import VGG16from keras.applications.vgg16 import preprocess_inputfrom keras.preprocessing import imagefrom numpy import linalgclass VGG16Net: def __init__(self): self.input_shape = (224, 224, 3) self.weight = 'imagenet' self.pooling = 'max' self.model_vgg = VGG16(weights=self.weight, input_shape=(self.input_shape[0], self.input_shape[1], self.input_shape[2],), pooling=self.pooling, include_top=False) self.model_vgg.predict(np.zeros((1, 224, 224, 3))) def detection(self, img_path): \"\"\" 提取VGG16最后一层卷积特征 \"\"\" # img = image.load_img(img_path, target_size=(self.input_shape[0], self.input_shape[1])) img = img_path.resize((self.input_shape[0], self.input_shape[1])) img = image.img_to_array(img) img = np.expand_dims(img, axis=0) img = preprocess_input(img) feat = self.model_vgg.predict(img) norm_feat = feat[0] / linalg.norm(feat[0]) return norm_feat.tolist()if __name__ == '__main__': img1 = '333.jpg' img2 = '555.jpg' img1 = Image.open(img1) img2 = Image.open(img2) vgg = VGG16Net() queryVec1 = np.array(vgg.detection(img1)) queryVec2 = np.array(vgg.detection(img2)) scores = np.dot(queryVec1, queryVec2) score2 = queryVec1.dot(queryVec2) / (np.linalg.norm(queryVec1) * np.linalg.norm(queryVec2)) print(scores) print(score2)","text":"思路 · 预先准备一份图片库，并对其中数据进行批处理操作，使用VGG16卷积神经网络提取图像的512维卷积特征，刷入数据库（ClickHouse）记录； · 上传目标图像进行识图，同样使用VGG16提取目标图像特征，使用CK数据库距离函数进行匹配，高于阈值即可返回识图结果 神经网络12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# -*- coding: utf-8 -*-# @Author : tianL.R# @Email : rtl1312@163.com# @Time : 2023.11.26import timeimport numpy as npfrom PIL import Imagefrom keras.applications.vgg16 import VGG16from keras.applications.vgg16 import preprocess_inputfrom keras.preprocessing import imagefrom numpy import linalgclass VGG16Net: def __init__(self): self.input_shape = (224, 224, 3) self.weight = 'imagenet' self.pooling = 'max' self.model_vgg = VGG16(weights=self.weight, input_shape=(self.input_shape[0], self.input_shape[1], self.input_shape[2],), pooling=self.pooling, include_top=False) self.model_vgg.predict(np.zeros((1, 224, 224, 3))) def detection(self, img_path): \"\"\" 提取VGG16最后一层卷积特征 \"\"\" # img = image.load_img(img_path, target_size=(self.input_shape[0], self.input_shape[1])) img = img_path.resize((self.input_shape[0], self.input_shape[1])) img = image.img_to_array(img) img = np.expand_dims(img, axis=0) img = preprocess_input(img) feat = self.model_vgg.predict(img) norm_feat = feat[0] / linalg.norm(feat[0]) return norm_feat.tolist()if __name__ == '__main__': img1 = '333.jpg' img2 = '555.jpg' img1 = Image.open(img1) img2 = Image.open(img2) vgg = VGG16Net() queryVec1 = np.array(vgg.detection(img1)) queryVec2 = np.array(vgg.detection(img2)) scores = np.dot(queryVec1, queryVec2) score2 = queryVec1.dot(queryVec2) / (np.linalg.norm(queryVec1) * np.linalg.norm(queryVec2)) print(scores) print(score2)","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}],"tags":[{"name":"VGG","slug":"VGG","permalink":"http://yoursite.com/tags/VGG/"},{"name":"卷积神经网络","slug":"卷积神经网络","permalink":"http://yoursite.com/tags/卷积神经网络/"}]},{"title":"卷积神经网络图像分类算法小集","slug":"卷积神经网络图像分类算法小集","date":"2024-01-01T02:12:35.000Z","updated":"2024-04-15T09:32:19.819Z","comments":true,"path":"2024/01/01/卷积神经网络图像分类算法小集/","link":"","permalink":"http://yoursite.com/2024/01/01/卷积神经网络图像分类算法小集/","excerpt":"目录结构训练结构· 在项目根目录下新建数据集文件夹data_set，建立子文件夹（数据集名称）用于存放训练集和测试集； · 在项目根目录下新建数据集文件夹class_j，用于存放分类json文件； · 在项目根目录下新建数据集文件夹models，用于存放训练好的模型文件； · 神经网络model.py； · 训练脚本train.py； · 预测脚本predict.py 12345678910111213141516171819202122# project├── data_set│ ├── data│ ├── train│ │ ├── 00001.jpg│ │ ├── 00002.jpg│ │ ├── 00003.jpg│ │ ├── ...│ │ └── 10000.jpg│ └── val│ ├── 00001.jpg│ ├── 00002.jpg│ ├── 00003.jpg│ ├── ...│ └── 01000.jpg├── class_j│ ├── class_indices.json├── models│ ├── model.pth├── model.py├── train.py└── predict.py 封装结构以GoogLeNet神经网络为例： 123456# GoogLeNet├── class_j│ ├── class_indices.json│── weights│ ├── GoogLeNet_GPU_v1.pth└── model.py","text":"目录结构训练结构· 在项目根目录下新建数据集文件夹data_set，建立子文件夹（数据集名称）用于存放训练集和测试集； · 在项目根目录下新建数据集文件夹class_j，用于存放分类json文件； · 在项目根目录下新建数据集文件夹models，用于存放训练好的模型文件； · 神经网络model.py； · 训练脚本train.py； · 预测脚本predict.py 12345678910111213141516171819202122# project├── data_set│ ├── data│ ├── train│ │ ├── 00001.jpg│ │ ├── 00002.jpg│ │ ├── 00003.jpg│ │ ├── ...│ │ └── 10000.jpg│ └── val│ ├── 00001.jpg│ ├── 00002.jpg│ ├── 00003.jpg│ ├── ...│ └── 01000.jpg├── class_j│ ├── class_indices.json├── models│ ├── model.pth├── model.py├── train.py└── predict.py 封装结构以GoogLeNet神经网络为例： 123456# GoogLeNet├── class_j│ ├── class_indices.json│── weights│ ├── GoogLeNet_GPU_v1.pth└── model.py 神经网络VGG神经网络model.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121\"\"\"VGG模型\"\"\"import osimport timeimport jsonfrom io import BytesIOfrom urllib.request import urlopenimport torchimport torch.nn as nnfrom torchvision import transformsfrom PIL import Image, ImageFilefrom settings import PR, vgg_modelImageFile.LOAD_TRUNCATED_IMAGES = True# 预训练权重模型model_urls = &#123; 'vgg11': 'https://download.pytorch.org/models/vgg11-bbd30ac9.pth', 'vgg13': 'https://download.pytorch.org/models/vgg13-c768596a.pth', 'vgg16': 'https://download.pytorch.org/models/vgg16-397923af.pth', 'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth'&#125;cfgs = &#123; 'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], 'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], 'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'], 'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],&#125;class VGG(nn.Module): def __init__(self, features, num_classes=1000, init_weights=False): super(VGG, self).__init__() self.features = features self.classifier = nn.Sequential( nn.Linear(512 * 7 * 7, 4096), # 第1线性层, 2048 减少参数 nn.ReLU(True), nn.Dropout(p=0.5), nn.Linear(4096, 4096), # 第2线性层 nn.ReLU(True), nn.Dropout(p=0.5), nn.Linear(4096, num_classes), # 第3线性层 ) if init_weights: self._initialize_weights() def forward(self, x): x = self.features(x) # N x 3 x 224 x 224 x = torch.flatten(x, start_dim=1) # N x 512 x 7 x 7 x = self.classifier(x) # N x 512*7*7 return x def _initialize_weights(self): for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.xavier_uniform_(m.weight) if m.bias is not None: nn.init.constant_(m.bias, 0) elif isinstance(m, nn.Linear): nn.init.xavier_uniform_(m.weight) nn.init.constant_(m.bias, 0)def make_features(cfg: list): layers = [] in_channels = 3 for v in cfg: if v == \"M\": layers += [nn.MaxPool2d(kernel_size=2, stride=2)] else: conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1) layers += [conv2d, nn.ReLU(True)] in_channels = v return nn.Sequential(*layers)def vgg(model_name=\"vgg16\", **kwargs): assert model_name in cfgs, \"Warning: model number &#123;&#125; not in cfgs dist!\".format(model_name) cfg = cfgs[model_name] model = VGG(make_features(cfg), **kwargs) return modelclass VGGNetImageClass: def __init__(self): # self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") self.device = torch.device(\"cpu\") self.data_transform = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) ]) self.json_path = os.path.join(PR, \"im_weight_vgg/class_j/class_indices.json\") self.weights_path = os.path.join(PR, f\"im_weight_vgg/weights/&#123;vgg_model&#125;\") self.model = vgg(model_name=\"vgg19\", num_classes=13).to(self.device) self.model.load_state_dict(torch.load(self.weights_path, map_location=self.device)) self.model.eval() with open(self.json_path, 'r') as f: self.class_indices = json.load(f) def detection(self, img): img = self.data_transform(img) # [N, C H, W] img = torch.unsqueeze(img, dim=0) with torch.no_grad(): output = torch.squeeze(self.model(img.to(self.device))).cpu() predict = torch.softmax(output, dim=0) predict_cla = torch.argmax(predict).numpy() result = &#123; 'class': predict_cla.tolist(), 'prob': predict[predict_cla].numpy().tolist() &#125; return result 训练train.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125\"\"\"训练(GPU)\"\"\"import osimport sysimport jsonimport timeimport torchimport torch.nn as nnfrom torchvision import transforms, datasets, utilsimport matplotlib.pyplot as pltimport numpy as npimport torch.optim as optimfrom tqdm import tqdmfrom model import vggfrom PIL import ImageFileImageFile.LOAD_TRUNCATED_IMAGES = Truedef main(): device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") print(f\"use device is &#123;device&#125;\") data_transform = &#123; \"train\": transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), ]), \"val\": transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), ]) &#125; data_root = os.path.abspath(os.path.join(os.getcwd(), \"./\")) image_path = os.path.join(data_root, \"data_set\", \"data\") assert os.path.exists(image_path), \"&#123;&#125; path does not exist.\".format(image_path) train_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"train\"), transform=data_transform[\"train\"] ) train_num = len(train_dataset) flower_list = train_dataset.class_to_idx cla_dict = dict((val, key) for key, val in flower_list.items()) json_str = json.dumps(cla_dict, indent=12) with open(\"class_j/class_indices.json\", 'w') as json_file: json_file.write(json_str) batch_size = 32 nw = min([os.cpu_count(), batch_size if batch_size &gt; 1 else 0, 8]) # 线程数计算 nw = 0 print(f\"Using &#123;nw&#125; dataloader workers every process.\") train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=nw ) val_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"val\"), transform=data_transform[\"val\"] ) val_num = len(val_dataset) val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=nw ) print(f\"Using &#123;train_num&#125; images for training, &#123;val_num&#125; images for validation.\") model_name = \"vgg19\" net = vgg(model_name=model_name, num_classes=13, init_weights=True) # 实例化网络(13分类) # \"\"\" 加载预训练模型权重 model_weight_path = './models/VGG19Net_GPU_v5.pth' net.load_state_dict(torch.load(model_weight_path, map_location='cpu')) # \"\"\" net.to(device) loss_function = nn.CrossEntropyLoss() optimizer = optim.Adam(net.parameters(), lr=0.0001) epochs = 300 save_path = \"./VGG19Net_GPU_RE.pth\" best_accuracy = 0.0 train_steps = len(train_loader) for epoch in range(epochs): net.train() running_loss = 0.0 train_bar = tqdm(train_loader, file=sys.stdout) for step, data in enumerate(train_bar): images, labels = data optimizer.zero_grad() outputs = net(images.to(device)) loss = loss_function(outputs, labels.to(device)) loss.backward() optimizer.step() running_loss += loss.item() train_bar.desc = \"train epoch [&#123;&#125;/&#123;&#125;] loss:&#123;:.3f&#125;\".format(epoch + 1, epochs, loss ) # 验证 net.eval() acc = 0.0 with torch.no_grad(): val_bar = tqdm(val_loader, file=sys.stdout) for val_data in val_bar: val_images, val_labels = val_data outputs = net(val_images.to(device)) predict_y = torch.max(outputs, dim=1)[1] acc += torch.eq(predict_y, val_labels.to(device)).sum().item() val_accuracy = acc / val_num print(\"[epoch %d ] train_loss: %3f val_accurancy: %3f\" % (epoch + 1, running_loss / train_steps, val_accuracy)) if val_accuracy &gt; best_accuracy: best_accuracy = val_accuracy torch.save(net.state_dict(), save_path+str(val_accuracy)) print(\"Finshed Training.\")if __name__ == '__main__': os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" main() 预测predict.py 1234567891011121314151617\"\"\"预测\"\"\"from io import BytesIOfrom urllib.request import urlopenfrom PIL import Image, ImageFilefrom im_weight_vgg.model import VGGNetImageClassif __name__ == '__main__': im_class = VGGNetImageClass() url = 'http://192.168.3.18:300/files/group1/M00/11/0E/wKgCBWRUgjWAGTfNAAEKiw0XSZc371.jpg' img_bytes = urlopen(url).read() img_pil = Image.open(BytesIO(img_bytes)) print(im_class.detection(img_pil)) print(im_class.detection(Image.open(\"-532576361772532412_120.jpg\").convert(\"RGB\"))) 附：vgg16+transformer 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108import torchfrom collections import namedtuplefrom torchvision import modelsimport torch.nn as nnimport torch.nn.functional as F# VGG16神经网络定义class VGG16(torch.nn.Module): \"\"\"Vgg16 Net\"\"\" def __init__(self, requires_grad=False): super(VGG16, self).__init__() vgg_pretrained_features = models.vgg16(pretrained=True).features self.slice1 = torch.nn.Sequential() self.slice2 = torch.nn.Sequential() self.slice3 = torch.nn.Sequential() self.slice4 = torch.nn.Sequential() for x in range(4): self.slice1.add_module(str(x), vgg_pretrained_features[x]) for x in range(4, 9): self.slice2.add_module(str(x), vgg_pretrained_features[x]) for x in range(9, 16): self.slice3.add_module(str(x), vgg_pretrained_features[x]) for x in range(16, 23): self.slice4.add_module(str(x), vgg_pretrained_features[x]) if not requires_grad: for param in self.parameters(): param.requires_grad = False def forward(self, X): h = self.slice1(X) h_relu1_2 = h h = self.slice2(h) h_relu2_2 = h h = self.slice3(h) h_relu3_3 = h h = self.slice4(h) h_relu4_3 = h vgg_outputs = namedtuple(\"VggOutputs\", [\"relu1_2\", \"relu2_2\", \"relu3_3\", \"relu4_3\"]) output = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3) return outputclass TransformerNet(torch.nn.Module): def __init__(self): super(TransformerNet, self).__init__() self.model = nn.Sequential( ConvBlock(3, 32, kernel_size=9, stride=1), ConvBlock(32, 64, kernel_size=3, stride=2), ConvBlock(64, 128, kernel_size=3, stride=2), ResidualBlock(128), ResidualBlock(128), ResidualBlock(128), ResidualBlock(128), ResidualBlock(128), ConvBlock(128, 64, kernel_size=3, upsample=True), ConvBlock(64, 32, kernel_size=3, upsample=True), ConvBlock(32, 3, kernel_size=9, stride=1, normalize=False, relu=False), ) def forward(self, x): return self.model(x)class ResidualBlock(torch.nn.Module): def __init__(self, channels): super(ResidualBlock, self).__init__() self.block = nn.Sequential( ConvBlock(channels, channels, kernel_size=3, stride=1, normalize=True, relu=True), ConvBlock(channels, channels, kernel_size=3, stride=1, normalize=True, relu=False), ) def forward(self, x): return self.block(x) + xclass ConvBlock(torch.nn.Module): def __init__(self, in_channels, out_channels, kernel_size, stride=1, upsample=False, normalize=True, relu=True): super(ConvBlock, self).__init__() self.upsample = upsample self.block = nn.Sequential( nn.ReflectionPad2d(kernel_size // 2), nn.Conv2d(in_channels, out_channels, kernel_size, (stride,)) ) self.norm = nn.InstanceNorm2d(out_channels, affine=True) if normalize else None self.relu = relu def forward(self, x): if self.upsample: x = F.interpolate(x, scale_factor=2) x = self.block(x) if self.norm is not None: x = self.norm(x) if self.relu: x = F.relu(x) return x if __name__ == '__main__': input1 = torch.rand([224, 3, 224, 224]) model_x = VGG16() print(model_x) GoogLeNetmodel.py 神经网络123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225import jsonimport torch.nn as nnimport torchimport torch.nn.functional as Ffrom torchvision import transformsfrom settings import GoogLeNet_model\"\"\"# 定义卷积+激活函数操作模板\"\"\"class BasicConv2d(nn.Module): def __init__(self, in_channels, out_channels, **kwargs): super(BasicConv2d, self).__init__() self.conv = nn.Conv2d(in_channels, out_channels, **kwargs) self.relu = nn.ReLU(inplace=True) def forward(self, x): x = self.conv(x) x = self.relu(x) return x\"\"\"# 定义 Iception 辅助分类器模板\"\"\"class InceptionAux(nn.Module): def __init__(self, in_channels, num_classes): super(InceptionAux, self).__init__() self.averagePool = nn.AvgPool2d(kernel_size=5, stride=3) self.conv = BasicConv2d(in_channels, 128, kernel_size=1) # output = [batch, 128, 4, 4] self.fc1 = nn.Linear(2048, 1024) self.fc2 = nn.Linear(1024, num_classes) def forward(self, x): # aux1: N*512*14*14, aux2: N*528*14*14 x = self.averagePool(x) # aux1: N*512*4*4, aux2: N*528*4*4 x = self.conv(x) # N*128*4*4 x = torch.flatten(x, 1) x = F.dropout(x, 0.5, training=self.training) # N*2048 x = F.relu(self.fc1(x), inplace=True) x = F.dropout(x, 0.5, training=self.training) # N*1024 x = self.fc2(x) # N*num_classes return x\"\"\"# Inception 模板 \"\"\"class Inception(nn.Module): def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj): super(Inception, self).__init__() self.branch1 = BasicConv2d(in_channels, ch1x1, kernel_size=1) self.branch2 = nn.Sequential( BasicConv2d(in_channels, ch3x3red, kernel_size=1), BasicConv2d(ch3x3red, ch3x3, kernel_size=3, padding=1) # 保证输出大小等于输入大小 ) self.branch3 = nn.Sequential( BasicConv2d(in_channels, ch5x5red, kernel_size=1), # 官方 3x3, https://github.com/pytorch/vision/issues/906 BasicConv2d(ch5x5red, ch5x5, kernel_size=5, padding=2) # 输出大小=输入大小 ) self.branch4 = nn.Sequential( nn.MaxPool2d(kernel_size=3, stride=1, padding=1), BasicConv2d(in_channels, pool_proj, kernel_size=1) ) def forward(self, x): branch1 = self.branch1(x) branch2 = self.branch2(x) branch3 = self.branch3(x) branch4 = self.branch4(x) outputs = [branch1, branch2, branch3, branch4] return torch.cat(outputs, 1) # 拼接数据\"\"\"# GoogLeNet 模型\"\"\"class GoogLeNet(nn.Module): def __init__(self, num_classes=1000, aux_logits=True, init_weights=False): super(GoogLeNet, self).__init__() self.aux_logits = aux_logits self.conv1 = BasicConv2d(3, 64, kernel_size=7, stride=2, padding=3) self.maxpool1 = nn.MaxPool2d(3, stride=2, ceil_mode=True) self.conv2 = BasicConv2d(64, 64, kernel_size=1) self.conv3 = BasicConv2d(64, 192, kernel_size=3, padding=1) self.maxpool2 = nn.MaxPool2d(3, stride=2, ceil_mode=True) self.inception3a = Inception(192, 64, 96, 128, 16, 32, 32) self.inception3b = Inception(256, 128, 128, 192, 32, 96, 64) self.maxpool3 = nn.MaxPool2d(3, stride=2, ceil_mode=True) self.inception4a = Inception(480, 192, 96, 208, 16, 48, 64) self.inception4b = Inception(512, 160, 112, 224, 24, 64, 64) self.inception4c = Inception(512, 128, 128, 256, 24, 64, 64) self.inception4d = Inception(512, 112, 144, 288, 32, 64, 64) self.inception4e = Inception(528, 256, 160, 320, 32, 128, 128) self.maxpool4 = nn.MaxPool2d(3, stride=2, ceil_mode=True) self.inception5a = Inception(832, 256, 160, 320, 32, 128, 128) self.inception5b = Inception(832, 384, 192, 384, 48, 128, 128) if self.aux_logits: self.aux1 = InceptionAux(512, num_classes) self.aux2 = InceptionAux(528, num_classes) self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) self.dropout = nn.Dropout(0.4) self.fc = nn.Linear(1024, num_classes) if init_weights: self._initialize_weights() def forward(self, x): # N*3*224*224 x = self.conv1(x) # N*64*112*112 x = self.maxpool1(x) # N*64*56*56 x = self.conv2(x) # N*64*56*56 x = self.conv3(x) # N*192*56*56 x = self.maxpool2(x) # N*192*28*28 x = self.inception3a(x) # N*256*28*28 x = self.inception3b(x) # N*480*28*28 x = self.maxpool3(x) # N*480*14*14 x = self.inception4a(x) # N*512*14*14 if self.training and self.aux_logits: # eval model lose this layer aux1 = self.aux1(x) x = self.inception4b(x) # N*512*14*14 x = self.inception4c(x) # N*512*14*14 x = self.inception4d(x) # N*528*14*14 if self.training and self.aux_logits: # eval model lose this layer aux2 = self.aux2(x) x = self.inception4e(x) # N*832*14*14 x = self.maxpool4(x) # N*832*7*7 x = self.inception5a(x) # N*832*7*7 x = self.inception5b(x) # N*1024*7*7 x = self.avgpool(x) # N*1024*1*1 x = torch.flatten(x, 1) # N*1024 x = self.dropout(x) x = self.fc(x) # N*1000 (num_classes) if self.training and self.aux_logits: # eval model lose this layer return x, aux2, aux1 return x def _initialize_weights(self): for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') if m.bias is not None: nn.init.constant_(m.bias, 0) elif isinstance(m, nn.Linear): nn.init.normal_(m.weight, 0, 0.01) nn.init.constant_(m.bias, 0)class GoogLeNetImageClass: def __init__(self): # self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") self.device = torch.device(\"cpu\") self.data_transform = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) ]) self.json_path = \"im_weight_gln/class_j/class_indices.json\" self.weights_path = f\"im_weight_gln/weights/&#123;GoogLeNet_model&#125;\" self.model = GoogLeNet(num_classes=13, aux_logits=False).to(self.device) self.model.load_state_dict(torch.load(self.weights_path, map_location=self.device), strict=False) self.model.eval() with open(self.json_path, 'r') as f: self.class_indices = json.load(f) def detection(self, img): img = self.data_transform(img) # [N, C H, W] img = torch.unsqueeze(img, dim=0) with torch.no_grad(): output = torch.squeeze(self.model(img.to(self.device))).cpu() predict = torch.softmax(output, dim=0) predict_cla = torch.argmax(predict).numpy() result = &#123; 'class': predict_cla.tolist(), 'prob': predict[predict_cla].numpy().tolist() &#125; return result 训练train.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119\"\"\"训练(GPU)\"\"\"import osimport sysimport jsonimport timeimport torchimport torch.nn as nnfrom torchvision import transforms, datasets, utilsimport matplotlib.pyplot as pltimport numpy as npimport torch.optim as optimfrom tqdm import tqdmfrom model import GoogLeNetdef main(): device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") print(f\"use device is &#123;device&#125;\") data_transform = &#123; \"train\": transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), ]), \"val\": transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), ]) &#125; data_root = os.path.abspath(os.path.join(os.getcwd(), \"./\")) image_path = \"/exp/work/algorithm/vgg/data_set/data\" assert os.path.exists(image_path), \"&#123;&#125; path does not exist.\".format(image_path) train_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"train\"), transform=data_transform[\"train\"] ) train_num = len(train_dataset) flower_list = train_dataset.class_to_idx cla_dict = dict((val, key) for key, val in flower_list.items()) json_str = json.dumps(cla_dict, indent=12) with open(\"calss_indices.json\", 'w') as json_file: json_file.write(json_str) batch_size = 32 nw = min([os.cpu_count(), batch_size if batch_size &gt; 1 else 0, 8]) # 线程数计算 nw = 0 print(f\"Using &#123;nw&#125; dataloader workers every process.\") train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=nw ) val_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"val\"), transform=data_transform[\"val\"] ) val_num = len(val_dataset) val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=nw ) print(f\"Using &#123;train_num&#125; images for training, &#123;val_num&#125; images for validation.\") net = GoogLeNet(num_classes=13, aux_logits=True, init_weights=True) # 实例化网络(5分类) net.to(device) loss_function = nn.CrossEntropyLoss() optimizer = optim.Adam(net.parameters(), lr=0.0001) epochs = 300 save_path = \"./GoogLeNet_GPU.pth\" best_accuracy = 0.0 train_steps = len(train_loader) for epoch in range(epochs): net.train() running_loss = 0.0 train_bar = tqdm(train_loader, file=sys.stdout) for step, data in enumerate(train_bar): images, labels = data optimizer.zero_grad() logits, aux_logits2, aux_logits1 = net(images.to(device)) loss0 = loss_function(logits, labels.to(device)) loss1 = loss_function(aux_logits1, labels.to(device)) loss2 = loss_function(aux_logits2, labels.to(device)) loss = loss0 + loss1 * 0.3 + loss2 * 0.3 loss.backward() optimizer.step() running_loss += loss.item() train_bar.desc = \"train epoch [&#123;&#125;/&#123;&#125;] loss:&#123;:.3f&#125;\".format(epoch + 1, epochs, loss ) # 验证 net.eval() acc = 0.0 with torch.no_grad(): val_bar = tqdm(val_loader, file=sys.stdout) for val_data in val_bar: val_images, val_labels = val_data outputs = net(val_images.to(device)) predict_y = torch.max(outputs, dim=1)[1] acc += torch.eq(predict_y, val_labels.to(device)).sum().item() val_accuracy = acc / val_num print(\"[epoch %d ] train_loss: %3f val_accurancy: %3f\" % (epoch + 1, running_loss / train_steps, val_accuracy)) if val_accuracy &gt; best_accuracy: best_accuracy = val_accuracy torch.save(net.state_dict(), save_path) print(\"Finshed Training.\")if __name__ == '__main__': os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" main() 预测predict.py 1234567891011121314151617\"\"\"预测\"\"\"from io import BytesIOfrom urllib.request import urlopenfrom PIL import Image, ImageFilefrom im_weight_gln.model import GoogLeNetImageClassif __name__ == '__main__': im_class = GoogLeNetImageClass() url = 'http://192.168.3.18:300/files/group1/M00/11/0E/wKgCBWRUgjWAGTfNAAEKiw0XSZc371.jpg' img_bytes = urlopen(url).read() img_pil = Image.open(BytesIO(img_bytes)) print(im_class.detection(img_pil)) print(im_class.detection(Image.open(\"-532576361772532412_120.jpg\").convert(\"RGB\"))) ResNet/ResNext神经网络model.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305\"\"\"ResNetr模型\"\"\"import osimport timeimport jsonfrom io import BytesIOfrom urllib.request import urlopenimport torchimport torch.nn as nnfrom torchvision import transformsfrom PIL import Image, ImageFilefrom settings import PR, ResNet_modelImageFile.LOAD_TRUNCATED_IMAGES = Truemodel_urls = &#123; 'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth', 'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth', 'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth', 'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth', 'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',&#125;\"\"\"# 定义 BasicBlock 模块# ResNet18/34的残差结构, 用的是2个3x3大小的卷积\"\"\"class BasicBlock(nn.Module): expansion = 1 # 残差结构中, 判断主分支的卷积核个数是否发生变化，不变则为1 def __init__(self, in_channel, out_channel, stride=1, downsample=None, **kwargs): # downsample 对应虚线残差结构 super(BasicBlock, self).__init__() self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel, kernel_size=3, stride=stride, padding=1, bias=False ) self.bn1 = nn.BatchNorm2d(out_channel) self.relu = nn.ReLU() self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel, kernel_size=3, stride=1, padding=1, bias=False ) self.bn2 = nn.BatchNorm2d(out_channel) self.downsample = downsample def forward(self, x): identity = x if self.downsample is not None: # 虚线残差结构，需要下采样 identity = self.downsample(x) # 捷径分支short cut out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out += identity out = self.relu(out) return out\"\"\"# 定义 Bottleneck 模块# ResNet50/101/152的残差结构，用的是1x1+3x3+1x1的卷积\"\"\"class Bottleneck(nn.Module): \"\"\" # 注意：原论文中，在虚线残差结构的主分支上，第一个1x1卷积层的步距是2，第二个3x3卷积层步距是1。 # 但在pytorch官方实现过程中是第一个1x1卷积层的步距是1，第二个3x3卷积层步距是2， # 这么做的好处是能够在top1上提升大概0.5%的准确率。 # 可参考Resnet v1.5 https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch \"\"\" expansion = 4 # 残差结构中第三层卷积核个数是第1/2层卷积核个数的4倍 def __init__(self, in_channel, out_channel, stride=1, downsample=None, groups=1, width_per_group=64): super(Bottleneck, self).__init__() width = int(out_channel * (width_per_group / 64.)) * groups self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=width, kernel_size=1, stride=1, bias=False) self.bn1 = nn.BatchNorm2d(width) self.conv2 = nn.Conv2d(in_channels=width, out_channels=width, groups=groups, kernel_size=3, stride=stride, bias=False, padding=1 ) self.bn2 = nn.BatchNorm2d(width) self.conv3 = nn.Conv2d(in_channels=width, out_channels=out_channel * self.expansion, kernel_size=1, stride=1, bias=False) self.bn3 = nn.BatchNorm2d(out_channel * self.expansion) self.relu = nn.ReLU(inplace=True) self.downsample = downsample def forward(self, x): identity = x if self.downsample is not None: identity = self.downsample(x) # 捷径分支short cut out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.relu(out) out = self.conv3(out) out = self.bn3(out) out += identity out = self.relu(out) return out\"\"\"# 残差网络结构\"\"\"class ResNet(nn.Module): # block = BasicBlock or Bottleneck # blocks_num 为残差结构中 conv2_x~conv5_x 中残差块个数, 一个列表 def __init__(self, block, blocks_num, num_classes=1000, include_top=True, groups=1, width_per_group=64): super(ResNet, self).__init__() self.include_top = include_top self.in_channel = 64 self.groups = groups self.width_per_group = width_per_group self.conv1 = nn.Conv2d(3, self.in_channel, kernel_size=7, stride=2, padding=3, bias=False) self.bn1 = nn.BatchNorm2d(self.in_channel) self.relu = nn.ReLU(inplace=True) self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) self.layer1 = self._make_layer(block, 64, blocks_num[0]) self.layer2 = self._make_layer(block, 128, blocks_num[1], stride=2) self.layer3 = self._make_layer(block, 256, blocks_num[2], stride=2) self.layer4 = self._make_layer(block, 512, blocks_num[3], stride=2) if self.include_top: self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) # output size = (1, 1) self.fc = nn.Linear(512 * block.expansion, num_classes) for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') # channel 为残差结构中第1层卷积核个数 def _make_layer(self, block, channel, block_num, stride=1): downsample = None # ResNet50/101/152 的残差结构, block.expansion=4 if stride != 1 or self.in_channel != channel * block.expansion: downsample = nn.Sequential( nn.Conv2d(self.in_channel, channel * block.expansion, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(channel * block.expansion) ) layers = [] layers.append(block(self.in_channel, channel, downsample=downsample, stride=stride, groups=self.groups, width_per_group=self.width_per_group, )) self.in_channel = channel * block.expansion for _ in range(1, block_num): layers.append(block(self.in_channel, channel, groups=self.groups, width_per_group=self.width_per_group, )) return nn.Sequential(*layers) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.maxpool(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) if self.include_top: x = self.avgpool(x) x = torch.flatten(x, 1) x = self.fc(x) return x\"\"\"# resnet34 结构# https://download.pytorch.org/models/resnet34-333f7ec4.pth\"\"\"def resnet34(num_classes=1000, include_top=True): return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)\"\"\"# resnet50 结构# https://download.pytorch.org/models/resnet50-19c8e357.pth\"\"\"def resnet50(num_classes=1000, include_top=True): return ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)\"\"\"# resnet101 结构# https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\"\"\"def resnet101(num_classes=1000, include_top=True): return ResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, include_top=include_top)\"\"\"# resnet152 结构# https://download.pytorch.org/models/resnet152-b121ed2d.pth\"\"\"def resnet152(num_classes=1000, include_top=True): return ResNet(Bottleneck, [3, 8, 36, 3], num_classes=num_classes, include_top=include_top)\"\"\"# resnext50_32x4d 结构# https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\"\"\"def resnext50_32x4d(num_classes=1000, include_top=True): groups = 32 width_per_group = 4 return ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top, groups=groups, width_per_group=width_per_group)\"\"\"# resnext101_32x8d 结构# https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth\"\"\"def resnext101_32x8d(num_classes=1000, include_top=True): groups = 32 width_per_group = 8 return ResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, include_top=include_top, groups=groups, width_per_group=width_per_group)class ResNetImageClass: def __init__(self): # self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") self.device = torch.device(\"cpu\") self.data_transform = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) ]) self.json_path = \"im_weight_Res/class_j/class_indices.json\" self.weights_path = f\"im_weight_Res/weights/&#123;ResNet_model&#125;\" self.model = resnet152(num_classes=13).to(self.device) self.model.load_state_dict(torch.load(self.weights_path, map_location=self.device)) self.model.eval() with open(self.json_path, 'r') as f: self.class_indices = json.load(f) def detection(self, img): img = self.data_transform(img) # [N, C H, W] img = torch.unsqueeze(img, dim=0) with torch.no_grad(): output = torch.squeeze(self.model(img.to(self.device))).cpu() predict = torch.softmax(output, dim=0) predict_cla = torch.argmax(predict).numpy() result = &#123; 'class': predict_cla.tolist(), 'prob': predict[predict_cla].numpy().tolist() &#125; return result 训练train.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133\"\"\"训练(GPU)\"\"\"import osimport sysimport jsonimport timeimport torchimport torch.nn as nnfrom torchvision import transforms, datasets, utilsimport matplotlib.pyplot as pltimport numpy as npimport torch.optim as optimfrom tqdm import tqdmfrom model import resnet152def main(): device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") print(f\"use device is &#123;device&#125;\") data_transform = &#123; \"train\": transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), \"val\": transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), ]) &#125; data_root = os.path.abspath(os.path.join(os.getcwd(), \"./\")) image_path = \"/exp/work/algorithm/vgg/data_set/data\" assert os.path.exists(image_path), \"&#123;&#125; path does not exist.\".format(image_path) train_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"train\"), transform=data_transform[\"train\"] ) train_num = len(train_dataset) flower_list = train_dataset.class_to_idx cla_dict = dict((val, key) for key, val in flower_list.items()) json_str = json.dumps(cla_dict, indent=12) with open(\"calss_indices.json\", 'w') as json_file: json_file.write(json_str) batch_size = 16 nw = min([os.cpu_count(), batch_size if batch_size &gt; 1 else 0, 8]) # 线程数计算 nw = 0 print(f\"Using &#123;nw&#125; dataloader workers every process.\") train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=nw ) val_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"val\"), transform=data_transform[\"val\"] ) val_num = len(val_dataset) val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=nw ) print(f\"Using &#123;train_num&#125; images for training, &#123;val_num&#125; images for validation.\") net = resnet152() # 实例化网络 # load pretrain weights # download url: https://download.pytorch.org/models/resnet34-333f7ec4.pth # \"\"\" model_weight_path = \"./ResNet152_GPU_v2.pth\" assert os.path.exists(model_weight_path), \"file &#123;&#125; does not exist.\".format(model_weight_path) net.load_state_dict(torch.load(model_weight_path, map_location='cpu')) # \"\"\" # for param in net.parameters(): # param.requires_grad = False # change fc layer structure in_channel = net.fc.in_features net.fc = nn.Linear(in_channel, 13) # (5分类) net.to(device) loss_function = nn.CrossEntropyLoss() # construct an optimizer # optimizer = optim.Adam(net.parameters(), lr=0.0001) params = [p for p in net.parameters() if p.requires_grad] optimizer = optim.Adam(params, lr=0.0001) epochs = 300 save_path = \"./ResNet152_GPU_RE.pth\" best_accuracy = 0.0 train_steps = len(train_loader) for epoch in range(epochs): net.train() running_loss = 0.0 train_bar = tqdm(train_loader, file=sys.stdout) for step, data in enumerate(train_bar): images, labels = data optimizer.zero_grad() logits = net(images.to(device)) loss = loss_function(logits, labels.to(device)) # 计算损失函数 loss.backward() optimizer.step() running_loss += loss.item() train_bar.desc = \"train epoch [&#123;&#125;/&#123;&#125;] loss:&#123;:.3f&#125;\".format(epoch + 1, epochs, loss ) # 验证 net.eval() acc = 0.0 with torch.no_grad(): val_bar = tqdm(val_loader, file=sys.stdout) for val_data in val_bar: val_images, val_labels = val_data outputs = net(val_images.to(device)) predict_y = torch.max(outputs, dim=1)[1] acc += torch.eq(predict_y, val_labels.to(device)).sum().item() val_accuracy = acc / val_num print(\"[epoch %d ] train_loss: %3f val_accurancy: %3f\" % (epoch + 1, running_loss / train_steps, val_accuracy)) if val_accuracy &gt; best_accuracy: best_accuracy = val_accuracy torch.save(net.state_dict(), save_path) print(\"Finished Training.\")if __name__ == '__main__': main() 预测predict.py 1234567891011121314151617\"\"\"预测\"\"\"from io import BytesIOfrom urllib.request import urlopenfrom PIL import Image, ImageFilefrom im_weight_Res.model import ResNetImageClassif __name__ == '__main__': im_class = ResNetImageClass() url = 'http://192.168.3.18:300/files/group1/M00/11/0E/wKgCBWRUgjWAGTfNAAEKiw0XSZc371.jpg' img_bytes = urlopen(url).read() img_pil = Image.open(BytesIO(img_bytes)) print(im_class.detection(img_pil)) print(im_class.detection(Image.open(\"-532576361772532412_120.jpg\").convert(\"RGB\"))) AlexNet神经网络model.py 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import torch.nn as nnimport torchclass AlexNet(nn.Module): def __init__(self, num_classes=1000, init_weights=False): super(AlexNet, self).__init__() \"\"\" 特征提取 \"\"\" self.features = nn.Sequential( nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=2), # 输入[3, 224, 224] 输出[48, 55, 55] nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), # 输出 [48,27,27] nn.Conv2d(48, 128, kernel_size=5, padding=2), # 输出 [128, 27, 27] nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), # 输出 [128, 13, 13] nn.Conv2d(128, 192, kernel_size=3, padding=1), # 输出[192, 13, 13] nn.ReLU(inplace=True), nn.Conv2d(192, 192, kernel_size=3, padding=1), # 输出[192, 13, 13] nn.ReLU(inplace=True), nn.Conv2d(192, 128, kernel_size=3, padding=1), # 输出[128, 13, 13] nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2) # 输出 [128, 6, 6] ) \"\"\" 分类器 \"\"\" self.classifier = nn.Sequential( nn.Dropout(p=0.5), # Dropout 随机失活神经元, 比例诶0.5 nn.Linear(128 * 6 * 6, 2048), nn.ReLU(inplace=True), nn.Dropout(p=0.5), nn.Linear(2048, 2048), nn.ReLU(inplace=True), nn.Linear(2048, num_classes) ) if init_weights: self._initialize_weights() def forward(self, x): x = self.features(x) x = torch.flatten(x, start_dim=1) x = self.classifier(x) return x \"\"\" 权重初始化 \"\"\" def _initialize_weights(self): for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') if m.bias is not None: nn.init.constant_(m.bias, 0) elif isinstance(m, nn.Linear): nn.init.normal_(m.weight, 0.01) nn.init.constant_(m.bias, 0) 训练train.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115\"\"\"训练(GPU)\"\"\"import osimport sysimport jsonimport timeimport torchimport torch.nn as nnfrom torchvision import transforms, datasets, utilsimport matplotlib.pyplot as pltimport numpy as npimport torch.optim as optimfrom tqdm import tqdmfrom model import AlexNetdef main(): device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") print(f\"use device is &#123;device&#125;\") data_transform = &#123; \"train\": transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), ]), \"val\": transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), ]) &#125; data_root = os.path.abspath(os.path.join(os.getcwd(), \"./\")) image_path = os.path.join(data_root, \"data_set\", \"flower_data\") assert os.path.exists(image_path), \"&#123;&#125; path does not exist.\".format(image_path) train_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"train\"), transform=data_transform[\"train\"] ) train_num = len(train_dataset) flower_list = train_dataset.class_to_idx cla_dict = dict((val, key) for key, val in flower_list.items()) json_str = json.dumps(cla_dict, indent=4) with open(\"calss_indices.json\", 'w') as json_file: json_file.write(json_str) batch_size = 32 nw = min([os.cpu_count(), batch_size if batch_size &gt; 1 else 0, 8]) # 线程数计算 nw = 0 print(f\"Using &#123;nw&#125; dataloader workers every process.\") train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=nw ) val_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"val\"), transform=data_transform[\"val\"] ) val_num = len(val_dataset) val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=nw ) print(f\"Using &#123;train_num&#125; images for training, &#123;val_num&#125; images for validation.\") net = AlexNet(num_classes=5, init_weights=True) net.to(device) loss_function = nn.CrossEntropyLoss() optimizer = optim.Adam(net.parameters(), lr=0.0002) epochs = 10 save_path = \"./AlexNet.pth\" best_accuracy = 0.0 train_steps = len(train_loader) for epoch in range(epochs): net.train() running_loss = 0.0 train_bar = tqdm(train_loader, file=sys.stdout) for step, data in enumerate(train_bar): images, labels = data optimizer.zero_grad() outputs = net(images.to(device)) loss = loss_function(outputs, labels.to(device)) loss.backward() optimizer.step() running_loss += loss.item() train_bar.desc = \"train epoch [&#123;&#125;/&#123;&#125;] loss:&#123;:.3f&#125;\".format(epoch + 1, epochs, loss ) # 验证 net.eval() acc = 0.0 with torch.no_grad(): val_bar = tqdm(val_loader, file=sys.stdout) for val_data in val_bar: val_images, val_labels = val_data outputs = net(val_images.to(device)) predict_y = torch.max(outputs, dim=1)[1] acc += torch.eq(predict_y, val_labels.to(device)).sum().item() val_accuracy = acc / val_num print(\"[epoch %d ] train_loss: %3f val_accurancy: %3f\" % (epoch + 1, running_loss / train_steps, val_accuracy)) if val_accuracy &gt; best_accuracy: best_accuracy = val_accuracy torch.save(net.state_dict(), save_path) print(\"Finshed Training.\")if __name__ == '__main__': main() 预测（暂未封装） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253\"\"\"预测\"\"\"import osimport jsonimport torchfrom PIL import Imagefrom torchvision import transformsimport matplotlib.pyplot as pltfrom model import AlexNetdef main(): device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") data_transform = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) ]) image_path = \"./sunflowers01.jpg\" img = Image.open(image_path) plt.imshow(img) img = data_transform(img) # [N, C H, W] img = torch.unsqueeze(img, dim=0) # 维度扩展 # print(f\"img=&#123;img&#125;\") json_path = \"./calss_indices.json\" with open(json_path, 'r') as f: class_indict = json.load(f) # model = AlexNet(num_classes=5).to(device) # GPU model = AlexNet(num_classes=5) # CPU weights_path = \"./AlexNet.pth\" model.load_state_dict(torch.load(weights_path)) model.eval() # 关闭 Dorpout with torch.no_grad(): # output = torch.squeeze(model(img.to(device))).cpu() #GPU output = torch.squeeze(model(img)) # 维度压缩 predict = torch.softmax(output, dim=0) predict_cla = torch.argmax(predict).numpy() print_res = \"class: &#123;&#125; prob: &#123;:.3&#125;\".format(class_indict[str(predict_cla)], predict[predict_cla].numpy()) plt.title(print_res) # for i in range(len(predict)): # print(\"class: &#123;&#125; prob: &#123;:.3&#125;\".format(class_indict[str(predict_cla)], # predict[predict_cla].numpy())) plt.show()if __name__ == '__main__': main() LeNet神经网络model.py 12345678910111213141516171819202122232425262728293031323334353637import torch.nn as nnimport torch.nn.functional as Fclass LeNet(nn.Module): # 集成nn.Module父类 def __init__(self): super(LeNet, self).__init__() # 看一下具体的参数 self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=1, padding=0, bias=True ) self.pool1 = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(16, 32, 5) self.pool2 = nn.MaxPool2d(2, 2) self.fc1 = nn.Linear(32 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) # self.relu = nn.ReLU(inplace=True) # 正向传播 def forward(self, x): x = F.relu(self.conv1(x)) # 输入: (3, 32, 32), 输出: (16, 28, 28) x = self.pool1(x) # 输出: (16, 14, 14) x = F.relu(self.conv2(x)) # 输出: (32, 10, 10) x = self.pool2(x) # 输出: (32, 5, 5) x = x.view(-1, 32 * 5 * 5) # 输出: (32*5*5) x = F.relu(self.fc1(x)) # 输出: (120) x = F.relu(self.fc2(x)) # 输出: (84) x = self.fc3(x) # 输出(10) return x 训练train.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119\"\"\"训练\"\"\"import torchimport torchvisionimport torch.nn as nnfrom model import LeNetimport torch.optim as optimimport torchvision.transforms as transformsimport matplotlib.pyplot as pltimport numpy as npimport timedef main(): transform = transforms.Compose([ transforms.ToTensor(), # 数据转为张量 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # 标准化处理 ]) # 导入训练集数据(50000张图片) train_set = torchvision.datasets.CIFAR10(root='./data', # root: 数据集存储路径 train=True, # 数据集为训练集 download=False, # download: True时下载数据集(下载完成修改为False) transform=transform # 数据预处理 ) # 加载训练集 train_loader = torch.utils.data.DataLoader(train_set, # 加载训练集 batch_size=50, # batch 大小 shuffle=True, # 是否随机打乱训练集 num_workers=0 # 使用的线程数量 ) # 导入测试集(10000张图片) val_set = torchvision.datasets.CIFAR10(root='./data', train=False, # 数据集为测试集 download=False, transform=transform ) # 加载测试集数据 val_loader = torch.utils.data.DataLoader(val_set, batch_size=10000, # 测试集batch大小 shuffle=False, num_workers=0 ) # 获取测试集中的图片和标签 val_data_iter = iter(val_loader) # val_image, val_label = val_data_iter.next() val_image, val_label = next(val_data_iter) # python 3 \"\"\" # ------------------------------------------------------------------------------------------- 查看数据集, 注意修改查看数据集的 batch \"\"\" # 定义的分类标签 # class_labels = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') # 查看数据集的图片 # def img_show(img): # img = img / 2 + 0.5 # np_img = img.numpy() # plt.imshow(np.transpose(np_img, (1, 2, 0))) # plt.show() # # # 查看数据集中的5张图像 # print(''.join(\" %5s \" % class_labels[val_label[j]] for j in range(5))) # img_show(torchvision.utils.make_grid(val_image)) \"\"\" # ------------------------------------------------------------------------------------------- \"\"\" # 检查是否支持CPU # if torch.cuda.is_available(): # use_dev = torch.device(\"cuda\") # else: # use_dev = torch.device(\"cpu\") # print(use_dev) device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") net = LeNet() # 用于训练的网络模型 # 指定GPU or CPU 进行训练 net.to(\"cpu\") loss_function = nn.CrossEntropyLoss() # 损失函数(交叉熵函数) optimizer = optim.Adam(net.parameters(), lr=0.001) # 优化器(训练参数, 学习率) # 训练的轮数 for epoch in range(5): start_time = time.perf_counter() running_loss = 0.0 # 遍历训练集, 从0开始 for step, data in enumerate(train_loader, start=0): inputs, labels = data # 得到训练集图片和标签 optimizer.zero_grad() # 清除历史梯度 outputs = net(inputs) # 正向传播 loss = loss_function(outputs, labels) # 损失计算 loss.backward() # 反向传播 optimizer.step() # 优化器更新参数 # 用于打印精确率等评估参数 running_loss += loss.item() if step % 500 == 499: # 500步打印一次 with torch.no_grad(): outputs = net(val_image) # 传入测试集数据 predict_y = torch.max(outputs, dim=1)[1] accuracy = torch.eq(predict_y, val_label).sum().item() / val_label.size(0) # 打印训练轮数、精确率等 print(\"[%d, %5d] train_loss: %.3f test_accuracy: %.3f\" % (epoch + 1, step + 1, running_loss / 500, accuracy) ) running_loss = 0.0 end_time = time.perf_counter() print(\"cost time = \", end_time - start_time) print(\"Finished trainning\") save_path = \"./LeNet.pth\" torch.save(net.state_dict(), save_path) # 保存训练输出的模型文件if __name__ == '__main__': main() 预测（暂未封装） 12345678910111213141516171819202122232425262728293031323334\"\"\"\"测试\"\"\"import torchimport torchvision.transforms as transformsfrom PIL import Imagefrom model import LeNetdef main(): transform = transforms.Compose([ transforms.Resize((32, 32)), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) ]) data_class = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') net = LeNet() net.load_state_dict(torch.load('LeNet.pth')) # net.load_state_dict(torch.load('LeNet.pth', map_location=torch.device(\"cpu\"))) test_image = Image.open('cat_test2.jpg') test_image = transform(test_image) # [C H W] test_image = torch.unsqueeze(test_image, dim=0) # [N C H W] with torch.no_grad(): outputs = net(test_image) predict = torch.max(outputs, dim=1)[1].numpy() print(f\"It is &#123;data_class[int(predict)]&#125;\")if __name__ == '__main__': main()","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}],"tags":[{"name":"VGG","slug":"VGG","permalink":"http://yoursite.com/tags/VGG/"},{"name":"AlexNet","slug":"AlexNet","permalink":"http://yoursite.com/tags/AlexNet/"},{"name":"LeNet","slug":"LeNet","permalink":"http://yoursite.com/tags/LeNet/"},{"name":"GoogLeNet","slug":"GoogLeNet","permalink":"http://yoursite.com/tags/GoogLeNet/"},{"name":"ResNet","slug":"ResNet","permalink":"http://yoursite.com/tags/ResNet/"},{"name":"卷积神经网络","slug":"卷积神经网络","permalink":"http://yoursite.com/tags/卷积神经网络/"}]},{"title":"CentOS-LibreOffice工具包安装","slug":"CentOS-LibreOffice工具包安装","date":"2023-12-28T06:43:49.000Z","updated":"2023-12-28T07:05:48.623Z","comments":true,"path":"2023/12/28/CentOS-LibreOffice工具包安装/","link":"","permalink":"http://yoursite.com/2023/12/28/CentOS-LibreOffice工具包安装/","excerpt":"· 系统： CentOS7 · LibreOffice： 7.4.5.1 稳定版 资源下载· 官方网站： https://zh-cn.libreoffice.org/download/libreoffice/ · 下载地址：https://downloadarchive.documentfoundation.org/libreoffice/old/7.4.5.1/rpm/x86_64/ 选择LibreOffice_7.4.5.1_Linux_x86-64_rpm.tar.gz安装包和LibreOffice_7.4.5.1_Linux_x86-64_rpm_langpack_zh-CN.tar.gz中文语言包并下载 安装进入安装包下载目录进行解压，这里为/usr/local/ 123cd /usr/local/ 进入目录tar -zxvf LibreOffice_7.4.6.1_Linux_x86-64_rpm.tar.gz 解压libreofficetar -zxvf LibreOffice7.4.6.1_Linux_x86-64_rpm_langpack_zh-CN.tar.gz 解压中文语言包 安装libreoffice和语言包的rpm包，默认安装目录为/opt/libreoffice7.4 1234cd /usr/local/LibreOffice_7.4.6.1_Linux_x86-64_rpm/RPMS/yum -y install *.rpmcd /usr/local/LibreOffice_7.4.6.1_Linux_x86-64_rpm_langpack_zh-CN/RPMSyum -y install *.rpm 安装soffice，进入/opt/libreoffice7.4/program目录执行 1234cd /opt/libreoffice7.4/program/yum install cairo yum install cups-libsyum install libSM 检查 1/opt/libreoffice7.4/program/soffice -help 正常输出，安装成功，接下来将soffice添加到环境变量 1vim /etc/profile 123# libreofficeexport LibreOffice_PATH=/opt/libreoffice7.4/programexport PATH=$LibreOffice_PATH:$PATH 1source /etc/profile","text":"· 系统： CentOS7 · LibreOffice： 7.4.5.1 稳定版 资源下载· 官方网站： https://zh-cn.libreoffice.org/download/libreoffice/ · 下载地址：https://downloadarchive.documentfoundation.org/libreoffice/old/7.4.5.1/rpm/x86_64/ 选择LibreOffice_7.4.5.1_Linux_x86-64_rpm.tar.gz安装包和LibreOffice_7.4.5.1_Linux_x86-64_rpm_langpack_zh-CN.tar.gz中文语言包并下载 安装进入安装包下载目录进行解压，这里为/usr/local/ 123cd /usr/local/ 进入目录tar -zxvf LibreOffice_7.4.6.1_Linux_x86-64_rpm.tar.gz 解压libreofficetar -zxvf LibreOffice7.4.6.1_Linux_x86-64_rpm_langpack_zh-CN.tar.gz 解压中文语言包 安装libreoffice和语言包的rpm包，默认安装目录为/opt/libreoffice7.4 1234cd /usr/local/LibreOffice_7.4.6.1_Linux_x86-64_rpm/RPMS/yum -y install *.rpmcd /usr/local/LibreOffice_7.4.6.1_Linux_x86-64_rpm_langpack_zh-CN/RPMSyum -y install *.rpm 安装soffice，进入/opt/libreoffice7.4/program目录执行 1234cd /opt/libreoffice7.4/program/yum install cairo yum install cups-libsyum install libSM 检查 1/opt/libreoffice7.4/program/soffice -help 正常输出，安装成功，接下来将soffice添加到环境变量 1vim /etc/profile 123# libreofficeexport LibreOffice_PATH=/opt/libreoffice7.4/programexport PATH=$LibreOffice_PATH:$PATH 1source /etc/profile 安装中文字体包windows系统字体包位于路径C:\\Windows\\Font下 centos系统字体包位于路径/usr/share/fonts下 进入/usr/share/fonts创建windows系统字体包目录 12cd /usr/share/fonts/mkdir windowsFont 将windows字体文件全部拷贝进该路径下，执行 123mkfontscalemkfontdirfc-cache 完成！可以在linux系统下顺利操作.docs、.pdf等文档文件，在langchain-chatchat中添加文档形知识库。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"linux工具","slug":"linux工具","permalink":"http://yoursite.com/tags/linux工具/"},{"name":"LibreOffice","slug":"LibreOffice","permalink":"http://yoursite.com/tags/LibreOffice/"}]},{"title":"LangChain + ChatGLM2-6B的本地知识问答库","slug":"LangChain + ChatGLM2-6B的本地知识问答库","date":"2023-10-24T01:14:34.000Z","updated":"2023-11-06T08:31:28.442Z","comments":true,"path":"2023/10/24/LangChain + ChatGLM2-6B的本地知识问答库/","link":"","permalink":"http://yoursite.com/2023/10/24/LangChain + ChatGLM2-6B的本地知识问答库/","excerpt":"原项目Github：https://github.com/imClumsyPanda/langchain-ChatGLM 项目部署· v 0.2.6 机器配置：· python 环境：anaconda3 + python3.10.12 · GPU：RTX3090*2 + CUDA11.7 · torch：2.0.1（CUDA未升至12） · conda：py310_dtglm 模型下载· m3e https://huggingface.co/moka-ai/m3e-base/tree/main · chatglm2-6b https://huggingface.co/THUDM/chatglm2-6b/tree/main chatglm清华源 https://cloud.tsinghua.edu.cn/d/674208019e314311ab5c/?p=%2F&amp;mode=list (这里将模型全部下载至/root/huggingface下) 创建虚拟环境，安装依赖123456conda create -n py310_dtglm python=3.10.12conda activate py310_dtglmpip install --use-pep517 -r requirements.txt -i https://mirror.baidu.com/pypi/simplepip install --use-pep517 -r requirements_api.txt -i https://mirror.baidu.com/pypi/simplepip install --use-pep517 -r requirements_webui.txt -i https://mirror.baidu.com/pypi/simple 修改配置、模型路径复制配置文件 1python copy_config_example.py 修改配置文件 · model_config.py 123456789101112131415161718MODEL_ROOT_PATH = \"/root/huggingface\"MODEL_PATH = &#123; \"embed_model\": &#123; ... \"m3e-base\": \"/root/huggingface/m3e-base\", # 修改m3e模型路径 ... &#125;, # TODO: add all supported llm models \"llm_model\": &#123; ... \"chatglm2-6b\": \"/root/huggingface/chatglm2-6b\", # 修改chatglm2-6b模型路径 ... &#125;,&#125;EMBEDDING_MODEL = \"m3e-base\" # 可以尝试最新的嵌入式sota模型：bge-large-zh-v1.5LLM_MODEL = \"chatglm2-6b\"","text":"原项目Github：https://github.com/imClumsyPanda/langchain-ChatGLM 项目部署· v 0.2.6 机器配置：· python 环境：anaconda3 + python3.10.12 · GPU：RTX3090*2 + CUDA11.7 · torch：2.0.1（CUDA未升至12） · conda：py310_dtglm 模型下载· m3e https://huggingface.co/moka-ai/m3e-base/tree/main · chatglm2-6b https://huggingface.co/THUDM/chatglm2-6b/tree/main chatglm清华源 https://cloud.tsinghua.edu.cn/d/674208019e314311ab5c/?p=%2F&amp;mode=list (这里将模型全部下载至/root/huggingface下) 创建虚拟环境，安装依赖123456conda create -n py310_dtglm python=3.10.12conda activate py310_dtglmpip install --use-pep517 -r requirements.txt -i https://mirror.baidu.com/pypi/simplepip install --use-pep517 -r requirements_api.txt -i https://mirror.baidu.com/pypi/simplepip install --use-pep517 -r requirements_webui.txt -i https://mirror.baidu.com/pypi/simple 修改配置、模型路径复制配置文件 1python copy_config_example.py 修改配置文件 · model_config.py 123456789101112131415161718MODEL_ROOT_PATH = \"/root/huggingface\"MODEL_PATH = &#123; \"embed_model\": &#123; ... \"m3e-base\": \"/root/huggingface/m3e-base\", # 修改m3e模型路径 ... &#125;, # TODO: add all supported llm models \"llm_model\": &#123; ... \"chatglm2-6b\": \"/root/huggingface/chatglm2-6b\", # 修改chatglm2-6b模型路径 ... &#125;,&#125;EMBEDDING_MODEL = \"m3e-base\" # 可以尝试最新的嵌入式sota模型：bge-large-zh-v1.5LLM_MODEL = \"chatglm2-6b\" · server_config.py 12345678910111213141516171819202122232425262728293031323334353637383940414243# webui.py serverWEBUI_SERVER = &#123; \"host\": DEFAULT_BIND_HOST, \"port\": 8501,&#125;# api.py serverAPI_SERVER = &#123; \"host\": DEFAULT_BIND_HOST, \"port\": 7861,&#125;# fastchat openai_api serverFSCHAT_OPENAI_API = &#123; \"host\": DEFAULT_BIND_HOST, \"port\": 20000,&#125;FSCHAT_MODEL_WORKERS = &#123; \"default\": &#123; \"host\": DEFAULT_BIND_HOST, \"port\": 20002, \"device\": LLM_DEVICE, \"infer_turbo\": False, # model_worker多卡加载需要配置的参数 \"gpus\": \"0,1\", # 使用的GPU，以str的格式指定，如\"0,1\"，如失效请使用CUDA_VISIBLE_DEVICES=\"0,1\"等形式指定 \"num_gpus\": 2, # 使用GPU的数量 \"max_gpu_memory\": \"20GiB\", # 每个GPU占用的最大显存 &#125;, \"zhipu-api\": &#123; # 请为每个要运行的在线API设置不同的端口 \"port\": 21001, &#125;,&#125;# fastchat controller serverFSCHAT_CONTROLLER = &#123; \"host\": DEFAULT_BIND_HOST, \"port\": 20001, \"dispatch_method\": \"shortest_queue\",&#125; 初始化默认知识库样例知识库文件位置：knowledge_base/samples/content/test.txt 1python init_database.py --recreate-vs 启动项目1python startup.py -a 通过fastapi接口添加知识库http://host:7861/knowledge_base/upload_docs 12345678910111213curl -X 'POST' \\ 'http://host:7861/knowledge_base/upload_docs' \\ -H 'accept: application/json' \\ -H 'Content-Type: multipart/form-data' \\ -F 'to_vector_store=true' \\ -F 'override=false' \\ -F 'not_refresh_vs_cache=false' \\ -F 'chunk_size=250' \\ -F 'chunk_overlap=50' \\ -F 'zh_title_enhance=true' \\ -F 'files=@分体式M录AI智能分析设备建设方案.docx;type=application/vnd.openxmlformats-officedocument.wordprocessingml.document' \\ -F 'knowledge_base_name=琅琊' \\ -F 'docs=' 选择知识库问答 代码调整百川大模型接入调整./configs/model_config.py 12345678\"llm_model\": &#123; ... 'baichuan-13b-chat':'/home/Baichuan2-main/baichuan-inc/Baichuan2-13B-Chat', ...&#125; # LLM 名称LLM_MODEL = \"baichuan-13b-chat\" 接口流式输出安装sse_starlette pip install sse-starlette -i https://mirror.baidu.com/pypi/simple 进入./chat/*.py，修改接口 123456from sse_starlette.sse import EventSourceResponse...return EventSourceResponse(chat_iterator(query=query, history=history, model_name=model_name, prompt_name=prompt_name)) 1/*注释内容 */","categories":[{"name":"GPT","slug":"GPT","permalink":"http://yoursite.com/categories/GPT/"}],"tags":[{"name":"langchain-chatchat","slug":"langchain-chatchat","permalink":"http://yoursite.com/tags/langchain-chatchat/"},{"name":"chat-glm","slug":"chat-glm","permalink":"http://yoursite.com/tags/chat-glm/"},{"name":"baichuan","slug":"baichuan","permalink":"http://yoursite.com/tags/baichuan/"}]},{"title":"Nebula3集群版新旧版本多开","slug":"Nebula3集群版版本多开","date":"2023-09-28T02:10:10.000Z","updated":"2023-11-03T09:32:18.875Z","comments":true,"path":"2023/09/28/Nebula3集群版版本多开/","link":"","permalink":"http://yoursite.com/2023/09/28/Nebula3集群版版本多开/","excerpt":"","text":"· 系统：CentOS7 · 已有nebula版本：2.6.1（开源社区版） · 已有nebula-console版本：2.6.0 · 已有nebula-graph-studio版本：3.2.3 · 多开nebula版本：3.6.0（开源社区版） · 多开nebula-graph-studio版本：3.2.3 · 多开nebula-console版本：3.6.0 集群部署· 参考单机部署方式，对配置文件--meta_server_addrs做扩展，添加meta机器 · 区分2.6.1版本已被占用的端口，找到配置文件默认的9559、19559、9669、19669、9779、19779端口，修改为8559、18559、8669、18669、8779、18779 · 启动集群 · 配置nebula-graph-studio默认端口为7002 · 注：双开nebula后使用同版本nebula-graph-studio即使更换了端口，也不能同时运行，可以安装nebula-console来同时启动nebula控制台 chmod 111 nebula-console ./nebula-console --addr &lt;host&gt; --port 9669 -u root -p nebula ./nebula-console --addr &lt;host&gt; --port 8669 -u root -p nebula","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"}],"tags":[{"name":"database","slug":"database","permalink":"http://yoursite.com/tags/database/"},{"name":"nebula","slug":"nebula","permalink":"http://yoursite.com/tags/nebula/"}]},{"title":"Nebula3单机版快速安装","slug":"Nebula3单机版快速安装","date":"2023-09-27T08:12:31.000Z","updated":"2024-02-01T06:59:02.195Z","comments":true,"path":"2023/09/27/Nebula3单机版快速安装/","link":"","permalink":"http://yoursite.com/2023/09/27/Nebula3单机版快速安装/","excerpt":"· 系统：CentOS7 · nebula版本：3.6.0（开源社区版） · nebula-graph-studio版本：3.2.3 单机部署tar包源码下载wget https://oss-cdn.nebula-graph.com.cn/package/3.6.0/nebula-graph-3.6.0.el7.x86_64.tar.gz 解压并重命名tar -xvzf nebula-graph-3.6.0.el7.x86_64.tar.gz mv nebula-graph-3.6.0.el7.x86_64 nebula 修改配置文件cd nebula/etc mv nebula-graphd.conf.default nebula-graphd.conf mv nebula-metad.conf.default nebula-metad.conf mv nebula-storaged.conf.default nebula-storaged.conf 修改对应文件存储位置、节点ip地址，集群同理","text":"· 系统：CentOS7 · nebula版本：3.6.0（开源社区版） · nebula-graph-studio版本：3.2.3 单机部署tar包源码下载wget https://oss-cdn.nebula-graph.com.cn/package/3.6.0/nebula-graph-3.6.0.el7.x86_64.tar.gz 解压并重命名tar -xvzf nebula-graph-3.6.0.el7.x86_64.tar.gz mv nebula-graph-3.6.0.el7.x86_64 nebula 修改配置文件cd nebula/etc mv nebula-graphd.conf.default nebula-graphd.conf mv nebula-metad.conf.default nebula-metad.conf mv nebula-storaged.conf.default nebula-storaged.conf 修改对应文件存储位置、节点ip地址，集群同理 示例： · nebula-graphd.conf 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134########## basics ########### Whether to run as a daemon process--daemonize=true# The file to host the process id--pid_file=pids/nebula-graphd.pid# Whether to enable optimizer--enable_optimizer=true# The default charset when a space is created--default_charset=utf8# The default collate when a space is created--default_collate=utf8_bin# Whether to use the configuration obtained from the configuration file--local_config=true########## logging ########### The directory to host logging files--log_dir=/home/nebula/graph/logs# Log level, 0, 1, 2, 3 for INFO, WARNING, ERROR, FATAL respectively--minloglevel=0# Verbose log level, 1, 2, 3, 4, the higher of the level, the more verbose of the logging--v=0# Maximum seconds to buffer the log messages--logbufsecs=0# Whether to redirect stdout and stderr to separate output files--redirect_stdout=true# Destination filename of stdout and stderr, which will also reside in log_dir.--stdout_log_file=graphd-stdout.log--stderr_log_file=graphd-stderr.log# Copy log messages at or above this level to stderr in addition to logfiles. The numbers of severity levels INFO, WARNING, ERROR, and FATAL are 0, 1, 2, and 3, respectively.--stderrthreshold=3# wether logging files&apos; name contain time stamp.--timestamp_in_logfile_name=true########## query ########### Whether to treat partial success as an error.# This flag is only used for Read-only access, and Modify access always treats partial success as an error.--accept_partial_success=false# Maximum sentence length, unit byte--max_allowed_query_size=4194304########## networking ########### Comma separated Meta Server Addresses--meta_server_addrs=192.168.9.103:9559# Local IP used to identify the nebula-graphd process.# Change it to an address other than loopback if the service is distributed or# will be accessed remotely.--local_ip=192.168.9.103# Network device to listen on--listen_netdev=any# Port to listen on--port=9669# To turn on SO_REUSEPORT or not--reuse_port=false# Backlog of the listen socket, adjust this together with net.core.somaxconn--listen_backlog=1024# The number of seconds Nebula service waits before closing the idle connections--client_idle_timeout_secs=28800# The number of seconds before idle sessions expire# The range should be in [1, 604800]--session_idle_timeout_secs=28800# The number of threads to accept incoming connections--num_accept_threads=1# The number of networking IO threads, 0 for # of CPU cores--num_netio_threads=0# Max active connections for all networking threads. 0 means no limit.# Max connections for each networking thread = num_max_connections / num_netio_threads--num_max_connections=0# The number of threads to execute user queries, 0 for # of CPU cores--num_worker_threads=0# HTTP service ip--ws_ip=0.0.0.0# HTTP service port--ws_http_port=19669# storage client timeout--storage_client_timeout_ms=60000# slow query threshold in us--slow_query_threshold_us=200000# Port to listen on Meta with HTTP protocol, it corresponds to ws_http_port in metad&apos;s configuration file--ws_meta_http_port=19559########## authentication ########### Enable authorization--enable_authorize=false# User login authentication type, password for nebula authentication, ldap for ldap authentication, cloud for cloud authentication--auth_type=password########## memory ########### System memory high watermark ratio, cancel the memory checking when the ratio greater than 1.0--system_memory_high_watermark_ratio=0.8########## metrics ##########--enable_space_level_metrics=false########## experimental feature ########### if use experimental features--enable_experimental_feature=false# if use balance data feature, only work if enable_experimental_feature is true--enable_data_balance=true# enable udf, written in c++ only for now--enable_udf=true# set the directory where the .so files of udf are stored, when enable_udf is true--udf_path=/home/nebula/dev/nebula/udf/########## session ########### Maximum number of sessions that can be created per IP and per user--max_sessions_per_ip_per_user=300########## memory tracker ########### trackable memory ratio (trackable_memory / (total_memory - untracked_reserved_memory) )--memory_tracker_limit_ratio=0.8# untracked reserved memory in Mib--memory_tracker_untracked_reserved_memory_mb=50# enable log memory tracker stats periodically--memory_tracker_detail_log=false# log memory tacker stats interval in milliseconds--memory_tracker_detail_log_interval_ms=60000# enable memory background purge (if jemalloc is used)--memory_purge_enabled=true# memory background purge interval in seconds--memory_purge_interval_seconds=10########## performance optimization ########### The max job size in multi job mode--max_job_size=1# The min batch size for handling dataset in multi job mode, only enabled when max_job_size is greater than 1--min_batch_size=8192# if true, return directly without go through RPC--optimize_appendvertices=false# number of paths constructed by each thread--path_batch_size=10000 · nebula-metad.conf 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253########## basics ########### Whether to run as a daemon process--daemonize=true# The file to host the process id--pid_file=pids/nebula-metad.pid########## logging ########### The directory to host logging files--log_dir=/home/nebula/meta/logs# Log level, 0, 1, 2, 3 for INFO, WARNING, ERROR, FATAL respectively--minloglevel=0# Verbose log level, 1, 2, 3, 4, the higher of the level, the more verbose of the logging--v=0# Maximum seconds to buffer the log messages--logbufsecs=0# Whether to redirect stdout and stderr to separate output files--redirect_stdout=true# Destination filename of stdout and stderr, which will also reside in log_dir.--stdout_log_file=metad-stdout.log--stderr_log_file=metad-stderr.log# Copy log messages at or above this level to stderr in addition to logfiles. The numbers of severity levels INFO, WARNING, ERROR, and FATAL are 0, 1, 2, and 3, respectively.--stderrthreshold=3# wether logging files&apos; name contain time stamp, If Using logrotate to rotate logging files, than should set it to true.--timestamp_in_logfile_name=true########## networking ########### Comma separated Meta Server addresses--meta_server_addrs=192.168.9.103:9559# Local IP used to identify the nebula-metad process.# Change it to an address other than loopback if the service is distributed or# will be accessed remotely.--local_ip=192.168.9.103# Meta daemon listening port--port=9559# HTTP service ip--ws_ip=0.0.0.0# HTTP service port--ws_http_port=19559# Port to listen on Storage with HTTP protocol, it corresponds to ws_http_port in storage&apos;s configuration file--ws_storage_http_port=19779########## storage ########### Root data path, here should be only single path for metad--data_path=/home/nebula/data/meta########## Misc ########## The default number of parts when a space is created--default_parts_num=100# The default replica factor when a space is created--default_replica_factor=1--heartbeat_interval_secs=10--agent_heartbeat_interval_secs=60 · nebula-metad.conf 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143########## basics ########### Whether to run as a daemon process--daemonize=true# The file to host the process id--pid_file=pids/nebula-storaged.pid# Whether to use the configuration obtained from the configuration file--local_config=true########## logging ########### The directory to host logging files--log_dir=/home/nebula/storage/logs# Log level, 0, 1, 2, 3 for INFO, WARNING, ERROR, FATAL respectively--minloglevel=0# Verbose log level, 1, 2, 3, 4, the higher of the level, the more verbose of the logging--v=0# Maximum seconds to buffer the log messages--logbufsecs=0# Whether to redirect stdout and stderr to separate output files--redirect_stdout=true# Destination filename of stdout and stderr, which will also reside in log_dir.--stdout_log_file=storaged-stdout.log--stderr_log_file=storaged-stderr.log# Copy log messages at or above this level to stderr in addition to logfiles. The numbers of severity levels INFO, WARNING, ERROR, and FATAL are 0, 1, 2, and 3, respectively.--stderrthreshold=3# Wether logging files&apos; name contain time stamp.--timestamp_in_logfile_name=true########## networking ########### Comma separated Meta server addresses--meta_server_addrs=192.168.9.103:9559# Local IP used to identify the nebula-storaged process.# Change it to an address other than loopback if the service is distributed or# will be accessed remotely.--local_ip=192.168.9.103# Storage daemon listening port--port=9779# HTTP service ip--ws_ip=0.0.0.0# HTTP service port--ws_http_port=19779# heartbeat with meta service--heartbeat_interval_secs=10######### Raft ########## Raft election timeout--raft_heartbeat_interval_secs=30# RPC timeout for raft client (ms)--raft_rpc_timeout_ms=500## recycle Raft WAL--wal_ttl=14400########## Disk ########### Root data path. Split by comma. e.g. --data_path=/disk1/path1/,/disk2/path2/# One path per Rocksdb instance.--data_path=/home/nebula/data/storage# Minimum reserved bytes of each data path--minimum_reserved_bytes=268435456# The default reserved bytes for one batch operation--rocksdb_batch_size=4096# The default block cache size used in BlockBasedTable.# The unit is MB.--rocksdb_block_cache=4# The type of storage engine, `rocksdb&apos;, `memory&apos;, etc.--engine_type=rocksdb# Compression algorithm, options: no,snappy,lz4,lz4hc,zlib,bzip2,zstd# For the sake of binary compatibility, the default value is snappy.# Recommend to use:# * lz4 to gain more CPU performance, with the same compression ratio with snappy# * zstd to occupy less disk space# * lz4hc for the read-heavy write-light scenario--rocksdb_compression=lz4# Set different compressions for different levels# For example, if --rocksdb_compression is snappy,# &quot;no:no:lz4:lz4::zstd&quot; is identical to &quot;no:no:lz4:lz4:snappy:zstd:snappy&quot;# In order to disable compression for level 0/1, set it to &quot;no:no&quot;--rocksdb_compression_per_level=# Whether or not to enable rocksdb&apos;s statistics, disabled by default--enable_rocksdb_statistics=false# Statslevel used by rocksdb to collection statistics, optional values are# * kExceptHistogramOrTimers, disable timer stats, and skip histogram stats# * kExceptTimers, Skip timer stats# * kExceptDetailedTimers, Collect all stats except time inside mutex lock AND time spent on compression.# * kExceptTimeForMutex, Collect all stats except the counters requiring to get time inside the mutex lock.# * kAll, Collect all stats--rocksdb_stats_level=kExceptHistogramOrTimers# Whether or not to enable rocksdb&apos;s prefix bloom filter, enabled by default.--enable_rocksdb_prefix_filtering=true# Whether or not to enable rocksdb&apos;s whole key bloom filter, disabled by default.--enable_rocksdb_whole_key_filtering=false############## rocksdb Options ############### rocksdb DBOptions in json, each name and value of option is a string, given as &quot;option_name&quot;:&quot;option_value&quot; separated by comma--rocksdb_db_options=&#123;&#125;# rocksdb ColumnFamilyOptions in json, each name and value of option is string, given as &quot;option_name&quot;:&quot;option_value&quot; separated by comma--rocksdb_column_family_options=&#123;&quot;write_buffer_size&quot;:&quot;67108864&quot;,&quot;max_write_buffer_number&quot;:&quot;4&quot;,&quot;max_bytes_for_level_base&quot;:&quot;268435456&quot;&#125;# rocksdb BlockBasedTableOptions in json, each name and value of option is string, given as &quot;option_name&quot;:&quot;option_value&quot; separated by comma--rocksdb_block_based_table_options=&#123;&quot;block_size&quot;:&quot;8192&quot;&#125;############### misc ##################### Whether turn on query in multiple thread--query_concurrently=true# Whether remove outdated space data--auto_remove_invalid_space=true# Network IO threads number--num_io_threads=16# Max active connections for all networking threads. 0 means no limit.# Max connections for each networking thread = num_max_connections / num_netio_threads--num_max_connections=0# Worker threads number to handle request--num_worker_threads=32# Maximum subtasks to run admin jobs concurrently--max_concurrent_subtasks=10# The rate limit in bytes when leader synchronizes snapshot data--snapshot_part_rate_limit=10485760# The amount of data sent in each batch when leader synchronizes snapshot data--snapshot_batch_size=1048576# The rate limit in bytes when leader synchronizes rebuilding index--rebuild_index_part_rate_limit=4194304# The amount of data sent in each batch when leader synchronizes rebuilding index--rebuild_index_batch_size=1048576########## memory tracker ########### trackable memory ratio (trackable_memory / (total_memory - untracked_reserved_memory) )--memory_tracker_limit_ratio=0.8# untracked reserved memory in Mib--memory_tracker_untracked_reserved_memory_mb=50# enable log memory tracker stats periodically--memory_tracker_detail_log=false# log memory tacker stats interval in milliseconds--memory_tracker_detail_log_interval_ms=60000# enable memory background purge (if jemalloc is used)--memory_purge_enabled=true# memory background purge interval in seconds--memory_purge_interval_seconds=10 启动服务回到nebula根目录 cd .. 执行启动脚本 ./scripts/nebula.service start all 查看服务状态 ./scripts/nebula.service status all 重启服务 ./scripts/nebula.service restart all 关闭服务 ./scripts/nebula.service stop all nebula-graph-studio部署v3.2.3下载地址（免费使用图探索的最高社区版本） https://github.com/vesoft-inc/nebula-studio/releases/tag/v3.2.3 源码安装后，进入对应目录，执行启动脚本 ./service","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"}],"tags":[{"name":"database","slug":"database","permalink":"http://yoursite.com/tags/database/"},{"name":"nebula","slug":"nebula","permalink":"http://yoursite.com/tags/nebula/"}]},{"title":"基于VGG16神经网络实现图像艺术风格转换","slug":"基于VGG16神经网络实现图像艺术风格转换","date":"2023-09-07T07:04:43.000Z","updated":"2024-04-18T10:07:42.612Z","comments":true,"path":"2023/09/07/基于VGG16神经网络实现图像艺术风格转换/","link":"","permalink":"http://yoursite.com/2023/09/07/基于VGG16神经网络实现图像艺术风格转换/","excerpt":"","text":"基本原理通过vgg16或其他神经网络提取图像特征，并使用格拉姆矩阵（Gram matrix）进行图像风格的迁移。 VGG16不必多说，2014年ImageNet图像分类竞赛亚军，定位竞赛冠军；VGG网络采用连续的小卷积核（3x3）和池化层构建深度神经网络，网络深度可以达到16层或19层，其中VGG16和VGG19最为著名。VGG16和VGG19网络架构非常相似，都由多个卷积层和池化层交替堆叠而成，最后使用全连接层进行分类。两者的区别在于网络的深度和参数量，VGG19相对于VGG16增加了3个卷积层和一个全连接层，参数量也更多。 可在keras直接使用vgg16/19源码，自动下载相关预训练模型 12from keras.applications.vgg16 import VGG16from keras.applications.vgg19 import VGG19 这里结合transform，在torch中构建神经网络 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112import torchfrom collections import namedtuplefrom torchvision import modelsimport torch.nn as nnimport torch.nn.functional as F# VGG16神经网络定义class VGG16(torch.nn.Module): \"\"\"Vgg16 Net\"\"\" def __init__(self, requires_grad=False): super(VGG16, self).__init__() vgg_pretrained_features = models.vgg16(pretrained=True).features self.slice1 = torch.nn.Sequential() self.slice2 = torch.nn.Sequential() self.slice3 = torch.nn.Sequential() self.slice4 = torch.nn.Sequential() for x in range(4): self.slice1.add_module(str(x), vgg_pretrained_features[x]) for x in range(4, 9): self.slice2.add_module(str(x), vgg_pretrained_features[x]) for x in range(9, 16): self.slice3.add_module(str(x), vgg_pretrained_features[x]) for x in range(16, 23): self.slice4.add_module(str(x), vgg_pretrained_features[x]) if not requires_grad: for param in self.parameters(): param.requires_grad = False def forward(self, X): h = self.slice1(X) h_relu1_2 = h h = self.slice2(h) h_relu2_2 = h h = self.slice3(h) h_relu3_3 = h h = self.slice4(h) h_relu4_3 = h vgg_outputs = namedtuple(\"VggOutputs\", [\"relu1_2\", \"relu2_2\", \"relu3_3\", \"relu4_3\"]) output = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3) return outputclass TransformerNet(torch.nn.Module): def __init__(self): super(TransformerNet, self).__init__() self.model = nn.Sequential( ConvBlock(3, 32, kernel_size=9, stride=1), ConvBlock(32, 64, kernel_size=3, stride=2), ConvBlock(64, 128, kernel_size=3, stride=2), ResidualBlock(128), ResidualBlock(128), ResidualBlock(128), ResidualBlock(128), ResidualBlock(128), ConvBlock(128, 64, kernel_size=3, upsample=True), ConvBlock(64, 32, kernel_size=3, upsample=True), ConvBlock(32, 3, kernel_size=9, stride=1, normalize=False, relu=False), ) def forward(self, x): return self.model(x)class ResidualBlock(torch.nn.Module): def __init__(self, channels): super(ResidualBlock, self).__init__() self.block = nn.Sequential( ConvBlock(channels, channels, kernel_size=3, stride=1, normalize=True, relu=True), ConvBlock(channels, channels, kernel_size=3, stride=1, normalize=True, relu=False), ) def forward(self, x): return self.block(x) + xclass ConvBlock(torch.nn.Module): def __init__(self, in_channels, out_channels, kernel_size, stride=1, upsample=False, normalize=True, relu=True): super(ConvBlock, self).__init__() self.upsample = upsample self.block = nn.Sequential( nn.ReflectionPad2d(kernel_size // 2), nn.Conv2d(in_channels, out_channels, kernel_size, (stride,)) ) self.norm = nn.InstanceNorm2d(out_channels, affine=True) if normalize else None self.relu = relu def forward(self, x): if self.upsample: x = F.interpolate(x, scale_factor=2) x = self.block(x) if self.norm is not None: x = self.norm(x) if self.relu: x = F.relu(x) return x\"\"\"测试模型\"\"\"if __name__ == '__main__': input1 = torch.rand([224, 3, 224, 224]) model_x = VGG16() print(model_x) 格拉姆矩阵格拉姆矩阵（Gram matrix）即n维欧式空间中任意k个向量之间两两的内积所组成的矩阵，是一个对称矩阵。 更直观的理解： 输入图像的feature map为[ ch, h, w]。我们经过flatten（即是将hw进行平铺成一维向量）和矩阵转置操作，可以变形为[ ch, hw]和[ h*w, ch]的矩阵。再对两个作内积得到格拉姆矩阵。 使用格拉姆矩阵进行风格迁移： 1.准备目标图像和目标风格图像； 2.使用深层网络加白噪声提取目标图像和风格目标的特征向量。对两个图像的特征向量计算格拉姆矩阵，以矩阵差异最小化为优化目标，不断调整目标图像，使风格不断相似。 torch中格拉姆矩阵代码： 123456def gram_matrix(y): (b, c, h, w) = y.size() features = y.view(b, c, w * h) features_t = features.transpose(1, 2) gram = features.bmm(features_t) / (c * h * w) return gram 开始训练准备训练文件和风格图片，例如随机图像*20和梵高名作星月夜 utils.py工具配置训练参数： 12345678910111213parser = argparse.ArgumentParser(description=\"Parser 4 Training\")parser.add_argument(\"--style\", type=str, default=\"images/styles/the_starry_night.jpg\", help=\"Path 2 style image\")parser.add_argument(\"--dataset\", type=str, help=\"path 2 training dataset\")parser.add_argument(\"--epochs\", type=int, default=1, help=\"Number of training epochs\")parser.add_argument(\"--batch_size\", type=int, default=4, help=\"Batch size 4 training\")parser.add_argument(\"--image_size\", type=int, default=256, help=\"Size of training images\")parser.add_argument(\"--style_size\", type=int, help=\"Size of style image\")parser.add_argument(\"--lr\", type=float, default=1e-3, help=\"Learning rate\")parser.add_argument(\"--lambda_img\", type=float, default=1e5, help=\"Weight 4 image loss\")parser.add_argument(\"--lambda_style\", type=float, default=1e10, help=\"Weight 4 style loss\")parser.add_argument(\"--model_path\", type=str, help=\"Optional path 2 checkpoint model\")parser.add_argument(\"--model_checkpoint\", type=int, default=1000, help=\"Batches 4 saving model\")parser.add_argument(\"--result_checkpoint\", type=int, default=1000, help=\"Batches 4 saving image result\") 使用神经网络进行风格训练 12345678910def train_transform(image_size): transform = transforms.Compose( [ transforms.Resize(int(image_size * 1.15)), transforms.RandomCrop(image_size), transforms.ToTensor(), transforms.Normalize(mean, std), ] ) return transform 使用神经网络进行风格转换 1234def style_transform(image_size=None): resize = [transforms.Resize(image_size)] if image_size else [] transform = transforms.Compose(resize + [transforms.ToTensor(), transforms.Normalize(mean, std)]) return transform 使用均值和标准对图像张量进行反规范化 1234def denormalize(tensors): for c in range(3): tensors[:, c].mul_(std[c]).add_(mean[c]) return tensors train.py训练脚本训练配置 12345678train_args = TrainArgs()args = train_args.initialize().parse_args()args.dataset = './dataset'args.style = './images/styles/the_starry_night.jpg'args.epochs = 2400 # epochs*(数据集/batch_size)是1000的公倍数args.batch_size = 4args.image_size = 256 训练流程 1234567891011121314151617181920style_name = args.style.split(\"/\")[-1].split(\".\")[0]os.makedirs(f\"images/train/&#123;style_name&#125;_training\", exist_ok=True)os.makedirs(f\"checkpoints\", exist_ok=True)device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")train_dataset = datasets.ImageFolder(args.dataset, train_transform(args.image_size))dataloader = DataLoader(train_dataset, batch_size=args.batch_size)transformer = TransformerNet().to(device)vgg = VGG16(requires_grad=False).to(device)if args.model_path: transformer.load_state_dict(torch.load(args.model_path))optimizer = Adam(transformer.parameters(), args.lr)l2_loss = torch.nn.MSELoss().to(device)style = style_transform(args.style_size)(Image.open(args.style))style = style.repeat(args.batch_size, 1, 1, 1).to(device)features_style = vgg(style)gram_style = [gram_matrix(y) for y in features_style]image_samples = []for path in random.sample(glob.glob(f\"&#123;args.dataset&#125;/*/*\"), len(train_dataset)): image_samples += [style_transform(args.image_size)(Image.open(path).resize((224, 224)))]image_samples = torch.stack(image_samples) 启动训练 1234567891011121314151617181920212223242526272829303132333435363738394041def save_result(sample): transformer.eval() with torch.no_grad(): output = transformer(image_samples.to(device)) image_rgb = denormalize(torch.cat((image_samples.cpu(), output.cpu()), 2)) save_image(image_rgb, f\"images/train/&#123;style_name&#125;_training/&#123;sample&#125;.jpg\", nrow=4) transformer.train()def save_model(sample): torch.save(transformer.state_dict(), f\"checkpoints/&#123;style_name&#125;_&#123;sample&#125;.pth\")for epoch in range(args.epochs): for line in range(len(dataloader)): batch_i = line batches_done = epoch * len(dataloader) + batch_i + 1 images = list(dataloader)[line][0] optimizer.zero_grad() images_original = images.to(device) images_transformed = transformer(images_original) features_original = vgg(images_original) features_transformed = vgg(images_transformed) img_loss = args.lambda_img * l2_loss(features_transformed.relu2_2, features_original.relu2_2) style_loss = 0 for ft_y, gm_s in zip(features_transformed, gram_style): gm_y = gram_matrix(ft_y) style_loss += l2_loss(gm_y, gm_s[: images.size(0), :, :]) style_loss *= args.lambda_style total_loss = img_loss + style_loss total_loss.backward() optimizer.step() if batches_done % args.result_checkpoint == 0: save_result(batches_done) if args.model_checkpoint &gt; 0 and batches_done % args.model_checkpoint == 0: save_model(batches_done) 第1000次迭代 第12000次迭代（2400epoch * (20/batch_size)），效果明显 到这一步，训练结束，可以预测结果 预测：配置预测参数 1234predict_args = PredictArgs()args = predict_args.initialize().parse_args()args.image_path = './images/input/001.jpg'args.model_path = './checkpoints/the_starry_night_12000.pth' 预测代码 1234567891011121314os.makedirs(\"images/output\", exist_ok=True) device = torch.device('cpu')#(\"cuda\" if torch.cuda.is_available() else \"cpu\") transform = style_transform() transformer = TransformerNet().to(device) transformer.load_state_dict(torch.load(mod_path)) transformer.eval() image_tensor = Variable(transform(Image.open(img_path))).to(device) image_tensor = image_tensor.unsqueeze(0) with torch.no_grad(): output_image = denormalize(transformer(image_tensor)).cpu() name = img_path.split(\"/\")[-1] save_image(output_image, f\"images/output/output_&#123;name&#125;\") 思路·参考https://github.com/elleryqueenhomels/fast_neural_style_transfer/tree/master https://github.com/AaronJny/DeepLearningExamples/tree/master/tf2-neural-style-transfer https://github.com/Huage001/PaintTransformer https://github.com/eriklindernoren/Fast-Neural-Style-Transfer/tree/master https://github.com/NeverGiveU/PaintTransformer-Pytorch-master https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}],"tags":[{"name":"VGG","slug":"VGG","permalink":"http://yoursite.com/tags/VGG/"},{"name":"卷积神经网络","slug":"卷积神经网络","permalink":"http://yoursite.com/tags/卷积神经网络/"}]},{"title":"paddleDetection Demo","slug":"paddleDetection Demo","date":"2023-08-31T04:20:20.000Z","updated":"2024-01-25T09:14:54.938Z","comments":true,"path":"2023/08/31/paddleDetection Demo/","link":"","permalink":"http://yoursite.com/2023/08/31/paddleDetection Demo/","excerpt":"PPHuman行人属性识别行人属性cfg： 12345678910111213141516171819202122232425crop_thresh: 0.5attr_thresh: 0.5kpt_thresh: 0.2visual: Truewarmup_frame: 50DET: model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip batch_size: 1MOT: model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip tracker_config: /exp/work/video/PaddleDetection/deploy/pipeline/config/tracker_config.yml batch_size: 1 skip_frame_num: -1 # preferably no more than 3 enable: TrueKPT: model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/dark_hrnet_w32_256x192.zip batch_size: 8ATTR: model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/PPLCNet_x1_0_person_attribute_945_infer.zip batch_size: 8 enable: True cli： 1python deploy/pipeline/pipeline.py --config deploy/pipeline/config/cache/cfg_human.yml --device=gpu --video_file=demo_input/human.mp4 --output_dir=demo_output/","text":"PPHuman行人属性识别行人属性cfg： 12345678910111213141516171819202122232425crop_thresh: 0.5attr_thresh: 0.5kpt_thresh: 0.2visual: Truewarmup_frame: 50DET: model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip batch_size: 1MOT: model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip tracker_config: /exp/work/video/PaddleDetection/deploy/pipeline/config/tracker_config.yml batch_size: 1 skip_frame_num: -1 # preferably no more than 3 enable: TrueKPT: model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/dark_hrnet_w32_256x192.zip batch_size: 8ATTR: model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/PPLCNet_x1_0_person_attribute_945_infer.zip batch_size: 8 enable: True cli： 1python deploy/pipeline/pipeline.py --config deploy/pipeline/config/cache/cfg_human.yml --device=gpu --video_file=demo_input/human.mp4 --output_dir=demo_output/ 行人行为识别跌倒cfg： 12345678910111213141516171819202122crop_thresh: 0.5kpt_thresh: 0.2visual: Truewarmup_frame: 50MOT: model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_s_36e_pipeline.zip tracker_config: /exp/work/video/PaddleDetection/deploy/pipeline/config/tracker_config.yml batch_size: 1 enable: True KPT: model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/dark_hrnet_w32_256x192.zip batch_size: 8SKELETON_ACTION: model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/STGCN.zip batch_size: 1 max_frames: 50 display_frames: 80 coord_size: [384, 512] enable: True cli： 1python deploy/pipeline/pipeline.py --config deploy/pipeline/config/cache/cfg_fall.yml --device=gpu --video_file=demo_input/fall.mp4 --output_dir=demo_output/ 接打电话cfg： 12345678910111213141516171819202122crop_thresh: 0.5kpt_thresh: 0.2visual: Truewarmup_frame: 50MOT: model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_s_36e_pipeline.zip tracker_config: /exp/work/video/PaddleDetection/deploy/pipeline/config/tracker_config.yml batch_size: 1 enable: TrueID_BASED_CLSACTION: model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/PPHGNet_tiny_calling_halfbody.zip batch_size: 8 threshold: 0.8 display_frames: 80 skip_frame_num: 2 enable: TrueKPT: model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/dark_hrnet_w32_256x192.zip batch_size: 8 cli： 1python deploy/pipeline/pipeline.py --config deploy/pipeline/config/cache/cfg_call.yml --device=gpu --video_file=demo_input/call.mp4 --output_dir=demo_output/ 吸烟cfg： 12345678910111213141516171819202122crop_thresh: 0.5kpt_thresh: 0.2visual: Truewarmup_frame: 50MOT: model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_s_36e_pipeline.zip tracker_config: /exp/work/video/PaddleDetection/deploy/pipeline/config/tracker_config.yml batch_size: 1 enable: TrueID_BASED_DETACTION: model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/ppyoloe_crn_s_80e_smoking_visdrone.zip batch_size: 8 threshold: 0.6 display_frames: 80 skip_frame_num: 2 enable: TrueKPT: model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/dark_hrnet_w32_256x192.zip batch_size: 8 cli： 1python deploy/pipeline/pipeline.py --config deploy/pipeline/config/cache/cfg_smoke.yml --device=gpu --video_file=demo_input/smoke.mp4 --output_dir=demo_output/ 打架cfg： 1234567891011121314151617181920212223crop_thresh: 0.5kpt_thresh: 0.2visual: Truewarmup_frame: 50MOT: model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_s_36e_pipeline.zip tracker_config: /exp/work/video/PaddleDetection/deploy/pipeline/config/tracker_config.yml batch_size: 1 enable: TrueKPT: model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/dark_hrnet_w32_256x192.zip batch_size: 8VIDEO_ACTION: model_dir: https://videotag.bj.bcebos.com/PaddleVideo-release2.3/ppTSM_fight.zip batch_size: 1 frame_len: 8 sample_freq: 7 short_size: 340 target_size: 320 enable: True cli： 1python deploy/pipeline/pipeline.py --config deploy/pipeline/config/cache/cfg_fight.yml --device=gpu --video_file=demo_input/fight.mp4 --output_dir=demo_output/ 行人进入cfg： 12345678910crop_thresh: 0.5visual: Truewarmup_frame: 50MOT: model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip tracker_config: /exp/work/video/PaddleDetection/backend/pp_config/tracker_config.yml batch_size: 1 skip_frame_num: -1 # preferably no more than 3 enable: True cli： 1python deploy/pipeline/pipeline.py --config deploy/pipeline/config/cache/cfg_entrance.yml --region_type=bottom --do_entrance_counting --device=gpu --video_file=demo_input/entrance.mp4 --output_dir=demo_output/ 禁区cfg： 12345678910crop_thresh: 0.5visual: Truewarmup_frame: 50MOT: model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip tracker_config: /exp/work/video/PaddleDetection/backend/pp_config/tracker_config.yml batch_size: 1 skip_frame_num: -1 # preferably no more than 3 enable: True cli： 1python deploy/pipeline/pipeline.py --config deploy/pipeline/config/cache/cfg_entrance.yml --do_break_in_counting=true --region_type=custom --region_polygon 355 700 1100 700 915 900 50 900 --device=gpu --video_file=demo_input/forb.mp4 --output_dir=demo_output/ PPVehicle车辆属性识别车辆属性cfg： 1234567891011121314151617181920212223242526272829303132333435crop_thresh: 0.5visual: Truewarmup_frame: 50DET: model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_s_36e_ppvehicle.zip batch_size: 1MOT: model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_s_36e_ppvehicle.zip tracker_config: /exp/work/video/PaddleDetection/backend/pp_config/tracker_config.yml batch_size: 1 skip_frame_num: 3 # preferably no more than 3 enable: TrueVEHICLE_PLATE: det_model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/ch_PP-OCRv3_det_infer.tar.gz det_limit_side_len: 736 det_limit_type: \"min\" rec_model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/ch_PP-OCRv3_rec_infer.tar.gz rec_image_shape: [3, 48, 320] rec_batch_num: 6 word_dict_path: /exp/work/video/PaddleDetection/deploy/pipeline/ppvehicle/rec_word_dict.txt enable: TrueVEHICLE_ATTR: model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/vehicle_attribute_model.zip batch_size: 8 color_threshold: 0.5 type_threshold: 0.5 enable: TrueLANE_SEG: lane_seg_config: deploy/pipeline/config/lane_seg_config.yml model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/pp_lite_stdc2_bdd100k.zip cli： 1python deploy/pipeline/pipeline.py --config deploy/pipeline/config/cache/cfg_car.yml --device=gpu --video_file=demo_input/car.mp4 --output_dir=demo_output/ 车辆行为识别禁止停车cfg： 12345678910111213141516171819crop_thresh: 0.5visual: Truewarmup_frame: 50MOT: model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_s_36e_ppvehicle.zip tracker_config: /exp/work/video/PaddleDetection/deploy/pipeline/config/tracker_config.yml batch_size: 1 enable: TrueVEHICLE_PLATE: det_model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/ch_PP-OCRv3_det_infer.tar.gz det_limit_side_len: 736 det_limit_type: \"min\" rec_model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/ch_PP-OCRv3_rec_infer.tar.gz rec_image_shape: [3, 48, 320] rec_batch_num: 6 word_dict_path: /exp/work/video/PaddleDetection/deploy/pipeline/ppvehicle/rec_word_dict.txt enable: True cli： 1python deploy/pipeline/pipeline.py --config deploy/pipeline/config/cache/cfg_park.yml --region_type=custom --FLAGS.illegal_parking_time=5 --region_polygon 600 200 1400 200 1400 900 600 900 --device=gpu --video_file=demo_input/park.mp4 --output_dir=demo_output/ 人脸识别人脸识别cfg（需paddleDetection集成arcface）： 1234567891011crop_thresh: 0.5attr_thresh: 0.5kpt_thresh: 0.2visual: Truewarmup_frame: 50FACE_DET: enable: TrueFACE_REC: enable: False cli： 1python deploy/pipeline/pipeline.py --config deploy/pipeline/config/cache/cfg_face.yml --device=gpu --video_file=demo_input/face.mp4 --output_dir=demo_output/ OCR视频OCRcfg（需paddleDetection集成paddleOCR）： 12345678crop_thresh: 0.5attr_thresh: 0.5kpt_thresh: 0.2visual: Truewarmup_frame: 50OCR: enable: True cli： 1python deploy/pipeline/pipeline.py --config deploy/pipeline/config/cache/cfg_ocr.yml --device=gpu --video_file=demo_input/ocr.mp4 --output_dir=demo_output/","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}],"tags":[{"name":"paddlepaddle","slug":"paddlepaddle","permalink":"http://yoursite.com/tags/paddlepaddle/"},{"name":"opencv","slug":"opencv","permalink":"http://yoursite.com/tags/opencv/"}]},{"title":"LiveGBS国标GB/T28181视频流媒体平台","slug":"LiveGBS国标GB-T28181视频流媒体平台","date":"2023-08-18T08:34:45.000Z","updated":"2023-08-18T08:35:21.653Z","comments":true,"path":"2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/","link":"","permalink":"http://yoursite.com/2023/08/18/LiveGBS国标GB-T28181视频流媒体平台/","excerpt":"软件包下载LiveGBS GB28181流媒体服务下载地址：https://www.liveqing.com/docs/download/LiveGBS.html#%E7%89%88%E6%9C%AC%E4%B8%8B%E8%BD%BD 选择windows版本的LiveGBS 信令服务和LiveGBS流媒体服务，免费版授权周期为26天，届时需要手动更新软件服务 安装LiveGBS GB28281解压下载好的软件包，分别启动LiveCMS.exe和LiveSMS.exe，如果有默认端口被占用的情况可以修改对应的livecms.ini或livesms.ini配置文件，这里我将LiveGBS的默认端口从10000修改为10005 成功启动后后台出现livecms和livesms的图标","text":"软件包下载LiveGBS GB28181流媒体服务下载地址：https://www.liveqing.com/docs/download/LiveGBS.html#%E7%89%88%E6%9C%AC%E4%B8%8B%E8%BD%BD 选择windows版本的LiveGBS 信令服务和LiveGBS流媒体服务，免费版授权周期为26天，届时需要手动更新软件服务 安装LiveGBS GB28281解压下载好的软件包，分别启动LiveCMS.exe和LiveSMS.exe，如果有默认端口被占用的情况可以修改对应的livecms.ini或livesms.ini配置文件，这里我将LiveGBS的默认端口从10000修改为10005 成功启动后后台出现livecms和livesms的图标 配置LiveGBS进入&lt;host&gt;:10005，点击基础配置，修改信令服务配置和流媒体服务配置 配置视频设备进入网络连接，选择对应的以太网，右键属性，选择Internet 协议版本 4 (TCP/IPv4)，选择高级，添加近端设备和摄像头的网段 进入设备IP，修改对应的网络配置、近端配置和摄像头配置 配置完成以后点击用户admin，选择重启设备 视频拉流回到&lt;host&gt;:10005，此时已经可以访问摄像头 点击国标设备，查看通道 看到已经配置好的摄像头信息 可从右下角获取视频拉流","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://yoursite.com/categories/计算机视觉/"}],"tags":[{"name":"LiveGBS","slug":"LiveGBS","permalink":"http://yoursite.com/tags/LiveGBS/"}]},{"title":"paddleDetection-视频OCR","slug":"paddleDetection-视频OCR","date":"2023-08-14T03:15:55.000Z","updated":"2023-08-29T09:18:54.671Z","comments":true,"path":"2023/08/14/paddleDetection-视频OCR/","link":"","permalink":"http://yoursite.com/2023/08/14/paddleDetection-视频OCR/","excerpt":"PPOCR_V4安装百度最新ppocr_v4库，使用虚拟环境为py39_vio，本虚拟环境不可与人脸识别（py38_arcface）兼容（opencv版本不兼容） 1pip install paddleocr --user -i https://mirror.baidu.com/pypi/simple 代码cfg_utils.py新增cfg--ocr，设置True为开启，默认False 12345parser.add_argument( \"--ocr\", type=bool, default=False, help=\"use paddlepaddle-ocr\") pipeline.py 1from python.visualize import visualize_box_mask, visualize_attr, visualize_pose, visualize_action, visualize_vehicleplate, visualize_vehiclepress, visualize_lane, visualize_vehicle_retrograde, visualize_ocr 123class PipePredictor(object): def __init__(self, args, cfg, is_video=True, multi_camera=False): self.ocr = args.ocr 1234567891011121314151617181920212223def visualize_video(self, image_rgb, result, collector, frame_id, fps, entrance=None, records=None, center_traj=None, do_illegal_parking_recognition=False, illegal_parking_dict=None): image = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR) mot_res = copy.deepcopy(result.get('mot')) if self.ocr: lock.acquire() # 加锁，paddleOCR是线程不安全的 ocr_result = ocr.ocr(image, cls=True)[0] lock.release() ocr_boxes = [line[0] for line in ocr_result] ocr_txts = [line[1][0] for line in ocr_result] ocr_scores = [line[1][1] for line in ocr_result] image = visualize_ocr(image, ocr_boxes, ocr_txts, ocr_scores) visualize.py 12345678910111213141516171819202122232425262728293031323334353637383940414243def visualize_ocr(im, boxes, texts, score): if isinstance(im, str): im = Image.open(im) im = np.ascontiguousarray(np.copy(im)) im = cv2.cvtColor(im, cv2.COLOR_RGB2BGR) else: im = np.ascontiguousarray(np.copy(im)) # 创建透明图层，为图像添加文字水印 im = Image.fromarray(im) im = im.convert('RGBA') im_canvas = Image.new('RGBA', im.size, (255, 255, 255, 0)) for i, res in enumerate(texts): if boxes is not None: box = boxes[i] text = res if text == \"\": continue text_scale = max(1.0, int(box[2][1] - box[1][1])) draw = ImageDraw.Draw(im_canvas) draw.text( (box[0][0], box[0][1]), text, font=ImageFont.truetype(font_file, size=int(text_scale)), fill=(255, 255, 0, 85)) # 第四位是透明度 try: draw.rectangle( ((box[0][0], box[0][1]), (box[2][0], box[2][1])), fill=None, outline=(255, 255, 0), width=1) except ValueError: pass # 复合图层 im = Image.alpha_composite(im, im_canvas) im = im.convert('RGB') # 还原连续存储数组 im = np.ascontiguousarray(np.copy(im)) return im","text":"PPOCR_V4安装百度最新ppocr_v4库，使用虚拟环境为py39_vio，本虚拟环境不可与人脸识别（py38_arcface）兼容（opencv版本不兼容） 1pip install paddleocr --user -i https://mirror.baidu.com/pypi/simple 代码cfg_utils.py新增cfg--ocr，设置True为开启，默认False 12345parser.add_argument( \"--ocr\", type=bool, default=False, help=\"use paddlepaddle-ocr\") pipeline.py 1from python.visualize import visualize_box_mask, visualize_attr, visualize_pose, visualize_action, visualize_vehicleplate, visualize_vehiclepress, visualize_lane, visualize_vehicle_retrograde, visualize_ocr 123class PipePredictor(object): def __init__(self, args, cfg, is_video=True, multi_camera=False): self.ocr = args.ocr 1234567891011121314151617181920212223def visualize_video(self, image_rgb, result, collector, frame_id, fps, entrance=None, records=None, center_traj=None, do_illegal_parking_recognition=False, illegal_parking_dict=None): image = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR) mot_res = copy.deepcopy(result.get('mot')) if self.ocr: lock.acquire() # 加锁，paddleOCR是线程不安全的 ocr_result = ocr.ocr(image, cls=True)[0] lock.release() ocr_boxes = [line[0] for line in ocr_result] ocr_txts = [line[1][0] for line in ocr_result] ocr_scores = [line[1][1] for line in ocr_result] image = visualize_ocr(image, ocr_boxes, ocr_txts, ocr_scores) visualize.py 12345678910111213141516171819202122232425262728293031323334353637383940414243def visualize_ocr(im, boxes, texts, score): if isinstance(im, str): im = Image.open(im) im = np.ascontiguousarray(np.copy(im)) im = cv2.cvtColor(im, cv2.COLOR_RGB2BGR) else: im = np.ascontiguousarray(np.copy(im)) # 创建透明图层，为图像添加文字水印 im = Image.fromarray(im) im = im.convert('RGBA') im_canvas = Image.new('RGBA', im.size, (255, 255, 255, 0)) for i, res in enumerate(texts): if boxes is not None: box = boxes[i] text = res if text == \"\": continue text_scale = max(1.0, int(box[2][1] - box[1][1])) draw = ImageDraw.Draw(im_canvas) draw.text( (box[0][0], box[0][1]), text, font=ImageFont.truetype(font_file, size=int(text_scale)), fill=(255, 255, 0, 85)) # 第四位是透明度 try: draw.rectangle( ((box[0][0], box[0][1]), (box[2][0], box[2][1])), fill=None, outline=(255, 255, 0), width=1) except ValueError: pass # 复合图层 im = Image.alpha_composite(im, im_canvas) im = im.convert('RGB') # 还原连续存储数组 im = np.ascontiguousarray(np.copy(im)) return im 启动在deploy/pipeline/config下创建视频ocr的yml文件infer_cfg_ppocr.yml，写入基本参数 123crop_thresh: 0.5visual: Truewarmup_frame: 50 1python deploy/pipeline/pipeline.py --config deploy/pipeline/config/infer_cfg_ppocr.yml --device=gpu --video_file=demo_input/car_t1.mp4 --output_dir=demo_output --ocr=True","categories":[],"tags":[{"name":"OCR","slug":"OCR","permalink":"http://yoursite.com/tags/OCR/"},{"name":"paddlepaddle","slug":"paddlepaddle","permalink":"http://yoursite.com/tags/paddlepaddle/"}]},{"title":"paddleDetection:OpenCV检测框转中文","slug":"paddleDetection-OpenCV检测框转中文","date":"2023-08-09T08:29:45.000Z","updated":"2024-01-25T09:20:13.155Z","comments":true,"path":"2023/08/09/paddleDetection-OpenCV检测框转中文/","link":"","permalink":"http://yoursite.com/2023/08/09/paddleDetection-OpenCV检测框转中文/","excerpt":"注：OpenCV不能直接显示中文，通过PIL转换会损失一部分算力性能 修改源码（可视化）./deploy/python/visualize.py 增加导入字体库和字体文件123from PIL import Image, ImageDraw, ImageFile, ImageFontfont_file = '/exp/work/video/PaddleDetection/deploy/pipeline/SourceHanSansCN-Medium.otf' visualize_attr123456789101112131415161718192021222324252627282930313233343536373839def visualize_attr(im, results, boxes=None, is_mtmct=False): if isinstance(im, str): im = Image.open(im) im = np.ascontiguousarray(np.copy(im)) im = cv2.cvtColor(im, cv2.COLOR_RGB2BGR) else: im = np.ascontiguousarray(np.copy(im)) line_inter = im.shape[0] / 40. text_scale = max(0.5, im.shape[0] / 100.) # 将nparray图像转PIL图像 im = Image.fromarray(im) for i, res in enumerate(results): print(i, res) if boxes is None: text_w = 3 text_h = 1 elif is_mtmct: box = boxes[i] # multi camera, bbox shape is x,y, w,h text_w = int(box[0]) + 3 text_h = int(box[1]) else: box = boxes[i] # single camera, bbox shape is 0, 0, x,y, w,h text_w = int(box[2]) + 3 text_h = int(box[3]) for text in res: text_h += int(line_inter) text_loc = (text_w, text_h) # 写入 draw = ImageDraw.Draw(im) draw.text( text_loc, text, font=ImageFont.truetype(font_file, size=int(text_scale)), # 字体位置 fill=(0, 255, 255)) # 还原连续存储数组 im = np.ascontiguousarray(np.copy(im)) return im","text":"注：OpenCV不能直接显示中文，通过PIL转换会损失一部分算力性能 修改源码（可视化）./deploy/python/visualize.py 增加导入字体库和字体文件123from PIL import Image, ImageDraw, ImageFile, ImageFontfont_file = '/exp/work/video/PaddleDetection/deploy/pipeline/SourceHanSansCN-Medium.otf' visualize_attr123456789101112131415161718192021222324252627282930313233343536373839def visualize_attr(im, results, boxes=None, is_mtmct=False): if isinstance(im, str): im = Image.open(im) im = np.ascontiguousarray(np.copy(im)) im = cv2.cvtColor(im, cv2.COLOR_RGB2BGR) else: im = np.ascontiguousarray(np.copy(im)) line_inter = im.shape[0] / 40. text_scale = max(0.5, im.shape[0] / 100.) # 将nparray图像转PIL图像 im = Image.fromarray(im) for i, res in enumerate(results): print(i, res) if boxes is None: text_w = 3 text_h = 1 elif is_mtmct: box = boxes[i] # multi camera, bbox shape is x,y, w,h text_w = int(box[0]) + 3 text_h = int(box[1]) else: box = boxes[i] # single camera, bbox shape is 0, 0, x,y, w,h text_w = int(box[2]) + 3 text_h = int(box[3]) for text in res: text_h += int(line_inter) text_loc = (text_w, text_h) # 写入 draw = ImageDraw.Draw(im) draw.text( text_loc, text, font=ImageFont.truetype(font_file, size=int(text_scale)), # 字体位置 fill=(0, 255, 255)) # 还原连续存储数组 im = np.ascontiguousarray(np.copy(im)) return im visualize_vehicleplate12345678910111213141516171819202122232425262728293031323334def visualize_vehicleplate(im, results, boxes=None): if isinstance(im, str): im = Image.open(im) im = np.ascontiguousarray(np.copy(im)) im = cv2.cvtColor(im, cv2.COLOR_RGB2BGR) else: im = np.ascontiguousarray(np.copy(im)) text_scale = max(1.0, im.shape[0] / 400.) line_inter = im.shape[0] / 40. # 将nparray图像转PIL图像 im = Image.fromarray(im) for i, res in enumerate(results): if boxes is None: text_w = 3 text_h = 1 else: box = boxes[i] text = res if text == \"\": continue text_w = int(box[2]) text_h = int(box[5] + box[3]) text_loc = (text_w, text_h) # 写入 draw = ImageDraw.Draw(im) draw.text( text_loc, text, font=ImageFont.truetype(font_file, size=int(text_scale)), # 字体位置 fill=(0, 255, 255)) # 还原连续存储数组 im = np.ascontiguousarray(np.copy(im)) return im visualize_action1234567891011121314151617181920212223242526272829303132333435363738394041424344def visualize_action(im, mot_boxes, action_visual_collector=None, action_text=\"\", video_action_score=None, video_action_text=\"\"): im = cv2.imread(im) if isinstance(im, str) else im im_h, im_w = im.shape[:2] text_scale = max(1, im.shape[1] / 40.) text_thickness = 2 # 将nparray图像转PIL图像 im = Image.fromarray(im) if action_visual_collector: id_action_dict = &#123;&#125; for collector, action_type in zip(action_visual_collector, action_text): id_detected = collector.get_visualize_ids() for pid in id_detected: id_action_dict[pid] = id_action_dict.get(pid, []) id_action_dict[pid].append(action_type) for mot_box in mot_boxes: # mot_box is a format with [mot_id, class, score, xmin, ymin, w, h] if mot_box[0] in id_action_dict: text_position = (int(mot_box[3] + mot_box[5] * 0.75), int(mot_box[4] - 10)) display_text = ', '.join(id_action_dict[mot_box[0]]) draw = ImageDraw.Draw(im) draw.text( text_position, display_text, font=ImageFont.truetype(size=int(text_scale)), # 字体位置 fill=(0, 0, 255)) if video_action_score: draw = ImageDraw.Draw(im) draw.text( (int(im_w / 2), int(15 * text_scale) + 5), video_action_text + ': %.2f' % video_action_score, font=ImageFont.truetype(font_file, size=int(text_scale)), # 字体位置 fill=(0, 0, 255)) # 还原连续存储数组 im = np.ascontiguousarray(np.copy(im)) return im 修改源码（属性）修改对应中文模块 ./deploy/pipeline/ pipeline.py ./deploy/pipeline/ppvehicle vehicle_attr.py vehicle_plate.py ./deploy/pipeline/pphuman attr_infer.py 执行脚本12345# pphumanpython deploy/pipeline/pipeline.py --config deploy/pipeline/config/infer_cfg_pphuman.yml --video_file=demo_input/act1.mp4 --device=gpu --output_dir=demo_output# ppvehiclepython deploy/pipeline/pipeline.py --config deploy/pipeline/config/infer_cfg_ppvehicle.yml --video_file=demo_input/car_t1.mp4 --device=gpu --output_dir=demo_output","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}],"tags":[{"name":"paddlepaddle","slug":"paddlepaddle","permalink":"http://yoursite.com/tags/paddlepaddle/"},{"name":"opencv","slug":"opencv","permalink":"http://yoursite.com/tags/opencv/"}]},{"title":"arcface_paddle","slug":"arcface-paddle","date":"2023-08-08T09:12:11.000Z","updated":"2023-08-09T10:03:27.504Z","comments":true,"path":"2023/08/08/arcface-paddle/","link":"","permalink":"http://yoursite.com/2023/08/08/arcface-paddle/","excerpt":"环境GPU（物理） NVIDIA 3090*2 显卡驱动 515.43.04 CUDA版本 11.7 CUDAtoolkit (cuda_11.7.0_515.43.04_linux) cuDNN (v8.4.1) paddlepaddle 多卡训练需要NCLL支持 (ncll v2.12.12 cuda11.7) paddlepaddle版本 paddlepaddle-gpu==2.2.0rc0（虚拟环境cuda11.2） python环境 CentOS7.9 anaconda3 python3.8 anaconda安装insightface重要：pillow版本建议选择9.5 否则过高会导致安装insightface报错 （错误原因：pillow10移除了getsize方法，需要修改对应位置源码为getbbox或 getlength） 告警信息： 12tools/test_recognition.py:627: DeprecationWarning: getsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use getbbox or getlength instead. tw = font.getsize(text)[0] 环境安装12345678# .conda install paddlepaddle-gpu==2.2.0rc0 cudatoolkit=11.2 -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/Paddle/ -c conda-forge# insightfacepip install -r requirements.txt -i https://mirror.baidu.com/pypi/simple# insightface/recongition/arcface_paddle/pip install -r requirement.txt -i https://mirror.baidu.com/pypi/simple# insightface-paddlepip install insightface-paddle -i https://mirror.baidu.com/pypi/simple","text":"环境GPU（物理） NVIDIA 3090*2 显卡驱动 515.43.04 CUDA版本 11.7 CUDAtoolkit (cuda_11.7.0_515.43.04_linux) cuDNN (v8.4.1) paddlepaddle 多卡训练需要NCLL支持 (ncll v2.12.12 cuda11.7) paddlepaddle版本 paddlepaddle-gpu==2.2.0rc0（虚拟环境cuda11.2） python环境 CentOS7.9 anaconda3 python3.8 anaconda安装insightface重要：pillow版本建议选择9.5 否则过高会导致安装insightface报错 （错误原因：pillow10移除了getsize方法，需要修改对应位置源码为getbbox或 getlength） 告警信息： 12tools/test_recognition.py:627: DeprecationWarning: getsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use getbbox or getlength instead. tw = font.getsize(text)[0] 环境安装12345678# .conda install paddlepaddle-gpu==2.2.0rc0 cudatoolkit=11.2 -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/Paddle/ -c conda-forge# insightfacepip install -r requirements.txt -i https://mirror.baidu.com/pypi/simple# insightface/recongition/arcface_paddle/pip install -r requirement.txt -i https://mirror.baidu.com/pypi/simple# insightface-paddlepip install insightface-paddle -i https://mirror.baidu.com/pypi/simple 获取数据集https://github.com/deepinsight/insightface/tree/master/recognition/_datasets_ MS1M_v2: MS1M-ArcFace MS1M_v3: MS1M-RetinaFace 从 MXNet 格式数据集抽取图像 1python tools/mx_recordio_2_images.py --root_dir ms1m-retinaface-t1/ --output_dir MS1M_v3/ 数据抽取完成后，格式如下 12345678910MS1M_v3|_ images| |_ 00000001.jpg| |_ ...| |_ 05179510.jpg|_ label.txt|_ agedb_30.bin|_ cfp_ff.bin|_ cfp_fp.bin|_ lfw.bin 标签数据格式如下 1234# 图像路径与标签的分隔符: &quot;\\t&quot;# 以下是 label.txt 每行的格式images/00000001.jpg 0... 模型训练使用双卡 1export CUDA_VISIBLE_DEVICES=0,1 训练脚本scripts/train_static.sh 12# 降级scipy,scipy版本过高会报错pip install scipy==1.7.1 -i https://mirror.baidu.com/pypi/simple 训练静态模型 123456789101112131415161718192021222324252627python -m paddle.distributed.launch --gpus=1 tools/train.py \\ --config_file configs/ms1mv3_r50.py \\ --is_static True \\ --backbone FresResNet50 \\ --classifier LargeScaleClassifier \\ --embedding_size 512 \\ --model_parallel True \\ --dropout 0.0 \\ --sample_ratio 0.1 \\ --loss ArcFace \\ --batch_size 64 \\ --dataset MS1M_v3 \\ --num_classes 93431 \\ --data_dir MS1M_v3/ \\ --label_file MS1M_v3/label.txt \\ --is_bin False \\ --log_interval_step 100 \\ --validation_interval_step 2000 \\ --fp16 True \\ --use_dynamic_loss_scaling True \\ --init_loss_scaling 27648.0 \\ --num_workers 8 \\ --train_unit 'epoch' \\ --warmup_num 0 \\ --train_num 25 \\ --decay_boundaries \"10,16,22\" \\ --output MS1M_v3_arcface_static_0.1 3090单卡容易爆显存 batch_size可由128调整至64，或开启多卡训练，需ncll 模型评价sh scripts/validation_static.sh 12345678python tools/validation.py \\ --is_static True \\ --backbone FresResNet50 \\ --embedding_size 512 \\ --checkpoint_dir MS1M_v3_arcface_static_0.1/FresResNet50/24 \\ --data_dir MS1M_v3/ \\ --val_targets lfw,cfp_fp,agedb_30 \\ --batch_size 64 模型导出sh scripts/export_static.sh 1234567python tools/export.py \\ --is_static True \\ --export_type paddle \\ --backbone FresResNet50 \\ --embedding_size 512 \\ --checkpoint_dir MS1M_v3_arcface_static_0.1/FresResNet50/24 \\ --output_dir MS1M_v3_arcface_static_0.1/FresResNet50/exported_model 模型推理sh scripts/inference.sh 12345python tools/inference.py \\ --export_type paddle \\ --model_file MS1M_v3_arcface_static_0.1/FresResNet50/exported_model/FresResNet50.pdmodel \\ --params_file MS1M_v3_arcface_static_0.1/FresResNet50/exported_model/FresResNet50.pdiparams \\ --image_path MS1M_v3/images/00000001.jpg 构建人像索引 建立图像文件夹 123456789101112fridends|_ gallery| |_ Chandler| |_ Chandler01.jpg| |_ ...| |_ Chandler50.jpg| |_ ...| |_ Ross| |_ Ross01.jpg| |_ ...| |_ Ross50.jpg|_ label.txt 建立索引文件label.txt 123456789101112131415161718./Chandler/Chandler00037.jpg Chandler./Chandler/Chandler00021.png Chandler./Chandler/Chandler00040.jpg Chandler./Chandler/Chandler00041.jpg Chandler./Chandler/Chandler00004.png Chandler./Chandler/Chandler00034.png Chandler./Chandler/Chandler00008.png Chandler..../Ross/Ross00016.jpg Ross./Ross/Ross00022.jpg Ross./Ross/Ross00019.jpg Ross./Ross/Ross00024.jpg Ross./Ross/Ross00001.jpg Ross./Ross/Ross00039.jpg Ross./Ross/Ross00038.jpg Ross./Ross/Ross00017.jpg Ross./Ross/Ross00034.jpg Ross./Ross/Ross00002.png Ross 构建索引 1insightfacepaddle --build_index ./demo/friends/index.bin --img_dir ./demo/friends/gallery --label ./demo/friends/gallery/label.txt 检测图片1python tools/test_recognition.py --det --rec --index=./demo/friends/index.bin --input=./test/测试2.jpg --output=./output 预测图片1python tools/test_recognition.py --det --rec --index=./demo/friends/index.bin --input=./test/测试2.jpg --output=./output 预测视频1python tools/test_recognition.py --det --rec --index=./demo/friends/index.bin --input=./test/mp4v.mp4 --output=./output python脚本脚本封装位置：arcface_paddle/python 构建索引 1234567891011import osimport insightface_paddle as faceimport loggingparser = face.parser()args = parser.parse_args()args.build_index = \"./demo/friends/index.bin\"args.img_dir = \"./demo/friends/gallery\"args.label = \"./demo/friends/gallery/label.txt\"predictor = face.InsightFace(args)predictor.build_index() 视频 123456789101112131415161718192021import osimport insightface_paddle as faceimport loggingPR = os.path.dirname(os.path.abspath(__file__))logging.basicConfig(level=logging.INFO)parser = face.parser()args = parser.parse_args()args.det = Trueargs.rec = Trueargs.index = os.path.join(PR, \"demo/friends/index.bin\")args.output = os.path.join(PR, \"output\")input_path = os.path.join(PR, \"test/MP4V.mp4\")predictor = face.InsightFace(args)res = predictor.predict(input_path, print_info=True)for _ in res: print(_.get('labels'))","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}],"tags":[{"name":"paddlepaddle","slug":"paddlepaddle","permalink":"http://yoursite.com/tags/paddlepaddle/"}]},{"title":"paddleDetection前置","slug":"paddleDetection前置","date":"2023-08-07T08:20:53.000Z","updated":"2024-01-25T08:50:19.855Z","comments":true,"path":"2023/08/07/paddleDetection前置/","link":"","permalink":"http://yoursite.com/2023/08/07/paddleDetection前置/","excerpt":"环境GPU NVIDIA 3090*2 显卡驱动 515.43.04 CUDA版本 11.7 CUDAtoolkit (cuda_11.7.0_515.43.04_linux) cuDNN (v8.4.1) paddlepaddle 多卡训练需要NCLL支持 (ncll v2.12.12 cuda11.7) paddlepaddle版本 v2.4.2 paddleDetection版本 v2.6.0 python环境 CentOS7.9 anaconda3 python3.8 普通视频处理h264格式视频被opencv解析帧率超过65535报错 源码： ./deploy/pipeline/pipeline.py predict_video 123out_path = os.path.join(self.output_dir, video_out_name + \".mp4\")fourcc = cv2.VideoWriter_fourcc(* 'mp4v')writer = cv2.VideoWriter(out_path, fourcc, fps, (width, height)) 输入：cv2.VideoCapture() 输出：cv2.VideoWriter(） 本GPU模式下对时长5分钟的行人检测视频处理时间约20分钟（高精度模型），视频体积增大13倍（100M-&gt;1.3G）","text":"环境GPU NVIDIA 3090*2 显卡驱动 515.43.04 CUDA版本 11.7 CUDAtoolkit (cuda_11.7.0_515.43.04_linux) cuDNN (v8.4.1) paddlepaddle 多卡训练需要NCLL支持 (ncll v2.12.12 cuda11.7) paddlepaddle版本 v2.4.2 paddleDetection版本 v2.6.0 python环境 CentOS7.9 anaconda3 python3.8 普通视频处理h264格式视频被opencv解析帧率超过65535报错 源码： ./deploy/pipeline/pipeline.py predict_video 123out_path = os.path.join(self.output_dir, video_out_name + \".mp4\")fourcc = cv2.VideoWriter_fourcc(* 'mp4v')writer = cv2.VideoWriter(out_path, fourcc, fps, (width, height)) 输入：cv2.VideoCapture() 输出：cv2.VideoWriter(） 本GPU模式下对时长5分钟的行人检测视频处理时间约20分钟（高精度模型），视频体积增大13倍（100M-&gt;1.3G） 视频流问题 h264格式存在问题（需要重新编译opencv），实时帧数过高（&gt;65535）cv2.VideoWriter报错，无法保存视频 问题原因：libx264基于GPL，ffmpeg编码器要使用libx264，必须--enable-gpl，而opencv使用的是MIT许可 如果视频流传输加载较慢，获取不到视频报错(修改pipeline.py predict_video) 1234567891011121314151617class FrameQueueEmptyException(Exception): \"\"\"An Exception for check frame queue\"\"\" def __init__(self, time): self.time = time def __str__(self): print(f\"&#123;self.time&#125;秒无法获取frame队列，请检查摄像头是否正常运转\")for i in range(1, 11): if framequeue.empty(): time.sleep(1) print(f\"Couldn't catch framequeue for &#123;i&#125; seconds\") else: break if i == 10: raise FrameQueueEmptyException 摄像头 &amp; 应用以liveCMS应用为例（国标设备见文档：LiveGBS国标GB-T28181视频流媒体平台） 每26天需更新一次 liveCMS应用安装在192.168.9.165机器 国标设备服务端口：192.168.9.165:10005账号：admin，密码：A12345678 近端设备地址：192.168.9.207 设备主板ip：192.168.177.177账户：root，密码：admin 摄像头设备地址：192.168.1.131、192.168.1.133 摄像头推流摄像头推流api：http://&lt;livecms_host&gt;:&lt;ip&gt;/api/v1/stream/list 12345678910111213141516171819202122232425262728293031323334353637383940414243&#123; \"StreamCount\": 1, \"Streams\": [ &#123; \"AudioEnable\": false, \"CDN\": \"\", \"CascadeSize\": 0, \"ChannelID\": \"34020000001320000131\", \"ChannelName\": \"192.168.1.131\", \"CloudRecord\": false, \"DeviceID\": \"34020000002000000207\", \"Duration\": 20520, \"FLV\": \"http://192.168.9.165:10005/sms/34020000002020000001/flv/hls/34020000002000000207_34020000001320000131.flv\", \"HLS\": \"http://192.168.9.165:10005/sms/34020000002020000001/hls/34020000002000000207_34020000001320000131/live.m3u8\", \"InBitRate\": 914, \"InBytes\": 2196788210, \"NumOutputs\": 1, \"Ondemand\": true, \"OutBytes\": 2308731853, \"RTMP\": \"rtmp://192.168.9.165:11935/hls/34020000002000000207_34020000001320000131\", \"RTPCount\": 2049812, \"RTPLostCount\": 3526, \"RTPLostRate\": 0.6256656017039404, \"RTSP\": \"rtsp://192.168.9.165:554/34020000002000000207_34020000001320000131\", \"RecordStartAt\": \"\", \"RelaySize\": 0, \"SMSID\": \"34020000002020000001\", \"SnapURL\": \"/sms/34020000002020000001/snap/34020000002000000207/34020000001320000131.jpg?t=1690332332290541800\", \"SourceAudioCodecName\": \"aac\", \"SourceAudioSampleRate\": 8000, \"SourceVideoCodecName\": \"h264\", \"SourceVideoFrameRate\": 30, \"SourceVideoHeight\": 720, \"SourceVideoWidth\": 1280, \"StartAt\": \"2023-07-26 08:45:30\", \"StreamID\": \"stream:34020000002000000207:34020000001320000131\", \"Transport\": \"UDP\", \"VideoFrameCount\": 605632, \"WEBRTC\": \"webrtc://192.168.9.165:10005/sms/34020000002020000001/rtc/34020000002000000207_34020000001320000131\", \"WS_FLV\": \"ws://192.168.9.165:10005/sms/34020000002020000001/ws-flv/hls/34020000002000000207_34020000001320000131.flv\" &#125; ]&#125; 执行paddleDetection –rtst指令1python deploy/pipeline/pipeline.py --config deploy/pipeline/config/examples/infer_cfg_human_attr.yml --rtsp http://192.168.9.165:10005/sms/34020000002020000001/flv/hls/34020000002000000207_34020000001320000131.flv --device=gpu 解决rtsp视频流中断问题（样例）pipline.py - class PipePredictor 1self.rtsp = args.rtsp pipline.py - predict_video 12345678910111213141516171819202122232425262728293031323334353637383940414243while True: flag = False if not framequeue.empty(): ... else: if self.rtsp: count = 1 capture = cv2.VideoCapture(video_file) try: fps = int(capture.get(cv2.CAP_PROP_FPS)) except OverflowError: fps = 20 frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT)) print(\"restart camera fps: %d, frame_count: %d\" % (fps, frame_count)) framequeue = queue.Queue(10) thread = threading.Thread( target=self.capturevideo, args=(capture, framequeue, execute_time)) thread.start() time.sleep(1) while True: if framequeue.empty(): time.sleep(1) print(f\"Couldn't catch framequeue for &#123;count&#125; seconds\") count += 1 if count == 11: flag = True break else: break else: flag = True if self.dbconfig: if self.dbconfig.rtsp: frame_thread_ = threading.Thread( target=self.video_composition, args=(self.frame_dir, self.out_path, video_fps) ) if not os.listdir(os.path.dirname(self.out_path)): if not self.frame_flag: frame_thread_.start() if flag: break 使用Python脚本pipeline/pipeline.py在对应config文件中将MOT的tracker_config修改为绝对路径 12345MOT: model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip tracker_config: /exp/work/video/PaddleDetection/deploy/pipeline/config/tracker_config.yml batch_size: 1 enable: True 修改pipeline.py启动脚本 123456789101112131415if __name__ == '__main__': paddle.enable_static() # parse params from command parser = argsparser() FLAGS = parser.parse_args() FLAGS.device = 'gpu' FLAGS.config = '/exp/work/video/PaddleDetection/deploy/pipeline/config/examples/infer_cfg_human_attr.yml' FLAGS.video_file = '/exp/work/video/PaddleDetection/demo_input/t2.mp4' FLAGS.output_dir = '/exp/work/video/PaddleDetection/demo_output/' FLAGS.device = FLAGS.device.upper() assert FLAGS.device in ['CPU', 'GPU', 'XPU', 'NPU' ], \"device should be CPU, GPU, XPU or NPU\" main() 修改cfg_utils.py argsparser；parse_args，修改和注释掉config作为必要条件 123456def parse_args(self, argv=None): args = super(ArgsParser, self).parse_args(argv) # assert args.config is not None, \\ # \"Please specify --config=configure_file_path.\" args.opt = self._parse_opt(args.opt) return args 123456parser.add_argument( \"--config\", type=str, default=None, help=(\"Path of configure\"), required=False) # True ——&gt; False 启动脚本： 1python pipeline.py 或将其封装为接口 python/infer.py导出预测模型 123456789101112131415# 导出YOLOv3检测模型python tools/export_model.py -c configs/yolov3/yolov3_darknet53_270e_coco.yml --output_dir=./inference_model \\ -o weights=https://paddledet.bj.bcebos.com/models/yolov3_darknet53_270e_coco.pdparams# 导出HigherHRNet(bottom-up)关键点检测模型python tools/export_model.py -c configs/keypoint/higherhrnet/higherhrnet_hrnet_w32_512.yml -o weights=https://paddledet.bj.bcebos.com/models/keypoint/higherhrnet_hrnet_w32_512.pdparams# 导出HRNet(top-down)关键点检测模型python tools/export_model.py -c configs/keypoint/hrnet/hrnet_w32_384x288.yml -o weights=https://paddledet.bj.bcebos.com/models/keypoint/hrnet_w32_384x288.pdparams# 导出FairMOT多目标跟踪模型python tools/export_model.py -c configs/mot/fairmot/fairmot_dla34_30e_1088x608.yml -o weights=https://paddledet.bj.bcebos.com/models/mot/fairmot_dla34_30e_1088x608.pdparams# 导出ByteTrack多目标跟踪模型(相当于只导出检测器)python tools/export_model.py -c configs/mot/bytetrack/detector/ppyoloe_crn_l_36e_640x640_mot17half.yml -o weights=https://paddledet.bj.bcebos.com/models/mot/ppyoloe_crn_l_36e_640x640_mot17half.pdparams 预测 修改deploy/python/infer.py启动脚本 12345678910111213141516171819if __name__ == '__main__': paddle.enable_static() parser = argsparser() FLAGS = parser.parse_args() FLAGS.device = 'gpu' FLAGS.video_file = '/exp/work/video/PaddleDetection/demo_input/t2.mp4' FLAGS.output_dir = '/exp/work/video/PaddleDetection/demo_output/' FLAGS.model_dir = '/exp/work/video/PaddleDetection/inference_model/yolov3_darknet53_270e_coco' print_arguments(FLAGS) FLAGS.device = FLAGS.device.upper() assert FLAGS.device in ['CPU', 'GPU', 'XPU', 'NPU' ], \"device should be CPU, GPU, XPU or NPU\" assert not FLAGS.use_gpu, \"use_gpu has been deprecated, please use --device\" assert not ( FLAGS.enable_mkldnn == False and FLAGS.enable_mkldnn_bfloat16 == True ), 'To enable mkldnn bfloat, please turn on both enable_mkldnn and enable_mkldnn_bfloat16' main() pp-human行人检测模块，对应配置文件位置：deploy/pipeline/config/examples/infer_cfg_pphuman.yml 通过命令启动检测脚本 1python deploy/pipeline/pipeline.py --config deploy/pipeline/config/examples/infer_cfg_human_attr.yml --device=gpu --video_file=demo_input/t1.mp4 --output_dir=demo_output/ pp-vehicle车辆检测模块，对应配置文件位置：deploy/pipeline/config/examples/infer_cfg_ppvehicle.yml 通过命令启动检测脚本 1python deploy/pipeline/pipeline.py --config deploy/pipeline/config/infer_cfg_ppvehicle.yml --video_file=demo_input/car.mp4 --device=gpu --output_dir=demo_output","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}],"tags":[{"name":"paddlepaddle","slug":"paddlepaddle","permalink":"http://yoursite.com/tags/paddlepaddle/"},{"name":"opencv","slug":"opencv","permalink":"http://yoursite.com/tags/opencv/"}]},{"title":"搭建nebula-CentOS集群（极速版）","slug":"搭建nebula-CentOS集群（极速版）","date":"2023-07-10T01:05:06.000Z","updated":"2023-08-10T01:38:30.425Z","comments":true,"path":"2023/07/10/搭建nebula-CentOS集群（极速版）/","link":"","permalink":"http://yoursite.com/2023/07/10/搭建nebula-CentOS集群（极速版）/","excerpt":"环境准备 系统 CentOS7.9 机器 三台，分别为nebula01、nebula02、nebula03、 安装位置 /usr/local nebula版本 2.6.1 nebula-graph-studio版本 3.2.3 快速开始 下载tar.gz文件 123cd /usr/localwget https://oss-cdn.nebula-graph.com.cn/package/2.6.1/nebula-graph-2.6.1.el7.x86_64.tar.gzwget https://oss-cdn.nebula-graph.com.cn/nebula-graph-studio/3.2.3/nebula-graph-studio-3.2.3.x86_64.tar.gz 解压缩并重命名文件 12tar -zxvf nebula-graph-2.6.1.el7.x86_64.tar.gz &amp;&amp; mv nebula-graph-2.6.1.el7.x86_64 nebulatar -zxvf nebula-graph-studio-3.2.3.x86_64.tar.gz &amp;&amp; mv nebula-graph-studio-3.2.3.x86_64 nebula-graph-studio 复制配置文件 1234cd nebula/etccp nebula-graphd.conf.default nebula-graphd.confcp nebula-metad.conf.default nebula-metad.confcp nebula-storaged.conf.default nebula-storaged.conf","text":"环境准备 系统 CentOS7.9 机器 三台，分别为nebula01、nebula02、nebula03、 安装位置 /usr/local nebula版本 2.6.1 nebula-graph-studio版本 3.2.3 快速开始 下载tar.gz文件 123cd /usr/localwget https://oss-cdn.nebula-graph.com.cn/package/2.6.1/nebula-graph-2.6.1.el7.x86_64.tar.gzwget https://oss-cdn.nebula-graph.com.cn/nebula-graph-studio/3.2.3/nebula-graph-studio-3.2.3.x86_64.tar.gz 解压缩并重命名文件 12tar -zxvf nebula-graph-2.6.1.el7.x86_64.tar.gz &amp;&amp; mv nebula-graph-2.6.1.el7.x86_64 nebulatar -zxvf nebula-graph-studio-3.2.3.x86_64.tar.gz &amp;&amp; mv nebula-graph-studio-3.2.3.x86_64 nebula-graph-studio 复制配置文件 1234cd nebula/etccp nebula-graphd.conf.default nebula-graphd.confcp nebula-metad.conf.default nebula-metad.confcp nebula-storaged.conf.default nebula-storaged.conf 修改配置文件 1vim nebula-graphd.conf 1234567891011...# The directory to host logging files--log_dir=/data/nebula/logs...# Comma separated Meta Server Addresses--meta_server_addrs=nebula01:9559,nebula02:9559,nebula03:9559# Local IP used to identify the nebula-graphd process.# Change it to an address other than loopback if the service is distributed or# will be accessed remotely.--local_ip=nebula01... 1vim nebula-metad.conf 1234567891011121314...# The directory to host logging files--log_dir=/data/nebula/logs...# Comma separated Meta Server addresses--meta_server_addrs=nebula01:9559,nebula02:9559,nebula03:9559# Local IP used to identify the nebula-metad process.# Change it to an address other than loopback if the service is distributed or# will be accessed remotely.--local_ip=nebula01...# Root data path, here should be only single path for metad--data_path=/data/nebula/meta... 1vim nebula-storaged.conf 123456789101112131415...# The directory to host logging files--log_dir=/data/nebula/logs...# Comma separated Meta server addresses--meta_server_addrs=nebula01:9559,nebula02:9559,nebula03:9559# Local IP used to identify the nebula-storaged process.# Change it to an address other than loopback if the service is distributed or# will be accessed remotely.--local_ip=nebula01...# Root data path. Split by comma. e.g. --data_path=/disk1/path1/,/disk2/path2/# One path per Rocksdb instance.--data_path=/data/nebula/storage... 分发nebula文件，并修改对应的--local_ip 123cd ../..scp -r nebula nebula02:`pwd`/scp -r nebula nebula03:`pwd`/ 启动集群 注：因为meta机器选择了nebula01、02、03共三台机器，启动时需要将三台同时执行命令拉起 1nebula/scripts/nebula.service start all 检查集群状态 1nebula/scripts/nebula.service status all 123[INFO] nebula-metad(de03025): Running as 24680, Listening on 9559 [INFO] nebula-graphd(de03025): Running as 25453, Listening on 9669 [INFO] nebula-storaged(de03025): Running as 24787, Listening on 9779 此时nebula文件夹下生成cluster.id文件，集群成功启动 启动nebula-graph-studio 12cd nebula-graph-studionohup ./server &amp; 注：同样可用于集群数据恢复（/data下的数据已备份）","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"}],"tags":[{"name":"database","slug":"database","permalink":"http://yoursite.com/tags/database/"},{"name":"nebua","slug":"nebua","permalink":"http://yoursite.com/tags/nebua/"}]},{"title":"Nebula-Spark和图算法","slug":"Nebula-Spark","date":"2023-03-24T06:34:47.000Z","updated":"2024-01-08T07:52:12.236Z","comments":true,"path":"2023/03/24/Nebula-Spark/","link":"","permalink":"http://yoursite.com/2023/03/24/Nebula-Spark/","excerpt":"Nebula Spark Connector下载地址&amp;官方文档：【https://github.com/vesoft-inc/nebula-spark-connector】 环境· nebula：2.6.1· hadoop：2.7· spark：2.4.7· pyspark：2.4.7· python：3.7.16· nebula-spark-connector：2.6.1 编译打包nebula-spark-connector12$ cd nebula-spark-connector-2.6.1/nebula-spark-connector$ mvn clean package -Dmaven.test.skip=true -Dgpg.skip -Dmaven.javadoc.skip=true 成功后在nebula-spark-connector/target/ 目录下得到 nebula-spark-connector-2.6.1.jar文件 123456789101112(base) [root@root target]# lltotal 106792drwxr-xr-x 3 root root 17 Mar 11 14:14 classes-rw-r--r-- 1 root root 1 Mar 11 14:14 classes.-497386701.timestamp-rw-r--r-- 1 root root 1 Mar 11 14:14 classes.timestamp-rw-r--r-- 1 root root 30701 Mar 11 14:15 jacoco.execdrwxr-xr-x 2 root root 28 Mar 11 14:15 maven-archiver-rw-r--r-- 1 root root 108375457 Mar 11 14:16 nebula-spark-connector-2.6.1.jar-rw-r--r-- 1 root root 583482 Mar 11 14:16 nebula-spark-connector-2.6.1-javadoc.jar-rw-r--r-- 1 root root 36358 Mar 11 14:16 nebula-spark-connector-2.6.1-sources.jar-rw-r--r-- 1 root root 315392 Mar 11 14:15 original-nebula-spark-connector-2.6.1.jardrwxr-xr-x 4 root root 37 Mar 11 14:15 site PySpark 读取 NebulaGraph 数据从 metaAddress 为 &quot;metad0:9559&quot; 的 Nebula Graph 中读取整个 tag 下的数据为一个 dataframe： 12345678df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() 然后可以像这样 show 这个 dataframe： 12345678&gt;&gt;&gt; df.show(n=2)+---------+--------------+---+|_vertexId| name|age|+---------+--------------+---+|player105| Danny Green| 31||player109|Tiago Splitter| 34|+---------+--------------+---+only showing top 2 rows","text":"Nebula Spark Connector下载地址&amp;官方文档：【https://github.com/vesoft-inc/nebula-spark-connector】 环境· nebula：2.6.1· hadoop：2.7· spark：2.4.7· pyspark：2.4.7· python：3.7.16· nebula-spark-connector：2.6.1 编译打包nebula-spark-connector12$ cd nebula-spark-connector-2.6.1/nebula-spark-connector$ mvn clean package -Dmaven.test.skip=true -Dgpg.skip -Dmaven.javadoc.skip=true 成功后在nebula-spark-connector/target/ 目录下得到 nebula-spark-connector-2.6.1.jar文件 123456789101112(base) [root@root target]# lltotal 106792drwxr-xr-x 3 root root 17 Mar 11 14:14 classes-rw-r--r-- 1 root root 1 Mar 11 14:14 classes.-497386701.timestamp-rw-r--r-- 1 root root 1 Mar 11 14:14 classes.timestamp-rw-r--r-- 1 root root 30701 Mar 11 14:15 jacoco.execdrwxr-xr-x 2 root root 28 Mar 11 14:15 maven-archiver-rw-r--r-- 1 root root 108375457 Mar 11 14:16 nebula-spark-connector-2.6.1.jar-rw-r--r-- 1 root root 583482 Mar 11 14:16 nebula-spark-connector-2.6.1-javadoc.jar-rw-r--r-- 1 root root 36358 Mar 11 14:16 nebula-spark-connector-2.6.1-sources.jar-rw-r--r-- 1 root root 315392 Mar 11 14:15 original-nebula-spark-connector-2.6.1.jardrwxr-xr-x 4 root root 37 Mar 11 14:15 site PySpark 读取 NebulaGraph 数据从 metaAddress 为 &quot;metad0:9559&quot; 的 Nebula Graph 中读取整个 tag 下的数据为一个 dataframe： 12345678df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() 然后可以像这样 show 这个 dataframe： 12345678&gt;&gt;&gt; df.show(n=2)+---------+--------------+---+|_vertexId| name|age|+---------+--------------+---+|player105| Danny Green| 31||player109|Tiago Splitter| 34|+---------+--------------+---+only showing top 2 rows PySpark 写 NebulaGraph 数据默认不指定的情况下 writeMode 是 insert： 写入点1234567891011df.write.format(\"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"vidPolicy\", \"\").option( \"vertexField\", \"_vertexId\").option( \"batch\", 1).option( \"metaAddress\", \"metad0:9559\").option( \"graphAddress\", \"graphd1:9669\").option( \"passwd\", \"nebula\").option( \"user\", \"root\").save() 删除点如果想指定 delete 或者 update 的非默认写入模式，增加 writeMode 的配置，比如 delete 的例子： 123456789101112df.write.format(\"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"vidPolicy\", \"\").option( \"vertexField\", \"_vertexId\").option( \"batch\", 1).option( \"metaAddress\", \"metad0:9559\").option( \"graphAddress\", \"graphd1:9669\").option( \"passwd\", \"nebula\").option( \"writeMode\", \"delete\").option( \"user\", \"root\").save() 写入边12345678910111213141516df.write.format(\"com.vesoft.nebula.connector.NebulaDataSource\")\\ .mode(\"overwrite\")\\ .option(\"srcPolicy\", \"\")\\ .option(\"dstPolicy\", \"\")\\ .option(\"metaAddress\", \"metad0:9559\")\\ .option(\"graphAddress\", \"graphd:9669\")\\ .option(\"user\", \"root\")\\ .option(\"passwd\", \"nebula\")\\ .option(\"type\", \"edge\")\\ .option(\"spaceName\", \"basketballplayer\")\\ .option(\"label\", \"server\")\\ .option(\"srcVertexField\", \"srcid\")\\ .option(\"dstVertexField\", \"dstid\")\\ .option(\"rankField\", \"\")\\ .option(\"batch\", 100)\\ .option(\"writeMode\", \"insert\").save() # delete to delete edge, update to update edge 删除边12345678910111213141516df.write.format(\"com.vesoft.nebula.connector.NebulaDataSource\")\\ .mode(\"overwrite\")\\ .option(\"srcPolicy\", \"\")\\ .option(\"dstPolicy\", \"\")\\ .option(\"metaAddress\", \"metad0:9559\")\\ .option(\"graphAddress\", \"graphd:9669\")\\ .option(\"user\", \"root\")\\ .option(\"passwd\", \"nebula\")\\ .option(\"type\", \"edge\")\\ .option(\"spaceName\", \"basketballplayer\")\\ .option(\"label\", \"server\")\\ .option(\"srcVertexField\", \"srcid\")\\ .option(\"dstVertexField\", \"dstid\")\\ .option(\"randkField\", \"\")\\ .option(\"batch\", 100)\\ .option(\"writeMode\", \"delete\").save() # delete to delete edge, update to update edge 关于 PySpark 读写的 option对于其他的 option，比如删除点的时候的 withDeleteEdge 可以参考 [nebula/connector/NebulaOptions.scala ](https://github.com/vesoft-inc/nebula-spark-connector/blob/master/nebula-spark-connector/src/main/scala/com/vesoft/nebula/connector/NebulaOptions.scala) 的字符串配置定义，我们可以看到它的字符串定义字段是 deleteEdge ： 12345678910111213141516/** write config */val RATE_LIMIT: String = \"rateLimit\"val VID_POLICY: String = \"vidPolicy\"val SRC_POLICY: String = \"srcPolicy\"val DST_POLICY: String = \"dstPolicy\"val VERTEX_FIELD = \"vertexField\"val SRC_VERTEX_FIELD = \"srcVertexField\"val DST_VERTEX_FIELD = \"dstVertexField\"val RANK_FIELD = \"randkField\"val BATCH: String = \"batch\"val VID_AS_PROP: String = \"vidAsProp\"val SRC_AS_PROP: String = \"srcAsProp\"val DST_AS_PROP: String = \"dstAsProp\"val RANK_AS_PROP: String = \"rankAsProp\"val WRITE_MODE: String = \"writeMode\"val DELETE_EDGE: String = \"deleteEdge\" PySpark 调用 Nebula Spark ConnectorPycharm12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# 导入SparkSession、findspark自动获取$&#123;SPARK_HOME&#125;，定义虚拟环境import osimport findsparkfrom pyspark.sql import SparkSessionos.environ[\"HADOOP_CONF_DIR\"] = \"/exp/server/hadoop-2.7.7/etc/hadoop\"os.environ[\"YARN_CONF_DIR\"] = \"/exp/server/hadoop-2.7.7/etc/hadoop\"os.environ[\"SPARK_HOME\"] = \"/exp/server/spark-2.4.7-bin-hadoop2.7\"os.environ[\"PYSPARK_PYTHON\"] = \"/root/anaconda3/envs/pyspark38/bin/python\"os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/root/anaconda3/envs/pyspark38/bin/python\"findspark.init()# 构建SparkSession对象，导入target下的jar包spark = SparkSession.builder.config( \"spark.jars\", \"/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar\").config( \"spark.driver.extraClassPath\", \"/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar\").appName( \"nebula-connector\").getOrCreate()# 读取nebula-graph数据# read vertexdf_tag = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"sep\", \"\\t\").option( \"type\", \"vertex\").option( \"spaceName\", \"space\").option( \"label\", \"tag\").option( \"returnCols\", \"\").option( \"metaAddress\", \"metahost:9559\").option( \"partitionNumber\", 1).load()# read edgedf_edge = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"sep\", \"\\t\").option( \"type\", \"edge\").option( \"spaceName\", \"space\").option( \"label\", \"edge\").option( \"returnCols\", \"\").option( \"metaAddress\", \"metahost:9559\").option( \"partitionNumber\", 1).load()# 写回nebula-graph# write vertexdf_tag.write.format(\"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"space\").option( \"label\", \"tag\").option( \"vidPolicy\", \"\").option( \"vertexField\", \"_vertexId\").option( \"batch\", 1).option( \"metaAddress\", \"metahost:9559\").option( \"graphAddress\", \"graphhost:9669\").option( \"passwd\", \"nebula\").option( \"writeMode\", \"update\").option( \"user\", \"root\").save()df_tag.show()df_edge.show() Spark-submit1234$&#123;SPARK_HOME&#125;/bin/spark-submit --master local[*] \\--deploy-mode client \\--jars file:/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar \\/exp/work/pyspark/nebula/nebula_reader.py NebulaAlgorithm下载地址&amp;官方文档：【&lt;https://github.com/vesoft-inc/nebula-algorithm】 环境· nebula：2.6.1· hadoop：2.7· spark：2.4.7· pyspark：2.4.7· python：3.7.16· nebula-spark-connector：2.6.1· nebula-algorithm：2.6.1 编译打包nebula-spark-connector12$ cd nebula-algorithm-2.6.1/nebula-algorithm$ mvn clean package -Dgpg.skip -Dmaven.javadoc.skip=true -Dmaven.test.skip=true 成功后在nebula-algorithm/target/ 目录下得到 nebula-algorithm-2.6.1.jar文件 PySpark 调用 Nebula AlgorithmPycharm123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115import osimport findsparkfrom pyspark.sql import SparkSession, DataFramefrom pyspark.sql.functions import *from pyspark.sql.window import Windowfrom py4j.java_gateway import java_importos.environ[\"HADOOP_CONF_DIR\"] = \"/exp/server/hadoop-2.7.7/etc/hadoop\"os.environ[\"YARN_CONF_DIR\"] = \"/exp/server/hadoop-2.7.7/etc/hadoop\"os.environ[\"SPARK_HOME\"] = \"/exp/server/spark-2.4.7-bin-hadoop2.7\"os.environ[\"PYSPARK_PYTHON\"] = \"/root/anaconda3/envs/pyspark37/bin/python\"os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/root/anaconda3/envs/pyspark37/bin/python\"findspark.init()spark = SparkSession.builder.config( \"spark.jars\", \"/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar\").config( \"spark.driver.extraClassPath\", \"/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar\").config( \"spark.jars\", \"/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar\").config( \"spark.driver.extraClassPath\", \"/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar\").appName( \"nebula-connector\").getOrCreate()# spark = SparkSession.builder.appName(\"PageRankExample\").getOrCreate()jspark = spark._jsparkSession# import \"com.vesoft.nebula.algorithm.config.SparkConfig\"java_import(spark._jvm, \"com.vesoft.nebula.algorithm.config.SparkConfig\")# import \"com.vesoft.nebula.algorithm.config.PRConfig\"java_import(spark._jvm, \"com.vesoft.nebula.algorithm.config.PRConfig\")# import \"com.vesoft.nebula.algorithm.lib.PageRankAlgo\"java_import(spark._jvm, \"com.vesoft.nebula.algorithm.lib.PageRankAlgo\")# 将string类型vid转int类型viddef convert_string_id_to_long_id(df): df = df.drop(\"display_desc\").drop(\"creation_time\") src_id_df = df.select(\"_srcId\").withColumnRenamed(\"_srcId\", \"id\") dst_id_df = df.select(\"_dstId\").withColumnRenamed(\"_dstId\", \"id\") id_df = src_id_df.union(dst_id_df).distinct() encode_id = id_df.withColumn(\"encodedId\", dense_rank().over(Window.orderBy(\"id\"))) # encode_id.write.option(\"header\", True).csv(\"file:///tmp/encodeId.csv\") src_join_df = df.join(encode_id, df._srcId == encode_id.id) \\ .drop(\"_srcId\") \\ .drop(\"id\") \\ .withColumnRenamed(\"encodedId\", \"_srcId\") df_sv = df.join(encode_id, df._srcId == encode_id.id) \\ .drop(\"_srcId\") \\ .drop(\"_rank\") \\ .drop(\"_dstId\") \\ .withColumnRenamed(\"id\", \"src_vid\") \\ .withColumnRenamed(\"encodedId\", \"_srcId\") df_dv = src_join_df.join(encode_id, src_join_df._dstId == encode_id.id) \\ .drop(\"_dstId\") \\ .drop(\"_rank\") \\ .drop(\"degree\") \\ .withColumnRenamed(\"encodedId\", \"_dstId\") \\ .withColumnRenamed(\"_srcID\", \"_src\") \\ .withColumnRenamed(\"id\", \"dst_vid\") dst_join_df = src_join_df.join(encode_id, src_join_df._dstId == encode_id.id) \\ .drop(\"_dstId\") \\ .drop(\"_rank\") \\ .drop(\"degree\") \\ .withColumnRenamed(\"encodedId\", \"_dstId\").drop(\"id\") df_v = df_dv.join(df_sv, df_dv._src == df_sv._srcId).drop(\"_src\") return dst_join_df, df_vdf = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( # \"sep\", \"\\t\").option( \"type\", \"edge\").option( \"spaceName\", \"DWD_GRAPH_LY_V3_2023\").option( \"label\", \"edge_phone\").option( \"returnCols\", \"\").option( \"metaAddress\", \"192.168.100.45:9559\").option( \"partitionNumber\", 1).load()df.orderBy(\"creation_time\").show()df_int, df_v = convert_string_id_to_long_id(df)prConfig = spark._jvm.PRConfig(1, 0.8)prResult = spark._jvm.PageRankAlgo.apply(jspark, df_int._jdf, prConfig, False)df_v.show(20, False)prResult.show(20, False)# 将jdf转dfprResult = prResult.toDF()pagerank_df = DataFrame(prResult, spark)pagerank_df.sort(col(\"pagerank\").desc()).show(60, False)# TODO: PYSPARK (pyspark=2.4.7, python=3.7)# conda activate pyspark37# TODO: SPARK-SUBMIT# $&#123;SPARK_HOME&#125;/bin/spark-submit --master local[*] \\# --deploy-mode client \\# --driver-class-path file:/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar \\# --driver-class-path file:/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar \\# --jars file:/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar \\# --jars file:/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar \\# /exp/work/pyspark/nebula/pagerank.py Spark-submit1234567$&#123;SPARK_HOME&#125;/bin/spark-submit --master local[*] \\--deploy-mode client \\--driver-class-path file:/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar \\--driver-class-path file:/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar \\--jars file:/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar \\--jars file:/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar \\/exp/work/pyspark/nebula/pagerank.py Result123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158ssh://root@192.168.100.43:22/root/anaconda3/envs/pyspark37/bin/python -u /exp/work/pyspark/nebula/pagerank.pySLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/exp/server/spark-2.4.7-bin-hadoop2.7/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]23/03/24 14:42:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableSetting default log level to &quot;WARN&quot;.To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).+--------------------+--------------------+-----+-------------+------------+| _srcId| _dstId|_rank|creation_time|display_desc|+--------------------+--------------------+-----+-------------+------------+|dwd_accounttelegr...|dwd_phone66802018560| 0| 1678692201| 注册手机号||dwd_accounttelegr...|dwd_phone12366827389| 0| 1678692201| 注册手机号||dwd_accounttelegr...|dwd_phone86151162...| 0| 1678692201| 注册手机号||dwd_accounttelegr...|dwd_phone79653470394| 0| 1678692210| 注册手机号||dwd_accounttelegr...|dwd_phone99890918...| 0| 1678692210| 注册手机号||dwd_accounttelegr...|dwd_phone99890535...| 0| 1678692210| 注册手机号||dwd_accounttelegr...|dwd_phone85620578...| 0| 1678692210| 注册手机号||dwd_accounttelegr...|dwd_phone99899666...| 0| 1678692210| 注册手机号||dwd_accounttelegr...|dwd_phone88802607465| 0| 1678692219| 注册手机号||dwd_accounttelegr...|dwd_phone62831556...| 0| 1678692228| 注册手机号||dwd_accounttelegr...|dwd_phone95965067...| 0| 1678692228| 注册手机号||dwd_accounttelegr...|dwd_phone85595212324| 0| 1678692228| 注册手机号||dwd_accounttelegr...|dwd_phone86132873...| 0| 1678692279| 注册手机号||dwd_accounttelegr...|dwd_phone63948774...| 0| 1678692279| 注册手机号||dwd_accounttelegr...|dwd_phone85590645678| 0| 1678692279| 注册手机号||dwd_accounttelegr...|dwd_phone63948229...| 0| 1678692288| 注册手机号||dwd_accounttelegr...|dwd_phone99899913...| 0| 1678692288| 注册手机号||dwd_accounttelegr...|dwd_phone99890302...| 0| 1678692288| 注册手机号||dwd_accounttelegr...|dwd_phone99890986...| 0| 1678692288| 注册手机号||dwd_accounttelegr...|dwd_phone16728042478| 0| 1678692297| 注册手机号|+--------------------+--------------------+-----+-------------+------------+only showing top 20 rows23/03/24 14:42:39 WARN window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.23/03/24 14:42:39 WARN window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.[Stage 3:&gt; (0 + 1) / 1][Stage 4:&gt; (0 + 1) / 1]23/03/24 14:42:49 WARN storage.BlockManager: Block rdd_29_0 already exists on this machine; not re-adding it23/03/24 14:42:50 WARN window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.23/03/24 14:42:50 WARN window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.23/03/24 14:42:50 WARN window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.+----------------------+------+---------------------------------+------+|dst_vid |_dstId|src_vid |_srcId|+----------------------+------+---------------------------------+------+|dwd_phone8613951884225|18141 |dwd_accountctrip1187326021 |1 ||dwd_phone8618705166775|19486 |dwd_accountctrip18705166775 |2 ||dwd_phone8613006584273|17582 |dwd_accountctripM2295153681 |3 ||dwd_phone8615668271999|18526 |dwd_accountctripM2494074526 |4 ||dwd_phone8616653556969|18757 |dwd_accountctripM2682271769 |5 ||dwd_phone8613562835888|17938 |dwd_accountctripM3011519721 |6 ||dwd_phone8618506413343|19344 |dwd_accountctripM4761127290 |7 ||dwd_phone8618112957512|19178 |dwd_accountctripM4915141465 |8 ||dwd_phone8615119464037|18222 |dwd_accountctripM548327259 |9 ||dwd_phone8613123368388|17719 |dwd_accountctripM601536478 |10 ||dwd_phone8617605442050|18905 |dwd_accountctrip_WeChat2269232661|11 ||dwd_phone8613123368388|17719 |dwd_accountdidi101108284 |12 ||dwd_phone8616653556969|18757 |dwd_accountdidi1641492054156 |13 ||dwd_phone8615851895366|18609 |dwd_accountdidi1746082933402 |14 ||dwd_phone8617561929739|18889 |dwd_accountdidi17598416475664 |15 ||dwd_phone8615119464037|18222 |dwd_accountdidi25441524 |16 ||dwd_phone8613006584273|17582 |dwd_accountdidi2881218873540 |17 ||dwd_phone8613951884225|18141 |dwd_accountdidi3099925 |18 ||dwd_phone8618600764544|19397 |dwd_accountdidi486186360832 |19 ||dwd_phone8618705166775|19486 |dwd_accountdidi772722 |20 |+----------------------+------+---------------------------------+------+only showing top 20 rows+-----+------------------+|_id |pagerank |+-----+------------------+|19021|1.1105466561585526||9831 |0.8884373249268421||5354 |0.8884373249268421||4926 |0.8884373249268421||21377|1.1105466561585526||14609|1.1105466561585526||11852|1.1105466561585526||8390 |0.8884373249268421||10837|0.8884373249268421||4992 |0.8884373249268421||20894|1.1105466561585526||21780|1.1105466561585526||1813 |0.8884373249268421||9025 |0.8884373249268421||14554|1.1105466561585526||1780 |0.8884373249268421||16132|1.1105466561585526||22467|1.1105466561585526||2117 |0.8884373249268421||16321|1.1105466561585526|+-----+------------------+only showing top 20 rows+-----+------------------+|_id |pagerank |+-----+------------------+|18222|3.331639968475657 ||19178|2.8874213060122367||19486|2.8874213060122367||12765|1.9989839810853947||13332|1.9989839810853947||13505|1.9989839810853947||17582|1.9989839810853947||18889|1.776874649853684 ||19176|1.776874649853684 ||12470|1.776874649853684 ||19397|1.5547653186219736||18141|1.5547653186219736||11750|1.5547653186219736||18555|1.5547653186219736||13216|1.5547653186219736||18905|1.5547653186219736||17688|1.5547653186219736||19734|1.5547653186219736||12768|1.5547653186219736||18609|1.5547653186219736||18347|1.332655987390263 ||19473|1.332655987390263 ||12007|1.332655987390263 ||12100|1.332655987390263 ||16268|1.332655987390263 ||19298|1.332655987390263 ||23258|1.332655987390263 ||17938|1.332655987390263 ||20042|1.332655987390263 ||12639|1.332655987390263 ||13309|1.332655987390263 ||12739|1.332655987390263 ||18037|1.332655987390263 ||18757|1.332655987390263 ||18344|1.332655987390263 ||12461|1.332655987390263 ||20037|1.332655987390263 ||13754|1.332655987390263 ||18864|1.332655987390263 ||21713|1.332655987390263 ||16872|1.332655987390263 ||15710|1.332655987390263 ||12229|1.332655987390263 ||19183|1.332655987390263 ||17928|1.332655987390263 ||19118|1.332655987390263 ||20243|1.332655987390263 ||12223|1.332655987390263 ||23219|1.332655987390263 ||12529|1.332655987390263 ||20865|1.332655987390263 ||16538|1.332655987390263 ||13253|1.332655987390263 ||13329|1.332655987390263 ||13198|1.332655987390263 ||17719|1.332655987390263 ||14661|1.1105466561585526||18584|1.1105466561585526||12532|1.1105466561585526||20466|1.1105466561585526|+-----+------------------+only showing top 60 rows 封装并写回nebula-graph实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185import osimport findsparkfrom pyspark.sql import SparkSession, DataFramefrom pyspark.sql.functions import *from pyspark.sql.window import Windowfrom py4j.java_gateway import java_importfrom config import nebula_config# TODO: environment variablesos.environ[\"HADOOP_CONF_DIR\"] = \"/exp/server/hadoop-2.7.7/etc/hadoop\"os.environ[\"YARN_CONF_DIR\"] = \"/exp/server/hadoop-2.7.7/etc/hadoop\"os.environ[\"SPARK_HOME\"] = \"/exp/server/spark-2.4.7-bin-hadoop2.7\"os.environ[\"PYSPARK_PYTHON\"] = \"/root/anaconda3/envs/pyspark37/bin/python\"os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/root/anaconda3/envs/pyspark37/bin/python\"findspark.init()class PageRank: \"\"\" pagerank \"\"\" def __init__(self): self.df = None self.dfc = None self.tag = None self.id_df = None self.df_int = None self.encode_id = None self.src_id_df = None self.dst_id_df = None self.src_join_df = None self.dst_join_df = None # TODO: spark object with nebula-spark-connector &amp; nebula-algorithm self.spark = SparkSession.builder.appName( \"pagerank\")\\ .master( \"local[*]\")\\ .config( \"spark.jars\", \"/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar\")\\ .config( \"spark.driver.extraClassPath\", \"/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar\")\\ .config( \"spark.jars\", \"/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar\")\\ .config( \"spark.driver.extraClassPath\", \"/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar\")\\ .getOrCreate() # TODO: java import self.jspark = self.spark._jsparkSession # import \"com.vesoft.nebula.algorithm.config.SparkConfig\" java_import(self.spark._jvm, \"com.vesoft.nebula.algorithm.config.SparkConfig\") # import \"com.vesoft.nebula.algorithm.config.PRConfig\" java_import(self.spark._jvm, \"com.vesoft.nebula.algorithm.config.PRConfig\") # import \"com.vesoft.nebula.algorithm.lib.PageRankAlgo\" java_import(self.spark._jvm, \"com.vesoft.nebula.algorithm.lib.PageRankAlgo\") def set_tag(self, tag: list): self.tag = tag def exe(self): self.sdf_create() # TODO: covert fixed_string vid 2 long vid def convert(self, df): self.df = df.drop(\"creation_time\") self.src_id_df = self.df.select(\"_srcId\").withColumnRenamed(\"_srcId\", \"id\") self.dst_id_df = self.df.select(\"_dstId\").withColumnRenamed(\"_dstId\", \"id\") self.id_df = self.src_id_df.union(self.dst_id_df).distinct() self.encode_id = self.id_df.withColumn(\"encodedId\", dense_rank().over(Window.orderBy(\"id\"))) self.src_join_df = self.df.join(self.encode_id, self.df._srcId == self.encode_id.id) \\ .drop(\"_srcId\") \\ .drop(\"id\") \\ .withColumnRenamed(\"encodedId\", \"_srcId\") self.dst_join_df = self.src_join_df.join(self.encode_id, self.src_join_df._dstId == self.encode_id.id) \\ .drop(\"_dstId\") \\ .drop(\"_rank\") \\ .drop(\"degree\") \\ .withColumnRenamed(\"encodedId\", \"_dstId\").drop(\"id\") # self.dst_join_df.write.option(\"header\", True).csv(\"file:/exp/work/pyspark/nebula/pr_file/1.csv\") # TODO: sdf build by nebula_reader def sdf_create(self): for i in range(len(self.tag)): self.dfc = self.spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"edge\").option( \"spaceName\", nebula_config.get(\"space\")).option( \"label\", f\"&#123;self.tag[i]&#125;\").option( \"returnCols\", \"creation_time\").option( \"metaAddress\", nebula_config.get(\"metaAddress\")).option( \"partitionNumber\", nebula_config.get(\"partitionNumber\")).load() if i == 0: self.df = self.dfc else: self.df = self.df.unionByName(self.dfc) self.convert(self.df)# TODO: NEBULA-READERdef nebula_reader(sth, tag): dataframe = sth.spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", nebula_config.get(\"space\")).option( \"label\", f\"&#123;tag&#125;\").option( \"returnCols\", \"pagerank\").option( \"metaAddress\", nebula_config.get(\"metaAddress\")).option( \"partitionNumber\", nebula_config.get(\"partitionNumber\")).load().drop(\"pagerank\") return dataframe# TODO: UPDATE NEBULA-GRAPHdef nebula_writer(tag, dataframe): dataframe.write.format(\"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", nebula_config.get(\"space\")).option( \"label\", f\"&#123;tag&#125;\").option( \"vidPolicy\", \"\").option( \"vertexField\", \"_vertexId\").option( \"writeMode\", \"update\").option( \"batch\", nebula_config.get(\"batch\")).option( \"metaAddress\", nebula_config.get(\"metaAddress\")).option( \"graphAddress\", nebula_config.get(\"graphAddress\")).option( \"passwd\", nebula_config.get(\"passwd\")).option( \"user\", nebula_config.get(\"user\")).save()# TODO: process datadef process(tag: list, *args): # TODO: algorithm object obj = PageRank() obj.set_tag(tag) obj.exe() encode_id = obj.encode_id df_int = obj.dst_join_df df_spark = df_int # TODO: nebula-algorithm config = obj.spark._jvm.PRConfig(1, 0.8) result = obj.spark._jvm.PageRankAlgo.apply(obj.jspark, df_spark._jdf, config, False) # TODO: jdf to sdf result = result.toDF() algo_df = DataFrame(result, obj.spark) # TODO: long vid mapping fixed_string vid df = encode_id.join(algo_df, encode_id.encodedId == algo_df._id) \\ .drop(\"encodedId\") \\ .drop(\"_id\") \\ .withColumnRenamed(\"id\", \"_vertexId\") for arg in args: df_ = nebula_reader(obj, arg) df_ = df_.join(df, df_._vertexId == df._vertexId, \"leftsemi\") df__ = df.join(df_, df._vertexId == df_._vertexId, \"leftsemi\") nebula_writer(arg, df__)if __name__ == '__main__': process([\"edge_phone\"], \"dwd_phone\", \"dwd_account\")# TODO: PYSPARK (pyspark=2.4.7, python=3.7)# conda activate pyspark37# TODO: SPARK-SUBMIT# $&#123;SPARK_HOME&#125;/bin/spark-submit --master local[*] \\# --deploy-mode client \\# --driver-class-path file:/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar \\# --driver-class-path file:/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar \\# --jars file:/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar \\# --jars file:/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar \\# /exp/work/pyspark/nebula/pagerank.py fastapi接口实例`pythonimport os import findspark from pyspark.sql import SparkSessionfrom pyspark.sql import DataFramefrom pyspark.sql.functions import *from typing import Listfrom fastapi import FastAPI, Queryfrom fastapi.middleware.cors import CORSMiddlewarefrom pydantic import BaseModel from config import nebula_config, pagerank_dictfrom algorithm.pagerank import PageRankfrom algorithm.louvain import Louvain app_router = FastAPI(docs_url=None, redoc_url=None) CORSorigins = [ “*”] app_router.add_middleware( CORSMiddleware, allow_origins=origins, allow_credentials=True, allow_methods=[““], allow_headers=[““]) class NebulaAlgorithm(BaseModel): algo: str tag_list: List[str] algorithm_dict = { “pagerank”: PageRank(), “louvain”: Louvain()} @app_router.post(“/nebula/algo”)async def nebula_algorithm( args: NebulaAlgorithm): # TODO: algorithm object tags = [] for line in args.tag_list: for item in pagerank_dict.get(line): tags.append(item) algo = algorithm_dict.get(args.algo) obj = algo obj.set_tag(tags) obj.exe() # TODO: nebula-algorithm encode_id = obj.encode_id df_int = obj.dst_join_df if algo.__doc__.strip() == &apos;pagerank&apos;: if len(tags) == 1: df_spark = df_int else: df_pd = df_int.toPandas() values = df_pd.values.tolist() columns = df_pd.columns.tolist() df_spark = obj.spark.createDataFrame(values, columns) config = obj.spark._jvm.PRConfig(1, 0.8) result = obj.spark._jvm.PageRankAlgo.apply(obj.jspark, df_spark._jdf, config, False) elif algo.__doc__.strip() == &apos;louvain&apos;: if len(tags) == 1: df_spark = df_int else: df_pd = df_int.toPandas() values = df_pd.values.tolist() columns = df_pd.columns.tolist() df_spark = obj.spark.createDataFrame(values, columns) config = obj.spark._jvm.LouvainConfig(20, 10, 0.5) result = obj.spark._jvm.LouvainAlgo.apply(obj.jspark, df_spark._jdf, config, False) else: return # TODO: jdf to sdf result = result.toDF() algo_df = DataFrame(result, obj.spark) # TODO: long vid mapping fixed_string vid df = encode_id.join(algo_df, encode_id.encodedId == algo_df._id) \\ .drop(&quot;encodedId&quot;) \\ .drop(&quot;_id&quot;) \\ .withColumnRenamed(&quot;id&quot;, &quot;_vertexId&quot;) df.show() if name == “main“: import uvicorn uvicorn.run(app=”main:app_router”, reload=True, host=’0.0.0.0’, port=7792) TODO: PYSPARK (pyspark=2.4.7, python=3.7)conda activate pyspark37TODO: SPARK-SUBMIT${SPARK_HOME}/bin/spark-submit –master local[*] \\–deploy-mode client \\–driver-class-path file:/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar \\–driver-class-path file:/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar \\–jars file:/exp/work/pyspark/nebula-spark-connector/nebula-spark-connector/target/nebula-spark-connector-2.6.1.jar \\–jars file:/exp/work/pyspark/nebula-algorithm/nebula-algorithm/target/nebula-algorithm-2.6.1.jar \\/exp/work/pyspark/nebula/main.py","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"}],"tags":[{"name":"nebula","slug":"nebula","permalink":"http://yoursite.com/tags/nebula/"},{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"},{"name":"图算法","slug":"图算法","permalink":"http://yoursite.com/tags/图算法/"}]},{"title":"ElasticSearch环境搭建","slug":"ElasticSearch环境搭建","date":"2023-02-16T03:16:07.000Z","updated":"2024-02-01T09:14:50.041Z","comments":true,"path":"2023/02/16/ElasticSearch环境搭建/","link":"","permalink":"http://yoursite.com/2023/02/16/ElasticSearch环境搭建/","excerpt":"* 配合nebula全文检索测试环境而搭建的es单机环境 安装下载地址【https://www.elastic.co/cn/downloads/elasticsearch#ga-release】 (或前往elastic中文社区下载中心【https://elasticsearch.cn/download/】) 选择linux版本 安装ES 解压缩 1$ tar xf elasticsearch-7.14.2-linux-x86_64.tar.gz 创建es用户 1$ useradd es &amp;&amp; passwd es 更名 1$ mv elasticsearch-7.14.2 elasticsearch 赋予es用户权限 1$ chown -R es:es elasticsearch 配置* 可使用es自带的java环境：ES_JAVA_HOME 1$ vim /etc/profile 123# ES_JAVA_HOMEexport ES_JAVA_HOME=/data/elasticsearch/jdk/export PATH=$ES_JAVA_HOME/bin:$PATH 1$ source /etc/profile elasticsearch config 12$ cd elasticsearch/config$ vim elasticsearch.yml 1234567891011node.name: node-1 ##节点名称path.data: /usr/local/elasticsearch/data ##数据存放路径path.logs: /usr/local/elasticsearch/logs ##日志存放路径 bootstrap.memory_lock: true ##避免es使用swap交换分区indices.requests.cache.size: 5% ##缓存配置indices.queries.cache.size: 10% ##缓存配置network.host: 192.168.80.128 ##本机IPhttp.port: 9200 ##默认端口cluster.initial_master_nodes: [\"node-1\"] ##设置符合主节点条件的节点的主机名或 IP 地址来引导启动集群http.cors.enabled: true ##跨域http.cors.allow-origin: \"*\" 将当前用户软硬限制调大 1$ vim /etc/security/limits.conf 1234es soft nofile 65535es hard nofile 65537es soft memlock unlimitedes hard memlock unlimited 修改vm.max_map_count内存 1vim /etc/sysctl.conf 1vm.max_map_count=655360 1sysctl -p 启动123$ su es$ cd ../bin$ ./elasticsearch -d","text":"* 配合nebula全文检索测试环境而搭建的es单机环境 安装下载地址【https://www.elastic.co/cn/downloads/elasticsearch#ga-release】 (或前往elastic中文社区下载中心【https://elasticsearch.cn/download/】) 选择linux版本 安装ES 解压缩 1$ tar xf elasticsearch-7.14.2-linux-x86_64.tar.gz 创建es用户 1$ useradd es &amp;&amp; passwd es 更名 1$ mv elasticsearch-7.14.2 elasticsearch 赋予es用户权限 1$ chown -R es:es elasticsearch 配置* 可使用es自带的java环境：ES_JAVA_HOME 1$ vim /etc/profile 123# ES_JAVA_HOMEexport ES_JAVA_HOME=/data/elasticsearch/jdk/export PATH=$ES_JAVA_HOME/bin:$PATH 1$ source /etc/profile elasticsearch config 12$ cd elasticsearch/config$ vim elasticsearch.yml 1234567891011node.name: node-1 ##节点名称path.data: /usr/local/elasticsearch/data ##数据存放路径path.logs: /usr/local/elasticsearch/logs ##日志存放路径 bootstrap.memory_lock: true ##避免es使用swap交换分区indices.requests.cache.size: 5% ##缓存配置indices.queries.cache.size: 10% ##缓存配置network.host: 192.168.80.128 ##本机IPhttp.port: 9200 ##默认端口cluster.initial_master_nodes: [\"node-1\"] ##设置符合主节点条件的节点的主机名或 IP 地址来引导启动集群http.cors.enabled: true ##跨域http.cors.allow-origin: \"*\" 将当前用户软硬限制调大 1$ vim /etc/security/limits.conf 1234es soft nofile 65535es hard nofile 65537es soft memlock unlimitedes hard memlock unlimited 修改vm.max_map_count内存 1vim /etc/sysctl.conf 1vm.max_map_count=655360 1sysctl -p 启动123$ su es$ cd ../bin$ ./elasticsearch -d Kibana下载地址[https://elasticsearch.cn/download/] 选择和es相同版本 安装kibana 解压缩 1$ tar -zxvf kibana-7.14.2-linux-x86_64.tar.gz 更名 1$ mv kibana-7.14.2-linux-x86_64 kibana 配置12cd kibanavim config/kibana.yml 1234server.port: 5601server.host: \"0.0.0.0\"elasticsearch.hosts: \"http://192.168.80.128:9200\"kibana.index: \".kibana\" 12345678910# 修改/etc/sudoers文件，进入超级用户, 给予es用户写权限(base) [root@localhost kibana]# chmod u+w /etc/sudoers# 编辑/etc/sudoers文件(base) [root@localhost kibana]# vim /etc/sudoers# 赋予es用户权限es ALL=(ALL) ALL(base) [root@localhost kibana]# chmod u-w /etc/sudoers# 撤销es用户文件写权限(base) [root@localhost kibana]# sudo chown -R es:es /usr/local/kibana(base) [root@localhost kibana]# su es 启动12[es@localhost kibana]$ cd bin[es@localhost bin]$ ./kibana 进入kibana http:192.168.80.128:5601","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"}],"tags":[{"name":"elastic search","slug":"elastic-search","permalink":"http://yoursite.com/tags/elastic-search/"}]},{"title":"Spark-SQL","slug":"Spark-SQL","date":"2022-12-08T08:50:52.000Z","updated":"2022-12-09T08:58:34.011Z","comments":true,"path":"2022/12/08/Spark-SQL/","link":"","permalink":"http://yoursite.com/2022/12/08/Spark-SQL/","excerpt":"SparkSQLSparkSQL是spark的一个用于处理海量结构化数据的模块 支持SQL语言 自动优化 性能强 兼容HIVE API流程简单 支持标准化JDBC和ODBC连接 … SparkSQL数据抽象 Pandas · DataFrame · 二维表数据结构 · 单机（本地）集合 SparkCore · RDD · 无标准数据结构 · 分布式（分区）集合 SparkSQL · DataFrame · 二维表数据结构 · 分布式（分区）集合 SparkSession对象RDD程序的执行入口对象：SparkContext 在Spark2.0以后，推出了SparkSession对象，来作为Spark编码的统一入口对象。SparkSession： 用于SparkSQL编程，作为入口对象 用于SparkCore编程，通过SparkSession对象获取SparkContext 构建SparkSession对象： 1234567from pyspark.sql import SparkSession# 通过builder方法来构建SparkSession对象# appName：设置程序名称# config：配置常用属性# getOrCreate：完成创建SparkSession对象spark = SparkSession.builder.appName(\"test\").master(\"local[*]\").config(\"spark.sql.shuffle.partitions\", \"4\").getOrCreate() 通过SparkSesion对象获取SparkContext对象： 12345from pyspark.sql import SparkSessionspark = SparkSession.builder.appName(\"test\").master(\"local[*]\").config(\"spark.sql.shuffle.partitions\", \"4\").getOrCreate()sc = spark.sparkContext","text":"SparkSQLSparkSQL是spark的一个用于处理海量结构化数据的模块 支持SQL语言 自动优化 性能强 兼容HIVE API流程简单 支持标准化JDBC和ODBC连接 … SparkSQL数据抽象 Pandas · DataFrame · 二维表数据结构 · 单机（本地）集合 SparkCore · RDD · 无标准数据结构 · 分布式（分区）集合 SparkSQL · DataFrame · 二维表数据结构 · 分布式（分区）集合 SparkSession对象RDD程序的执行入口对象：SparkContext 在Spark2.0以后，推出了SparkSession对象，来作为Spark编码的统一入口对象。SparkSession： 用于SparkSQL编程，作为入口对象 用于SparkCore编程，通过SparkSession对象获取SparkContext 构建SparkSession对象： 1234567from pyspark.sql import SparkSession# 通过builder方法来构建SparkSession对象# appName：设置程序名称# config：配置常用属性# getOrCreate：完成创建SparkSession对象spark = SparkSession.builder.appName(\"test\").master(\"local[*]\").config(\"spark.sql.shuffle.partitions\", \"4\").getOrCreate() 通过SparkSesion对象获取SparkContext对象： 12345from pyspark.sql import SparkSessionspark = SparkSession.builder.appName(\"test\").master(\"local[*]\").config(\"spark.sql.shuffle.partitions\", \"4\").getOrCreate()sc = spark.sparkContext e.g. 1234567891011121314151617from pyspark.sql import SparkSessionspark = SparkSession.builder.appName(\"test\").master(\"local[*]\").config(\"spark.sql.shuffle.partitions\", \"4\").getOrCreate()sc = spark.sparkContextdf1 = spark.read.csv(\"hdfs://master:8020/input/pip_file.txt\", sep=',', header=False)df2 = df1.toDF(\"id\", \"package\", \"version\")df2.printSchema() # 表结构df2.show() # 展示表df2.createTempView(\"pip\") # 创建\"pip\"表,保存在内存中# SQL风格spark.sql(\"\"\" SELECT * FROM pip WHERE package='pyspark'\"\"\").show# DSL风格df2.where(\"package='pyspark'\").limit(5).show() 123456789101112131415161718192021222324252627282930313233343536root |-- id: string (nullable = true) |-- package: string (nullable = true) |-- version: string (nullable = true)+---+-------------+--------+| id| package| version|+---+-------------+--------+| 1| flask| 1.1.4|| 2| fastapi| 0.78.0|| 3| h5py| 2.10.0|| 4| keras| 2.6.0|| 5| jieba| 0.42.1|| 6| numpy| 1.23.3|| 7|opencv-python|4.6.0.66|| 8| pandas| 1.4.3|| 9| pillow| 9.2.0|| 10| py4j|0.10.9.5|| 11| pyspark| 3.3.1|| 12| sklearn| 0.0|| 13| tensorfolw| 2.6.2|| 14| requests| 2.28.1|| 15| redis| 3.5.3|+---+-------------+--------++---+-------+-------+| id|package|version|+---+-------+-------+| 11|pyspark| 3.3.1|+---+-------+-------++---+-------+-------+| id|package|version|+---+-------+-------+| 11|pyspark| 3.3.1|+---+-------+-------+ DataFrameDataFrame的组成DataFrame是一个二维表结构，在结构层面： StructureType对象描述整个DataFrame的表结构 StructField对象描述一个列的信息 在数据层面： Row对象记录一行数据 Column对象记录一列数据并包含列的信息 DataFrame的构建RDD转换DataFrame对象可以从RDD转换而来 1df = spark.createDataFrame(rdd, schema=['id', 'package']) e.g. 123456789101112from pyspark.sql import SparkSessionspark = SparkSession.builder.appName(\"test\").master(\"local[*]\").getOrCreate()sc = spark.sparkContextrdd = sc.textFile(\"hdfs://master:8020/input/pip_file.txt\").map(lambda x: x.split(\",\")).map(lambda x:(int(x[0]), x[1]))df = spark.createDataFrame(rdd, schema=['id', 'package'])df.printSchema()df.show(10, False) # 输出前十行数据，要全部显示默认设置为Truedf.createOrReplaceTempView(\"pip\")spark.sql(\"SELECT * FROM pip WHERE id &lt; 5\").show() 12345678910111213141516171819202122232425262728root |-- id: long (nullable = true) |-- package: string (nullable = true)+---+-------------+|id |package |+---+-------------+|1 |flask ||2 |fastapi ||3 |h5py ||4 |keras ||5 |jieba ||6 |numpy ||7 |opencv-python||8 |pandas ||9 |pillow ||10 |py4j |+---+-------------+only showing top 10 rows+---+-------+| id|package|+---+-------+| 1| flask|| 2|fastapi|| 3| h5py|| 4| keras|+---+-------+ StructType通过StructType对象来定义DataFrame的表结构，转换RDD 12345# 导入StructType对象和类型from pyspark.sql.types import StructType, StringType, IntegerType# 定义表结构schema = StructType().add(\"id\", IntegerType(), nullable=True).add(\"package\", StringType(), nullable=True) e.g. 12345678910111213from pyspark.sql import SparkSessionfrom pyspark.sql.types import StructType, StringType, IntegerTypespark = SparkSession.builder.appName(\"test\").master(\"local[*]\").getOrCreate()sc = spark.sparkContextrdd = sc.textFile(\"hdfs://master:8020/input/pip_file.txt\").map(lambda x: x.split(\",\")).map(lambda x:(int(x[0]), x[1]))schema = StructType().add(\"id\", IntegerType(), nullable=True).add(\"package\", StringType(), nullable=True)df = spark.createDataFrame(rdd, schema=schema)df.printSchema()df.show() 1234567891011121314151617181920212223root |-- id: integer (nullable = true) |-- package: string (nullable = true)+---+-------------+| id| package|+---+-------------+| 1| flask|| 2| fastapi|| 3| h5py|| 4| keras|| 5| jieba|| 6| numpy|| 7|opencv-python|| 8| pandas|| 9| pillow|| 10| py4j|| 11| pyspark|| 12| sklearn|| 13| tensorfolw|| 14| requests|| 15| redis|+---+-------------+ toDF12345678910111213141516171819from pyspark.sql import SparkSessionfrom pyspark.sql.types import StructType, StringType, IntegerTypespark = SparkSession.builder.appName(\"test\").master(\"local[*]\").getOrCreate()sc = spark.sparkContextrdd = sc.textFile(\"hdfs://master:8020/input/pip_file.txt\").map(lambda x: x.split(\",\")).map(lambda x:(int(x[0]), x[1]))# toDF快速构建DataFrame# 对列类型不敏感，默认string类型df1 = rdd.toDF([\"id\", \"package\"])df1.printSchema()df1.show(5, False)# 设置schema通过toDF构建DataFrameschema = StructType().add(\"id\", IntegerType(), nullable=True).add(\"package\", StringType(), nullable=True)df2 = rdd.toDF(schema=schema)df2.printSchema()df2.show(5, False) 1234567891011121314151617181920212223242526272829root |-- id: long (nullable = true) |-- package: string (nullable = true)+---+-------+|id |package|+---+-------+|1 |flask ||2 |fastapi||3 |h5py ||4 |keras ||5 |jieba |+---+-------+only showing top 5 rowsroot |-- id: integer (nullable = true) |-- package: string (nullable = true)+---+-------+|id |package|+---+-------+|1 |flask ||2 |fastapi||3 |h5py ||4 |keras ||5 |jieba |+---+-------+only showing top 5 rows Pandas1234567891011121314151617import pandas as pdfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName(\"test\").master(\"local[*]\").getOrCreate()sc = spark.sparkContextpddf = pd.DataFrame( &#123; \"id\": [1, 2, 3], \"package\": [\"flask\", \"fastapi\", \"h5py\"], \"version\": [\"1.1.4\", \"0.78.0\", \"2.10.0\"] &#125;)df = spark.createDataFrame(pddf)df.printSchema()df.show() 123456789101112root |-- id: long (nullable = true) |-- package: string (nullable = true) |-- version: string (nullable = true)+---+-------+-------+| id|package|version|+---+-------+-------+| 1| flask| 1.1.4|| 2|fastapi| 0.78.0|| 3| h5py| 2.10.0|+---+-------+-------+ 通过文件构建DataFrametext1sparksession.read.format(\"text|csv|json|jdbc|...\").option(\"K\", \"V\").schema(StructType|String).load(\"文件路径，支持本地文件系统和HDFS\") e.g. 1234567891011from pyspark.sql import SparkSessionfrom pyspark.sql.types import StructType, StringTypespark = SparkSession.builder.appName(\"test\").master(\"local[*]\").getOrCreate()sc = spark.sparkContext# 构建StructType，text数据源，读取数据的特点是将一整行当作一列来读取，默认列名为value，类型为stringschema = StructType().add(\"data\", StringType(), nullable=True)df = spark.read.format(\"text\").schema(schema=schema).load(\"hdfs://master:8020/input/pip_file.txt\")df.printSchema()df.show() 12345678910111213141516171819202122root |-- data: string (nullable = true)+--------------------+| data|+--------------------+| 1,flask,1.1.4|| 2,fastapi,0.78.0|| 3,h5py,2.10.0|| 4,keras,2.6.0|| 5,jieba,0.42.1|| 6,numpy,1.23.3||7,opencv-python,4...|| 8,pandas,1.4.3|| 9,pillow,9.2.0|| 10,py4j,0.10.9.5|| 11,pyspark,3.3.1|| 12,sklearn,0.0|| 13,tensorfolw,2.6.2|| 14,requests,2.28.1|| 15,redis,3.5.3|+--------------------+ jsonjson文件自带一定数据结构 12345678from pyspark.sql import SparkSessionspark = SparkSession.builder.appName(\"test\").master(\"local[*]\").getOrCreate()sc = spark.sparkContextdf = spark.read.format(\"json\").load(\"hdfs://master:8020/input/pip_file.json\")df.printSchema()df.show() 1234567891011121314root |-- id: long (nullable = true) |-- package: string (nullable = true) |-- version: string (nullable = true)+---+-------+-------+| id|package|version|+---+-------+-------+| 1| flask| 1.1.4|| 2|fastapi| 0.78.0|| 3| h5py| 2.10.0|| 4| keras| 2.6.0|| 5| jieba| 0.42.1|+---+-------+-------+ csv123456789from pyspark.sql import SparkSessionspark = SparkSession.builder.appName(\"test\").master(\"local[*]\").getOrCreate()sc = spark.sparkContext# 通过.option指定属性df = spark.read.format(\"csv\").option(\"sep\", \";\").option(\"header\", True).option(\"encoding\", \"utf-8\").schema(\"id INT, package STRING, version STRING\").load(\"hdfs://master:8020/input/pip_file.csv\")df.printSchema()df.show() 123456789101112131415161718192021222324root |-- id: integer (nullable = true) |-- package: string (nullable = true) |-- version: string (nullable = true)+---+-------------+--------+| id| package| version|+---+-------------+--------+| 1| flask| 1.1.4|| 2| fastapi| 0.78.0|| 3| h5py| 2.10.0|| 4| keras| 2.6.0|| 5| jieba| 0.42.1|| 6| numpy| 1.23.3|| 7|opencv-python|4.6.0.66|| 8| pandas| 1.4.3|| 9| pillow| 9.2.0|| 10| py4j|0.10.9.5|| 11| pyspark| 3.3.1|| 12| sklearn| 0.0|| 13| tensorfolw| 2.6.2|| 14| requests| 2.28.1|| 15| redis| 3.5.3|+---+-------------+--------+ parquetparquet是spark中常用的一种列示存储文件格式 内置schema（列名，列类型，是否为空） 存储是以列作为存储格式 存储是序列化存储在文件中（有压缩属性体积小） 123456789from pyspark.sql import SparkSessionspark = SparkSession.builder.appName(\"test\").master(\"local[*]\").getOrCreate()sc = spark.sparkContext# 通过.option指定属性df = spark.read.format(\"parquet\").load(\"hdfs://master:8020/input/suers.parquet\")df.printSchema()df.show() 123456789101112root |-- name: string (nullable = true) |-- favorite_color: string (nullable = true) |-- favorite_numbers: array (nullable = true) | |-- element: integer (containsNull = true)+------+--------------+----------------+| name|favorite_color|favorite_numbers|+------+--------------+----------------+|Alyssa| null| [3, 9, 15, 20]|| Ben| red| []|+------+--------------+----------------+ DataFrame编程DataFrame支持两种编程风格 DSL风格 被称为领域特定语言，是DataFrame的特有API SQL风格 使用SQL语句来直接处理DataFrame DSL风格12345678910111213141516171819202122232425262728293031323334353637from pyspark.sql import SparkSessionspark = SparkSession.builder.appName(\"test\").master(\"local[*]\").getOrCreate()sc = spark.sparkContextdf = spark.read.format(\"csv\").schema(\"id INT, package STRING, version STRING\").load(\"hdfs://master:8020/input/pip_file.txt\")# 获取Colum对象id_colum = df['id']package_colum = df['package']# DSL风格# select APIdf.select(['id', 'package']).show() # listdf.select('id', 'package').show() # 可变参数df.select(id_colum, package_colum).show() # Colum对象# filter APIdf.filter('id &lt; 5').show()df.filter(df['id'] &lt; 5).show() # Colum对象# where APIdf.where('id &lt; 5').show()df.where(df['id'] &lt; 5).show() # Colum对象# groupBy APIdf.groupBy('package').count().show()df.groupBy(df['package']).count().show() # Colum对象# 返回值不是DataFrame，而是GroupedData对象# 是一个有分组关系的数据结构，提供API做分组聚合# SQL：group by后接聚合：sum、avg、count、min、max# GroupedData类似于SQL分组后的数据结构，同样拥有上述5种聚合方法# GroupedData调用聚合方法后，返回值依旧是DataFrame对象# GroupedData只是一个中转对象，最终还是要获得DataFrame对象r = df.groupBy('package')print(r.sum().show(), r.avg().show(), r.count().show(), r.min().show(), r.max().show()) SQL风格使用SQL风格，需要将DataFrame提前注册成表 123df.createTempView(\"pip\") # 注册一个临时视图（表）df.createOrReplaceTempView(\"pip\") # 注册一个临时表，如果已存在则进行替换df.createGlobalTempView(\"pip\") # 注册一个全局表 全局表：可跨SparkSession对象使用，在一个程序内的多个SparkSession中均可调用，查询时带上前缀global_temp 临时表：仅在当前SparkSession中可用 123456789101112131415from pyspark.sql import SparkSessionspark = SparkSession.builder.appName(\"test\").master(\"local[*]\").getOrCreate()sc = spark.sparkContextdf = spark.read.format(\"csv\").schema(\"id INT, package STRING, version STRING\").load(\"hdfs://master:8020/input/pip_file.txt\")# 注册临时表df.createTempView(\"pip\")df.createOrReplaceTempView(\"pip\")df.createGlobalTempView(\"pip_2\")# 通过SparkSession SQL API执行sql语句spark.sql(\"SELECT package, COUNT(*) AS cnt FROM pip GROUP BY package\").show()spark.sql(\"SELECT package, COUNT(*) AS cnt FROM global_temp.pip_2 GROUP BY package\").show() pyspark.sql.functionsPySpark提供的pyspark.sql.functions包包含一系列可供SparkSQL使用的计算函数 1from pyspark.sql import functions as F 即可调用F对象调用函数进行计算。这些功能函数的返回值大多是Colum对象 WordCount示例1234567891011121314151617181920from pyspark.sql import SparkSessionfrom pyspark.sql import functions as Fspark = SparkSession.builder.appName(\"test\").master(\"local[*]\").getOrCreate()sc = spark.sparkContext# SQL风格处理rdd = sc.textFile(\"hdfs://master:8020/input/words.txt\").flatMap(lambda x: x.split(\" \")).map(lambda x: [x])df = rdd.toDF([\"word\"])df.createTempView(\"words\")spark.sql(\"SELECT word, COUNT(*) AS cnt FROM words GROUP BY word ORDER BY cnt DESC\").show()# DSL风格处理df = spark.read.format(\"text\").load(\"hdfs://master:8020/input/words.txt\")# withColumn方法# ：对已存在的列进行操作，返回与一个新的列，如果名字和之前相同则替换，否则新建列df.withColumn(\"value\", F.explode(F.split(df['value'], \" \"))).show()df2 = df.withColumn(\"value\", F.explode(F.split(df['value'], \" \"))).show()df2.groupBy(\"value\").count().show()df2.groupBy(\"value\").count().withColumnRenamed(\"value\", \"name\").withColumnRenamed(\"count\", \"cnt\").orderBy(\"cnt\", ascending=False).show() SparkSQL Shufflespark.sql.shuffle.partitions参数是在spark sql计算过程中，shuffle算子阶段默认的分区数是200个。对于集群模式，200较为合适，如果在local模式下运行，200较多，会在调度上带来额外的损耗，所以在local模式下建议修改较低，例如2/4/10。这个参数和RDD中设置并行度的参数相互独立 可以按优先级在三处设置： 代码设置 1spark = SparkSession.builder.appName(\"test\").master(\"local[*]\").config(\"spark.sql.shuffle.partitions\", \"2\").getOrCreate() 客户端参数设置 bin/spark-submit --conf &quot;spark.sql.shuffle.partitions=100&quot; 配置文件设置 conf/spark-defaults.conf spark.sql.shuffle.partitions 100 SparkSQL数据清洗数据去重APIdropDuplicates 1234# DataFrame API# 不设置参数，对全局的列联合起来进行比较，去除重复值，只保留一条df.dropDuplicates().show()df.dropDuplicates(['id', 'version']).show() 缺失值处理APIdropna 1234567# DataFrame API# 对缺失值的数据进行删除# 不设置参数，只要列中存在null即删除整行df.dropna().show()# tresh=3表示，最少满足3个有效列，不满足即删除当前数据df.dropna(thresh=3).show()df.dropna(tresh=2,subset['id', 'version']).show() fillna 1234567# DataFrame API# 对缺失值的数据进行填充df.fillna(\"loss\").show()# 对指定列进行填充df.fillna(\"M/A\", subset['version']).show()# 指定一个字典，对所有的列提供填充依据df.fillna(&#123;\"id\": \"unknown\", \"package\": \"unknown\", \"version\": \"none\"&#125;).show() DataFrame数据写出123456# mode：传入模式，append-追加，overwrite-覆盖，ignore-忽略，error-重复异常（默认）# format：传入格式：text/csv/json/parquet/orc/avro/jdbc# （text源仅支持单列df写出）# option：设置属性，如.option(\"sep\", \",\")# save：保存路径，支持本地文件系统和HDFSdf.write.mode().format().option(K, V).save(PATH) e.g. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from pyspark.sql import SparkSessionfrom pyspark.sql.types import StructType, StringType, IntegerTypefrom pyspark.sql import functions as F# 0. 构建执行环境入口对象SparkSessionspark = SparkSession.builder.\\ appName(\"test\").\\ master(\"local[*]\").\\ config(\"spark.sql.shuffle.partitions\", 2).\\ getOrCreate()sc = spark.sparkContext# 1. 读取数据集schema = StructType().add(\"user_id\", StringType(), nullable=True). \\ add(\"movie_id\", IntegerType(), nullable=True). \\ add(\"rank\", IntegerType(), nullable=True). \\ add(\"ts\", StringType(), nullable=True)df = spark.read.format(\"csv\"). \\ option(\"sep\", \"\\t\"). \\ option(\"header\", False). \\ option(\"encoding\", \"utf-8\"). \\ schema(schema=schema). \\ load(\"hdfs://master:8020/input/u.data\")# Write text 写出, 只能写出一个列的数据, 需要将df转换为单列dfdf.select(F.concat_ws(\"---\", \"user_id\", \"movie_id\", \"rank\", \"ts\")).\\ write.\\ mode(\"overwrite\").\\ format(\"text\").\\ save(\"hdfs://master:8020/output/sql/text\")# Write csvdf.write.mode(\"overwrite\").\\ format(\"csv\").\\ option(\"sep\", \";\").\\ option(\"header\", True).\\ save(\"hdfs://master:8020/output/sql/csv\")# Write jsondf.write.mode(\"overwrite\").\\ format(\"json\").\\ save(\"hdfs://master:8020/output/sql/json\")# Write parquetdf.write.mode(\"overwrite\").\\ format(\"parquet\").\\ save(\"hdfs://master:8020/output/sql/parquet\") DataFrame JDBC将mysql包添加进pyspark 12cd /usr/local/anaconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/jars/rz DataFrame读写数据库1234567891011121314151617181920# 将DataFrame通过JDBC写入mysqldf.write.mode(\"overwrite\").\\ format(\"jdbc\").\\ option(\"url\", \"jdbc:mysql://master:3306/...\").\\ option(\"dbtable\", \"test\").\\ option(\"user\", \"root\").\\ option(\"password\", \"123456\").\\ save()# JDBC会自动建表，因为DataFrame中含有表结构的信息# 读mysqldf = spark.read.format(\"jdbc\").\\ option(\"url\", \"jdbc:mysql://master:3306/...\").\\ option(\"dbtable\", \"test\").\\ option(\"user\", \"root\").\\ option(\"password\", \"123456\").\\ load()df.printSchema()df.show() SparkSQL函数定义SparkSQL定义UDF函数SparkSQL模块自带实现公共方法的位置在pyspark.sql.functions中，同时SparkSQL和Hive一样支持自定义函数：UDF和UDAF 目前python仅支持SparkSQL UDF自定函数 1234567891011# 注册的UDF可用于DSL和SQL风格# 返回值用于DSL风格，传参内的名称用于SQL风格# arg1：注册的UDF名称，仅可用于SQL风格# arg2：UDF处理逻辑，是一个单独的方法# arg3：声明UDF的返回值类型，UDF注册时，必须声明返回值类型，并且UDF的真实返回值一定要和声明的返回值一致# 返回值对象：是一个UDF对象，仅可用于DSL语法# 这种方式定义的UDF，可以通过arg1的名称用于SQL风格，通过返回值对象用于DSL风格sparksession.udf.register(arg1, arg2, arg3)# 仅能用于DSL风格pyspark.sql.functions.udf e.g. 12345678910111213141516171819202122232425262728293031from pyspark.sql import SparkSessionfrom pyspark.sql.types import IntegerTypefrom pyspark.sql import functions as Fspark = SparkSession.builder.appName(\"test\").master(\"local[*]\").config(\"spark.sql.shuffle.partitions\", 2).getOrCreate()sc = spark.sparkContextrdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).map(lambda x: [x])df = rdd.toDF(['num'])# sparksession.udf.register()def num_ride_10(num): return num * 10udf1 = spark.udf.register(\"udf1\", num_ride_10, IntegerType())# SQL风格使用# selectExpr：以SELECT的表达式执行（SQL字符串）# 普通select方法接受普通字符串字段名，或者返回值是Column对象的计算df.selectExpr(\"udf1(num)\").show()# DSL风格# 返回值UDF对象如果作为方法使用，传入的参数一定是Column对象df.select(udf1(df['num'])).show()# pyspark.sql.functions.udfudf2 = F.udf(num_ride_10, IntegerType())df.select(udf2(df['num'])).show()df.selectExpr(\"udf2(num)\").show() 1234567891011121314151617181920212223242526272829303132333435363738394041+---------+|udf1(num)|+---------+| 10|| 20|| 30|| 40|| 50|| 60|| 70|| 80|| 90|| 100|+---------++---------+|udf1(num)|+---------+| 10|| 20|| 30|| 40|| 50|| 60|| 70|| 80|| 90|| 100|+---------++----------------+|num_ride_10(num)|+----------------+| 10|| 20|| 30|| 40|| 50|| 60|| 70|+----------------+ 注册返回值为数组类型的UDF123456789101112131415161718192021222324252627282930from pyspark.sql import SparkSessionfrom pyspark.sql.types import StringType, ArrayTypeimport pandas as pdfrom pyspark.sql import functions as F# 0. 构建执行环境入口对象SparkSessionspark = SparkSession.builder.appName(\"test\").master(\"local[*]\").config(\"spark.sql.shuffle.partitions\", 2).getOrCreate()sc = spark.sparkContext# 构建一个RDDrdd = sc.parallelize([[\"hadoop spark flink\"], [\"hadoop flink java\"]])df = rdd.toDF([\"line\"])# 注册UDF, UDF的执行函数定义def split_line(data): return data.split(\" \") # 返回值是一个Array对象# TODO1 方式1 构建UDFudf2 = spark.udf.register(\"udf1\", split_line, ArrayType(StringType()))# DLS风格df.select(udf2(df['line'])).show()# SQL风格df.createTempView(\"lines\")spark.sql(\"SELECT udf1(line) FROM lines\").show(truncate=False)# TODO 2 方式2的形式构建UDFudf3 = F.udf(split_line, ArrayType(StringType()))df.select(udf3(df['line'])).show(truncate=False) 1234567891011121314151617181920+--------------------+| udf1(line)|+--------------------+|[hadoop, spark, f...||[hadoop, flink, j...|+--------------------++----------------------+|udf1(line) |+----------------------+|[hadoop, spark, flink]||[hadoop, flink, java] |+----------------------++----------------------+|split_line(line) |+----------------------+|[hadoop, spark, flink]||[hadoop, flink, java] |+----------------------+ 注册返回值为字典类型的UDF12345678910111213141516171819202122232425262728import stringfrom pyspark.sql import SparkSessionfrom pyspark.sql.types import StructType, StringType, IntegerType, ArrayType# 0. 构建执行环境入口对象SparkSessionspark = SparkSession.builder.master(\"local[*]\").config(\"spark.sql.shuffle.partitions\", 2).getOrCreate()sc = spark.sparkContext# 假设 有三个数字 1 2 3 我们传入数字 ,返回数字所在序号对应的 字母 然后和数字结合形成dict返回# 比如传入1 我们返回 &#123;\"num\":1, \"letters\": \"a\"&#125;rdd = sc.parallelize([[1], [2], [3]])df = rdd.toDF([\"num\"])# 注册UDFdef process(data): return &#123;\"num\": data, \"letters\": string.ascii_letters[data]&#125;\"\"\"UDF的返回值是字典的话, 需要用StructType来接收\"\"\"udf1 = spark.udf.register(\"udf1\", process, StructType().add(\"num\", IntegerType(), nullable=True).\\ add(\"letters\", StringType(), nullable=True))df.selectExpr(\"udf1(num)\").show(truncate=False)df.select(udf1(df['num'])).show(truncate=False) 123456789101112131415+---------+|udf1(num)|+---------+|&#123;1, b&#125; ||&#123;2, c&#125; ||&#123;3, d&#125; |+---------++---------+|udf1(num)|+---------+|&#123;1, b&#125; ||&#123;2, c&#125; ||&#123;3, d&#125; |+---------+ 通过RDD模拟UDAF1234567891011121314151617181920212223from pyspark.sql import SparkSession# 0. 构建执行环境入口对象SparkSessionspark = SparkSession.builder.appName(\"test\").master(\"local[*]\").config(\"spark.sql.shuffle.partitions\", 2).getOrCreate()sc = spark.sparkContextrdd = sc.parallelize([1, 2, 3, 4, 5], 3)df = rdd.map(lambda x: [x]).toDF(['num'])# 折中的方式 就是使用RDD的mapPartitions 算子来完成聚合操作# 如果用mapPartitions API 完成UDAF聚合, 一定要单分区single_partition_rdd = df.rdd.repartition(1)def process(iter): sum = 0 for row in iter: sum += row['num'] return [sum] # 一定要嵌套list, 因为mapPartitions方法要求的返回值是list对象print(single_partition_rdd.mapPartitions(process).collect()) 1[15] SparkSQL窗口函数开窗函数开窗函数的引入是为了既显示聚集前的数据又显示聚集后的数据，即在每一行的最后一列添加聚合函数的结果 开窗用于为行为定义一个窗口（运算将要操作的行为集合），对一组值进行操作，不需要使用GROUP BY字句对数据进行分组，能够在同一行中同时返回基础行的列和聚合列 聚合函数和开窗函数 聚合函数是将多行变为一行，count、avg…；如果要显示其他的列必须将列加入到GROUP BY中 开窗函数是将一行变成多行，可以不使用GROUP BY直接显示所有数据 开窗函数分类 聚合开窗函数 排序开窗函数 分区类型NTILE的窗口函数","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"Spark-Core","slug":"Spark-Core","date":"2022-12-07T08:58:49.000Z","updated":"2022-12-08T09:05:29.171Z","comments":true,"path":"2022/12/07/Spark-Core/","link":"","permalink":"http://yoursite.com/2022/12/07/Spark-Core/","excerpt":"广播变量本地对象被发送到同个Executor内每个分区的处理线程上使用，这样每个分区实际上存放了重复的数据。而Executor本质上是进程，进程内资源共享，没必要将本地对象分发给所有分区，造成内存浪费 解决方案：将本地对象设置为广播变量 12345# 1.将本地对象标记为广播变量broadcast = sc.broadcast(var)# 2.使用广播变量，从broadcast对象中取出本地对象value = broadcast.value# 当传输的是广播对象时，spark会只给每个Executor分发一份数据 当本地集合对象和分布式集合对象（RDD）进行关联时，需要将本地集合对象封装为广播变量 节省网络IO次数 降低Executor内存占用 累加器当执行累加操作时，各个分区累加自身的内容 12# spark提供累加器变量，参数是初始值acmlt = sc.accumulator(0) e.g. 123456789101112131415161718from pyspark import SparkConf, SparkContextconf = SparkConf().setAppName(\"test\").setMaster(\"local[*]\")sc = SparkContext(conf=conf)acmlt = sc.accumulator(0)def counts(data): global acmlt acmlt += 1 return datardd1 = sc.parallelize([1, 2, 3], 3)rdd2 = rdd1.map(counts)print(rdd2.collect())print(acmlt) 12[1, 2, 3]3 注：累加器可能因血缘关系导致重复的累加，例如一个RDD被释放后累加已经完成，此时再使用该RDD将会导致重复累加。可通过cache缓存机制来解决","text":"广播变量本地对象被发送到同个Executor内每个分区的处理线程上使用，这样每个分区实际上存放了重复的数据。而Executor本质上是进程，进程内资源共享，没必要将本地对象分发给所有分区，造成内存浪费 解决方案：将本地对象设置为广播变量 12345# 1.将本地对象标记为广播变量broadcast = sc.broadcast(var)# 2.使用广播变量，从broadcast对象中取出本地对象value = broadcast.value# 当传输的是广播对象时，spark会只给每个Executor分发一份数据 当本地集合对象和分布式集合对象（RDD）进行关联时，需要将本地集合对象封装为广播变量 节省网络IO次数 降低Executor内存占用 累加器当执行累加操作时，各个分区累加自身的内容 12# spark提供累加器变量，参数是初始值acmlt = sc.accumulator(0) e.g. 123456789101112131415161718from pyspark import SparkConf, SparkContextconf = SparkConf().setAppName(\"test\").setMaster(\"local[*]\")sc = SparkContext(conf=conf)acmlt = sc.accumulator(0)def counts(data): global acmlt acmlt += 1 return datardd1 = sc.parallelize([1, 2, 3], 3)rdd2 = rdd1.map(counts)print(rdd2.collect())print(acmlt) 12[1, 2, 3]3 注：累加器可能因血缘关系导致重复的累加，例如一个RDD被释放后累加已经完成，此时再使用该RDD将会导致重复累加。可通过cache缓存机制来解决 DAGSpark的核心是根据RDD来实现的，Spark Scheduler（spark任务调度）是spark核心实现的重要一环，其功能是组织处理RDD中每个分区的数据，根据RDD的依赖关系构建DAG有向无环图，再基于DAG划分Stage，将每个Stage中的任务发送到指定节点运行，合理规划资源的利用 DAG标准定义有向无环图：有方向而没有形成闭环的执行流程图 有向：具有执行方向 无环：没有闭环 Action算子和Job一个Action会产生一个DAG，如果代码中存在3个Action则会产生3个DAG； 每个DAG在应用程序运行时产生一个Job（应用程序内的子任务） 1个Action = 1个DAG = 1个Job 这样的代码运行起来在spark中被称为Application DAG和分区DAG的最终作用是为了构建spark详细执行的物理计划，由于spark是分布式多分区的，所以DAG和分区间也具有关联 DAG的宽窄依赖和阶段划分在Spark RDD前后之间的血缘关系，分为： 窄依赖：父RDD的一个分区，将全部数据发送给子RDD的一个分区； 宽依赖：父RDD的一个分区，将数据发送给子RDD的多个分区，别名：shuffle 对于spark，会根据DAG，按照宽依赖划分不同的DAG阶段。划分依据：从后向前，每遇到宽依赖就划分出一个阶段，称之为stage。在stage内部，一定是窄依赖 Spark的内存迭代计算窄依赖同一线程内走管道交互，进入宽依赖走网络IO交互 Spark默认收到全局并行度的限制，除了个别算子有特殊分区的情况，大部分算子都会遵循全局并行度的要求来划分自己的分区数。例如全局并行度是3，大部分算子的默认分区都是3-&gt;不建议再独立通过arg来指定分区数 Spark并行度Spark的并行：在同一时间内，有多少task在同时运行 Spark的并行度：并行能力，当设置为6，即共有6个task在并行运行，RDD的分区被规划为6个分区 Spark并行度设置（优先级由高到低）： 代码 客户端参数 配置文件 默认值（1），并不会全部以1来运行，多数情况下基于读取文件的分片数量来作为默认并行度 全局并行度配置参数： spark.default.parallelism 代码中设置： 12conf = SparkConf()conf.set(\"spark.default.parallelism\", \"100\") 客户端提交参数中设置bin/spark-submit --conf &quot;spark.default.parallelism=100&quot; 配置文件conf/spark-defaults.conf中设置spark.default.parallelism 100 注：全局并行度是推荐设置，不要针对RDD更改分区，可能会影响内存迭代管道的构建，或者产生额外的shuffle 针对RDD并行度的设置（不推荐）： · repartition算子 · coalesce算子 · partitionBy算子 规划Spark集群并行度设置为CPU总核心的2~10倍（或更高）* 比如集群可用的CPU核心数量为100个，建议并行度200~1000（确保是CPU核心的整数倍） 设置为最少2倍： CPU的一个核心同一时间只能做一件事，当拥有100个核心的情况下，设置100并行度，能利用全部的CPU，但task的压力不均衡，一旦某个task先执行完毕，会导致某个CPU核心的空闲。所以将task并行分配的数量增多，例如设置1000并行度，同一时间内有100个task在运行，900个在等待，但可以确保某个task运行完毕后会不断有task补上，不让CPU处于空闲状态，最大程度利用集群的资源 Spark任务调度Spark任务由Driver进行调度包括： 逻辑DAG产生 分区DAG产生 基于分区DAG构建线程task并划分 将task分配给Executor并监控其工作 Spark程序调度流程 构建Driver（Driver） 构建SparkContext执行环境入口对象（Driver） 基于DAG scheduler调度器构建逻辑task分配（Driver） 基于task scheduler调度器将逻辑task分配到各个Executor上执行并监控（Driver） Worker（Executor）被task scheduler管理监控，遵从指令干活并汇报进度（Worker） Driver内部组件 DAG调度器 将逻辑DAG进行处理，最终得到逻辑上的task划分 Task调度器 基于DAG调度器的产出，来规划这些逻辑的task应该在哪些物理的Executor上运行，以及监控它们 层级关系梳理 1个spark环境可运行多个Application； 1个代码成功运行生成一个Application； 1个Application内部有多个Job； 1个Action算子产生1个Job，每个Job有自己的DAG执行图； 1个Job的DAG基于宽窄依赖划分不同的阶段； 1个阶段里基于分区数量形成多个并行的内存迭代管道； 1个内存迭代管道形成1个task（DAG调度器划分将Job内划分出具体的task任务，1个Job被划分出的task在逻辑上被称为这个job的taskset）","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"Pyspark-RDD","slug":"Pyspark-RDD","date":"2022-12-07T05:45:49.000Z","updated":"2022-12-08T06:09:52.881Z","comments":true,"path":"2022/12/07/Pyspark-RDD/","link":"","permalink":"http://yoursite.com/2022/12/07/Pyspark-RDD/","excerpt":"RDDRDD（Resilient Distributed Dataset）弹性分布式数据集，是spark中最基本的数据抽象，代表一个不可变、可分区、其中元素可并行计算的集合 Resilient：RDD中的数据可存储再内存或磁盘中 Distributed：分布式存储数据（跨机器/跨进程），用于分布式计算 Dataset：一个用于存放数据的数据集合 特性分区RDD分区是RDD数据存储的最小单位，一份RDD数据本质上分隔成了多个分区 123# 存储9个数字，设立三个分区rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9], 3)rdd.glom().collect() 1[[1,2,3],[4,5,6],[7,8,9]] RDD方法会作用在其所有方法上1rdd.map(lambda x: x * 10).collect() 1[10,20,30,40,50,60,70,80,90] RDD之间具有依赖关系123456sc = SparkContext(conf=conf)rdd1 = sc.textFile(\"../test.text\")rdd2 = rdd1.flatMap(lambda x: x.split(' '))rdd3 = rdd2.map(lambda x: (x, 1))rdd4 = rdd3.reduceByKey(lambda a, b: a+b)print(rdd4.collect()) Key-Value型RDD可以有分区器KV型RDD：RDD内存储的数据是只有两个元素的二元元组 默认分区器：Hash分区规则，也可手动设置分区器：rdd.partitionBy()方法 注：不是所有RDD都是KV型 RDD的分区规划：会尽量靠近数据所在的服务器在初始RDD读取数据规划阶段，分区会尽量规划到存储数据所在服务器，直接读取本地数据，避免从网络读取数据 Spark会在确保并行计算能力的前提下，尽量确保本地读取 RDD创建 通过并行化集合创建（本地对象转化为分布式RDD） 读取外部数据源（读文件） 并行化创建123# arg1: 集合对象，如：list# arg2：可选，指定分区数量rdd = SparkContext.parallelize(arg1, arg2) 读取文件通过textFile API来读取本地或者hdfs的数据 1234# arg1: 文件路径# arg2：可选，最小分区数量# 当arg2超出spark允许范围，参数失效SparkContext.textFile(arg1, arg2) 通过wholeTextFile API来读取小文件，这个api偏向于少量分区读取数据，是pyspark基于小文件的优化 1234# arg1：文件路径# arg2：可选，最小分区数量# 当arg2超出spark允许范围，参数失效SparkContext.wholeTextFiles(arg1, arg2)","text":"RDDRDD（Resilient Distributed Dataset）弹性分布式数据集，是spark中最基本的数据抽象，代表一个不可变、可分区、其中元素可并行计算的集合 Resilient：RDD中的数据可存储再内存或磁盘中 Distributed：分布式存储数据（跨机器/跨进程），用于分布式计算 Dataset：一个用于存放数据的数据集合 特性分区RDD分区是RDD数据存储的最小单位，一份RDD数据本质上分隔成了多个分区 123# 存储9个数字，设立三个分区rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9], 3)rdd.glom().collect() 1[[1,2,3],[4,5,6],[7,8,9]] RDD方法会作用在其所有方法上1rdd.map(lambda x: x * 10).collect() 1[10,20,30,40,50,60,70,80,90] RDD之间具有依赖关系123456sc = SparkContext(conf=conf)rdd1 = sc.textFile(\"../test.text\")rdd2 = rdd1.flatMap(lambda x: x.split(' '))rdd3 = rdd2.map(lambda x: (x, 1))rdd4 = rdd3.reduceByKey(lambda a, b: a+b)print(rdd4.collect()) Key-Value型RDD可以有分区器KV型RDD：RDD内存储的数据是只有两个元素的二元元组 默认分区器：Hash分区规则，也可手动设置分区器：rdd.partitionBy()方法 注：不是所有RDD都是KV型 RDD的分区规划：会尽量靠近数据所在的服务器在初始RDD读取数据规划阶段，分区会尽量规划到存储数据所在服务器，直接读取本地数据，避免从网络读取数据 Spark会在确保并行计算能力的前提下，尽量确保本地读取 RDD创建 通过并行化集合创建（本地对象转化为分布式RDD） 读取外部数据源（读文件） 并行化创建123# arg1: 集合对象，如：list# arg2：可选，指定分区数量rdd = SparkContext.parallelize(arg1, arg2) 读取文件通过textFile API来读取本地或者hdfs的数据 1234# arg1: 文件路径# arg2：可选，最小分区数量# 当arg2超出spark允许范围，参数失效SparkContext.textFile(arg1, arg2) 通过wholeTextFile API来读取小文件，这个api偏向于少量分区读取数据，是pyspark基于小文件的优化 1234# arg1：文件路径# arg2：可选，最小分区数量# 当arg2超出spark允许范围，参数失效SparkContext.wholeTextFiles(arg1, arg2) RDD算子方法、函数：本地对象的API 算子：分布式集合对象的API RDD的算子分为两类： Transformation：转换算子 返回值仍旧是RDD的算子，构建执行计划 Action：行动算子 返回值不再是RDD，使执行计划开始工作 Transformation算子map将RDD的数据一条条处理（处理逻辑基于map算子接收的处理函数），返回新的RDD 1rdd.map(func) e.g. 12345678910111213from pyspark import SparkConf, SparkContextconf = SparkConf().setAppName(\"TEST\").setMaster(\"local[*]\")sc = SparkContext(conf=conf)rdd = sc.parallelize([1, 2, 3, 4, 5, 6], 3)def add(data): return data * 10print(rdd.map(add).collect()) 1[10, 20, 30, 40, 50, 60] flatMap对RDD执行map操作，接着进行解除嵌套： 1234# 嵌套lst = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]# 解除嵌套lst = [1, 2, 3, 4, 5, 6, 7, 8, 9] e.g. 123456789from pyspark import SparkConf, SparkContextconf = SparkConf().setAppName(\"test\").setMaster(\"local[*]\")sc = SparkContext(conf=conf)rdd = sc.parallelize([\"hadoop spark hadoop\", \"spark hadoop hadoop\", \"hadoop flink spark\"])# 得到所有的单词, 组成RDD, flatMap的传入参数 和map一致, 就是给map逻辑用的, 解除嵌套无需逻辑(传参)rdd2 = rdd.flatMap(lambda line: line.split(\" \"))print(rdd2.collect()) 1[&apos;hadoop&apos;, &apos;spark&apos;, &apos;hadoop&apos;, &apos;spark&apos;, &apos;hadoop&apos;, &apos;hadoop&apos;, &apos;hadoop&apos;, &apos;flink&apos;, &apos;spark&apos;] reduceByKey针对KV型RDD自动按照key进行分组，然后根据提供的聚合逻辑完成组内数据（value）聚合操作 12# 接受两个类型一致的传入参数，返回聚合值rdd.reduceByKey(func) e.g. 123456789from pyspark import SparkConf, SparkContextconf = SparkConf().setAppName(\"test\").setMaster(\"local[*]\")sc = SparkContext(conf=conf)rdd = sc.parallelize([('a', 1), ('a', 1), ('b', 1), ('b', 1), ('a', 1)])# reduceByKey 对相同key 的数据执行聚合相加print(rdd.reduceByKey(lambda a, b: a + b).collect()) 1[(&apos;b&apos;, 2), (&apos;a&apos;, 3)] mapValues针对二元元组RDD，对其内部的二元元组value值进行map 12# 传入二元元组的value值，func只对value进行处理rdd.mapValues(func) e.g. 123456789from pyspark import SparkConf, SparkContextconf = SparkConf().setAppName(\"test\").setMaster(\"local[*]\")sc = SparkContext(conf=conf)rdd = sc.parallelize([('a', 1), ('a', 1), ('b', 1), ('b', 1), ('a', 1)])# reduceByKey 对相同key 的数据执行聚合相加print(rdd.mapValues(lambda x: x * 10).collect()) 1[(&apos;a&apos;, 10), (&apos;a&apos;, 10), (&apos;b&apos;, 10), (&apos;b&apos;, 10), (&apos;a&apos;, 10)] groupBy将RDD的数据进行分组 123# func要求传入一个参数，返回一个值，类型不做要求。相同的返回值将被放入同一个组中。# 分组完成后，每一个组是一个二元元组，key就是返回值，所有同组数据放入一个迭代器对象中作为valuerdd.groupBy(func) e.g. 123456789101112from pyspark import SparkConf, SparkContextconf = SparkConf().setAppName(\"test\").setMaster(\"local[*]\")sc = SparkContext(conf=conf)rdd = sc.parallelize([('a', 1), ('a', 1), ('b', 1), ('b', 2), ('b', 3)])# 通过groupBy对数据进行分组# groupBy传入的函数的 意思是: 通过这个函数, 确定按照谁来分组(返回谁即可)# 分组规则 和SQL是一致的, 也就是相同的在一个组(Hash分组)result = rdd.groupBy(lambda t: t[0])print(result.map(lambda t:(t[0], list(t[1]))).collect()) 1[(&apos;b&apos;, [(&apos;b&apos;, 1), (&apos;b&apos;, 2), (&apos;b&apos;, 3)]), (&apos;a&apos;, [(&apos;a&apos;, 1), (&apos;a&apos;, 1)])] Filter过滤数据 12# func返回值为True的参数保留，False丢弃rdd.filter(func) e.g. 1234567891011from pyspark import SparkConf, SparkContextconf = SparkConf().setAppName(\"test\").setMaster(\"local[*]\")sc = SparkContext(conf=conf)rdd = sc.parallelize([1, 2, 3, 4, 5, 6])# 通过Filter算子, 过滤奇数result = rdd.filter(lambda x: x % 2 == 1)print(result.collect()) 1[1, 3, 5] distinct对RDD数据进行去重 12# arg：去重分区数量，一般省略rdd.distinct(arg) e.g. 123456789101112from pyspark import SparkConf, SparkContextconf = SparkConf().setAppName(\"test\").setMaster(\"local[*]\")sc = SparkContext(conf=conf)rdd = sc.parallelize([1, 1, 1, 2, 2, 2, 3, 3, 3])# distinct 进行RDD数据去重操作print(rdd.distinct().collect())rdd2 = sc.parallelize([('a', 1), ('a', 1), ('a', 3)])print(rdd2.distinct().collect()) 12[2, 1, 3][(&apos;a&apos;, 1), (&apos;a&apos;, 3)] union将两个RDD合并成一个RDD返回 123# 仅合并，不会去重# 可以合并不同类型的RDDrdd.union(other_rdd) e.g. 12345678910from pyspark import SparkConf, SparkContextconf = SparkConf().setAppName(\"test\").setMaster(\"local[*]\")sc = SparkContext(conf=conf)rdd1 = sc.parallelize([1, 1, 3, 3])rdd2 = sc.parallelize([\"a\", \"b\", \"a\"])rdd3 = rdd1.union(rdd2)print(rdd3.collect()) 1[1, 1, 3, 3, &apos;a&apos;, &apos;b&apos;, &apos;a&apos;] join对两个RDD执行JOIN操作（可实现SQL的内/外连接） 1234# jion算子只能用于二元元组rdd.join(other_rdd) # 内连接rdd.leftOuterJoin(other_rdd) # 左外连接rdd.rightOuterJoin(other_rdd) # 右外连接 e.g. 1234567891011121314from pyspark import SparkConf, SparkContextconf = SparkConf().setAppName(\"test\").setMaster(\"local[*]\")sc = SparkContext(conf=conf)rdd1 = sc.parallelize([ (1001, \"zhangsan\"), (1002, \"lisi\"), (1003, \"wangwu\"), (1004, \"zhaoliu\") ])rdd2 = sc.parallelize([ (1001, \"销售部\"), (1002, \"科技部\")])# 通过join算子来进行rdd之间的关联# 对于join算子来说 关联条件 按照二元元组的key来进行关联print(rdd1.join(rdd2).collect())# 左外连接, 右外连接 可以更换一下rdd的顺序 或者调用rightOuterJoin即可print(rdd1.leftOuterJoin(rdd2).collect()) 12[(1001, (&apos;zhangsan&apos;, &apos;销售部&apos;)), (1002, (&apos;lisi&apos;, &apos;科技部&apos;))][(1004, (&apos;zhaoliu&apos;, None)), (1001, (&apos;zhangsan&apos;, &apos;销售部&apos;)), (1002, (&apos;lisi&apos;, &apos;科技部&apos;)), (1003, (&apos;wangwu&apos;, None))] intersection求两个RDD的交集，返回一个新RDD 1rdd.intersection(other_rdd) e.g. 123456789101112from pyspark import SparkConf, SparkContextconf = SparkConf().setAppName(\"test\").setMaster(\"local[*]\")sc = SparkContext(conf=conf)rdd1 = sc.parallelize([('a', 1), ('a', 3)])rdd2 = sc.parallelize([('a', 1), ('b', 3)])# 通过intersection算子求RDD之间的交集, 将交集取出 返回新RDDrdd3 = rdd1.intersection(rdd2)print(rdd3.collect()) 1[(&apos;a&apos;, 1)] glom将RDD的数据按照分区加上嵌套 例如RDD数据[1,2,3,4,5]有两个分区，经过glom处理后变成：[[1,2,3]],[4,5]] 1rdd.glom() e.g. 123456789from pyspark import SparkConf, SparkContextconf = SparkConf().setAppName(\"test\").setMaster(\"local[*]\")sc = SparkContext(conf=conf)rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9], 2)print(rdd.glom().collect())print(rdd.glom().flatMap(lambda x: x).collect()) # tips：解嵌套 12[[1, 2, 3, 4], [5, 6, 7, 8, 9]][1, 2, 3, 4, 5, 6, 7, 8, 9] groupByKey针对KV型RDD自动按key分组 1rdd.groupByKey() e.g. 12345678910from pyspark import SparkConf, SparkContextconf = SparkConf().setAppName(\"test\").setMaster(\"local[*]\")sc = SparkContext(conf=conf)rdd = sc.parallelize([('a', 1), ('a', 1), ('b', 1), ('b', 1), ('b', 1)])rdd2 = rdd.groupByKey()print(rdd2.map(lambda x: (x[0], list(x[1]))).collect()) 1[(&apos;b&apos;, [1, 1, 1]), (&apos;a&apos;, [1, 1])] sortBy基于指定的排序函数对RDD数据进行排序 123# ascending：True-升序，False-降序# numPartitions：用于排序的分区数量，要进行全局排序，设置为1rdd.sortBy(func, ascending=False, numPartitions=1) e.g. 123456789101112131415161718from pyspark import SparkConf, SparkContextconf = SparkConf().setAppName(\"test\").setMaster(\"local[*]\")sc = SparkContext(conf=conf)rdd = sc.parallelize([('c', 3), ('f', 1), ('b', 11), ('c', 3), ('a', 1), ('c', 5), ('e', 1), ('n', 9), ('a', 1)], 3)# 使用sortBy对rdd执行排序# 按照value 数字进行排序# 参数1函数，告知Spark 按照数据的哪个列进行排序# 参数2: True表示升序 False表示降序# 参数3: 排序的分区数\"\"\"注意: 如果要全局有序, 排序分区数需设置为1\"\"\"print(rdd.sortBy(lambda x: x[1], ascending=True, numPartitions=1).collect())# 按照key来进行排序print(rdd.sortBy(lambda x: x[0], ascending=False, numPartitions=1).collect()) 12[(&apos;f&apos;, 1), (&apos;a&apos;, 1), (&apos;e&apos;, 1), (&apos;a&apos;, 1), (&apos;c&apos;, 3), (&apos;c&apos;, 3), (&apos;c&apos;, 5), (&apos;n&apos;, 9), (&apos;b&apos;, 11)][(&apos;n&apos;, 9), (&apos;f&apos;, 1), (&apos;e&apos;, 1), (&apos;c&apos;, 3), (&apos;c&apos;, 3), (&apos;c&apos;, 5), (&apos;b&apos;, 11), (&apos;a&apos;, 1), (&apos;a&apos;, 1)] sortByKey针对KV型RDD按照key进行排序 1234# ascending：True-升序，False-降序# numPartitions：用于排序的分区数量，要进行全局排序，设置为1# keyfunc：在排序前对key进行的处理rdd.sortByKey(ascending=True, numPartitions=None, keyfunc=&lt;function RDD.&lt;lambda&gt;&gt;) e.g. 12345678910from pyspark import SparkConf, SparkContextconf = SparkConf().setAppName(\"test\").setMaster(\"local[*]\")sc = SparkContext(conf=conf)rdd = sc.parallelize([('a', 1), ('E', 1), ('C', 1), ('D', 1), ('b', 1), ('g', 1), ('f', 1), ('y', 1), ('u', 1), ('i', 1), ('o', 1), ('p', 1), ('m', 1), ('n', 1), ('j', 1), ('k', 1), ('l', 1)], 3)print(rdd.sortByKey(ascending=True, numPartitions=1, keyfunc=lambda key: str(key).lower()).collect()) 1[(&apos;a&apos;, 1), (&apos;b&apos;, 1), (&apos;C&apos;, 1), (&apos;D&apos;, 1), (&apos;E&apos;, 1), (&apos;f&apos;, 1), (&apos;g&apos;, 1), (&apos;i&apos;, 1), (&apos;j&apos;, 1), (&apos;k&apos;, 1), (&apos;l&apos;, 1), (&apos;m&apos;, 1), (&apos;n&apos;, 1), (&apos;o&apos;, 1), (&apos;p&apos;, 1), (&apos;u&apos;, 1), (&apos;y&apos;, 1)] repartition &amp; coalesce对RDD的分区执行重新分区（仅数量） 注：尽量避免使用，影响并行计算性能。在合并到1个分区进行全局排序等场景下使用，尽可能避免增加分区，可能破坏内存迭代的计算管道 1234# n：决定新的分区数量# coalesce中增加分区必须指定shuffle=Truerdd.repartition(n)rdd.coalesce(n, shuffle) e.g. 12345678910111213141516from pyspark import SparkConf, SparkContextconf = SparkConf().setAppName(\"test\").setMaster(\"local[*]\")sc = SparkContext(conf=conf)rdd = sc.parallelize([1, 2, 3, 4, 5], 3)# repartition 修改分区print(rdd.repartition(1).getNumPartitions())print(rdd.repartition(5).getNumPartitions())# coalesce 修改分区print(rdd.coalesce(1).getNumPartitions())print(rdd.coalesce(5, shuffle=True).getNumPartitions()) 12341515 Action算子countByKey用于统计key出现的次数（一般适用于KV型RDD） 1rdd.countByKey() e.g. 12345678910111213from pyspark import SparkConf, SparkContextconf = SparkConf().setAppName(\"test\").setMaster(\"local[*]\")sc = SparkContext(conf=conf)rdd = sc.textFile(\"hdfs://master:8020/input/words.txt\")rdd2 = rdd.flatMap(lambda x: x.split(\" \")).map(lambda x: (x, 1))# 通过countByKey来对key进行计数, 这是一个Action算子result = rdd2.countByKey()print(result)print(type(result)) 12defaultdict(&lt;class &apos;int&apos;&gt;, &#123;&apos;hadoop&apos;: 7, &apos;spark&apos;: 5, &apos;flink&apos;: 3&#125;)&lt;class &apos;collections.defaultdict&apos;&gt; collect将RDD各个分区内的数据统一收集到Driver中，形成一个list对象 1rdd.collect() 注：数据集大小不能超过Driver内存 reduce对RDD数据集按照func逻辑进行聚合 12# 对func：传入2个参数得到1个返回值，要求返回值和参数的类型保持一致rdd.reduce(func) e.g. 12345678from pyspark import SparkConf, SparkContextconf = SparkConf().setAppName(\"test\").setMaster(\"local[*]\")sc = SparkContext(conf=conf)rdd = sc.parallelize([1, 2, 3, 4, 5])print(rdd.reduce(lambda a, b: a + b)) 115 fold同reduce，接受传入逻辑进行聚合，但是聚合是带有初始值的。这个初始值的聚合作用在： 分区内聚合 分区间聚合 例如：[[1, 2, 3], [4, 5, 6], [7, 8, 9]] 数据分布在三个分区上 分区1：123聚合时带上10作为初始值得到16 分区2：456聚合时带上10作为初始值得到25 分区3：789聚合时带上10作为初始值得到34 最后再做3个分区间的聚合：16+25+34得到85 1rdd.fold(src, func) e.g. 12345678from pyspark import SparkConf, SparkContextconf = SparkConf().setAppName(\"test\").setMaster(\"local[*]\")sc = SparkContext(conf=conf)rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9], 3)print(rdd.fold(10, lambda a, b: a + b)) 185 first·take·count·topfirst 取出RDD的第一个元素 12&gt;&gt;&gt;sc.parallelize([3, 2, 1]).first()3 take 取出RDD的前n个元素，组合成list返回 12&gt;&gt;&gt;sc.parallelize([1, 2, 3, 4, 5, 6]).take(5)[1, 2, 3, 4, 5] count 计算RDD有多少条数据，返回值是一个数字 12&gt;&gt;&gt;sc.parallelize([3, 2, 1, 4, 5, 6]).count()6 top 对RDD数据集进行降序排序，取结果前n个 12&gt;&gt;&gt;sc.parallelize([3, 2, 1, 4, 5, 6]).top(3)[6, 5, 4] takeSample随机抽样RDD数据，可用于数据检查 1234# arg1：True表示运行取同一个数据，False表示不允许取同一个数据（和数据内容无关，是否重复表示的是同一个位置的数据）# arg2：抽样数目# arg3：可选，随机数种子，随意传进一个数字takeSample(arg1：True/False, arg2:采样数, arg3:随机数种子) e.g. 12345678from pyspark import SparkConf, SparkContextconf = SparkConf().setAppName(\"test\").setMaster(\"local[*]\")sc = SparkContext(conf=conf)rdd = sc.parallelize([1, 3, 5, 3, 1, 3, 2, 6, 7, 8, 6], 1)print(rdd.takeSample(False, 5, 1)) 1[2, 7, 6, 6, 3] takeOrdered对RDD进行排序，取结果前n个 123# arg1：需要几个数据# arg2：对排序的数据进行更改（不会更改数据本身，仅在排序时使用）rdd.takeOrdered(arg1, arg2) e.g. 12345678910from pyspark import SparkConf, SparkContextconf = SparkConf().setAppName(\"test\").setMaster(\"local[*]\")sc = SparkContext(conf=conf)rdd = sc.parallelize([1, 3, 2, 4, 7, 9, 6], 1)print(rdd.takeOrdered(3))print(rdd.takeOrdered(3, lambda x: -x)) 12[1, 2, 3][9, 7, 6] foreach对RDD的每一个元素执行提供的逻辑操作（同map），无返回值 1rdd.foreach(func) e.g. 12345678from pyspark import SparkConf, SparkContextconf = SparkConf().setAppName(\"test\").setMaster(\"local[*]\")sc = SparkContext(conf=conf)rdd = sc.parallelize([1, 3, 2, 4, 7, 9, 6], 1)result = rdd.foreach(lambda x: print(x * 10)) 123456710302040709060 特性：由Executor直接输出 saveAsTextFile将RDD数据写入文本文件 支持：本地写出或hdfs等文件系统 1rdd.saveAsTextFile() e.g. 12345678from pyspark import SparkConf, SparkContextconf = SparkConf().setAppName(\"test\").setMaster(\"local[*]\")sc = SparkContext(conf=conf)rdd = sc.parallelize([1, 3, 2, 4, 7, 9, 6], 3)rdd.saveAsTextFile(\"hdfs://master:8020/output/out_test1\") 12hadoop fs -ls /output/out_test1hadoop fs -cat /output/out_test1/* 12345Found 4 items-rw-r--r-- 3 root supergroup 0 2022-12-06 17:44 /output/out_test1/_SUCCESS-rw-r--r-- 3 root supergroup 4 2022-12-06 17:44 /output/out_test1/part-00000-rw-r--r-- 3 root supergroup 4 2022-12-06 17:44 /output/out_test1/part-00001-rw-r--r-- 3 root supergroup 6 2022-12-06 17:44 /output/out_test1/part-00002 12345671324796 特性：由Executor直接写入文件 mapPartitions不同于map每次操作一个分区的单一对象，mapPartitions一次操作一整个分区的数据，作为一个迭代器对象传入进来 1rdd.mapPartitions(func) e.g. 1234567891011121314151617from pyspark import SparkConf, SparkContextconf = SparkConf().setAppName(\"test\").setMaster(\"local[*]\")sc = SparkContext(conf=conf)rdd = sc.parallelize([1, 3, 2, 4, 7, 9, 6], 3)def process(iter): result = list() for it in iter: result.append(it * 10) return result print(rdd.mapPartitions(process).collect()) 1[10, 20, 70] foreachPartition和foreach一致，foreach一条条处理，而foreachPartition一次处理一整个分区的数据，类似于没有返回值的mapPartitions 1rdd.foreachPartition(func) e.g. 1234567891011121314151617from pyspark import SparkConf, SparkContextconf = SparkConf().setAppName(\"test\").setMaster(\"local[*]\")sc = SparkContext(conf=conf)rdd = sc.parallelize([1, 3, 2, 4, 7, 9, 6], 3)def process(iter): result = list() for it in iter: result.append(it * 10) print(result)rdd.foreachPartition(process) 1234567[20][20, 40][10][10, 30][70][70, 90][70, 90, 60] partitionBy对RDD进行自定义分区操作 123# arg1：重新分区后的分区数量# arg2：自定义分区规则，通过函数传入，函数返回值必须位int型（分区编号从0开始，不得超过分区数-1）rdd.partitionBy(arg1, arg2) e.g. 12345678910111213141516from pyspark import SparkConf, SparkContextconf = SparkConf().setAppName(\"test\").setMaster(\"local[*]\")sc = SparkContext(conf=conf)rdd = sc.parallelize([('hadoop', 1), ('spark', 1), ('hello', 1), ('flink', 1), ('hadoop', 1), ('spark', 1)])# 使用partitionBy 自定义 分区def process(k): if 'hadoop' == k or 'hello' == k: return 0 if 'spark' == k: return 1 return 2print(rdd.partitionBy(3, process).glom().collect()) 1[[(&apos;hadoop&apos;, 1), (&apos;hello&apos;, 1), (&apos;hadoop&apos;, 1)], [(&apos;spark&apos;, 1), (&apos;spark&apos;, 1)], [(&apos;flink&apos;, 1)]] RDD的数据是过程数据RDD之间进行相互迭代计算（Transformation的转换），当执行开启以后，新RDD生成，旧RDD消失。所以RDD的数据是过程数据，仅在处理的过程中存在，一旦处理完成便会被释放，旨在最大化的合理利用系统资源 当RDD被释放后需要被重新使用，会从头开始执行 RDD缓存防止当RDD被释放而又要被重新调用的情况下，避免从头执行代码，使用RDD缓存API 12345678910rdd.cache() # 缓存到内存中rdd.persist(StorageLevel.MEMORY_ONLY) # 仅在内存缓存rdd.persist(StorageLevel.MEMORY_ONLY_2) # 仅在内存缓存，生成2个副本rdd.persist(StorageLevel.DISK_ONLY) # 仅缓存到硬盘rdd.persist(StorageLevel.DISK_ONLY_2) # 仅缓存到硬盘，生成2个副本rdd.persist(StorageLevel.DISK_ONLY_3) # 仅缓存到硬盘，生成3个副本rdd.persist(StorageLevel.MEMORY_AND_DISK) # 先在内存缓存，内存不够缓存到硬盘rdd.persist(StorageLevel.MEMORY_AND_DISK_2) # 先在内存缓存，内存不够缓存到硬盘，生成2个副本rdd.persist(StorageLevel.OFF_HEAP) # 堆外内存rdd.unpersist() # 主动清理缓存 e.g. 1234567891011121314151617181920212223import timefrom pyspark import SparkConf, SparkContextfrom pyspark.storagelevel import StorageLevelconf = SparkConf().setAppName(\"test\").setMaster(\"local[*]\")sc = SparkContext(conf=conf)rdd1 = sc.textFile(\"hdfs://master:8020/input/words.txt\")rdd2 = rdd1.flatMap(lambda x: x.split(\" \"))rdd3 = rdd2.map(lambda x: (x, 1))rdd3.cache()rdd3.persist(StorageLevel.MEMORY_AND_DISK_2)rdd4 = rdd3.reduceByKey(lambda a, b: a + b)print(rdd4.collect())rdd5 = rdd3.groupByKey()rdd6 = rdd5.mapValues(lambda x: sum(x))print(rdd6.collect())rdd3.unpersist()time.sleep(100000) 12[(&apos;hadoop&apos;, 1), (&apos;hello&apos;, 3), (&apos;spark&apos;, 1), (&apos;flink&apos;, 1)][(&apos;hadoop&apos;, 1), (&apos;hello&apos;, 3), (&apos;spark&apos;, 1), (&apos;flink&apos;, 1)] 注：缓存分散存储在各Executor所在服务器中。缓存从设计上来说是不安全的，缓存一旦丢失，需要重新计算缓存，必须保留被缓存RDD的前置血缘关系 RDD CheckPoint用于保存RDD数据，仅支持硬盘存储，且可以写入HDFS（cache不行），从设计上来说是安全的，不保留RDD的前置血缘关系 ChickPoint集中收集各个分区的数据进行存储，而非cache的分散存储 e.g. 12345678910111213141516171819202122232425import timefrom pyspark import SparkConf, SparkContextfrom pyspark.storagelevel import StorageLevelconf = SparkConf().setAppName(\"test\").setMaster(\"local[*]\")sc = SparkContext(conf=conf)# 1. 告知spark, 开启CheckPoint功能sc.setCheckpointDir(\"hdfs://master:8020/output/ckp\")rdd1 = sc.textFile(\"hdfs://master:8020/input/words.txt\")rdd2 = rdd1.flatMap(lambda x: x.split(\" \"))rdd3 = rdd2.map(lambda x: (x, 1))# 调用checkpoint API 保存数据即可rdd3.checkpoint()rdd4 = rdd3.reduceByKey(lambda a, b: a + b)print(rdd4.collect())rdd5 = rdd3.groupByKey()rdd6 = rdd5.mapValues(lambda x: sum(x))print(rdd6.collect())rdd3.unpersist()time.sleep(100000) 12[(&apos;hadoop&apos;, 1), (&apos;hello&apos;, 3), (&apos;spark&apos;, 1), (&apos;flink&apos;, 1)][(&apos;hadoop&apos;, 1), (&apos;hello&apos;, 3), (&apos;spark&apos;, 1), (&apos;flink&apos;, 1)]","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"},{"name":"pyspark","slug":"pyspark","permalink":"http://yoursite.com/tags/pyspark/"}]},{"title":"Pyspark","slug":"Pyspark","date":"2022-12-05T08:45:40.000Z","updated":"2022-12-07T05:47:29.586Z","comments":true,"path":"2022/12/05/Pyspark/","link":"","permalink":"http://yoursite.com/2022/12/05/Pyspark/","excerpt":"Pyspark库安装（本文基于上文spark基础） Python库安装在三台机器分别安装pyspark 12conda activate pysparkpip install pyspark -i https://mirror.baidu.com/pypi/simple Windows补丁将hadoop.dll置于C:/windows/system32/目录下，然后配置hadoop工具包的环境变量 安装相关python库 1pip install pyspark pyhive pymysql jieba -i https://mirror.baidu.com/pypi/simple 为pycharm添加ssh解释器环境","text":"Pyspark库安装（本文基于上文spark基础） Python库安装在三台机器分别安装pyspark 12conda activate pysparkpip install pyspark -i https://mirror.baidu.com/pypi/simple Windows补丁将hadoop.dll置于C:/windows/system32/目录下，然后配置hadoop工具包的环境变量 安装相关python库 1pip install pyspark pyhive pymysql jieba -i https://mirror.baidu.com/pypi/simple 为pycharm添加ssh解释器环境 SparkContext对象Spark Application程序的入口为SparkContext。任何一个spark应用都要先构建SparkContext对象： 创建SparkConf对象 基于SparkConf创建SparkContext 12conf = SparkConf().setAppName(appName).setMaster(master)sc = SparkContext(conf=conf) WorldCount测试程序Windows在pycharm中新建python脚本，通过解释器执行 1234567891011121314151617181920212223242526272829303132# coding:utf8from pyspark import SparkConf, SparkContext# 提交到yarn集群执行时，需配置环境变量# import os# os.environ[\"HADOOP_CONF_DIR\"] = \"/usr/local/hadoop/etc/hadoop\"# os.environ[\"YARN_CONF_DIR\"] = \"/usr/local/hadoop/etc/hadoop\"if __name__ == '__main__': # 通过脚本执行时无需在代码中指定Master # conf = SparkConf().setAppName(\"WordCountHelloWorld\") # 直接在pycharm执行 conf = SparkConf().setAppName(\"WordCountHelloWorld\").setMaster(\"local[*]\") # 通过SparkConf对象构建SparkContext对象 sc = SparkContext(conf=conf) # 需求 : wordcount单词计数, 读取HDFS上的words.txt文件, 对其内部的单词统计出现 的数量 # 读取hdfs文件 file_rdd = sc.textFile(\"hdfs://master:8020/input/words.txt\") # 读取本地文件 # file_rdd = sc.textFile(\"/usr/local/words.txt\") # 将单词进行切割, 得到一个存储全部单词的集合对象 words_rdd = file_rdd.flatMap(lambda line: line.split(\" \")) # 将单词转换为元组对象, key是单词, value是数字1 words_with_one_rdd = words_rdd.map(lambda x: (x, 1)) # 将元组的value 按照key来分组, 对所有的value执行聚合操作(相加) result_rdd = words_with_one_rdd.reduceByKey(lambda a, b: a + b) # 通过collect方法收集RDD的数据打印输出结果 print(result_rdd.collect()) CentOS在根目录创建一份py脚本，通过spark客户端执行 12/usr/local/spark/bin/spark-submit --master local[*] /root/helloworld.py/usr/local/spark/bin/spark-submit --master yarn /root/helloworld.py 基本原理Code构建SparkContext对象等非任务处理由Driver执行，RDD数据任务处理由Executor执行，再由Driver处理分布式计算结果 Master Nodespark自身的JVM框架JVM Driver和JVM Executor之间可以相互通讯，Python通过构建SparkContext对象与JVM Driver进行连接（Python的Driver代码翻译成JVM代码-py4j库，变成JVM Driver） Worker NodeDriver的操作指令发送给JVM Executor（RPC），JVM Executor再通过pyspark守护进程将指令发送给pyspark守护进程，pyspark守护进程将指令调度到运行的python进程中去。Executor端本质上是由python进程再工作 Driver段是直接由py4j直接翻译过去，Executor端则是转发","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"},{"name":"pyspark","slug":"pyspark","permalink":"http://yoursite.com/tags/pyspark/"}]},{"title":"Spark基础","slug":"Spark基础","date":"2022-12-01T07:40:17.000Z","updated":"2023-04-13T01:43:42.844Z","comments":true,"path":"2022/12/01/Spark基础/","link":"","permalink":"http://yoursite.com/2022/12/01/Spark基础/","excerpt":"定义：Apache Spark是用于大规模数据处理的统一分析引擎。其核心数据结构：弹性分布式数据集（RDD）能够在大规模集群中做内存运算，且具有一定容错方式。 Spark框架组成· Spark Core：以RDD为数据抽象，提供Python、Java、Scala、R语言的API和Spark的核心功能，是Spark运行的基础； · SparkSQL：基于SparkCore，提供机构化数据处理模块，支持以SQL语言对数据进行处理；同时可以作为StructuredStreaming模块的基础，进行数据流式计算； · SparkStreaming：基于SparkCore，提供数据流式计算； · MLlib：基于SparkCore，内置大量机器学习库和算法API，进行机器学习计算； · GraphX：基于SparkCore，提供了大量图计算API，用于分布式图计算 运行模式· 本地模式（单机/local）：以一个独立进程，通过内部的多线程模拟Spark运行环境 · Standalone模式（集群）：Spark的各个角色以独立进程形式存在，组成集群环境 · Hadoop YARN模式（集群）：Spark的各个角色运行在YARN容器内部，组成集群环境 · Kubernetes模式（容器集群）：Spark的各个角色运行在Kubernetes容器内部，组成集群环境 · 云服务模式 角色 资源 · Master角色：集群资源管理 · Worker角色：单机资源管理(所在服务器资源管理) 任务 · Driver角色：单个任务管理 · Executor角色：单个任务计算","text":"定义：Apache Spark是用于大规模数据处理的统一分析引擎。其核心数据结构：弹性分布式数据集（RDD）能够在大规模集群中做内存运算，且具有一定容错方式。 Spark框架组成· Spark Core：以RDD为数据抽象，提供Python、Java、Scala、R语言的API和Spark的核心功能，是Spark运行的基础； · SparkSQL：基于SparkCore，提供机构化数据处理模块，支持以SQL语言对数据进行处理；同时可以作为StructuredStreaming模块的基础，进行数据流式计算； · SparkStreaming：基于SparkCore，提供数据流式计算； · MLlib：基于SparkCore，内置大量机器学习库和算法API，进行机器学习计算； · GraphX：基于SparkCore，提供了大量图计算API，用于分布式图计算 运行模式· 本地模式（单机/local）：以一个独立进程，通过内部的多线程模拟Spark运行环境 · Standalone模式（集群）：Spark的各个角色以独立进程形式存在，组成集群环境 · Hadoop YARN模式（集群）：Spark的各个角色运行在YARN容器内部，组成集群环境 · Kubernetes模式（容器集群）：Spark的各个角色运行在Kubernetes容器内部，组成集群环境 · 云服务模式 角色 资源 · Master角色：集群资源管理 · Worker角色：单机资源管理(所在服务器资源管理) 任务 · Driver角色：单个任务管理 · Executor角色：单个任务计算 环境搭建Hadoop伪分布式搭建准备master虚拟机 配置静态ip 1vim /etc/sysconfig/network-scripts/ifcfg-ens33 1234567891011#修改BOOTPROTO=&quot;static&quot;ONBOOT=&quot;yes&quot;# 新增IPADDR=&quot;192.168.80.129&quot;GATEWAY=&quot;192.168.80.2&quot;NETMASK=&quot;255.255.255.0&quot;DNS1=&quot;8.8.8.8&quot;DNS2=&quot;114.114.114.114&quot;IPV6_PRIVACY=&quot;no&quot; 重启网卡服务 1service network restart 关闭防火墙，并禁止防火墙开机自启 12systemctl stop firewalldsystemctl disable firewalld 关闭selinux 1vim /etc/selinux/config 1234567891011# This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:# enforcing - SELinux security policy is enforced.# permissive - SELinux prints warnings instead of enforcing.# disabled - No SELinux policy is loaded.SELINUX=disabled# SELINUXTYPE= can take one of three values:# targeted - Targeted processes are protected,# minimum - Modification of targeted policy. Only selected processes are protected. # mls - Multi Level Security protection.SELINUXTYPE=targeted 重启master 1reboot 重启完成后将相关软件包（jdk、hadoop、spark等）导入至/usr/local目录下并解压 解压jdk 1tar -zxvf jdk-8u211-linux-x64.tar.gz -C /usr/local 配置jdk环境变量 1vim /etc/profile 12345#set java environment JAVA_HOME=/usr/local/jdk1.8.0_211CLASSPATH=.:$JAVA_HOME/lib PATH=$JAVA_HOME/bin:$PATH export JAVA_HOME CLASSPATH PATH 重启环境变量 1source /etc/profile 检查安装 1java -version 123java version &quot;1.8.0_211&quot;Java(TM) SE Runtime Environment (build 1.8.0_211-b12)Java HotSpot(TM) 64-Bit Server VM (build 25.211-b12, mixed mode) 配置linux集群将master完整克隆两台node1、node2机器，配置静态ip 配置主机名，分别执行： 1vim /etc/hostname 将三台机器的主机名设置为master、node1、node2 配置三台虚拟机的域名映射 1vim /etc/hosts 123456127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.80.129 master192.168.80.130 node1192.168.80.131 node2 此时三台机器已经可以互相ping通 123ping masterping node1ping node2 分别重启三台机器 生成三台机器的公钥和私钥，分别执行： 1ssh-keygen -t rsa 反复回车，在/root/.ssh隐藏目录下生成私钥id_rsa和公钥id_rsa.pub 123456789101112131415161718192021Generating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa): Created directory &apos;/root/.ssh&apos;.Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:SHA256:/Vd3sEsHhz3CcxGwbRp5lj6+srZ7OpwAvyKS+qoTi9Q root@masterThe key&apos;s randomart image is:+---[RSA 2048]----+| ..o.|| . +oo|| B+Oo|| o @=.|| . S + .oo=|| o E + ..++||o o . = +. ||.o o . . . B ..|| .oo+.. . . .=O. |+----[SHA256]-----+ 在三台虚拟机执行命令将公钥拷贝到master 1ssh-copy-id master 123456789101112The authenticity of host &apos;master (192.168.80.129)&apos; can&apos;t be established.ECDSA key fingerprint is SHA256:9UGNQgh5SWXh/1Z9iWTOzBSbqXf8kjbxc5SC73j9ct4.ECDSA key fingerprint is MD5:ee:b1:5d:3c:a5:2b:2e:08:cd:85:44:68:fe:c7:29:d9.Are you sure you want to continue connecting (yes/no)? yes/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysroot@master&apos;s password: Number of key(s) added: 1Now try logging into the machine, with: &quot;ssh &apos;master&apos;&quot;and check to make sure that only the key(s) you wanted were added. 将master的公钥拷贝到node上 12scp /root/.ssh/authorized_keys node1:/root/.sshscp /root/.ssh/authorized_keys node2:/root/.ssh 1234567The authenticity of host &apos;node1 (192.168.80.130)&apos; can&apos;t be established.ECDSA key fingerprint is SHA256:9UGNQgh5SWXh/1Z9iWTOzBSbqXf8kjbxc5SC73j9ct4.ECDSA key fingerprint is MD5:ee:b1:5d:3c:a5:2b:2e:08:cd:85:44:68:fe:c7:29:d9.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added &apos;node1,192.168.80.130&apos; (ECDSA) to the list of known hosts.root@node1&apos;s password: authorized_keys 100% 1177 1.2MB/s 00:00 测试ssh免密登录 123ssh node1ssh node2exit 安装hadoop解压安装包并重命名 12tar -zxvf hadoop-3.3.1.tar.gz -C /usr/localmv hadoop-3.3.1 hadoop 修改配置文件 1cd /usr/local/hadoop/etc/hadoop hadoop-env.sh 1vim hadoop-env.sh 123456export JAVA_HOME=/usr/local/jdk1.8.0_211export HDFS_NAMENODE_USER=rootexport HDFS_DATANODE_USER=rootexport HDFS_SECONDARYNAMENODE_USER=rootexport YARN_RESOURCEMANAGER_USER=rootexport YARN_NODEMANAGER_USER=root core-site.xml 1vim core-site.xml 123456789101112131415161718192021222324252627282930&lt;configuration&gt; &lt;!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:8020&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置Hadoop本地保存数据路径 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/local/data/hadoop&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置HDFS web UI用户身份 --&gt; &lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;!-- 整合hive 用户代理设置 --&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml 1vim hdfs-site.xml 1234567&lt;configuration&gt; &lt;!-- 设置SNN进程运行机器位置信息 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;node1:9868&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; mapred-site.xml 1vim mapred-site.xml 12345678910111213141516171819202122232425262728293031323334&lt;configuration&gt; &lt;!-- 设置MR程序默认运行模式： yarn集群模式 local本地模式 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;!-- MR程序历史服务器端地址 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;master:10020&lt;/value&gt; &lt;/property&gt; &lt;!-- 历史服务器web端地址 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;master:19888&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.map.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.reduce.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site.xml 1vim yarn-site.xml 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;configuration&gt; &lt;!-- 设置YARN集群主角色运行机器位置 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;master&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- 是否将对容器实施物理内存限制 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;!-- 是否将对容器实施虚拟内存限制。 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;!-- 开启日志聚集 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置yarn历史服务器地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://master:19888/jobhistory/logs&lt;/value&gt; &lt;/property&gt; &lt;!-- 保存的时间7天 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; workers 1vim workers 123masternode1node2 slaves 1vim workers 123masternode1node2 将安装包分发至其他机器 123cd /usr/localscp -r hadoop root@node1:/usr/localscp -r hadoop root@node2:/usr/local 配置hadoop环境变量，分发至其他机器 1vim /etc/profile 123# set hadoop envexport HADOOP_HOME=/usr/local/hadoopexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 12scp /etc/profile node1:/etc/scp /etc/profile node2:/etc/ 在三台机器分别重启环境变量 1source /etc/profile 首次启动，先格式化namenode 1./bin hdfs namenode -format 启动 123start-dfs.shstart-yarn.shmapred --daemon start historyserver 或者 12start-all.shmapred --daemon start historyserver 查看任务 1jps 12345661488 Jps60417 NameNode60597 DataNode60981 ResourceManager61133 NodeManager61567 JobHistoryServer 查看web页面（hadoop3.0版本以后web端口跟改为9870） master:9870 关闭任务 123stop-dfs.shstop-yarn.shmapred --daemon stop historyserver 或者 12stop-all.shmapred --daemon stop historyserver Anaconda3安装1sh ./Anaconda3-2021.05-Linux-x86_64.sh 配置清华源 1vim ~/.condarc 1234567891011121314channels: - defaultsshow_channel_urls: truedefault_channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2custom_channels: conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud 创建pyspark虚拟环境 1conda create -n pyspark python=3.8 Spark local模式搭建安装解压spark安装包并重命名 12tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /usr/localmv spark-3.2.0-bin-hadoop3.2 spark 配置环境变量 1vim /etc/profile 123export SPARK_HOME=/usr/local/sparkexport PYSPARK_PYTHON=/usr/local/anaconda3/envs/pyspark/bin/python3.8export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop 1source /etc/profile 注：查找pyspark虚拟环境位置： 1cd /usr/local/anaconda3/envs/pyspark/bin 1vim ~/.bashrc 12export JAVA_HOME=/usr/local/jdk1.8.0_211export PYSPARK_PYTHON=/usr/local/anaconda3/envs/pyspark/bin/python3.8 启动pyspark交互式解释器1./bin/pyspark 12345678910111213141516171819Python 3.8.8 (default, Apr 13 2021, 19:58:26) [GCC 7.3.0] :: Anaconda, Inc. on linuxType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.Using Spark&apos;s default log4j profile: org/apache/spark/log4j-defaults.propertiesSetting default log level to &quot;WARN&quot;.To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).22/12/02 17:38:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableWelcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ &apos;_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 3.2.0 /_/Using Python version 3.8.8 (default, Apr 13 2021 19:58:26)Spark context Web UI available at http://master:4040Spark context available as &apos;sc&apos; (master = local[*], app id = local-1669973937905).SparkSession available as &apos;spark&apos;.&gt;&gt;&gt; 进入浏览器任务页面master:4040，可以查看信息 执行一条pyspark指令后： Spark StandAlone模式搭建StandAloneStandAlone模式是Spark自带的集群模式，Master角色以Master进程形式存在，Worker角色以Worker进程形式存在。其中Driver角色运行在Master进程内，Executor角色运行在Worker进程内。此外，还可以开启第三个进程：历史服务器（HistoryServer），用于保存Spark app运行后的事件日志。 环境分发12scp Anaconda3-2021.05-Linux-x86_64.sh node1:`pwd`/scp Anaconda3-2021.05-Linux-x86_64.sh node2:`pwd`/ 进入node1、node2安装anaconda，同master：配置conda源，创建虚拟环境 将master:/etc/profile和bashrc中的环境变量复制到node1、2中 1234scp /etc/profile node1:/etc/profilescp /etc/profile node2:/etc/profilescp ~/.bashrc node1:~/scp ~/.bashrc node2:~/ 创建hadoop用户（仅有root用户可跳过）hadoop用户拥有yarn的最高权限 新建用户 12sudo adduser hadooppasswd hadoop 添加用户组 1sudo usermod -a -G hadoop hadoop 赋予root权限 1vim /etc/sudoers 123## Allow root to run any commands anywhere root ALL=(ALL) ALLhadoop ALL=(ALL) ALL 添加权限 123cd /usr/local/chown -R hadoop:hadoop hadoop*chown -R hadoop:hadoop spark* 配置spark配置文件1234su - hadoopcd /usr/local/spark/confmv workers.template workersvim workers 123masternode1node2 12mv spark-env.sh.template spark-env.shvim spark-env.sh 123456789101112131415161718192021222324252627## 设置JAVA安装目录JAVA_HOME=/usr/local/jdk1.8.0_211## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoopYARN_CONF_DIR=/usr/local/hadoop/etc/hadoop## 指定spark master的IP和提交任务的通信端口# 告知spark的master运行在哪个机器上export SPARK_MASTER_HOST=master# 告知sparkmaster的通讯端口export SPARK_MASTER_PORT=7077# 告知spark master的 webui端口SPARK_MASTER_WEBUI_PORT=8080# worker cpu可用核数SPARK_WORKER_CORES=1# worker可用内存SPARK_WORKER_MEMORY=1g# worker的工作通讯地址SPARK_WORKER_PORT=7078# worker的 webui地址SPARK_WORKER_WEBUI_PORT=8081## 设置历史服务器# 配置的意思是 将spark程序运行的历史日志 存到hdfs的/sparklog文件夹中SPARK_HISTORY_OPTS=\"-Dspark.history.fs.logDirectory=hdfs://master:8020/sparklog/ -Dspark.history.fs.cleaner.enabled=true\" 启动hadoop 1hadoop fs -ls / 此时没有sparklog文件。创建sparklog文件,赋予权限 12hadoop fs -mkdir /sparkloghadoop fs -chmod 777 /sparklog 继续配置 12mv spark-defaults.conf.template spark-defaults.confvim spark-defaults.conf 123456# 开启spark的日期记录功能spark.eventLog.enabled true# 设置spark日志记录的路径spark.eventLog.dir hdfs://master:8020/sparklog/ # 设置spark日志是否启动压缩spark.eventLog.compress true 12mv log4j.properties.template log4j.propertiesvim log4j.properties 将INFO改为WARN，减少冗余日志 12# Set everything to be logged to the consolelog4j.rootCategory=WARN, console 分发spark配置文件123cd /usr/localscp -r spark node1:`pwd`/scp -r spark node2:`pwd`/ 启动spark集群启动历史服务器进程 123cd sparksbin/start-history-server.shjps 123456749297 JobHistoryServer49778 HistoryServer48296 DataNode49835 Jps48734 ResourceManager48910 NodeManager48127 NameNode 启动集群 12sbin/start-all.shjps 12345678949297 JobHistoryServer50162 Jps50020 Master48296 DataNode50107 Worker48734 ResourceManager48910 NodeManager48127 NameNode49903 HistoryServer 进入master:8080web端口可以看到spark集群界面 StandAlone集群测试12cd /usr/local/spark/bin./pyspark --master spark://master:7077 12345678910111213141516Python 3.8.15 (default, Nov 24 2022, 15:19:38) [GCC 11.2.0] :: Anaconda, Inc. on linuxType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.22/12/05 11:54:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableWelcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ &apos;_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 3.2.0 /_/Using Python version 3.8.15 (default, Nov 24 2022 15:19:38)Spark context Web UI available at http://master:4040Spark context available as &apos;sc&apos; (master = spark://master:7077, app id = app-20221205115438-0000).SparkSession available as &apos;spark&apos;.&gt;&gt;&gt; 在/usr/local下创建一个words.txt文件，写入 123hadoop spark flinkhadoop spark flink hadoop hadoophadoop spark flink hadoop hadoop spark spark 创建input文件夹，将文件上传至hdfs 1234hadoop fs -mkdir /input/hdfs dfs -put /usr/local/words.txt /input/hadoop fs -ls /inputhadoop fs -cat /input/words.txt 执行： 1sc.textFile(\"hdfs://master:8020/input/words.txt\").flatMap(lambda line: line.split(\" \")).map(lambda x:(x,1)).reduceByKey(lambda a,b:a+b).collect() 1[(&apos;hadoop&apos;, 7), (&apos;spark&apos;, 5), (&apos;flink&apos;, 3)] Spark on YARN 将spark部署到yarn集群中可以提高对资源的利用率，无需部署spark集群，只需要一台充当spark客户端的服务器即可提交任务到yarn集群运行 Master角色由yarn的ResourceManager担任 Worker角色由yarn的NodeManager担任 Driver角色运行在yarn容器内或提交任务的客户端进程中 Executor运行在yarn提供的容器内 让spark计算任务运行在yarn容器内部，资源管理交友yarn的ResourceManager和NodeManager代替 启动Spark on YARN123cd /usr/local/spark./sbin/stop-all.sh #关闭standalone集群bin/pyspark --master yarn 12345678910111213141516171819Python 3.8.15 (default, Nov 24 2022, 15:19:38) [GCC 11.2.0] :: Anaconda, Inc. on linuxType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.22/12/05 15:27:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable22/12/05 15:27:49 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME. Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ &apos;_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 3.2.0 /_/Using Python version 3.8.15 (default, Nov 24 2022 15:19:38)Spark context Web UI available at http://master:4040Spark context available as &apos;sc&apos; (master = yarn, app id = application_1670210110128_0001).SparkSession available as &apos;spark&apos;.&gt;&gt;&gt; sc.parallelize([1,2,3,4,5]).map(lambda x:x*10).collect()[10, 20, 30, 40, 50] &gt;&gt;&gt; 执行程序测试，或通过spark客户端spark-submit提交代码，spark‘算法会运行在容器中 1./bin/spark-submit --master yarn /usr/local/spark/examples/src/main/python/pi.py 100 Spark on YARN部署模式 Cluster（集群模式） Driver运行在yarn容器内部，和ApplicationMaster在同一个容器内 Client（客户端模式） Driver运行在客户端进程中，例如Driver运行在spark-submit客户端的进程中 其中集群模式在容器内进行通讯，效率高，但是日志同样存放于容器内部 Client1bin/spark-submit --master yarn --deploy-mode client /usr/local/spark/examples/src/main/python/pi.py 100 Cluster1bin/spark-submit --master yarn --deploy-mode cluster /usr/local/spark/examples/src/main/python/pi.py 100","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"}]},{"title":"Nebula-全文检索","slug":"Nebula-全文检索","date":"2022-11-25T06:17:41.000Z","updated":"2022-11-25T08:35:41.947Z","comments":true,"path":"2022/11/25/Nebula-全文检索/","link":"","permalink":"http://yoursite.com/2022/11/25/Nebula-全文检索/","excerpt":"连接ES添加Nebula全文索引模板文件http://localhost:5601/app/dev_tools#/console 1234567891011121314151617curl -H \"Content-Type: application/json; charset=utf-8\" -XPUT http://localhost:9200/_template/nebula_index_template -d '&#123; \"template\": \"nebula*\", \"settings\": &#123; \"index\": &#123; \"number_of_shards\": 3, \"number_of_replicas\": 1 &#125; &#125;, \"mappings\": &#123; \"properties\" : &#123; \"tag_id\" : &#123; \"type\" : \"long\" &#125;, \"column_id\" : &#123; \"type\" : \"text\" &#125;, \"value\" :&#123; \"type\" : \"keyword\"&#125; &#125; &#125;&#125;' 登录ES客户端SIGN IN TEXT SERVICE (localhost:9200,&quot;admin&quot;,&quot;123456&quot;) 查看ES客户端详情SHOW TEXT SEARCH CLIENTS; 退出ES客户端SIGN OUT TEXT SERVICE 配置Nebula安装storage服务进入/usr/local/nebula根目录下，进入/etc子目录，找到nebula-storaged-listener.conf.production文件，复制一份并去除.production后缀。将文件中的listenr地址改为真实地址。 随后启动listener：./bin/nebula-storaged --flagfile etc/nebula-storaged-listener.conf 添加listenerADD LISTENER ELASTICSEARCH 192.168.100.1:9789,192.168.100.2:9789;(注：如果有多台图库集群，都要配置) 进入nebula-console，执行SHOW LISTENER可以查看图空间的所有listener。 要删除所有listener，执行：REMOVE LISTENER ELASTICSEARCH(注：一个图空间仅可执行一次)","text":"连接ES添加Nebula全文索引模板文件http://localhost:5601/app/dev_tools#/console 1234567891011121314151617curl -H \"Content-Type: application/json; charset=utf-8\" -XPUT http://localhost:9200/_template/nebula_index_template -d '&#123; \"template\": \"nebula*\", \"settings\": &#123; \"index\": &#123; \"number_of_shards\": 3, \"number_of_replicas\": 1 &#125; &#125;, \"mappings\": &#123; \"properties\" : &#123; \"tag_id\" : &#123; \"type\" : \"long\" &#125;, \"column_id\" : &#123; \"type\" : \"text\" &#125;, \"value\" :&#123; \"type\" : \"keyword\"&#125; &#125; &#125;&#125;' 登录ES客户端SIGN IN TEXT SERVICE (localhost:9200,&quot;admin&quot;,&quot;123456&quot;) 查看ES客户端详情SHOW TEXT SEARCH CLIENTS; 退出ES客户端SIGN OUT TEXT SERVICE 配置Nebula安装storage服务进入/usr/local/nebula根目录下，进入/etc子目录，找到nebula-storaged-listener.conf.production文件，复制一份并去除.production后缀。将文件中的listenr地址改为真实地址。 随后启动listener：./bin/nebula-storaged --flagfile etc/nebula-storaged-listener.conf 添加listenerADD LISTENER ELASTICSEARCH 192.168.100.1:9789,192.168.100.2:9789;(注：如果有多台图库集群，都要配置) 进入nebula-console，执行SHOW LISTENER可以查看图空间的所有listener。 要删除所有listener，执行：REMOVE LISTENER ELASTICSEARCH(注：一个图空间仅可执行一次) 刷表创建图空间1CREATE SPACE nebula_graph (partition_num = 20, replica_factor = 2, vid_type = FIXED_STRING(128)) Tag123CREATE TAG IF NOT EXISTS player(name string, age string);CREATE TAG INDEX IF NOT EXISTS name ON player(name(20),age(10));CREATE FULLTEXT TAG INDEX nebula_index_1 ON player(name,age); Edge Type1CREATE EDGE IF NOT EXISTS edge_player(created_time TIMESTAMP NULL DEFAULT now()); 全文索引注：1.要将图库索引同步到es中去，需要在创建索引时将索引以nebula为开头命名。 2.执行REBUILD FULLTEXT INDEX将索引同步到es，此时es中还未完成索引的创建，需要在图库录入数据后自动同步到es（可能存在延迟，时间不定） 3.仅可使用LOOKUP语句进行模糊检索 4.删除图空间，全文索引不会自动删除，需要手动执行：DROP FULLTEXT INDEX &lt;nebula_index*&gt; 查看全文索引SHOW FULLTEXT INDEXES 数据测试插入数据123456789101112INSERT VERTEX player(name, age) VALUES \\ \"Russell Westbrook\": (\"Russell Westbrook\", 30), \\ \"Chris Paul\": (\"Chris Paul\", 33),\\ \"Boris Diaw\": (\"Boris Diaw\", 36),\\ \"David West\": (\"David West\", 38),\\ \"Danny Green\": (\"Danny Green\", 31),\\ \"Tim Duncan\": (\"Tim Duncan\", 42),\\ \"James Harden\": (\"James Harden\", 29),\\ \"Tony Parker\": (\"Tony Parker\", 36),\\ \"Aron Baynes\": (\"Aron Baynes\", 32),\\ \"Ben Simmons\": (\"Ben Simmons\", 22),\\ \"Blake Griffin\": (\"Blake Griffin\", 30); 模糊检索LOOKUP ON player WHERE PREFIX(player.name, &quot;B&quot;); LOOKUP ON player WHERE WILDCARD(player.name, &quot;*ri*&quot;) YIELD player.name, player.age; LOOKUP ON player WHERE WILDCARD(player.name, &quot;*ri*&quot;) | YIELD count(*); LOOKUP ON player WHERE REGEXP(player.name, &quot;R.*&quot;) YIELD player.name, player.age; LOOKUP ON player WHERE REGEXP(player.name, &quot;.*&quot;); LOOKUP ON player WHERE FUZZY(player.name, &quot;Tim Dunncan&quot;, AUTO, OR, 100) YIELD player.name;","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"}],"tags":[{"name":"-database -nebula","slug":"database-nebula","permalink":"http://yoursite.com/tags/database-nebula/"}]},{"title":"Docker","slug":"Docker","date":"2022-09-16T07:15:39.000Z","updated":"2023-02-14T03:51:35.127Z","comments":true,"path":"2022/09/16/Docker/","link":"","permalink":"http://yoursite.com/2022/09/16/Docker/","excerpt":"CentOS Docker安装CentOS7要求64位，且内核版本高于3.10 查看CentOS内核版本：uname -r · 安装Docker：卸载旧版本 12345678$ sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine · 安装所需软件包 123$ sudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 · 设置Docker仓库（清华源） 123$ sudo yum-config-manager \\ --add-repo \\ https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/centos/docker-ce.repo · 安装Docker Engine-Community 1$ sudo yum install docker-ce docker-ce-cli containerd.io","text":"CentOS Docker安装CentOS7要求64位，且内核版本高于3.10 查看CentOS内核版本：uname -r · 安装Docker：卸载旧版本 12345678$ sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine · 安装所需软件包 123$ sudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 · 设置Docker仓库（清华源） 123$ sudo yum-config-manager \\ --add-repo \\ https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/centos/docker-ce.repo · 安装Docker Engine-Community 1$ sudo yum install docker-ce docker-ce-cli containerd.io 启动Docker1$ sudo systemctl start docker 通过运行自带的hello-world镜像来验证是否正确安装了Docker 1$ sudo docker run hello-world 运行应用程序使用docker run命令在容器内运行一个脚本 1$ docker run centos /bin/echo \"hello docker\" 其中centos为指定要运行的镜像。Docker 会先从本地主机上查找镜像是否存在，如果不存在，则到Docker Hub镜像仓库下载公共镜像 运行交互式容器使用docker命令的两个参数 -i -t 实现对运行容器的交互 · -t：在新的容器内指定一个伪终端或终端 · -i：允许对容器内的标准输入（STDIN）进行交互 1$ docker run -i -t centos /bin/bash 运行 exit 命令或者使用键盘CTRL+D退出该容器 后台模式运行容器使用下列命令创建一个以进程方式运行的容器 1$ docker run -d centos /bin/sh -c \"while true; do echo hello world; sleep 1; done\" 执行后并没有输出”hello world”，而是输出了一串字符（容器ID），可以通过该容器ID来查看对应的容器里执行了什么 执行docker ps命令查看运行的容器，其中： · CONTAINER ID ：容器ID · IMAGE：使用的镜像 · COMMAND：启动容器时执行的命令 · CREATED：容器创建时间 · STATUS：容器状态（created：已创建；restarting：重启中；running/Up：运行中；removing：迁移中；paused：暂停中；exited：已停止；dead：死亡） · PORTS：端口信息和使用的连接类型（tcp\\udp） · NAMES：自动分配的容器名称 要查看容器内部的标准输出，执行docker logs &lt;CONTAINER ID&gt; 停止容器1$ docker stop &lt;CONTAINER ID&gt; Docker 容器获取镜像假设本地没有ubuntu镜像，使用docker pull命令来载入一个ubuntu镜像 1$ docker pull ubuntu 启动容器1$ docker run -it ubuntu /bin/bash 查看全部容器1$ docker ps -a 启动停止运行的容器12$ docker ps -a # 获取容器ID$ docker start &lt;CONTAINER ID&gt; 后台运行容器-d可以指定容器的运行模式 1$ docker run -itd --name ubuntu-test ubuntu /bin/bash 停止容器1$ docker stop &lt;CONTAINER ID&gt; 重启容器1$ docker restart &lt;CONTAINER ID&gt; 进入容器使用了-d参数后，容器进入后台运行，此时需要进入容器，需要指令docker attach或docker exec 其中docker attach命令下如果容器退出，则容器会停止，而docker exec命令会退出容器终端，但不会导致容器的停止 12$ docker attach &lt;CONTAINER ID&gt;$ docker exec -it &lt;CONTAINER ID&gt; /bin/bash 导出与导入容器导出容器：导出本地某个容器 1$ docker export &lt;CONTAINER ID&gt; &gt; centos.tar 导入容器：将容器快照文件导入到镜像 1$ cat docker/centos.tar | docker import - test/centos:v1 或通过指定URL/目录来导入 1$ docker import http://example.com/exampleimage.tgz example/imagerepo 删除容器1$ docker rm -f &lt;CONTAINER ID&gt; 清理所有处于终止状态的容器： 1$ docker container prune Docker镜像运行容器时，使用的镜像如果在本地中不存在，docker会自动从docker镜像仓库中下载。默认Docker Hub公共镜像源 获取Docker镜像1$ docker images 其中： · REPOSITORY ：镜像仓库源 · TAG：镜像标签 · IMAGE ID：镜像ID · CREATED：镜像创建时间 · SIZE：镜像大小 同一仓库源可以有多个 TAG，代表这个仓库源的不同个版本，如python仓库源里，有3.6.7、3.9.0两个不同的版本了。使用 &lt;REPOSITORY> : &lt;TAG> 来定义不同的镜像 例：使用3.9.0版本的pythob来运行容器，执行： 1$ docker run -it python:3.9.0 /usr/bin/env python 镜像后的/usr/bin/env python是具体的命令，这里的作用是启动交互式python IDLE。如果不指定镜像的版本标签，docker将默认使用python:latest镜像 查找镜像1$ docker search centos 其中： · NAME ：镜像仓库源名称 · DESCRIPTION：镜像描述 · STARS：点赞数 · OFFICIAL：是否为Docker官方发布镜像 · AUTOMATED：自动构建 预加载镜像当在本地主机当中使用不存在的镜像时Docker将会自动下载镜像。如果要预加载镜像，可以使用docker pull命令 1$ docker pull centos:7 下载完成后，就可以直接使用这个镜像运行容器 删除镜像1$ docker rmi &lt;IMAGE ID&gt; 创建自己的镜像· 从已创建容器当中更新镜像通过镜像进入一个容器 1$ docker run -it centos:7 bin/bash 例：安装vim 1$ yum install vim 执行exit命令退出容器，接着使用docker commit命令提交容器副本 1$ docker commit -m=\"add vim\" -a=\"9.9\" &lt;CONTAINER ID&gt; centos7:vim 其中： · -m ：镜像描述信息 · -a：镜像作者 · CONTAINER ID：容器ID · centos7：镜像名称 · vim：镜像标签 启动安装vim的镜像容器 1$ docker run -it centos:vim /bin/bash · 使用Dockerfile构建镜像使用docker build命令，构建一个全新的镜像。需要在自定目录下创建Dockerfile文件，并添加构建指令 1234567FROM python:3.9.0ADD fastapi /WORKDIR /RUN /usr/local/bin/python -m pip install --upgrade pip -i https://pypi.tuna.tsinghua.edu.cn/simple --trusted-host pypi.tuna.tsinghua.edu.cnRUN pip install fastapi[all] -i https://pypi.tuna.tsinghua.edu.cn/simple --trusted-host pypi.tuna.tsinghua.edu.cnEXPOSE 8089CMD [\"python\",\"main.py\"] 1$ docker images 使用构建的镜像创建容器 1$ docker run -it -p 8089:8089 fastapi:v1.0 设置镜像标签1$ dcoker tag &lt;IMAGE ID&gt; &lt;REPOSITORY&gt;:&lt;TAG&gt; 1$ dcoker tag 8720FB78CF34 fastapi:v1.1 为已有标签的镜像添加标签，该镜像将共用两个标签而不会被替换 运行应用示例启动应用载入并运行镜像，其中-P命令使容器内部使用的网络端口随机映射到主机上 12$ docker pull training/webapp$ docker run -d -P training/webapp python app.py 1$ docker ps 可以通过-p参数来设置端口 1$ docker run -d -p 5000:5000 training/webapp python app.py 查看容器内端口的指令： 1$ docker port &lt;CONTAINER ID&gt; 查看应用程序日志/进程： 12$ docker logs -f &lt;CONTAINER ID&gt;$ docker top &lt;CONTAINER ID&gt; 查看Docker应用底层信息： 1$ docker inspect 停止应用容器1$ docker stop &lt;CONTAINER ID&gt; 重启应用容器对停止的应用：docker start 对运行中的应用：docker restart 查询最后一次创建的容器：docker ps -l 12docker start 707a9f61dd9adocker ps -l 移除应用容器注：容器必须是停止状态1$docker rm &lt;CONTAINER ID&gt;### Docker网络端口映射以fastapi镜像为例· -P：随机映射1$ docker run -it -P &lt;REPOSITORY&gt;:&lt;TAG&gt; ...· -p：指定端口映射、指定容器绑定的ip地址1$ docker run -it -p 8089:8089 &lt;REPOSITORY&gt;:&lt;TAG&gt; ...1$ docker run -it -p 127.0.0.1:8089:8089 &lt;REPOSITORY&gt;:&lt;TAG&gt; ...默认都是绑定tcp端口，如要绑定UPD协议端口，需要在端口后加上\\udp 1$ docker run -it -p 127.0.0.1:8089:8089/udp &lt;REPOSITORY&gt;:&lt;TAG&gt; ... · port：快速查看端口绑定情况 1$ docker port &lt;CONTAINER ID&gt; &lt;PORT&gt; 指定容器名称、用户： 1$ docker run -it -p 127.0.0.1:8089:8089 --name fastapi --user=root &lt;REPOSITORY&gt;:&lt;TAG&gt; ... DockerfileDockerfile是用于构建镜像的文本文件，其中包含了构建镜像时需要用到的各个指令和说明 Dockerfile的指令每执行一次都会在docker上新建一层镜像，如果执行过多会脏成镜像膨胀过大，例如之前的fastapi镜像 1234567FROM python:3.9.0ADD fastapi /WORKDIR /RUN /usr/local/bin/python -m pip install --upgrade pip -i https://pypi.tuna.tsinghua.edu.cn/simple --trusted-host pypi.tuna.tsinghua.edu.cnRUN pip install fastapi[all] -i https://pypi.tuna.tsinghua.edu.cn/simple --trusted-host pypi.tuna.tsinghua.edu.cnEXPOSE 8089CMD [\"python\",\"main.py\"] 可以将其修改为： 1234567FROM python:3.9.0ADD fastapi /WORKDIR /RUN /usr/local/bin/python -m pip install --upgrade pip -i https://pypi.tuna.tsinghua.edu.cn/simple --trusted-host pypi.tuna.tsinghua.edu.cn \\ &amp;&amp; pip install fastapi[all] -i https://pypi.tuna.tsinghua.edu.cn/simple --trusted-host pypi.tuna.tsinghua.edu.cnEXPOSE 8089CMD [\"python\",\"main.py\"] 以&amp;&amp;符号连接的命令，在执行后只会构建一层镜像 构建命令1$ docker build -t &lt;REPOSITORY&gt;:&lt;TAG&gt; . 最后.代表本次执行的上下文路径（打包本机目录下文件的全部内容） Dockerfile常用构建指令· COPY 复制指令，从上下文目录中复制文件或者目录到容器里指定路径 1COPY [--chown=&lt;user&gt;:&lt;group&gt;][&lt;source&gt;,... &lt;target&gt;] 1COPY fastapi / 其中： · [–chown=&lt;user&gt;:&lt;group&gt;] ：可选参数，用户改变复制到容器内文件的拥有者和属组 · &lt;source&gt;：源文件或源目录 · &lt;targer&gt;：目标路径，容器内的指定路径，如果不存在则会自动创建 · ADD 与COPY使用方法类似，主要用于执行的源文件为tar压缩文件情况下，会自动复制并解压到目标路径中 · CMD 123CMD &lt;shell 命令&gt; CMD [\"&lt;可执行文件或命令&gt;\",\"&lt;param1&gt;\",\"&lt;param2&gt;\",...] CMD [\"&lt;param1&gt;\",\"&lt;param2&gt;\",...] # 该写法是为 ENTRYPOINT 指令指定的程序提供默认参数 运行程序，在执行docker run时执行 · ENTRYPOINT 1ENTRYPOINT [\"&lt;executeable&gt;\",\"&lt;param1&gt;\",\"&lt;param2&gt;\",...] 执行docker run时接受命令行参数，但在命令行指定了 –entrypoint 时，将覆盖ENTRYPOINT指令指定的程序，且如果存在多个，仅有最后一个生效 · WORKDIR 指定工作目录。用WORKDIR指定的工作目录，会在构建镜像的每一层中都存在（WORKDIR指定的工作目录，必须是提前创建好的） docker build构建镜像过程中的，每一个RUN命令都是新建的一层。只有通过WORKDIR创建的目录才会一直存在 1WORKDIR &lt;工作目录路径&gt; · EXPOSE 声明端口。在运行时使用随机端口映射时，执行docker run -P，会自动随机映射EXPOSE的端口 1EXPOSE &lt;端口1&gt; [&lt;端口2&gt;...] · ENV 设置环境变量 12ENV &lt;key&gt; &lt;value&gt;ENV &lt;key1&gt;=&lt;value1&gt; &lt;key2&gt;=&lt;value2&gt;... · ARG 构建参数，与 ENV 作用一致。不过作用域不一样。ARG 设置的环境变量仅对 Dockerfile 内有效，也就是说只有 docker build 的过程中有效，构建好的镜像内不存在此环境变量 构建命令 docker build 中可以用 –build-arg &lt;参数名&gt;=&lt;值&gt; 来覆盖 1ARG &lt;参数名&gt;[=&lt;默认值&gt;] · VOLUME 定义匿名数据卷。在启动容器时忘记挂载数据卷，会自动挂载到匿名卷，用于避免重要的数据因容器重启而丢失、避免容器体积不断变大。在执行docker run时，可以通过-v参数修改挂载点 12VOLUME [\"&lt;路径1&gt;\", \"&lt;路径2&gt;\"...]VOLUME &lt;路径&gt; · USER 指定执行后续命令的用户和用户组，用于切换后续命令执行的用户（用户和用户组必须提前已经存在） 1USER &lt;用户名&gt;[:&lt;用户组&gt;] · HEALTHCHECK 指定某个程序或者指令来监控 docker 容器服务的运行状态 123HEALTHCHECK [选项] CMD &lt;命令&gt;：设置检查容器健康状况的命令HEALTHCHECK NONE：如果基础镜像有健康检查指令，使用这行可以屏蔽掉其健康检查指令HEALTHCHECK [选项] CMD &lt;命令&gt; : 这边 CMD 后面跟随的命令使用，可以参考 CMD 的用法。 · ONBUILD 用于延迟构建命令的执行。就是 Dockerfile 里使用ONBUILD指定的命令，在本次构建镜像的过程中不会执行（假设镜像为 test-build）。当有新的Dockerfile使用了之前构建的镜像FROM test-build ，这时执行新镜像的Dockerfile构建时候，会执行test-build的Dockerfile里ONBUILD指定的命令 1ONBUILD &lt;其它指令&gt; · LABEL LABEL 指令以键值对的形式给镜像添加一些元数据（metadata） 1LABEL &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; ... Docker打包打包tar1$ docker save -o &lt;filename&gt; &lt;REPOSITORY&gt;:&lt;TAG&gt; 1$ docker save -o fastapi.tar fastapi:v1.0 加载tar1$ docker load &lt; fastapi.tar 进入容器内部编辑容器运行一个容器 1$ docker run -d -P &lt;REPOSITORY&gt;:&lt;TAG&gt; 进入容器内部 1$ docker exec -it &lt;CONTAINER ID&gt; [bash]/[/bin/sh]/[/bin/bash] 退出容器内部 1$ quit 复制容器内代码文件1$ docker cp &lt;CONTAINER ID&gt;:/file/path/within/container /root/path/target","categories":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/tags/Docker/"}]},{"title":"Nebula-fastapi接口维护文档","slug":"Nebula-fastapi接口维护文档","date":"2022-08-19T09:40:40.000Z","updated":"2022-11-25T09:10:52.210Z","comments":true,"path":"2022/08/19/Nebula-fastapi接口维护文档/","link":"","permalink":"http://yoursite.com/2022/08/19/Nebula-fastapi接口维护文档/","excerpt":"普通接口1.导入fastapi、连接nebula连接池123456789101112131415161718192021222324from fastapi import FastAPIfrom nebula2.gclient.net import ConnectionPoolfrom nebula2.Config import Config# 关闭在线文档，防止攻击app_router = FastAPI(docs_url=None, redoc_url=None)config = Config()config.max_connection_pool_size = 10# 连接超时时间config.timeout = 60000# 关闭空闲连接时间config.idle_time = 0# 检查空闲连接时间间隔config.interval_check = -1# 初始化连接池connection_pool = ConnectionPool()# 如果给定的服务器正常，则返回true，否则返回falseok = connection_pool.init([('host', 9669)], config)if __name__ == \"__main__\": import uvicorn uvicorn.run(app=\"nebula_api:app_router\", reload=True, debug=True, host=host, port=port) 2.CORS跨域访问设置123456789101112131415from fastapi.middleware.cors import CORSMiddleware# CORSorigins = [ \"*\"]app_router.add_middleware( CORSMiddleware, allow_origins=origins, allow_credentials=True, allow_methods=[\"*\"], allow_headers=[\"*\"])","text":"普通接口1.导入fastapi、连接nebula连接池123456789101112131415161718192021222324from fastapi import FastAPIfrom nebula2.gclient.net import ConnectionPoolfrom nebula2.Config import Config# 关闭在线文档，防止攻击app_router = FastAPI(docs_url=None, redoc_url=None)config = Config()config.max_connection_pool_size = 10# 连接超时时间config.timeout = 60000# 关闭空闲连接时间config.idle_time = 0# 检查空闲连接时间间隔config.interval_check = -1# 初始化连接池connection_pool = ConnectionPool()# 如果给定的服务器正常，则返回true，否则返回falseok = connection_pool.init([('host', 9669)], config)if __name__ == \"__main__\": import uvicorn uvicorn.run(app=\"nebula_api:app_router\", reload=True, debug=True, host=host, port=port) 2.CORS跨域访问设置123456789101112131415from fastapi.middleware.cors import CORSMiddleware# CORSorigins = [ \"*\"]app_router.add_middleware( CORSMiddleware, allow_origins=origins, allow_credentials=True, allow_methods=[\"*\"], allow_headers=[\"*\"]) 3.一般查询接口1.单点查询1234567891011121314151617181920212223242526272829303132333435363738394041# 从连接池中获取会话session = connection_pool.get_session('root', 'nebula')session.execute('USE ai_project')data_final = []errorcode = 0result = &#123; \"data\": data_final, \"errorcode\": errorcode&#125;# 举例，查询标签exampledata_select = session.execute( str(f'match (v:example) return id(v),v.name;'))# 查询出错，错误代码1if not data_select.is_succeeded(): session.release() result[\"errorcode\"] = 1 return result# 查询结果不为空，显示数据if not data_select.is_empty(): size = data_select.row_size() for index in range(size): data1 = data_select.row_values(index)[0].as_string() data2 = data_select.row_values(index)[1].as_string() data_dict = &#123; \"entity_id\": data1, \"name\": data2, &#125; data_final.append(data_dict) session.release() return result# 查询类型下实体为空，错误代码2session.release()result[\"errorcode\"] = 2return result 2.树型查询主函数： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# 自定义类和函数from api_class import *session = connection_pool.get_session('root', 'nebula')session.execute('USE ai_project')data_final = []errorcode = 0result = &#123; \"data\": data_final, \"errorcode\": errorcode&#125;# 以example_id为根节点data_select = session.execute( str(f'match Ret=(v)-[e:edge_example*0..15]-&gt;(p) where id(v)==\"&#123;example_id&#125;\" return nodes(Ret);'))# 查询出错，错误代码1if not data_select.is_succeeded(): session.release() result[\"errorcode\"] = 1 return result# 查询不为空，显示数据if not data_select.is_empty(): size = data_select.row_size() data_list = [] for index in range(size): data1 = data_select.row_values(index)[0].as_list() item = re.finditer(r'\\(\\\"(.*?)\\\" :(.*?)\\&#123;(.*?)\\&#125;\\)', str(data1)) parent_id = '0' for match in item: kv_dict = &#123; 'parent_id': parent_id, 'entity_id': match.group(1).strip(), 'entity_type': match.group(2).strip() &#125; kvs = match.group(3).replace(\" \", \"\").split(\",\") for kv in kvs: kv_dict[kv.strip().split(':')[0]] = kv.split(':')[1][1:-1] if kv_dict not in data_list: data_list.append(kv_dict) parent_id = match.group(1).strip() # json对象列表转换为树形json对象 data_final = ApiFuncs.list_to_tree(data_list) # 树形json对象按参数排序 ApiFuncs.data_sort(data_final) for item in data_final: del item[\"parent_id\"] session.release() result[\"data\"] = data_final return resultsession.release()result[\"errorcode\"] = 2return result 将json对象列表转换为树形json对象的函数： 1234567891011121314151617181920212223242526272829303132333435363738class ApiFuncs: @staticmethod def list_to_tree(data): root = [] node = [] for d in data: if d.get(\"parent_id\") == '0': root.append(d) else: node.append(d) for p in root: ApiFuncs.add_node(p, node) if len(root) == 0: return node return root @staticmethod def add_node(p, node): p[\"children\"] = [] for n in node: if n.get(\"parent_id\") == p.get(\"entity_id\"): p[\"children\"].append(n) for t in p[\"children\"]: if not t.get(\"children\"): t[\"children\"] = [] t[\"children\"].append(ApiFuncs.add_node(t, node)) if not t[\"children\"]: del t[\"children\"] if len(p[\"children\"]) == 0: return 将树形json对象按参数排序的函数： 1234567891011121314151617class SerialNameError(Exception): def __init__(self, message): self.message = message... @staticmethod def data_sort(data): for elem in data: if elem[\"serial\"] == \"_NULL_\": elem[\"serial\"] = \"0\" try: data.sort(key=lambda x: int(x[\"serial\"])) except ValueError: raise SerialNameError(\"序号似乎无效\") for item in data: if \"children\" in item.keys(): ApiFuncs.data_sort(item[\"children\"]) return 4.一般编辑接口123456# 继承BaseModel类的参数from typing import Listfrom pydantic import BaseModelclass SampleEdit(BaseModel): example_kv: List[dict] 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859session = connection_pool.get_session('root', 'nebula')session.execute('USE ai_project')result = &#123; \"msg\": True, \"errorcode\": 0&#125;for item in eg.example_kv: # 新增 if item[\"type_operation\"] == 'add': \"\"\" ==========增========== \"\"\" temp_id = item[\"name\"] + \"_\" + ApiFuncs.create_time() temp_md5 = ApiFuncs.create_md5(temp_id) # 检索当前编号，创建实体时对编号自增 serial_list = session.execute( str(f'match (v:example) return v.seq;') ) data = [] size = serial_list.row_size() for index in range(size): if serial_list.row_values(index)[0].is_string(): data1 = serial_list.row_values(index)[0].as_string() data.append(int(data1)) if data: count = max(data) count += 1 else: count = 1 session.execute( str(f'INSERT VERTEX example ( name,seq ) VALUES \"&#123;temp_md5&#125;\":(\"&#123;item[\"name\"]&#125;\", \"&#123;count&#125;\");') ) # 删除 elif item[\"type_operation\"] == 'delete': \"\"\" ==========删========== \"\"\" session.execute(str(f'DELETE VERTEX \"&#123;item[\"id\"]&#125;\";')) # 修改 elif item[\"type_operation\"] == 'update': \"\"\" ==========改========== \"\"\" temp_md5 = item[\"id\"] session.execute( str(f'UPDATE VERTEX ON example \"&#123;temp_md5&#125;\" SET name = \"&#123;item[\"name\"]&#125;\";') )# 关闭连接池session.release()return result 测试接口： 1234567891011121314import requestsimport jsondata = &#123; 'example_kv': [ &#123;\"type_operation\": \"add\", \"name\": \"test\"&#125;, &#123;...&#125;, ... ],&#125;body = json.dumps(data)response = requests.post('http://127.0.0.1:8888/example', data=body)print(response.text) 5.功能函数查找树型json结构中关键字 12345678910import redef find_name(json_data, e): for element in json_data: match = re.search(e, element[\"name\"]) if match: id_collect.append(element[\"entity_id\"]) if \"children\" in element.keys(): find_name(element[\"children\"], e) return id_collect 分页 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495from __future__ import annotationsimport mathfrom fastapi import FastAPI, Dependsfrom fastapi_pagination import paginate, add_paginationfrom typing import TypeVar, Generic, Sequencefrom fastapi import Queryfrom fastapi_pagination.bases import AbstractPage, AbstractParams, RawParamsfrom pydantic import BaseModelT = TypeVar(\"T\")class Params(BaseModel, AbstractParams): page: int = Query(1, ge=1, description=\"Page number\") size: int = Query(17, gt=0, le=100, description=\"Page size\") def to_raw_params(self) -&gt; RawParams: return RawParams( limit=self.size, offset=self.size * (self.page - 1), )class Page(AbstractPage[T], Generic[T]): results: Sequence[T] total: int page: int size: int next: str previous: str total_pages: int __params_type__ = Params @classmethod def create( cls, results: results, total: int, params: Params, ) -&gt; Page[T]: page = params.page size = params.size total_pages = math.ceil(total / params.size) next = f\"?page=&#123;page + 1&#125;&amp;size=&#123;size&#125;\" if (page + 1) &lt;= total_pages else \"null\" previous = f\"?page=&#123;page - 1&#125;&amp;size=&#123;size&#125;\" if (page - 1) &gt;= 1 else \"null\" return cls(results=results, total=total, page=params.page, size=params.size, next=next, previous=previous, total_pages=total_pages)app = FastAPI()class User(BaseModel): sample_id: str sample_name: str time: strsample_data = [ &#123; \"sample_id\": \"000001\", \"sample_name\": \"test1\", \"time\": \"2022-08-11 21:15:33\" &#125;, &#123; \"sample_id\": \"000002\", \"sample_name\": \"test2\", \"time\": \"2022-08-12 13:45:56\" &#125;, &#123; \"sample_id\": \"000003\", \"sample_name\": \"test3\", \"time\": \"2022-08-12 13:45:59\" &#125;,]@app.get('/sample', response_model=Page[User])async def get_users(): return paginate(sample_data)add_pagination(app)if __name__ == \"__main__\": import uvicorn uvicorn.run(app=\"1:app\", reload=True, host='127.0.0.1', port=9999) 项目级文件目录结构│ api_class.py│ main.py│├─api│ │ nebuladb.py│ │ proj_1.py│ │ proj_2.py│ │ __init__.py│ ││ └─__pycache__│ nebuladb.cpython-39.pyc│ proj_1cpython-39.pyc│ proj_2.cpython-39.pyc│ __init__.cpython-39.pyc│├─data│ nebula_data.txt│├─logs│ 2022-xx-01.txt│ 2022-xx-02.txt│ 2022-xx-03.txt│ 2022-xx-04.txt│├─scripts│ post_1.py│ test_1.py│├─utils│ │ client.py│ │ creat_data.py│ │ log.py│ │ snapshot_day_by_day.py│ │ __init__.py│ ││ └─__pycache__│ log.cpython-39.pyc│ __init__.cpython-39.pyc│└─__pycache__ api_class.cpython-39.pyc main.cpython-39.pyc 主函数main.py12345678from api import mainfrom fastapi_pagination import add_paginationapp_router = main()add_pagination(app_router)if __name__ == \"__main__\": import uvicorn uvicorn.run(app=\"main:app_router\", reload=True, debug=True, host='0.0.0.0', port=8888) 日志utils/log.py123456789101112131415161718192021222324252627282930313233import osimport loggingimport logging.handlersimport timeclass LogInit(object): __instance = None def __init__(self): _log_dir = os.path.join(os.path.dirname(__file__), '../logs') _log_name = time.strftime('%Y-%m-%d', time.localtime(time.time())) + '.txt' self.logger = logging.getLogger(_log_name) self.logger.setLevel(logging.DEBUG) formatter = logging.Formatter( '%(asctime)s - %(levelname)s - %(filename)s - %(funcName)s - %(lineno)s - %(message)s') if not self.logger.handlers: file_log_handler = logging.handlers.RotatingFileHandler(os.path.join(_log_dir, _log_name), maxBytes=10 * 1024 * 1024, backupCount=3, encoding=\"utf-8\") file_log_handler.setLevel(logging.INFO) file_log_handler.setFormatter(formatter) self.logger.addHandler(file_log_handler) @staticmethod def set_logger(): if not LogInit.__instance: LogInit.__instance = LogInit() return LogInit.__instancelogger = LogInit.set_logger().logger 使用方式： 12345from utils.log import loggerlogger.info(\"信息\")logger.warning(\"警告\")logger.error(\"错误\") 数据库配置api/nebuladb.py1234567891011from nebula2.Config import Configfrom nebula2.gclient.net import ConnectionPool# NEBULAconfig = Config()config.max_connection_pool_size = 10config.timeout = 60000config.idle_time = 0config.interval_check = -1connection_pool = ConnectionPool()ok = connection_pool.init([('192.168.80.128', 9669)], config) 图谱快照保存脚本utils/snapshot_day_by_day.py123456789101112131415161718192021222324252627282930313233343536373839404142434445import scheduleimport timefrom nebula2.gclient.net import ConnectionPoolfrom nebula2.Config import Configconfig = Config()config.max_connection_pool_size = 10config.timeout = 60000config.idle_time = 0config.interval_check = -1connection_pool = ConnectionPool()ok = connection_pool.init([('192.168.80.128', 9669)], config)def add_snapshot(): session = connection_pool.get_session('root', 'nebula') session.execute('USE ai_project') session.execute(str('CREATE SNAPSHOT'))def del_snapshot(): session = connection_pool.get_session('root', 'nebula') session.execute('USE ai_project') data = session.execute(str('SHOW SNAPSHOTS')) datas = [] if not data.is_empty(): size = data.row_size() for index in range(size): data1 = data.row_values(index)[0].as_string() datas.append(data1) if len(datas) &gt; 3: data_final = datas[:-3] for i in data_final: session.execute(str(f'DROP SNAPSHOT &#123;i&#125;'))schedule.every().day.at(\"22:00\").do(add_snapshot)schedule.every().day.at(\"22:00\").do(del_snapshot)while True: schedule.run_pending() time.sleep(1) api包api/__init__.py配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950from fastapi import FastAPIfrom fastapi.middleware.cors import CORSMiddlewarefrom api.stdlib import stdlib_routerfrom api.quota import quota_routerdef main(): app = FastAPI(docs_url=None, redoc_url=None) app_cors(app) app_stdlib(app) app_quota(app) app_ocr(app) return appdef app_cors(app: FastAPI): \"\"\" CORS :param app: :return: \"\"\" origins = [ \"*\" ] app.add_middleware( CORSMiddleware, allow_origins=origins, allow_credentials=True, allow_methods=[\"*\"], allow_headers=[\"*\"] )def app_proj_1(app: FastAPI): \"\"\" 项目1 :param app: :return: \"\"\" app.include_router(proj_1_router) def app_proj_2(app: FastAPI): \"\"\" 项目2 :param app: :return: \"\"\" app.include_router(proj_2_router) 项目接口api/proj_1(2).py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657from fastapi import APIRouterfrom utils.log import loggerfrom api_class import *from .nebuladb import *proj_1_router = APIRouter( prefix=\"/api\",)@stdlib_router.api_route(\"/project/standard\", methods=['GET'])async def get_project_standard(): \"\"\" 获取工程规范 :return: \"\"\" # 从连接池中获取会话 session = connection_pool.get_session('root', 'nebula') session.execute('USE ai_project') data_final = [] errorcode = 0 result = &#123; \"data\": data_final, \"errorcode\": errorcode &#125; # 举例，查询标签example data_select = session.execute( str(f'match (v:example) return id(v),v.name;') ) # 查询出错，错误代码1 if not data_select.is_succeeded(): session.release() result[\"errorcode\"] = 1 return result # 查询结果不为空，显示数据 if not data_select.is_empty(): size = data_select.row_size() for index in range(size): data1 = data_select.row_values(index)[0].as_string() data2 = data_select.row_values(index)[1].as_string() data_dict = &#123; \"entity_id\": data1, \"name\": data2, &#125; data_final.append(data_dict) session.release() return result # 查询类型下实体为空，错误代码2 session.release() result[\"errorcode\"] = 2 return result","categories":[{"name":"Web","slug":"Web","permalink":"http://yoursite.com/categories/Web/"}],"tags":[{"name":"nebula","slug":"nebula","permalink":"http://yoursite.com/tags/nebula/"},{"name":"fastapi","slug":"fastapi","permalink":"http://yoursite.com/tags/fastapi/"}]},{"title":"PaddleOCR表格识别","slug":"PaddleOCR表格识别","date":"2022-07-21T06:15:22.000Z","updated":"2022-09-22T03:43:56.275Z","comments":true,"path":"2022/07/21/PaddleOCR表格识别/","link":"","permalink":"http://yoursite.com/2022/07/21/PaddleOCR表格识别/","excerpt":"PaddleOCR2.5根目录下的ppstructure文件模块是PaddleOCR提供的一个可用于复杂文档结构分析处理的OCR工具包github文档页面：https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.5/ppstructure/README_ch.md 安装依赖· 安装paddleocr version&gt;=2.51pip install &quot;paddleocr&gt;=2.5&quot; · 安装版面分析依赖包layoutparser1pip install -U https://paddleocr.bj.bcebos.com/whl/layoutparser-0.0.0-py3-none-any.whl · 安装DocVQA依赖包paddlenlp（DocVQA功能，选装）1pip install paddlenlp 快速开始在PaddleOCR/ppstructure目录下进入CMD命令行，或者创建python脚本启动· 表格识别 1paddleocr --image_dir=docs/table/table.jpg --type=structure --layout=false python脚本： 123456789101112131415import osimport cv2from paddleocr import PPStructure,save_structure_restable_engine = PPStructure(layout=False, show_log=True)save_folder = './output'img_path = 'PaddleOCR/ppstructure/docs/table/table.jpg'img = cv2.imread(img_path)result = table_engine(img)save_structure_res(result, save_folder, os.path.basename(img_path).split('.')[0])for line in result: line.pop('img') print(line) · 版面分析 1paddleocr --image_dir=docs/table/1.png --type=structure --table=false --ocr=false python脚本： 123456789101112131415import osimport cv2from paddleocr import PPStructure,save_structure_restable_engine = PPStructure(table=False, ocr=False, show_log=True)save_folder = './output'img_path = 'PaddleOCR/ppstructure/docs/table/1.png'img = cv2.imread(img_path)result = table_engine(img)save_structure_res(result, save_folder, os.path.basename(img_path).split('.')[0])for line in result: line.pop('img') print(line) · 版面分析+表格识别 1paddleocr --image_dir=docs/table/1.png --type=structure python脚本：1234567891011121314151617181920212223import osimport cv2from paddleocr import PPStructure, draw_structure_result, save_structure_resfrom PIL import Imagetable_engine = PPStructure(show_log=True)save_folder = './output'img_path = './ppstructure/docs/table/table.jpg'img = cv2.imread(img_path)result = table_engine(img)save_structure_res(result, save_folder, os.path.basename(img_path).split('.')[0])for line in result: line.pop('img') print(line)font_path = './doc/fonts/simfang.ttf'image = Image.open(img_path).convert('RGB')im_show = draw_structure_result(image, result, font_path=font_path)im_show = Image.fromarray(im_show)im_show.save('result.jpg')","text":"PaddleOCR2.5根目录下的ppstructure文件模块是PaddleOCR提供的一个可用于复杂文档结构分析处理的OCR工具包github文档页面：https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.5/ppstructure/README_ch.md 安装依赖· 安装paddleocr version&gt;=2.51pip install &quot;paddleocr&gt;=2.5&quot; · 安装版面分析依赖包layoutparser1pip install -U https://paddleocr.bj.bcebos.com/whl/layoutparser-0.0.0-py3-none-any.whl · 安装DocVQA依赖包paddlenlp（DocVQA功能，选装）1pip install paddlenlp 快速开始在PaddleOCR/ppstructure目录下进入CMD命令行，或者创建python脚本启动· 表格识别 1paddleocr --image_dir=docs/table/table.jpg --type=structure --layout=false python脚本： 123456789101112131415import osimport cv2from paddleocr import PPStructure,save_structure_restable_engine = PPStructure(layout=False, show_log=True)save_folder = './output'img_path = 'PaddleOCR/ppstructure/docs/table/table.jpg'img = cv2.imread(img_path)result = table_engine(img)save_structure_res(result, save_folder, os.path.basename(img_path).split('.')[0])for line in result: line.pop('img') print(line) · 版面分析 1paddleocr --image_dir=docs/table/1.png --type=structure --table=false --ocr=false python脚本： 123456789101112131415import osimport cv2from paddleocr import PPStructure,save_structure_restable_engine = PPStructure(table=False, ocr=False, show_log=True)save_folder = './output'img_path = 'PaddleOCR/ppstructure/docs/table/1.png'img = cv2.imread(img_path)result = table_engine(img)save_structure_res(result, save_folder, os.path.basename(img_path).split('.')[0])for line in result: line.pop('img') print(line) · 版面分析+表格识别 1paddleocr --image_dir=docs/table/1.png --type=structure python脚本：1234567891011121314151617181920212223import osimport cv2from paddleocr import PPStructure, draw_structure_result, save_structure_resfrom PIL import Imagetable_engine = PPStructure(show_log=True)save_folder = './output'img_path = './ppstructure/docs/table/table.jpg'img = cv2.imread(img_path)result = table_engine(img)save_structure_res(result, save_folder, os.path.basename(img_path).split('.')[0])for line in result: line.pop('img') print(line)font_path = './doc/fonts/simfang.ttf'image = Image.open(img_path).convert('RGB')im_show = draw_structure_result(image, result, font_path=font_path)im_show = Image.fromarray(im_show)im_show.save('result.jpg') 参数说明 字段 说明 默认值 output excel和识别结果保存的地址 ./output/table table_max_len 表格结构模型预测时，图像的长边resize尺度 488 table_model_dir 表格结构模型 inference 模型地址 None table_char_dict_path 表格结构模型所用字典地址 ../ppocr/utils/dict/table_structure_dict.txt layout_path_model 版面分析模型模型地址，可以为在线地址或者本地地址，当为本地地址时，需要指定 layout_label_map, 命令行模式下可通过–layout_label_map=’{0: “Text”, 1: “Title”, 2: “List”, 3:”Table”, 4:”Figure”}’ 指定 lp://PubLayNet/ppyolov2_r50vd_dcn_365e_publaynet/config layout_label_map 版面分析模型模型label映射字典 None model_name_or_path VQA SER模型地址 None max_seq_length VQA SER模型最大支持token长度 512 label_map_path VQA SER 标签文件地址 ./vqa/labels/labels_ser.txt layout 前向中是否执行版面分析 True table 前向中是否执行表格识别 True ocr 对于版面分析中的非表格区域，是否执行ocr。当layout为False时会被自动设置为False True structure_version 表格结构化模型版本，可选 PP-STRUCTURE。PP-STRUCTURE支持表格结构化模型 pp-structure 模型下载 模型类型 模型名称 模型简介 下载地址 版面分析模型 ppyolov2_r50vd_dcn_365e_publaynet PubLayNet 数据集训练的版面分析模型，可以划分文字、标题、表格、图片以及列表5类区域 推理模型/训练模型 OCR模型 ch_PP-OCRv3_det_infer PubLayNet数据集训练的中英文超轻量PP-OCRv3模型 推理模型/训练模型 OCR模型 en_ppocr_mobile_v2.0_table_rec PubLayNet数据集训练的中英文超轻量PP-OCRv3模型 推理模型/训练模型 表格识别模型 en_ppocr_mobile_v2.0_table_structure PubLayNet数据集训练的英文表格场景的表格结构预测 推理模型/训练模型 预测示例（以版面分析+表格为例）命令行1paddleocr --image_dir=docs/table/table.jpg --type=structure --layout=false Python脚本1234567891011121314151617181920212223import osimport cv2from paddleocr import PPStructure, draw_structure_result, save_structure_resfrom PIL import Imagetable_engine = PPStructure(show_log=True)save_folder = './output'img_path = './ppstructure/docs/table/table.jpg'img = cv2.imread(img_path)result = table_engine(img)save_structure_res(result, save_folder, os.path.basename(img_path).split('.')[0])for line in result: line.pop('img') print(line)font_path = './doc/fonts/simfang.ttf'image = Image.open(img_path).convert('RGB')im_show = draw_structure_result(image, result, font_path=font_path)im_show = Image.fromarray(im_show)im_show.save('result.jpg') 服务部署安装structure_system 1hub install deploy\\hubserving\\structure_system\\ 修改config.json，不使用GPU 123456789101112131415&#123; \"modules_info\": &#123; \"structure_system\": &#123; \"init_args\": &#123; \"version\": \"1.0.0\", \"use_gpu\": false &#125;, \"predict_args\": &#123; &#125; &#125; &#125;, \"port\": 8870, \"use_multiprocess\": false, \"workers\": 2&#125; 启动服务 1hub serving start -c ./deploy/hubserving/structure_system/config.json 版面分析+表格识别 1python tools/test_hubserving.py --server_url http://127.0.0.1:8870/predict/structure_system --image_dir ppstructure/docs/table/table.jpg 将识别得到的html标签内容复制另存为.html文件进行对比得到如下结果： 在Pycharm中部署服务并识别进入deploy/hubserving/structure_system/params.py，修改默认模型位置 1234567891011121314...from deploy.hubserving.structure_table.params import read_params as table_read_paramsdef read_params(): cfg = table_read_params() # params for layout parser model cfg.layout_path_model = 'lp://PubLayNet/ppyolov2_r50vd_dcn_365e_publaynet/config' cfg.layout_label_map = None cfg.mode = 'structure' cfg.output = './output' return cfg 可以看到，structure_system/params.py引用了structure_table/params.py下的read_params参数，再进入到structure_table/params.py文件中，该文件则是引用了ocr_system/params.py下的read_params参数，这些参数主要用作OCR识别，所以，如果在不同场景下要使用不同模型时，最好将各个params.py重写。修改后的文件如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# structure_system/params.pyfrom __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_functionclass Config(object): passdef read_params(): cfg = Config() #params for text detector cfg.det_algorithm = \"DB\" cfg.det_model_dir = \"C:/Users/9.9/.paddleocr/whl/det/ch/ch_PP-OCRv3_det_infer\" cfg.det_limit_side_len = 960 cfg.det_limit_type = 'max' #DB parmas cfg.det_db_thresh = 0.3 cfg.det_db_box_thresh = 0.5 cfg.det_db_unclip_ratio = 1.6 cfg.use_dilation = False cfg.det_db_score_mode = \"fast\" #EAST parmas cfg.det_east_score_thresh = 0.8 cfg.det_east_cover_thresh = 0.1 cfg.det_east_nms_thresh = 0.2 #params for text recognizer cfg.rec_algorithm = \"CRNN\" cfg.rec_model_dir = r\"C:/Users/9.9/.paddleocr/whl/rec/ch/ch_PP-OCRv3_rec_infer/\" cfg.rec_image_shape = \"3, 48, 320\" cfg.rec_batch_num = 6 cfg.max_text_length = 25 cfg.rec_char_dict_path = r\"C:/Users/9.9/software/PaddleOCR-release-2.5/ppocr/utils/ppocr_keys_v1.txt\" cfg.use_space_char = True #params for text classifier cfg.use_angle_cls = True cfg.cls_model_dir = r\"C:/Users/9.9/software/PaddleOCR-release-2.5/inference/ch_ppocr_mobile_v2.0_cls_infer/\" cfg.cls_image_shape = \"3, 48, 192\" cfg.label_list = ['0', '180'] cfg.cls_batch_num = 30 cfg.cls_thresh = 0.9 cfg.use_pdserving = False cfg.use_tensorrt = False cfg.drop_score = 0.5 # params for table structure model cfg.table_max_len = 488 cfg.table_model_dir = r'C:\\Users\\9.9\\.paddleocr\\whl\\table\\en_ppocr_mobile_v2.0_table_structure_infer/' cfg.table_char_dict_path = 'C:/Users/9.9/software/PaddleOCR-release-2.5/ppocr/utils/dict/table_structure_dict.txt' cfg.show_log = False # params for layout parser model cfg.layout_path_model = 'lp://PubLayNet/ppyolov2_r50vd_dcn_365e_publaynet/config' # cfg.layout_path_model = './inference/ppyolov2_r50vd_dcn_365e_publaynet' cfg.layout_label_map = None cfg.mode = 'structure' cfg.output = './output' return cfg 再新建python文件用于提取具体内容，通过pandas分析得到结构 1234567891011121314151617181920212223242526272829import osimport reimport pandas as pddef exec_ocr(cmd: str): pip = os.popen(cmd) return pip.buffer.read().decode(encoding='utf8')img_dir = r\"C:\\Users\\9.9\\software\\PaddleOCR-release-2.5\\ppstructure\\docs\\table\\table.jpg\"ocr_dir = r\"python C:\\Users\\9.9\\software\\PaddleOCR-release-2.5\\tools\\test_hubserving.py\"res = exec_ocr(fr\"&#123;ocr_dir&#125; --server_url http://127.0.0.1:8870/predict/structure_system --image_dir &#123;img_dir&#125;\")with open(\"system_res.txt\", \"w\", encoding=\"utf-8\") as f: f.write(res)ocr_file = open(\"system_res.txt\", \"r\", encoding='utf-8')alldata_res = ocr_file.read()system_res = re.search(r\"'html': '(.*)'&#125;, 'type': 'Table'&#125;\", alldata_res)print(system_res.group(1))with open(\"system_res.html\", \"w\", encoding='utf-8') as f: f.write(system_res.group(1))with open(\"system_res.html\", encoding='utf-8') as f: df = pd.read_html(f.read(), encoding='utf-8', index_col=0)[0] df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]print(df)df.to_csv('system_res.csv') PP-Structure表格模型训练安装前往Paddle的github主页下载安装PaddleDetection：https://github.com/PaddlePaddle/PaddleDetection，并执行pip install -r requirements.txt安装其他依赖 准备数据下载PubLayNet数据集，可通过链接（https://dax-cdn.cdn.appdomain.cloud/dax-publaynet/1.0.0/publaynet.tar.gz?_ga=2.104193024.1076900768.1622560733-649911202.1622560733）直接下载（约95GB） 配置文件修改configs/ppyolo/ppyolov2_r50vd_dcn_365e_coco.yml文件的配置进行训练： 12345678910_BASE_: [ '../datasets/coco_detection.yml', # 主要说明了训练数据和验证数据的路径 '../runtime.yml', # 主要说明了公共的运行参数，比如是否使用GPU、每多少个epoch存储checkpoint等 './_base_/ppyolov2_r50vd_dcn.yml', # 主要说明了学习率和优化器的配置 './_base_/optimizer_365e.yml', # 主要说明模型和主干网络的情况 './_base_/ppyolov2_reader.yml', # 主要说明数据读取器配置，如batch size，并发加载子进程数等，同时包含读取后预处理操作，如resize、数据增强等等]snapshot_epoch: 8weights: output/ppyolov2_r50vd_dcn_365e_coco/model_final 来到datasets/coco_detection.yml文件中，修改下载好的训练集的位置 1234567891011121314151617181920metric: COCOnum_classes: 80TrainDataset: !COCODataSet image_dir: C:/Users/9.9/software/publaynet/train anno_path: C:/Users/9.9/software/publaynet/train.json dataset_dir: dataset/coco data_fields: ['image', 'gt_bbox', 'gt_class', 'is_crowd']EvalDataset: !COCODataSet image_dir: C:/Users/9.9/software/publaynet/val anno_path: C:/Users/9.9/software/publaynet/val.json dataset_dir: dataset/cocoTestDataset: !ImageFolder anno_path: C:/Users/9.9/software/publaynet/val.json # also support txt (like VOC's label_list.txt) dataset_dir: dataset/coco # if set, anno_path will be 'dataset_dir/anno_path' 训练1python tools/train.py -c configs/ppyolo/ppyolov2_r50vd_dcn_365e_coco.yml","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"OCR","slug":"OCR","permalink":"http://yoursite.com/tags/OCR/"}]},{"title":"PaddleOCR","slug":"PaddleOCR","date":"2022-07-12T06:20:14.000Z","updated":"2022-09-22T03:43:08.898Z","comments":true,"path":"2022/07/12/PaddleOCR/","link":"","permalink":"http://yoursite.com/2022/07/12/PaddleOCR/","excerpt":"安装进入PaddleOCR的github页面（https://github.com/PaddlePaddle/PaddleOCR），进行下载和解压 使用pip进行安装，这里因为速度很慢推荐使用百度源1pip install paddlepaddle -i https://mirror.baidu.com/pypi/simple 安装shapely（https://www.lfd.uci.edu/~gohlke/pythonlibs/），下载shapely对应python和系统版本的安装包（我使用的是py39，windows_64），python版本目前不能超过3.9 将下载好的Shapely-1.8.2-cp39-cp39-win_amd64.whl放进python根目录下的libs文件夹内，通过cmd或pycharm终端使用pip执行安装1pip install Shapely-1.8.2-cp39-cp39-win_amd64.whl 完成以后接着来到PaddleOCR目录下，通过终端安装依赖，这里同样推荐使用百度源1pip install -r requirements.txt -i https://mirror.baidu.com/pypi/simple 到这里，PaddleOCR的安装完成了 如果再执行pip install -r requirements.txt -i https://mirror.baidu.com/pypi/simple到安装opencv4.4.0.46包时报错： error: subprocess-exited-with-error 说明python的版本可能存在问题，需要切换虚拟环境或回退python版本，因为opencv-python目前仅支持py3.6-3.9版本 opencv-python镜像：https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple/opencv-python/","text":"安装进入PaddleOCR的github页面（https://github.com/PaddlePaddle/PaddleOCR），进行下载和解压 使用pip进行安装，这里因为速度很慢推荐使用百度源1pip install paddlepaddle -i https://mirror.baidu.com/pypi/simple 安装shapely（https://www.lfd.uci.edu/~gohlke/pythonlibs/），下载shapely对应python和系统版本的安装包（我使用的是py39，windows_64），python版本目前不能超过3.9 将下载好的Shapely-1.8.2-cp39-cp39-win_amd64.whl放进python根目录下的libs文件夹内，通过cmd或pycharm终端使用pip执行安装1pip install Shapely-1.8.2-cp39-cp39-win_amd64.whl 完成以后接着来到PaddleOCR目录下，通过终端安装依赖，这里同样推荐使用百度源1pip install -r requirements.txt -i https://mirror.baidu.com/pypi/simple 到这里，PaddleOCR的安装完成了 如果再执行pip install -r requirements.txt -i https://mirror.baidu.com/pypi/simple到安装opencv4.4.0.46包时报错： error: subprocess-exited-with-error 说明python的版本可能存在问题，需要切换虚拟环境或回退python版本，因为opencv-python目前仅支持py3.6-3.9版本 opencv-python镜像：https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple/opencv-python/ 使用官方模型先使用官方模型对数据进行测试（v2.0）：https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.0/README_ch.md 其中推理模型（inference model）相当于已训练完成的模型，可以直接拿来预测，而预训练模型（trained model)属于半成品，在使用本地的数据训练模型时需要用到 将推理模型下载后，来到PaddleOCR目录下新建inference文件夹，用来存放模型 检查每个文件夹下是否存在inference.pdiparams、inference.pdiparams.info、inference.pdmodel三个文件，如果出现不和谐的文件夹是官方打包时出错，将文件夹内的内容提取出来即可 如果直接使用可能会报错： raise Exception(“not found any img file in {}”.format(img_file))Exception: not found any img file in ./doc/imgs/test.jpg 预测官方的快速开始教程：https://github.com/PaddlePaddle/PaddleOCR/blob/develop/doc/doc_ch/quickstart.md 首先将测试目标test.jpg存放进PaddleOCR/doc/imgs/目录下，到cmd或pycharm终端里执行：123456python tools/infer/predict_system.py --image_dir=\"./doc/imgs/test.jpg\" \\ --det_model_dir=\"./inference/ch_ppocr_server_v2.0_det_infer/\" \\ --rec_model_dir=\"./inference/ch_ppocr_server_v2.0_rec_infer/\" \\ --cls_model_dir=\"./inference/ch_ppocr_mobile_v2.0_cls_infer/\" \\ --use_angle_cls=True \\ --use_space_char=True （windows下使用官方文档里中的”python3 tools/…”会出不来结果，linux下没有问题） 使用pycharm预测需要修改的代码位于根目录下/tools/infer文件夹下，其中predict_det.py用于检测文本，predict_rec.py用于识别文本，predict_system.py可用于检测和识别，这3个.py文件共用同一个配置文件utility.py，需要到其中定位并修改：1234567891011121314151617181920212223242526272829303132333435# 是否使用gpuparser.add_argument(\"--use_gpu\", type=str2bool, default=True)# 图片文件位置parser.add_argument(\"--image_dir\", type=str, default=\"../../demo/test.jpg\")# 检测模型路径parser.add_argument(\"--det_model_dir\", type=str, default=\"../../inference/ch_ppocr_server_v2.0_det_infer/\")# 识别模型路径parser.add_argument(\"--rec_model_dir\", type=str, default=\"../../inference/ch_ppocr_server_v2.0_rec_infer\")# 分类模型路径parser.add_argument(\"--cls_model_dir\", type=str, default=\"../../inference/ch_ppocr_mobile_v2.0_cls_infer\")# 字典路径（ic15_dict.txt是英文字典，ppocr_keys_v1.txt是中文字典，检测一般不区分中英文，但是识别需要区分中英文）parser.add_argument( \"--rec_char_dict_path\", type=str, default=\"../../ppocr/utils/ic15_dict.txt\")# 字体路径，2处def draw_ocr_box_txt(image, boxes, txts, scores=None, drop_score=0.5, font_path=\"./doc/simfang.ttf\"):def text_visual(texts, scores, img_h=400, img_w=600, threshold=0., font_path=\"./doc/simfang.ttf\"):# 程序输出路径parser.add_argument( \"--draw_img_save_dir\", type=str, default=\"./inference_results\") 接着运行predict_system.py 服务部署（基于PaddleHub Serving）deploy/hubserving服务部署目录下包括检测（ocr_det）、识别（ocr_rec）、2阶段串联（ocr_system）三种服务包和分类模块服务包（ocr_cls），可以根据需求选择相应的服务包进行安装和启动。每个服务包下包括3个.py文件和一个config.json配置文件：- __init__.py- config.json：配置文件- module.py：主模块，必选，包含服务的完整逻辑- params.py：参数文件，必选，包含模型路径、前后处理参数等参数 准备环境1.使用pip安装paddlehub，python版本需要高于3.6.21pip3 install paddlehub==2.1.0 --upgrade -i https://pypi.tuna.tsinghua.edu.cn/simple 2.配置模型文件，将之前下载的模型路径修改到params.py当中123cfg.det_model_dir = \"./inference/ch_ppocr_server_v2.0_det_infer/\"cfg.rec_model_dir = \"./inference/ch_ppocr_server_v2.0_rec_infer/\"cfg.cls_model_dir = \"./inference/ch_ppocr_mobile_v2.0_cls_infer/\" 3.安装服务模块，进入paddleOCR根目录，执行：Linux环境1234567891011# 安装检测服务模块： $ hub install deploy/hubserving/ocr_det/# 或，安装分类服务模块： $ hub install deploy/hubserving/ocr_cls/# 或，安装识别服务模块： $ hub install deploy/hubserving/ocr_rec/# 或，安装检测+识别串联服务模块： $ hub install deploy/hubserving/ocr_system/ windows环境1234567891011# 安装检测服务模块： hub install deploy\\hubserving\\ocr_det\\# 或，安装分类服务模块： hub install deploy\\hubserving\\ocr_cls\\# 或，安装识别服务模块： hub install deploy\\hubserving\\ocr_rec\\# 或，安装检测+识别串联服务模块：hub install deploy\\hubserving\\ocr_system\\ 执行hub list可以查看已安装的模块： 启动服务方式1.命令行启动（仅支持CPU）1234$ hub serving start --modules [Module1==Version1, Module2==Version2, ...] \\ --port XXXX \\ --use_multiprocess \\ --workers \\ 参数 作用 –modules/-m PaddleHub Serving预安装模型，以多个Module==Version键值对的形式列出当不指定Version时，默认选择最新版本 –port/-p 服务端口，默认为8866 –use_multiprocess 是否启用并发方式，默认为单进程方式，推荐多核CPU机器使用此方式Windows操作系统只支持单进程方式 –workers 在并发方式下指定的并发任务数，默认为2*cpu_count-1，其中cpu_count为CPU核数 例：1hub serving start -m ocr_system 方式2.通过配置文件config.json启动（支持CPU、GPU）config.json内容格式如下：123456789101112131415&#123; \"modules_info\": &#123; \"ocr_system\": &#123; \"init_args\": &#123; \"version\": \"1.0.0\", \"use_gpu\": false &#125;, \"predict_args\": &#123; &#125; &#125; &#125;, \"port\": 8868, \"use_multiprocess\": false, \"workers\": 2&#125; · init_args中的可配参数与module.py中的_initialize函数接口保持一致，当use_gpu的值为true时，表示使用GPU启动服务 · predict_args中的可配参数与module.py中的predict函数接口保持一致 · 使用配置文件启动服务时，其他参数会被忽略 · 如果使用GPU预测(即，use_gpu置为true)，则需要在启动服务之前，设置CUDA_VISIBLE_DEVICES环境变量，如：export CUDA_VISIBLE_DEVICES=0，否则不用设置 · use_gpu不可与use_multiprocess同时为true 例：1hub serving start -c ./deploy/hubserving/ocr_system/config.json 发送预测请求需要通过POST方法传递2个参数：server_url 和 image_path1python tools/test_hubserving.py --server_url http://127.0.0.1:8868/predict/ocr_system --image_dir ./demo/ 如果在启动时遇到警告： C:\\Python39\\lib\\site-packages\\attrdict\\mapping.py:4: DeprecationWarning: Using or importing the ABCs from ‘collections’ instead of from ‘collections.abc’ is deprecated since Python 3.3, and in 3.10 it will stop working from collections import Mapping需要到对应的Python\\lib\\site-packages\\attrdict\\目录下修改default.py、mapping.py、merge.py 和 mixins.py文件中1from collections import ... 修改为1from collections.abc import ... 返回结构返回的结果为列表，其中每一项为一个字典，字典一共可能包含下列三种字段： 字段名 数据类型 含义 text str 文本内容 confidence float 文本识别置信度 text_region list 文本位置坐标 不同模块返回的字段不同，如，文本识别服务模块返回结果不含text_region字段 字段名 ocr_det ocr_cls ocr_rec ocr_system text √ √ confidence √ √ √ text_region √ √ 自定义修改服务模块· 停止服务：1hub serving stop --port/-p XXXX · 到相应的module.py 和 params.py等文件中根据实际需求修改代码例如，如果需要替换部署服务所用模型，则需要到params.py中修改模型路径参数det_model_dir 和 rec_model_dir，如果需要关闭文本方向分类器，则将参数use_angle_cls置为False，当然，同时可能还需要修改其他相关参数，强烈建议修改后先直接运行module.py调试，能正确运行预测后再启动服务测试 · 卸载旧服务包1hub uninstall ocr_system · 安装修改后的新服务包1hub install deploy/hubserving/ocr_system/ · 重新启动服务1hub serving start -m ocr_system","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"OCR","slug":"OCR","permalink":"http://yoursite.com/tags/OCR/"}]},{"title":"ClickHouse","slug":"ClickHouse","date":"2022-07-07T08:02:23.000Z","updated":"2022-09-22T03:47:24.441Z","comments":true,"path":"2022/07/07/ClickHouse/","link":"","permalink":"http://yoursite.com/2022/07/07/ClickHouse/","excerpt":"本文参考CK中文官方文档：https://clickhouse.com/docs/zh/ClickHouse是一个用于联机分析(OLAP)的列式数据库管理系统(DBMS) 在传统的行式数据库系统中，数据按如下顺序存储： ROW Title Code Note Datetime #0 Database 0 数据库 2022-07-07 16:03:00 #1 MySQL 1 关系型数据库 2022-07-07 16:03:10 #2 Nebula 1 图数据库 2022-07-07 16:03:20 #3 ClickHouse 1 列式数据库 2022-07-07 16:03:30 #N … 1 … … 处于同一行中的数据总是被物理的存储在一起。常见的行式数据库系统有：MySQL、Postgres 和 MS SQL Server 等 而在列式数据库系统中，数据则按如下顺序存储 ROW #0 #1 #2 #3 #4 Title Database MySQL Nebula ClickHouse … Code 0 1 1 1 1 Note 数据库 关系型数据库 图数据库 列式数据库 … Datetime 2022-0707 16:03:00 2022-0707 16:03:10 2022-0707 16:03:20 2022-07-07 16:03:30 … 这些示例只显示了数据的排列顺序，来自不同列的值被单独存储，来自同一列的数据被存储在一起。常见的列式数据库系统有：Vertica、Paraccel(Actian Matrix, Amazon Redshift)、Sybase IQ、Exasol、Infobright、InfiniDB、MonetDB(VectorWise, Actian Vector)、LucidDB、SAP HANA、Google Dremel、Google PowerDrill、Druid、kdb+ 等 不同的数据存储方式适用不同的业务场景，数据访问的场景包括：进行了何种查询、多久查询一次以及各类查询的比例；每种类型的查询(行、列和字节)读取多少数据；读取数据和更新之间的关系；使用的数据集大小以及如何使用本地的数据集；是否使用事务,以及它们是如何进行隔离的；数据的复制机制与数据的完整性要求；每种类型的查询要求的延迟与吞吐量等等 系统负载越高，依据使用场景进行定制化就越重要，并且定制将会变的越精细。没有一个系统能够同时适用所有不同的业务场景。如果系统适用于广泛的场景，在负载高的情况下，要兼顾所有的场景，那么将不得不做出选择：平衡 OR 效率 联机分析（OLAP）场景的关键特征· 绝大多数是读请求 · 数据以相当大的批次(&gt; 1000行)更新，而不是单行更新;或者根本没有更新。 · 已添加到数据库的数据不能修改。 · 对于读取，从数据库中提取相当多的行，但只提取列的一小部分。 · 宽表，即每个表包含着大量的列 · 查询相对较少(通常每台服务器每秒查询数百次或更少) · 对于简单查询，允许延迟大约50毫秒 · 列中的数据相对较小：数字和短字符串(例如，每个URL 60个字节) · 处理单个查询时需要高吞吐量(每台服务器每秒可达数十亿行) · 事务不是必须的 · 对数据一致性要求低 · 每个查询有一个大表。除了他以外，其他的都很小。 · 查询结果明显小于源数据。换句话说，数据经过过滤或聚合，因此结果适合于单个服务器的RAM中 很容易可以看出，OLAP场景与其他通常业务场景(例如,OLTP或K/V)有很大的不同， 因此想要使用OLTP或Key-Value数据库去高效的处理分析查询场景，并不是非常完美的适用方案。例如，使用OLAP数据库去处理分析请求通常要优于使用MongoDB或Redis去处理分析请求。","text":"本文参考CK中文官方文档：https://clickhouse.com/docs/zh/ClickHouse是一个用于联机分析(OLAP)的列式数据库管理系统(DBMS) 在传统的行式数据库系统中，数据按如下顺序存储： ROW Title Code Note Datetime #0 Database 0 数据库 2022-07-07 16:03:00 #1 MySQL 1 关系型数据库 2022-07-07 16:03:10 #2 Nebula 1 图数据库 2022-07-07 16:03:20 #3 ClickHouse 1 列式数据库 2022-07-07 16:03:30 #N … 1 … … 处于同一行中的数据总是被物理的存储在一起。常见的行式数据库系统有：MySQL、Postgres 和 MS SQL Server 等 而在列式数据库系统中，数据则按如下顺序存储 ROW #0 #1 #2 #3 #4 Title Database MySQL Nebula ClickHouse … Code 0 1 1 1 1 Note 数据库 关系型数据库 图数据库 列式数据库 … Datetime 2022-0707 16:03:00 2022-0707 16:03:10 2022-0707 16:03:20 2022-07-07 16:03:30 … 这些示例只显示了数据的排列顺序，来自不同列的值被单独存储，来自同一列的数据被存储在一起。常见的列式数据库系统有：Vertica、Paraccel(Actian Matrix, Amazon Redshift)、Sybase IQ、Exasol、Infobright、InfiniDB、MonetDB(VectorWise, Actian Vector)、LucidDB、SAP HANA、Google Dremel、Google PowerDrill、Druid、kdb+ 等 不同的数据存储方式适用不同的业务场景，数据访问的场景包括：进行了何种查询、多久查询一次以及各类查询的比例；每种类型的查询(行、列和字节)读取多少数据；读取数据和更新之间的关系；使用的数据集大小以及如何使用本地的数据集；是否使用事务,以及它们是如何进行隔离的；数据的复制机制与数据的完整性要求；每种类型的查询要求的延迟与吞吐量等等 系统负载越高，依据使用场景进行定制化就越重要，并且定制将会变的越精细。没有一个系统能够同时适用所有不同的业务场景。如果系统适用于广泛的场景，在负载高的情况下，要兼顾所有的场景，那么将不得不做出选择：平衡 OR 效率 联机分析（OLAP）场景的关键特征· 绝大多数是读请求 · 数据以相当大的批次(&gt; 1000行)更新，而不是单行更新;或者根本没有更新。 · 已添加到数据库的数据不能修改。 · 对于读取，从数据库中提取相当多的行，但只提取列的一小部分。 · 宽表，即每个表包含着大量的列 · 查询相对较少(通常每台服务器每秒查询数百次或更少) · 对于简单查询，允许延迟大约50毫秒 · 列中的数据相对较小：数字和短字符串(例如，每个URL 60个字节) · 处理单个查询时需要高吞吐量(每台服务器每秒可达数十亿行) · 事务不是必须的 · 对数据一致性要求低 · 每个查询有一个大表。除了他以外，其他的都很小。 · 查询结果明显小于源数据。换句话说，数据经过过滤或聚合，因此结果适合于单个服务器的RAM中 很容易可以看出，OLAP场景与其他通常业务场景(例如,OLTP或K/V)有很大的不同， 因此想要使用OLTP或Key-Value数据库去高效的处理分析查询场景，并不是非常完美的适用方案。例如，使用OLAP数据库去处理分析请求通常要优于使用MongoDB或Redis去处理分析请求。 列式数据库更适合OLAP场景的原因列式数据库更适合于OLAP场景(对于大多数查询而言，处理速度至少提高了100倍) I/O1.针对分析类查询，通常只需要读取表的一小部分列。在列式数据库中你可以只读取你需要的数据。例如，如果只需要读取100列中的5列，这将帮助你最少减少20倍的I/O消耗。 2.由于数据总是打包成批量读取的，所以压缩是非常容易的。同时数据按列分别存储这也更容易压缩。这进一步降低了I/O的体积。 3.由于I/O的降低，这将帮助更多的数据被系统缓存。 CPU由于执行一个查询需要处理大量的行，因此在整个向量上执行所有操作将比在每一行上执行所有操作更加高效。同时这将有助于实现一个几乎没有调用成本的查询引擎。如果你不这样做，使用任何一个机械硬盘，查询引擎都不可避免的停止CPU进行等待。所以，在数据按列存储并且按列执行是很有意义的。 有两种方法可以做到这一点： 1.向量引擎：所有的操作都是为向量而不是为单个值编写的。这意味着多个操作之间的不再需要频繁的调用，并且调用的成本基本可以忽略不计。操作代码包含一个优化的内部循环。 2.代码生成：生成一段代码，包含查询中的所有操作。 这是不应该在一个通用数据库中实现的，因为这在运行简单查询时是没有意义的。但是也有例外，例如，MemSQL使用代码生成来减少处理SQL查询的延迟(只是为了比较，分析型数据库通常需要优化的是吞吐而不是延迟)。 为了提高CPU效率，查询语言必须是声明型的(SQL或MDX)， 或者至少一个向量(J，K)。 查询应该只包含隐式循环，允许进行优化。 安装ClickHouse使用CentOS基于rpm的linux发行版官方预编译rpm包，添加官方存储库：123$ sudo yum install -y yum-utils$ sudo yum-config-manager --add-repo https://packages.clickhouse.com/rpm/clickhouse.repo$ sudo yum install -y clickhouse-server clickhouse-client 启动CK运行以下命令将在后台启动服务，日志文件将输出在/var/log/clickhouse-server/文件夹12$ sudo /etc/init.d/clickhouse-server start$ clickhouse-client # 或执行 &quot;clickhouse-client --password&quot; 来添加密码 要手动从控制台启动服务，执行：1$ clickhouse-server --config-file=/etc/clickhouse-server/config.xml 日志将被打印至控制台 当服务成功启动后，执行$ clickhouse-client来连接CK数据库，默认情况下，使用default用户并不携带密码连接到localhost:9000，还可以使用–host参数连接到指定服务器。 终端必须使用UTF-8编码。 ClickHouse数据库可视化工具Dbeaver目前navicat并不支持clickhouse的可视化查询，可以使用dbeaver数据库管理工具来代替navicat Dbeaver安装下载最新版本的DBeaver RPM安装包并使用rpm/dnf或yum软件包管理器进行安装12$ wget https://dbeaver.io/files/dbeaver-ce-latest-stable.x86_64.rpm$ sudo rpm -Uvh ./dbeaver-ce-latest-stable.x86_64.rpm 完成后即可在终端执行$ dbeaver来启动dbeaver，再连接到CK（需要安装CK在线包） ClickHouse数据库操作创建新的数据库TEST（删除为DROP）1localhost :) CREATE DATABASE TEST 查看数据库、数据表12localhost :) SHOW DATABASESlocalhost :) SHOW TABLES IN TEST 创建数据表ClickHouse有自己的数据类型，每个表都必须指定一个Engine属性来确定要创建的表的类1localhost :) CREATE TABLE TEST.testable (id String,time Date,name String,code FixedString(2))ENGINE=MergeTree() ORDER BY (time) 执行1localhost :) DESCRIBE TEST.testable 来描述表 添加数据1localhost :) INSERT INTO TEST.testable VALUES (&apos;001&apos;,&apos;2022-07-07&apos;,&apos;MySQL&apos;,&apos;01&apos;) 或指定数据所在的列：1localhost :) INSERT INTO TEST.testable(id,time,name,code) VALUES (&apos;002&apos;,&apos;2022-07-07&apos;,&apos;SQLite&apos;,&apos;01&apos;) 指定列不插入数据： 1localhost :) INSERT INTO TEST.testable(* EXCEPT(code)) VALUES (&apos;0000&apos;,&apos;2022-07-07&apos;,&apos;nosql&apos;) SELECT12localhost :) SELECT * FROM TEST.testablelocalhost :) SELECT * FROM TEST.testable WHERE name=&apos;MySQL&apos;","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"}],"tags":[{"name":"database","slug":"database","permalink":"http://yoursite.com/tags/database/"},{"name":"clickhouse","slug":"clickhouse","permalink":"http://yoursite.com/tags/clickhouse/"}]},{"title":"Nebula-CentOS安装与部署","slug":"Nebula-CentOS安装与部署","date":"2022-06-21T13:46:14.000Z","updated":"2022-09-22T03:41:08.505Z","comments":true,"path":"2022/06/21/Nebula-CentOS安装与部署/","link":"","permalink":"http://yoursite.com/2022/06/21/Nebula-CentOS安装与部署/","excerpt":"本文参考、提取自Nebula Graph Database官方文档(https://docs.nebula-graph.com.cn/2.6.0) 1.前期准备环境· nebula版本：2.6.0· 主机：CentOS7 64位· 内存：8GB· 处理器数量：4· 处理器内核数：4· 处理器内核总数：16· 硬盘：30GB，SSD 硬件要求· CPU架构：x86_64· 内存：4GB· 硬盘：10GB，SSD 软件 软件名称 版本 备注 glibc 2.17及以上 执行ldd --version检查版本 make 任意稳定版本 - m4 任意稳定版本 - git 任意稳定版本 - wget 任意稳定版本 - unzip 任意稳定版本 - xz 任意稳定版本 - redline-devel 任意稳定版本 - ncurses-devel 任意稳定版本 - zlib-devel 任意稳定版本 - gcc 7.5.0及以上 执行命令gcc -v检查版本 gcc-c++ 任意稳定版本 - cmake 3.9.0及以上 执行命令cmake --version检查版本 gettext 任意稳定版本 - curl 任意稳定版本 - redhat-lsb-core 任意稳定版本 - libstdc++-static 任意稳定版本 仅在CentOS 8+、RedHat 8+、Fedora中需要 libasan 任意稳定版本 仅在CentOS 8+、RedHat 8+、Fedora中需要 bzip2 任意稳定版本 - 需要保证gcc和cmake的版本足够高，其他第三方软件将在安装（cmake）阶段自动下载并安装到build目录中","text":"本文参考、提取自Nebula Graph Database官方文档(https://docs.nebula-graph.com.cn/2.6.0) 1.前期准备环境· nebula版本：2.6.0· 主机：CentOS7 64位· 内存：8GB· 处理器数量：4· 处理器内核数：4· 处理器内核总数：16· 硬盘：30GB，SSD 硬件要求· CPU架构：x86_64· 内存：4GB· 硬盘：10GB，SSD 软件 软件名称 版本 备注 glibc 2.17及以上 执行ldd --version检查版本 make 任意稳定版本 - m4 任意稳定版本 - git 任意稳定版本 - wget 任意稳定版本 - unzip 任意稳定版本 - xz 任意稳定版本 - redline-devel 任意稳定版本 - ncurses-devel 任意稳定版本 - zlib-devel 任意稳定版本 - gcc 7.5.0及以上 执行命令gcc -v检查版本 gcc-c++ 任意稳定版本 - cmake 3.9.0及以上 执行命令cmake --version检查版本 gettext 任意稳定版本 - curl 任意稳定版本 - redhat-lsb-core 任意稳定版本 - libstdc++-static 任意稳定版本 仅在CentOS 8+、RedHat 8+、Fedora中需要 libasan 任意稳定版本 仅在CentOS 8+、RedHat 8+、Fedora中需要 bzip2 任意稳定版本 - 需要保证gcc和cmake的版本足够高，其他第三方软件将在安装（cmake）阶段自动下载并安装到build目录中 2.安装编译1.安装依赖包1$ yum update 12345678910111213141516$ yum install -y make \\ m4 \\ git \\ wget \\ unzip \\ xz \\ readline-devel \\ ncurses-devel \\ zlib-devel \\ gcc \\ gcc-c++ \\ cmake \\ gettext \\ curl \\ redhat-lsb-core \\ bzip2 1$ yum install -y libstdc++-static libasan 安装完成后检查主机的g++和cmake版本12$ g++ --version$ cmake --version 如果cmake和g++的版本不够，可以先到github克隆nebula仓库12$ git clone --branch v2.6.0 https://github.com/vesoft-inc/nebula.git$ cd nebula 执行命令安装cmake和g++123$ ./third-party/install-cmake.sh cmake-install$ source cmake-install/bin/enable-cmake.sh$ sudo mkdir /opt/vesoft &amp;&amp; sudo chmod -R a+w /opt/vesoft 12$ ./third-party/install-gcc.sh --prefix=/opt$ source /opt/vesoft/toolset/gcc/7.5.0/enable 最后执行脚本install-third-party.sh1$ ./third-party/install-third-party.sh 编译源码在github克隆nebula仓库后，创建build目录并进入1$ mkdir build &amp;&amp; cd build 使用cmake生成makefile文件1$ cmake -DCMAKE_INSTALL_PREFIX=/usr/local/nebula -DENABLE_TESTING=OFF -DCMAKE_BUILD_TYPE=Release .. 进行编译，为了提高速度，-j参数（并行数量）为min(CPU核数，内存/2)，这里选择8/2=41$ make -j4 安装nebula graph1$ sudo make install 一切完成以后，来到默认的/usr/local/nebula/etc目录下，将配置文件（nebula-graph.conf.default、nebula-metad.conf.default、nebula-storaged.conf.default）后缀的.default删除，即使用默认的配置文件 3.启动Nebula Graph服务端启动Nebula Graph直接进行启动、查看状态和关闭服务的命令：123$ sudo /usr/local/nebula/scripts/nebula.service start all$ sudo /usr/local/nebula/scripts/nebula.service status all$ sudo /usr/local/nebula/scripts/nebula.service stop all 连接Nebula Graph进入nebula console下载界面https://github.com/vesoft-inc/nebula-console/releases，找到对应的2.6.0版本，点击Assets找到linux版本，这里选择nebula-console-linux-amd64-v2.6.0 1$ wget https://github.com/vesoft-inc/nebula-console/releases/download/v2.6.0/nebula-console-linux-amd64-v2.6.0 将下载好的nebula-console-linux-amd64-v2.6.0重命名为nebula-console 为用户授予nebula-console文件的执行权限1$ chmod 111 nebula-console 来到nebula-console所在目录下，执行命令连接nebula graph，其中参数分别对应ip地址、服务端的端口号（9669）、nebula用户名和密码1$ ./nebula-console -addr 192.168.80.128 -port 9669 -u root -p nebula 成功后进入到nebula console控制台界面（需要提前启动nebula graph） 可以通过console控制台执行命令，进行数据库的查看、编辑等 最后通过exit或者quit命令退出nebula console控制台 4.Nebula Graph Studio客户端nebula graph studio是一款可以通过 Web 访问的图数据库开源可视化工具，搭配 nebula graph 内核使用，能够提供构图、数据导入、编写 nGQL 查询、图探索等一站式服务 nebula graph studio 版本发布节奏独立于 nebula graph 内核，其命名方式也不参照内核命名的规则，目前仅支持 x86_64 架构 前期准备· 需要启动nebula graph服务 · 使用的Linux发行版为CentOS，安装有lsof和版本为v10.16.0及以上的Node.js · 确保7001和8080端口处于未被占用的状态 下载最新版本的node.js： 进入node.js官网，选择linux下64位版本 1$ wget https://nodejs.org/dist/v16.15.1/node-v16.15.1-linux-x64.tar.xz 下载到本地以后解压缩，将文件重命名为nodejs16，12$ tar xf node-v16.15.1-linux-x64.tar.xz$ mv node-v16.15.1-linux-x64 nodejs16 配置软连接，将nodejs16映射到usr/bin下，使全局可以使用node.js12$ ln -s /root/nodejs16/bin/node /usr/bin/ $ ln -s /root/nodejs16/bin/npm /usr/bin/ 最后可以查看node.js的版本，确认安装完成1$ /usr/bin/node -v 安装和部署nebula graph studio12$ https://oss-cdn.nebula-graph.com.cn/nebula-graph-studio/3.1.0/nebula-graph-studio-3.1.0.x86_64.rpm$ sudo rpm -i nebula-graph-studio-3.1.0.x86_64.rpm 成功后显示[egg-scripts] egg started on http://0.0.0.0:7001说明客户端正常启动了 刷入前面接口要调用到的数据 要手动关闭和启动服务，可以执行 12$ bash /usr/local/nebula-graph-studio/scripts/rpm/start.sh$ bash /usr/local/nebula-graph-studio/scripts/rpm/stop.sh windows访问客户端查看虚拟机状态，关闭虚拟机防火墙，禁止防火墙的开机启动123$ service iptables status$ systemctl stop firewalld.service$ systemctl disable firewalld.service 在浏览器中进入虚拟机本地ip，进入图库，可以检索到前面插入的数据 自启动进入系统相关配置文件，写入自启动命令和脚本1$ vim /etc/rc.d/rc.local 写入启动命令123#nebula-graphsudo /usr/local/nebula/scripts/nebula.service start allbash /usr/local/nebula-graph-studio/scripts/rpm/start.sh","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"}],"tags":[{"name":"database","slug":"database","permalink":"http://yoursite.com/tags/database/"},{"name":"nebula","slug":"nebula","permalink":"http://yoursite.com/tags/nebula/"}]},{"title":"FastApi","slug":"FastApi","date":"2022-05-23T03:49:29.000Z","updated":"2022-06-27T07:29:21.649Z","comments":true,"path":"2022/05/23/FastApi/","link":"","permalink":"http://yoursite.com/2022/05/23/FastApi/","excerpt":"FastAPI 是一个基于Python3.6+版本用于构建API的高性能web框架 官方文档链接：https://fastapi.tiangolo.com 安装执行pip install fastapi[all]来安装fastapi所需要的全部python包和模块 也可以分开来安装来将应用程序部署到成产环境，其中uvicorn可以用作运行代码的服务器12pip install fastapipip install uvicorn[standard]","text":"FastAPI 是一个基于Python3.6+版本用于构建API的高性能web框架 官方文档链接：https://fastapi.tiangolo.com 安装执行pip install fastapi[all]来安装fastapi所需要的全部python包和模块 也可以分开来安装来将应用程序部署到成产环境，其中uvicorn可以用作运行代码的服务器12pip install fastapipip install uvicorn[standard] 接口最简单的Fastapi接口可以如下所示，制作一个根节点下的欢迎目录Hello World123456789101112from fastapi import FastAPIapp = FastAPI()@app.get(\"/\")async def root(): return &#123;\"message\": \"Hello World\"&#125;if __name__ == \"__main__\": import uvicorn uvicorn.run(app=\"main:app\", reload=True, host='127.0.0.1', port=8888) 进入对应的本地地址后会获得200的响应并收到json信息：{“message”: “Hello World”} 继续跳转到http://localhost:8888/docs或者http://localhost:8888/redoc将进入由Swagger UI提供的交互式API在线文档","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"FastApi","slug":"FastApi","permalink":"http://yoursite.com/tags/FastApi/"}]},{"title":"Scrapy：爬取steam热门游戏资料","slug":"Scrapy：爬取steam热门游戏资料","date":"2022-05-01T04:00:35.000Z","updated":"2022-06-27T07:39:57.505Z","comments":true,"path":"2022/05/01/Scrapy：爬取steam热门游戏资料/","link":"","permalink":"http://yoursite.com/2022/05/01/Scrapy：爬取steam热门游戏资料/","excerpt":"有段时间没有做爬虫程序了，正好想起来还没有写过scrapy框架的博客，写个scrapy爬虫进行总结和回顾。本文代码已上传至github：https://github.com/elbadaernU404/steamgames 1.Scrapy简介搜索scrapy，可以很轻松的找到Scrapy的官方网站，正如其首页所描述的那样，scrapy是一款快速且强力的多线程异步爬虫框架，适合对静态页面（不配置中间件的情况下）的信息进行高速且大面积、高层次的抓取，避免了传统爬虫并发爬取网站数据的局限性，仅需要少量的代码即可完成爬虫工作，是一项高效率的主流爬虫工具。 2.Scrapy框架原理进入scrapy官网，可以找到官方文档中对框架最新的描述如下： 总结后就是：scrapy框架中共包括了引擎（Scrapy Engine）、Item 字段、调度器（Scheduler）、调度中间件（Scheduler Middewares）、下载器（Downloader）、下载器中间件（Downloader Middlewares）、爬虫程序（Spiders）、爬虫中间件（Spider Middlewares）和管道（Pipeline） 1.scrapy引擎会从爬虫程序中获取初始请求；2.scrapy引擎通过调度器（Scheduler）调度Requests并要求获取下一个Requests；3.调度器将下一个请求返回至scrapy引擎；4.scrapy引擎通过下载中间件将请求发送到下载器（Downloader）当中，完成一次下载后下载器会生成Response并发送给scrapy引擎；5.scrapy引擎接受下载器的Response交给爬虫程序。爬虫完成对Response的处理后，将抓取的数据和新的Requests返回给scrapy引擎，scrapy引擎将数据处理完毕后交给Item管道，再将处理的请求发送给调度器，执行下一轮工作；6.重复上述步骤，直到不再有来自调度器的请求，爬虫程序结束。","text":"有段时间没有做爬虫程序了，正好想起来还没有写过scrapy框架的博客，写个scrapy爬虫进行总结和回顾。本文代码已上传至github：https://github.com/elbadaernU404/steamgames 1.Scrapy简介搜索scrapy，可以很轻松的找到Scrapy的官方网站，正如其首页所描述的那样，scrapy是一款快速且强力的多线程异步爬虫框架，适合对静态页面（不配置中间件的情况下）的信息进行高速且大面积、高层次的抓取，避免了传统爬虫并发爬取网站数据的局限性，仅需要少量的代码即可完成爬虫工作，是一项高效率的主流爬虫工具。 2.Scrapy框架原理进入scrapy官网，可以找到官方文档中对框架最新的描述如下： 总结后就是：scrapy框架中共包括了引擎（Scrapy Engine）、Item 字段、调度器（Scheduler）、调度中间件（Scheduler Middewares）、下载器（Downloader）、下载器中间件（Downloader Middlewares）、爬虫程序（Spiders）、爬虫中间件（Spider Middlewares）和管道（Pipeline） 1.scrapy引擎会从爬虫程序中获取初始请求；2.scrapy引擎通过调度器（Scheduler）调度Requests并要求获取下一个Requests；3.调度器将下一个请求返回至scrapy引擎；4.scrapy引擎通过下载中间件将请求发送到下载器（Downloader）当中，完成一次下载后下载器会生成Response并发送给scrapy引擎；5.scrapy引擎接受下载器的Response交给爬虫程序。爬虫完成对Response的处理后，将抓取的数据和新的Requests返回给scrapy引擎，scrapy引擎将数据处理完毕后交给Item管道，再将处理的请求发送给调度器，执行下一轮工作；6.重复上述步骤，直到不再有来自调度器的请求，爬虫程序结束。 3.制作Scrapy爬虫程序3.1 目标网站：Steam商店&lt;s.team&gt;完整域名：https://store.steampowered.com/ 3.2 目标数据爬虫要提取的数据为Steam商店中全球热销榜中的游戏信息，包括：· 游戏名称· 链接· 发售日期· 好评率· 折扣· 价格（原价、折扣价，因为挂了梯子所以货币显示的是新台币） 检查元素，找到各数据的xpath 可以在chrome浏览器插件xpath helper中验证xpath是否能获取目标数据 3.3 创建Scrapy项目启动pycharm，在pycharm的终端中进入存放pycharm程序的目录，新建一个项目steamgames。在终端中输入： scrapy startproject steamgames 成功创建项目后，会得到爬虫项目的根目录steamgames文件夹，其中包含有框架已经创建号的python程序： 3.4 创建爬虫程序进入项目目录后，在终端中输入：scrapy genspider test store.steampowered.com 完成对爬虫程序的创建，命名为test 3.5 程序制作3.5.1 编写items.py首先在目录的items.py文件中添加要爬取数据的item字段,对应的信息分别为上述的· 游戏名称· 链接· 发售日期· 好评率· 折扣· 价格（原价、折扣价）123456789101112131415161718192021222324# -*- coding: utf-8 -*-# Define here the models for your scraped items## See documentation in:# https://doc.scrapy.org/en/latest/topics/items.htmlimport scrapyclass SteamgamesItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() name = scrapy.Field() link = scrapy.Field() time = scrapy.Field() evaluate = scrapy.Field() discount = scrapy.Field() price = scrapy.Field() #pass 3.5.2 修改settings.py首先因为是学习用途，关闭遵守robots协议 robots协议全称“网络爬虫排除标准”，又称爬虫协议、机器人协议等，它规定着搜索引擎抓取网站时所能抓取的内容，是网络爬虫爬行网站时第一个需要访问的文件，该文件位于网站的根目录下，文件名是robots.txt，主要用于保护网站的隐私，来防止网站重要的信息被泄露，对网站安全起到一定的作用。1ROBOTSTXT_OBEY = False 接下来启用管道程序，用于后续的数据存储。其中数字“300”表示每个管道执行的优先级，数字越小优先级越高，一般不超过1000123ITEM_PIPELINES = &#123; 'steamgames.pipelines.SteamgamesPipeline': 300,&#125; 3.5.3 scrapy shell可以通过scrapy shell + “linkurl”的方式进入scrapy shell，对项目进行测试，如测试使用的xpath是否正常获得了值（未受到反爬手段影响） 这里在终端中输入scrapy shell “https://store.steampowered.com/search/?filter=globaltopsellers&amp;os=win&quot; 成功进入scrapy shell： 进行xpath测试，输入： response.xpath(“//*[@id=’search_resultsRows’]/a/div[2]/div[4]/div[1]/span/text()”).extract()response.xpath(“//*[@id=’search_resultsRows’]/a/div[2]/div[1]/span/text()”).extract() 得到结果，测试成功： 3.5.4 制作爬虫程序test.py进入已经生成好的test.py程序当中，程序中已存在的代码123name = 'test'allowed_domains = ['store.steampowered.com']start_urls = ['https://store.steampowered.com/search/?filter=globaltopsellers&amp;os=win'] 分别表示爬虫的名称、爬取的域名界限（网站内）、爬虫的起始网址 爬虫程序可以有多个，爬取的网址子路由也可以有多个，但是不能超出一开始设定的范围之外 在类TestSpider中，函数parse名称不可变，因为其类本身继承自scrapy的Spider，我们只是改写其中的方法，实现爬虫功能。 为了防止在函数结尾return item导致整个函数终止运行，而不能继续完成之后的数据传输进管道程序中，故这里使用的是yield关键字实现一个迭代器1234567891011121314151617181920def parse(self, response): node_list = response.xpath(\"//*[@id='search_resultsRows']/a\") print(node_list) for node in node_list: item = SteamgamesItem() name = node.xpath(\"./div[2]/div[1]/span/text()\").extract() link = node.xpath(\"./@href\").extract() time = node.xpath(\"./div[2]/div[2]/text()\").extract() evaluate = node.xpath(\"./div[2]/div[3]/span/@data-tooltip-html\").extract() discount = node.xpath(\"./div[2]/div[4]/div[1]/span/text()\").extract() price = node.xpath('normalize-space(./div[2]/div[4]/div[2])').extract() item['name'] = name[0] item['link'] = link[0] tem['time'] = time[0] item['evaluate'] = evaluate[0] item['discount'] = discount[0] item['price'] = price[0] yield item 到这里可以使用命令scrapy crawl test -o steamgames.json来将结果直接输出为json格式的文件 但是测试后如果报错： · KeyError: Spider not found可能是由于运行的爬虫文件与test.py中的爬虫名称不符，或者创建的爬虫文件未能放入spiders文件夹当中，检查即可 · IndexError: list index out of range因为并不是所有游戏列出了折扣、折扣价格等情况，所以在得到空列表时会出现异常，增加异常处理，如果发生异常则输出为“暂时没有折扣”，或“详情请查阅游戏链接”即可 修改函数代码如下：1234567891011121314151617181920212223242526def parse(self, response): node_list = response.xpath(\"//*[@id='search_resultsRows']/a\") print(node_list) for node in node_list: item = SteamgamesItem() name = node.xpath(\"./div[2]/div[1]/span/text()\").extract() link = node.xpath(\"./@href\").extract() time = node.xpath(\"./div[2]/div[2]/text()\").extract() evaluate = node.xpath(\"./div[2]/div[3]/span/@data-tooltip-html\").extract() discount = node.xpath(\"./div[2]/div[4]/div[1]/span/text()\").extract() price = node.xpath('normalize-space(./div[2]/div[4]/div[2])').extract() item['name'] = name[0] item['link'] = link[0] try: item['time'] = time[0] except: item['time'] = 'See link for details' item['evaluate'] = evaluate[0] try: item['discount'] = discount[0] except: item['discount'] = 'No discount for now' item['price'] = price[0].strip() yield item 3.5.5 制作管道程序pipelines.py爬虫主体完成之后，需要编写数据存储的管道程序。这里使用open方法，创建一个以json格式保存的steamgames.csv文件,没完成一组数据下载，进行换行12345678910111213import jsonclass SteamgamesPipeline(object): def __init__(self): self.f=open('steamgames.csv','wb+') def process_item(self, item, spider): content=json.dumps(dict(item),ensure_ascii=False)+',\\n' self.f.write(content.encode(encoding='utf-8')) return item def close_spider(self,spider): self.f.close() 输出后得到结果，仔细查看有位置错位、长空格等情况： 发现问题的原因：因为json数据是以“，”进行分隔的，而steam商城中的年份、价格、人数等信息都是以“，”分隔千分位，以及折扣价格的标签中带有换行符，所以会造成误差。 为了实现正常的存储和格式的美观，继续完善test.py爬虫程序 可以通过xpath(‘normalize-space(./…)’)或使用字符串方法strip()的方式去除文字中多余的空格 使用列表推导式和字符串方法replace()结合，将数据中的“，”替换成空格或其他字符：[i.replace(“,”,“”) for i in node.xpath(‘…’).extract()] 完整的test.py文件如下：123456789101112131415161718192021222324252627282930313233343536import scrapyfrom steamgames.items import SteamgamesItemclass TestSpider(scrapy.Spider): name = 'test' allowed_domains = ['store.steampowered.com'] start_urls = ['https://store.steampowered.com/search/?filter=globaltopsellers&amp;os=win'] def parse(self, response): node_list = response.xpath(\"//*[@id='search_resultsRows']/a\") print(node_list) for node in node_list: item = SteamgamesItem() name = node.xpath(\"./div[2]/div[1]/span/text()\").extract() link = node.xpath(\"./@href\").extract() time = [i.replace(\",\",\" \") for i in node.xpath(\"./div[2]/div[2]/text()\").extract()] evaluate = [j.replace(\",\",\" \") for j in [i.replace(\"&lt;br&gt;\",\",\") for i in node.xpath(\"./div[2]/div[3]/span/@data-tooltip-html\").extract()]] discount = node.xpath(\"./div[2]/div[4]/div[1]/span/text()\").extract() price = [i.replace(\",\",\"\") for i in node.xpath('normalize-space(./div[2]/div[4]/div[2])').extract()] item['name'] = name[0] item['link'] = link[0] try: item['time'] = time[0] except: item['time'] = 'See link for details' item['evaluate'] = evaluate[0] try: item['discount'] = discount[0] except: item['discount'] = 'No discount for now' item['price'] = price[0].strip() yield item 4.结果完成scrapy爬虫后，执行scrapy crawl test最终得到一个.csv格式的输出结果： 至此，scrapy框架下的爬虫程序完成，已获取到初步的游戏资讯，可以据此对其进行进一步的数据清洗和分析 4.1 pandas数据清洗将文件重新输出一份json格式的steamgames.json文件，在steamgames当前目录下新建pd_steamgames.py文件准备用于数据的处理1234import pandas as pddf = pd.read_json('steamgames.json',encoding='utf-8')print(df.to_string()) 输出后得到一个pandas DataFrame类型的结果: 这里制定一个清洗目标：仅筛选出打折的热销游戏12345import pandas as pddf = pd.read_json('steamgames.json',encoding='utf-8')df = df[df['discount']!='No discount for now']print(df.to_string()) 进而对数据进行分析，拟找到折扣与好评率对应关系，研究折扣是否会对评价造成影响。这里丢弃掉除评价和折扣外的其他数据行1df = df[df['discount']!='No discount for now'].drop(['name','link','time','price'],axis=1) 利用正则表达式清洗掉评价数据中的人数和字符串等信息，同时给数据清除原始的索引：123for i in df.columns: df[i] = df[i].str.extract('(\\d+%?)')print(df.to_string(index=False)) 4.2 数据分析可视化完成数据的清洗后，导入matpoltlib库，对数据进行可视化展示，完整的pd_steamgames.py文件代码如下：1234567891011121314151617import pandas as pdimport matplotlib.pyplot as pltdf = pd.read_json('steamgames.json',encoding='utf-8')df = df[df['discount']!='No discount for now'].drop(['name','link','time','price'],axis=1)for i in df.columns: df[i] = df[i].str.extract('(\\d+)')df=df.astype(float)df.plot()my_y_ticks = [20,40,60,80,100]plt.yticks(my_y_ticks)plt.xlabel('Number of games')plt.ylabel('favorable rate(%)')plt.title('evaluate &amp; discount')plt.show() 至此，不难得出两者之间并没有严重的线性依赖关系。有的游戏折扣很高，好评率也一直很高，有的游戏给出很少折扣，好评率却依然处于中高水平。当然这和本次使用的数据为全球热销游戏也有一定关系，数据量也较为有限。时间关系这些都是之后将解决的问题，继续尝试多挖掘数据，再多多去做数据分析","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"scrapy","slug":"scrapy","permalink":"http://yoursite.com/tags/scrapy/"}]},{"title":"Python爬虫：识别网页的验证码（tesseract-OCR）","slug":"Python爬虫：识别网页的验证码（tesseract-OCR）","date":"2022-04-29T07:00:29.000Z","updated":"2022-06-27T07:39:04.522Z","comments":true,"path":"2022/04/29/Python爬虫：识别网页的验证码（tesseract-OCR）/","link":"","permalink":"http://yoursite.com/2022/04/29/Python爬虫：识别网页的验证码（tesseract-OCR）/","excerpt":"在实际的爬虫操作中，处于安全等原因网站会设置非常多的反爬虫手段来限制网络爬虫，最常见的比如设置图形验证码，来识别访客是否为机器人 但是由于python拥有许多强大的图像识别库，所以最简单也是最原始的图形验证码目前已经慢慢没落了，我找了很久发现中国知网的注册界面目前居然还在使用中~ ^ ^ 获取网页验证码首先是获取验证码图片的目标链接，我使用xpath找到html标签中验证码的src链接，读取内容后发现并不能得到生成验证码的对应id，以为是知网做了对爬虫头的限制，于是在headers里添加了“Referer”和“Accept-Encoding”等信息，但是并没有效果。抓包在Doc中发现网页HTTP请求的方法为”GET“方法，尝试不加id也得到了验证码图片 12345678910111213141516171819202122232425262728import requestsfrom lxml import etreeheaders = &#123; \"method\": \"GET\", \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\", \"Accept-Encoding\": \"gzip, deflate, br\", \"Cache-Control\": \"max-age=0\", \"scheme\": \"https\", \"accept-language\": \"zh-CN,zh;q=0.9\", \"Cookie\": \"U:M_distinctid=1809edfcf6e8ff-088ab0a6edb385-17333273-1fa400-1809edfcf6f9fe; _pk_ref=[\"\",\"\",1651932918,'https://www.baidu.com/link?url=PnlS4wubLFxXVNXrTBRCZMAhA0P5TZLlCBAhBvXfLke&amp;wd=&amp;eqid=e52482a8001102070000000562767ee2']; Ecp_ClientId=1220507221503170167; Ecp_IpLoginFail=22050736.161.51.128; Ecp_ClientIp=36.161.51.128; language=chs; _pk_id=2ce0e7fb-229f-4440-a78b-57771deac2fe.1651932918.1.1651933578.1651932918.; ASP.NET_SessionId=he1no3kmgo3bwuktv1xa1lhs; SID_mycnki=020101\", \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36\", \"Referer\": \"https://www.cnki.net/\" \"Sec-Fetch-Dest:\" 'document', \"Sec-Fetch-Mode\": 'navigate', \"Sec-Fetch-Site\": 'same-origin', \"Sec-Fetch-User\": '?1', \"Upgrade-Insecure-Requests\": '1'&#125;url = \"https://my.cnki.net/Register/CommonRegister.aspx?returnUrl=https://www.cnki.net#\"r = requests.get(url, headers=headers).content.decode('utf-8')html = etree.HTML(r)link=html.xpath('//*[@id=\"commonRe\"]/div[10]/div[3]/a/img/@src')[0]img_url = \"https://my.cnki.net/Register/\" + linkimg = requests.get(img_url, headers=headers,stream=True)with open('checkcode.jpg','wb')as file: file.write(img.raw.read())","text":"在实际的爬虫操作中，处于安全等原因网站会设置非常多的反爬虫手段来限制网络爬虫，最常见的比如设置图形验证码，来识别访客是否为机器人 但是由于python拥有许多强大的图像识别库，所以最简单也是最原始的图形验证码目前已经慢慢没落了，我找了很久发现中国知网的注册界面目前居然还在使用中~ ^ ^ 获取网页验证码首先是获取验证码图片的目标链接，我使用xpath找到html标签中验证码的src链接，读取内容后发现并不能得到生成验证码的对应id，以为是知网做了对爬虫头的限制，于是在headers里添加了“Referer”和“Accept-Encoding”等信息，但是并没有效果。抓包在Doc中发现网页HTTP请求的方法为”GET“方法，尝试不加id也得到了验证码图片 12345678910111213141516171819202122232425262728import requestsfrom lxml import etreeheaders = &#123; \"method\": \"GET\", \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\", \"Accept-Encoding\": \"gzip, deflate, br\", \"Cache-Control\": \"max-age=0\", \"scheme\": \"https\", \"accept-language\": \"zh-CN,zh;q=0.9\", \"Cookie\": \"U:M_distinctid=1809edfcf6e8ff-088ab0a6edb385-17333273-1fa400-1809edfcf6f9fe; _pk_ref=[\"\",\"\",1651932918,'https://www.baidu.com/link?url=PnlS4wubLFxXVNXrTBRCZMAhA0P5TZLlCBAhBvXfLke&amp;wd=&amp;eqid=e52482a8001102070000000562767ee2']; Ecp_ClientId=1220507221503170167; Ecp_IpLoginFail=22050736.161.51.128; Ecp_ClientIp=36.161.51.128; language=chs; _pk_id=2ce0e7fb-229f-4440-a78b-57771deac2fe.1651932918.1.1651933578.1651932918.; ASP.NET_SessionId=he1no3kmgo3bwuktv1xa1lhs; SID_mycnki=020101\", \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36\", \"Referer\": \"https://www.cnki.net/\" \"Sec-Fetch-Dest:\" 'document', \"Sec-Fetch-Mode\": 'navigate', \"Sec-Fetch-Site\": 'same-origin', \"Sec-Fetch-User\": '?1', \"Upgrade-Insecure-Requests\": '1'&#125;url = \"https://my.cnki.net/Register/CommonRegister.aspx?returnUrl=https://www.cnki.net#\"r = requests.get(url, headers=headers).content.decode('utf-8')html = etree.HTML(r)link=html.xpath('//*[@id=\"commonRe\"]/div[10]/div[3]/a/img/@src')[0]img_url = \"https://my.cnki.net/Register/\" + linkimg = requests.get(img_url, headers=headers,stream=True)with open('checkcode.jpg','wb')as file: file.write(img.raw.read()) Tesserocr库Tesserocr是python的一个OCR识别库，是通过对tesseract库做API封装来实现功能，核心仍是tesseract。所以需提前安装tesseract库来进行支持 安装Tesseract进入官网链接https://digi.bib.uni-mannheim.de/tesseract/，找到当前合适的版本，下载安装程序 安装过程中可以勾选安装额外的语言文件，添加对包括中文等多语种的支持 安装完成后去系统设置中添加环境变量 新增一个变量TESSDATA_PREFIX 打开CMD，输入tesseract -v，此时出现版本信息，tesseract已经安装完成了 如果添加后控制台仍提示”tesseract不是内部或者外部命令”，继续到系统变量的path中添加“%SystemRoot%\\system32;%SystemRoot%;%SystemRoot%\\System32\\Wbem“即可 安装Tesserocr安装完Tesseract后，继续安装python的OCR识别库,打开CMD或Pycharm12pip install Pillowpip install pytesseract 完成后找到pytesseract文件，将tesseract.exe程序的路径添加进去（注：不同版本路径代码在程序中的位置不一定相同） 最后再执行安装tesserocr 1pip install tesserocr 如果报错，进入tesserocr在github的官网https://github.com/simonflueckiger/tesserocr-windows_build/releases，下载对应版本的whl文件进行手动安装 进入对应路径，来到控制台执行pip install tesserocr-2.5.2-cp37-cp37m-win_amd64.whl 最后显示tesserocr安装成功 图像处理灰度图使用Image的open方法将下载的checkcode.jpg打开，得到一个PIL.Image.Image对象，可以通过convert方法来将RGB彩色图片转换为灰度图1234567from PIL import Imageimport tesserocrimg = Image.open(\"checkcode.jpg\")img.show()img_grey = img.convert(\"L\")img_grey.show() 得到原始图片和灰度图像，使用tesserocr库进行识别，由于图像的干绕因素和噪点还比较多，目前还不能识别出准确结果123456789101112from PIL import Imageimport tesserocrimg = Image.open(\"checkcode.jpg\")img.show()img_grey = img.convert(\"L\")img_grey.show()img_grey.save('checkcode_grey.jpg')checkcode = tesserocr.image_to_text(img)checkcode_grey = tesserocr.image_to_text(img_grey)print(f\"checkcode:&#123;checkcode&#125;\")print(f\"checkcode_grey:&#123;checkcode_grey&#125;\") 二值化灰度从得到的灰度图结果来看，需要识别的数字和字母（主体部分）明显要比干绕部分清晰的，灰度也更重，所以接下来可对灰度图进行二值化，即设定一个阈值，每个像素的灰度将依照和阈值的关系修改为黑色或白色，这样就可以很好的凸显出主体部分123456789101112131415from PIL import Imagedef bin_table(threshold=125): table = [] for i in range(256): if i &lt; threshold: table.append(0) else: table.append(1) return tableimg = Image.open('checkcode_grey.jpg')table = bin_table()bin = img.point(table, '1')bin.save('checkcode_bin.jpg') 此时再对图像进行识别，得到的结果为”g4YS”,已经很接近了，但是问题出在哪里呢？ 将图像放大： 可以发现图像中还要很多噪点 图像降噪py中降噪的方法有很多，像椒盐降噪、高斯降噪、均值滤波降噪等，需要找到合适的降噪方法。这里根据图片，使用8邻域降噪的方法，代码如下1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253from PIL import Imagedef sum_9_region_new(img, x, y): '''确定噪点 ''' cur_pixel = img.getpixel((x, y)) # 当前像素点的值 width = img.width height = img.height if cur_pixel == 1: # 如果当前点为白色区域,则不统计邻域值 return 0 # 因当前图片的四周都有黑点，所以周围的黑点可以去除 if y &lt; 3: # 本例中，前两行的黑点都可以去除 return 1 elif y &gt; height - 3: # 最下面两行 return 1 else: # y不在边界 if x &lt; 3: # 前两列 return 1 elif x == width - 1: # 右边非顶点 return 1 else: # 具备9领域条件的 sum = img.getpixel((x - 1, y - 1)) \\ + img.getpixel((x - 1, y)) \\ + img.getpixel((x - 1, y + 1)) \\ + img.getpixel((x, y - 1)) \\ + cur_pixel \\ + img.getpixel((x, y + 1)) \\ + img.getpixel((x + 1, y - 1)) \\ + img.getpixel((x + 1, y)) \\ + img.getpixel((x + 1, y + 1)) return 9 - sumdef collect_noise_point(img): '''收集所有的噪点''' noise_point_list = [] for x in range(img.width): for y in range(img.height): res_9 = sum_9_region_new(img, x, y) if (0 &lt; res_9 &lt; 3) and img.getpixel((x, y)) == 0: # 找到孤立点 pos = (x, y) noise_point_list.append(pos) return noise_point_listdef remove_noise_pixel(img, noise_point_list): '''根据噪点的位置信息，消除二值图片的黑点噪声''' for item in noise_point_list: img.putpixel((item[0], item[1]), 1)bin = Image.open('checkcode_bin.jpg')noise_point_list = collect_noise_point(bin)remove_noise_pixel(bin, noise_point_list)bin.save('checkcode_fin.jpg') 不过运行完成后任然带噪点，我才反应过来是保存的.jpg格式对图像进行了压缩造成的，不过整体思路没有问题。将代码整合到一起，再保存为.png格式，完整代码如下12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970from PIL import Imagedef sum_9_region_new(img, x, y): '''确定噪点 ''' cur_pixel = img.getpixel((x, y)) # 当前像素点的值 width = img.width height = img.height if cur_pixel == 1: # 如果当前点为白色区域,则不统计邻域值 return 0 # 因当前图片的四周都有黑点，所以周围的黑点可以去除 if y &lt; 3: # 本例中，前两行的黑点都可以去除 return 1 elif y &gt; height - 3: # 最下面两行 return 1 else: # y不在边界 if x &lt; 3: # 前两列 return 1 elif x == width - 1: # 右边非顶点 return 1 else: # 具备9领域条件的 sum = img.getpixel((x - 1, y - 1)) \\ + img.getpixel((x - 1, y)) \\ + img.getpixel((x - 1, y + 1)) \\ + img.getpixel((x, y - 1)) \\ + cur_pixel \\ + img.getpixel((x, y + 1)) \\ + img.getpixel((x + 1, y - 1)) \\ + img.getpixel((x + 1, y)) \\ + img.getpixel((x + 1, y + 1)) return 9 - sumdef collect_noise_point(img): '''收集所有的噪点''' noise_point_list = [] for x in range(img.width): for y in range(img.height): res_9 = sum_9_region_new(img, x, y) if (0 &lt; res_9 &lt; 3) and img.getpixel((x, y)) == 0: # 找到孤立点 pos = (x, y) noise_point_list.append(pos) return noise_point_listdef remove_noise_pixel(img, noise_point_list): '''根据噪点的位置信息，消除二值图片的黑点噪声''' for item in noise_point_list: img.putpixel((item[0], item[1]), 1)def bin_table(threshold=125): '''获取灰度转二值的映射table,0表示黑色,1表示白色''' table = [] for i in range(256): if i &lt; threshold: table.append(0) else: table.append(1) return tabledef main(): img = Image.open('checkcode.jpg') img_grey = img.convert('L') table = bin_table() bin = img_grey.point(table, '1') noise_point_list = collect_noise_point(bin) remove_noise_pixel(bin, noise_point_list) bin.save('checkcode_fin.png')if __name__ == '__main__': main() 放大以后没有干扰： 结果1234567import pytesseractfrom PIL import Imageim = Image.open('checkcode_bin.png')string = pytesseract.image_to_string(im)print(string)","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"OCR","slug":"OCR","permalink":"http://yoursite.com/tags/OCR/"}]},{"title":"Numpy & Pandas","slug":"Numpy-Pandas","date":"2022-03-01T05:13:22.000Z","updated":"2022-06-27T07:35:40.856Z","comments":true,"path":"2022/03/01/Numpy-Pandas/","link":"","permalink":"http://yoursite.com/2022/03/01/Numpy-Pandas/","excerpt":"本文主要记录一些Numpy和Pandas的方法 Numpy1.惯用写法：1import numpy as np 2.从列表生成一个矩阵：123import numpy as npprint(np.array([[0,1,2,3,4,5],[6,7,8,9,10,11]])) 3.创建全0或全1矩阵：1234import numpy as npprint(np.zeros((3,4)))print(np.ones((5,5))) 4.通过.shape得到当前矩阵的尺寸，通过.reshape()将当前矩阵转置12345import numpy as npprint(np.zeros((3,4)))print(np.zeros((3,4)).shape)print(np.zeros((3,4)).reshape(4,3)) 5.使用arange()创建递增/递减的数列123import numpy as npprint(np.arange(1,10)) 6.使用linspace()获取一个等分的区间序列，第三个参数是输出样本的总数123import numpy as npprint(np.linspace(1,100,10)) 7.通过random.rand()生成随机数组123import numpy as npprint(np.random.rand(3,4)) 8.Numpy中默认的数据类型为64位浮点数，可以在创建数组时使用dtype参数指定其他的数据类型123import numpy as npprint(np.ones((3,4),dtype=np.float32)) 9.对先有的数组，也可以使用astype()来更改数据类型123456import numpy as nparray0=np.ones((3,4),dtype=np.float32)array1=array0.astype(int)print(array0,array1) 10.Numpy数组可以直接与一个数进行运算，这个过程称之为广播；两个不同尺寸的数组也可以直接进行运算，这期间numpy会自动将两个数组扩展到相同的尺寸12345678910import numpy as nparray0=np.array([1,2,3])array1=np.array([4,5,6])array2=np.array([[7], [8], [9]])print(array0*10)print(array1+array2)","text":"本文主要记录一些Numpy和Pandas的方法 Numpy1.惯用写法：1import numpy as np 2.从列表生成一个矩阵：123import numpy as npprint(np.array([[0,1,2,3,4,5],[6,7,8,9,10,11]])) 3.创建全0或全1矩阵：1234import numpy as npprint(np.zeros((3,4)))print(np.ones((5,5))) 4.通过.shape得到当前矩阵的尺寸，通过.reshape()将当前矩阵转置12345import numpy as npprint(np.zeros((3,4)))print(np.zeros((3,4)).shape)print(np.zeros((3,4)).reshape(4,3)) 5.使用arange()创建递增/递减的数列123import numpy as npprint(np.arange(1,10)) 6.使用linspace()获取一个等分的区间序列，第三个参数是输出样本的总数123import numpy as npprint(np.linspace(1,100,10)) 7.通过random.rand()生成随机数组123import numpy as npprint(np.random.rand(3,4)) 8.Numpy中默认的数据类型为64位浮点数，可以在创建数组时使用dtype参数指定其他的数据类型123import numpy as npprint(np.ones((3,4),dtype=np.float32)) 9.对先有的数组，也可以使用astype()来更改数据类型123456import numpy as nparray0=np.ones((3,4),dtype=np.float32)array1=array0.astype(int)print(array0,array1) 10.Numpy数组可以直接与一个数进行运算，这个过程称之为广播；两个不同尺寸的数组也可以直接进行运算，这期间numpy会自动将两个数组扩展到相同的尺寸12345678910import numpy as nparray0=np.array([1,2,3])array1=np.array([4,5,6])array2=np.array([[7], [8], [9]])print(array0*10)print(array1+array2) 11.Numpy中两个相同尺寸的数组可以直接进行四则运算123456789import numpy as nparray0=np.array([1,2,3])array1=np.array([4,5,6])array2=np.array([7,8,9])print(array0+array1)print(array0*array2)print(array2/array0) 12.使用np.dot()函数来将两个数组元素进行向量的点乘运算，或使用 @ 符号将两个矩阵进行矩阵的乘法1234567891011import numpy as nparray0=np.array([1,2,3])array1=np.array([4,5,6])array2=np.array([[7,8], [9,10]])array3=np.array([[11,12], [13,14]])print(array0.dot(array1))print(array2 @ array3) 13.基本初等运算还可以使用： · np.sin()/np.cos()/np.tan()进行三角函数运算 · np.sqrt()进行开方运算 · np.log()进行对数运算 · np.power()进行指数运算12345678import numpy as nparray0=np.array([1,2,3])print(np.sin(array0),np.cos(array0),np.tan(array0))print(np.sqrt(array0))print(np.log(array0))print(np.power(array0,2)) 14.argmin()和argmax()可以返回数组中最小/最大元素的索引123456import numpy as nparray0=np.array([1,2,3])print(array0.argmin())print(array0.argmax()) 15.统计运算使用： · sum()获取所有元素和 · mean()获取数据平均数 · np.median()获取数据中位数 · var()获取数据方差 · std()获取数据标准差 其中可以通过修改参数axis的值来获取对应矩阵的行或列，也可以使用类似Python数组的表示或列表的切片来获取对应的元素/矩阵对应的行列1234567891011import numpy as nparray0=np.array([[1,2,3,4,5,6,7,8,9,10], [11,12,13,14,15,16,17,18,19,20], [21,22,23,24,25,26,27,28,29,30]])print(array0[1:2,0:5].sum(axis=0),array0.sum(axis=1))print(array0[1,:].mean(),array0.mean(axis=0))print(np.median(array0[2,:]),np.median(array0,axis=1))print(array0[:,5:-1].var(),array0.var(axis=0))print(array0.std(axis=0),array0.std(axis=1)) 16.筛选数据可以通过如下方式进行数据筛选，如在方括号中输入array0&gt;6则返回所有&gt;6的数据。也可以配合逻辑运算符一起使用1234567import numpy as nparray0=np.array([[1,2,3,4,5,6,7,8,9,10], [11,12,13,14,15,16,17,18,19,20], [21,22,23,24,25,26,27,28,29,30]])print(array0[array0&gt;6]) 17.Numpy图片处理通常对于灰度图片，可以将其当作二维数组处理，数组中的每个元素代表对应像素的亮度。对于RGB彩色图片，可以用三维数组处理，第三维分别存储像素点对应的R/G/B值 使用pillow库打开一张图片1234from PIL import Imageimg=Image.open('kite.png')img.show() 1234from PIL import Imageimg=Image.open('kite.png')img.show() 将图像转化为numpy数组形式，下面以flower.png为例1234567import numpy as npfrom PIL import Imageimg=Image.open('flower.png')img=np.array(img)print(img.shape) 可以看到这个图像共有3456行，5184列像素，每个像素包括RGB三个颜色的分量 通过类似列表切片的方式提取出图像的所有绿色分量12345678import numpy as npfrom PIL import Imageimg=Image.open('flower.png')img=np.array(img)img_red=img[:,:,1]Image.fromarray(img_red).show() 也可以对图像进行裁剪12345678import numpy as npfrom PIL import Imageimg=Image.open('flower.png')img=np.array(img)img_cut=img[1000:2500,900:3200,:]Image.fromarray(img_cut).show() 通过广播的方式做图像增强，常用作机器学习领域1234567import numpy as npfrom PIL import Imageimg=Image.open('flower.png')img=np.array(img)*15Image.fromarray(img).show() Pandas1.惯用写法：1import pandas as pd 2.创建pandas序列1234import numpy as npimport pandas as pdprint(pd.Series([1,2,3,np.nan,5,6])) 3.创建pandas数据结构DataFrame可以通过一般形式创建或以一个类似于字典的形式创建pandas DataFrame1234567891011121314import numpy as npimport pandas as pddates=pd.date_range('20220301',periods=5)df0=pd.DataFrame(np.random.randn(5,10),index=dates,columns=['A','B','C','D','E','F','G','H','I','I'])df1=pd.DataFrame(&#123;'A':1., 'B':pd.Timestamp('20220301'), 'C':pd.Series(1,index=list(range(4)),dtype='float32'), 'D':np.array([3]*4,dtype='int32'), 'E':pd.Categorical([\"test\",\"train\",\"test\",\"train\"]), 'F':'foo'&#125;)print(df0)print(df1) 也可以直接得出DataFrame的一些属性123456789101112131415import numpy as npimport pandas as pddates=pd.date_range('20220301',periods=5)df0=pd.DataFrame(np.random.randn(5,10),index=dates,columns=['A','B','C','D','E','F','G','H','I','I'])df1=pd.DataFrame(&#123;'A':1., 'B':pd.Timestamp('20220301'), 'C':pd.Series(1,index=list(range(4)),dtype='float32'), 'D':np.array([3]*4,dtype='int32'), 'E':pd.Categorical([\"test\",\"train\",\"test\",\"train\"]), 'F':'foo'&#125;)print(df0.columns)print(df1.values)print(df1.describe) 4.获取DataFrame元素下列print()中的两种方式均可：1234567891011import numpy as npimport pandas as pddf1=pd.DataFrame(&#123;'A':1., 'B':pd.Timestamp('20220301'), 'C':pd.Series(1,index=list(range(4)),dtype='float32'), 'D':np.array([3]*4,dtype='int32'), 'E':pd.Categorical([\"test\",\"train\",\"test\",\"train\"]), 'F':'foo'&#125;)print(df1.A,df1['A']) 也可以通过切片索引的方式来获取1234567891011import numpy as npimport pandas as pddf1=pd.DataFrame(&#123;'A':1., 'B':pd.Timestamp('20220301'), 'C':pd.Series(1,index=list(range(4)),dtype='float32'), 'D':np.array([3]*4,dtype='int32'), 'E':pd.Categorical([\"test\",\"train\",\"test\",\"train\"]), 'F':'foo'&#125;)print(df1[1:4]) 还可以通过标签来获取1234567import numpy as npimport pandas as pddates=pd.date_range('20220301',periods=6)df0=pd.DataFrame(np.arange(24).reshape((6,4)),index=dates,columns=['A','B','C','D'])print(df0.loc['20220302'],['A','B']) 通过位置来获取12345678import numpy as npimport pandas as pddates=pd.date_range('20220301',periods=6)df0=pd.DataFrame(np.arange(24).reshape((6,4)),index=dates,columns=['A','B','C','D'])print(df0.iloc[[1,3,5],[1,3]])![获取DataFrame元素4](/7.png) 4.修改DataFrame中的值可以通过之前查找标签和位置的方式对值进行修改：12345678910import numpy as npimport pandas as pddates=pd.date_range('20220301',periods=6)df0=pd.DataFrame(np.arange(24).reshape((6,4)),index=dates,columns=['A','B','C','D'])df0.loc['20220304','B']=9999df0.iloc[2,3]=1000print(df0) 也可以通过逻辑表达式修改数据：123456789import numpy as npimport pandas as pddates=pd.date_range('20220301',periods=6)df0=pd.DataFrame(np.arange(24).reshape((6,4)),index=dates,columns=['A','B','C','D'])df0[df0.B&gt;10]=100print(df0) 5.删除缺失数据可以通过dropna()来清除缺失数据，其中参数how的值为‘any’表示有缺失即丢弃，值为‘all’则表示全部缺失后才丢弃123456789import numpy as npimport pandas as pddates=pd.date_range('20220301',periods=6)df0=pd.DataFrame(np.arange(24).reshape((6,4)),index=dates,columns=['A','B','C','D'])df0.iloc[0,1]=np.nandf0.iloc[2,3]=np.nanprint(df0.dropna(axis=0,how='any')) 6.填补缺失数据可以通过fillna()来填补缺失数据123456789import numpy as npimport pandas as pddates=pd.date_range('20220301',periods=6)df0=pd.DataFrame(np.arange(24).reshape((6,4)),index=dates,columns=['A','B','C','D'])df0.iloc[0,1]=np.nandf0.iloc[2,3]=np.nanprint(df0.fillna(value=0)) 7.审查缺失数据通过isnull()来检查数据是否为空，检查结果会返回布尔值123456789import numpy as npimport pandas as pddates=pd.date_range('20220301',periods=6)df0=pd.DataFrame(np.arange(24).reshape((6,4)),index=dates,columns=['A','B','C','D'])df0.iloc[0,1]=np.nandf0.iloc[2,3]=np.nanprint(df0.isnull()) 如果要检查数据中是否存在丢失数据，可以这样123456789import numpy as npimport pandas as pddates=pd.date_range('20220301',periods=6)df0=pd.DataFrame(np.arange(24).reshape((6,4)),index=dates,columns=['A','B','C','D'])df0.iloc[0,1]=np.nandf0.iloc[2,3]=np.nanprint(np.any(df0.isnull()==True)) 一旦返回结果为True，则意味着数据中有缺失 8.Pandas的数据导入导出通常使用内置函数如： · read_csv()：打开csv文件 · read_excel()：打开excel文件 · read_json()：打开json文件 · read_html()：打开html文件 · read_pickle()：打开pickle文件 · read_sql()：打开数据库文件 … 类似的，要存储为对应的文件，可以使用： · to_csv()：存储为csv文件 · to_excel()：存储为excel文件 · to_json()：存储为json文件 · to_html()：存储为html文件 · to_pickle()：存储为pickle文件 · to_sql()：存储为数据库文件 … 9.Pandas数据合并可以使用pd.concat()来合并pandas的DataFrame12345678910import numpy as npimport pandas as pddf0=pd.DataFrame(np.ones((3,4))*0,columns=['A','B','C','D'])df1=pd.DataFrame(np.ones((3,4))*1,columns=['A','B','C','D'])df2=pd.DataFrame(np.ones((3,4))*2,columns=['A','B','C','D'])print(df1,'\\n',df1,'\\n',df2)print(pd.concat([df0,df1,df2],axis=0)) 如果两组数据的索引、类型不同，也可以进一步将它们合并 其中join中‘inner’为裁剪模式，默认‘out’为合并模式，会为两者间缺失的部分赋空值 将ignore_index的值赋为True可以重置索引123456789import numpy as npimport pandas as pddf0=pd.DataFrame(np.ones((3,4))*0,columns=['A','B','C','D'],index=[1,2,3])df1=pd.DataFrame(np.ones((3,4))*1,columns=['B','C','D','E'],index=[2,3,5])print(df1,'\\n',df1)print(pd.concat([df0,df1],join='inner',ignore_index=True))","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"numpy、pandas","slug":"numpy、pandas","permalink":"http://yoursite.com/tags/numpy、pandas/"}]},{"title":"Django项目：订单管理系统","slug":"Django项目：订单管理系统","date":"2022-02-26T18:21:08.000Z","updated":"2022-06-27T07:28:27.634Z","comments":true,"path":"2022/02/27/Django项目：订单管理系统/","link":"","permalink":"http://yoursite.com/2022/02/27/Django项目：订单管理系统/","excerpt":"1.Django简介Django是一款开源的python web应用框架，采用MTV(Model-Template-View)模式，其中Model负责对象与数据库的映射(ORM)，Template通过接口等方式将前端页面传递给用户，View视图函数对应业务逻辑，在合适的时候调用Model或Template 除去MTV模型以外，Django还有一个URL控制器，分别将不同的URL请求传递给不同的View视图函数处理，再通过各自的View函数来调用对应的Model和Template 2.Django项目：订单管理系统1.Django项目的创建在python 3中安装完成Django库后，可以在CMD或Pycharm终端里执行：django-admin startproject itemproj来创建Django项目 也可以在Pycharm专业版本直接创建一个Django项目，这样会自动生成一个空的Template文件夹，以及在settings.py中生成Template对应的路径。还可以直接创建初始APP，这里没有做 (本文使用的Django版本为3.2.4) 执行创建完成后，系统会为我们生成这样一个初始目录： 其中： · manage.py是Django项目管理的工具脚本，执行Django命令时通过这个文件进行管理操作 · itemproj/settings.py是Django项目的配置文件，包括了App配置、Django中间件、数据库等重要设置，在实际开发中要经常修改其中内容 · itemproj/urls.py是URL路由映射文件，其中声明了前端发送的各种http请求，再通过各级路由文件来寻找到对应的View视图函数来进行处理 · itemproj/wsgi.py，WSGI是Python web服务网关接口规范(Web Server Gateway Interface)的简称，由wsgi web server和wsgi web application两部分组成，是运行在同一个python进程中的两个模块 wsgi web sever接收了http请求以后调用wsgi web application接口，处理请求的具体内容，处理完成后再将结果返回给wsgi web server，通过wsgi web server将请求传递给前端，如图所示(Django官方文档)： · itemproj/asgi.py，ASGI即异步网关协议接口(Asynchronous Server Gateway Interface)，是一个介于网络协议服务和python应用之间的标准接口，用于处理通用类型的协议。参考自Django官方文档的资料，ASGI是为了支持异步网络服务器和应用而出现的新的python标准，可以将ASGI理解为WSGI协议的扩展 2.Django生命周期了解了Django项目基本组成后，Django的生命周期也基本明朗：前端发起URL请求 ➞ WSGI ➞ Django中间件 ➞ URL路由 ➞ View视图函数 ➞ Model数据库交互 ➞ Template模板进行html渲染 ➞ Django中间件 ➞ WSGI ➞ 前端页面 3.Django项目启动通过在Pycharm终端中执行python manage.py runserver来启动Django服务，系统默认在:8000端口上执行 可以在settings.py中增加ALLOWED_HOSTS的参数，来增加支持的hosts，这里新增一个localhost。也可以自定义端口号等1ALLOWED_HOSTS = ['localhost']","text":"1.Django简介Django是一款开源的python web应用框架，采用MTV(Model-Template-View)模式，其中Model负责对象与数据库的映射(ORM)，Template通过接口等方式将前端页面传递给用户，View视图函数对应业务逻辑，在合适的时候调用Model或Template 除去MTV模型以外，Django还有一个URL控制器，分别将不同的URL请求传递给不同的View视图函数处理，再通过各自的View函数来调用对应的Model和Template 2.Django项目：订单管理系统1.Django项目的创建在python 3中安装完成Django库后，可以在CMD或Pycharm终端里执行：django-admin startproject itemproj来创建Django项目 也可以在Pycharm专业版本直接创建一个Django项目，这样会自动生成一个空的Template文件夹，以及在settings.py中生成Template对应的路径。还可以直接创建初始APP，这里没有做 (本文使用的Django版本为3.2.4) 执行创建完成后，系统会为我们生成这样一个初始目录： 其中： · manage.py是Django项目管理的工具脚本，执行Django命令时通过这个文件进行管理操作 · itemproj/settings.py是Django项目的配置文件，包括了App配置、Django中间件、数据库等重要设置，在实际开发中要经常修改其中内容 · itemproj/urls.py是URL路由映射文件，其中声明了前端发送的各种http请求，再通过各级路由文件来寻找到对应的View视图函数来进行处理 · itemproj/wsgi.py，WSGI是Python web服务网关接口规范(Web Server Gateway Interface)的简称，由wsgi web server和wsgi web application两部分组成，是运行在同一个python进程中的两个模块 wsgi web sever接收了http请求以后调用wsgi web application接口，处理请求的具体内容，处理完成后再将结果返回给wsgi web server，通过wsgi web server将请求传递给前端，如图所示(Django官方文档)： · itemproj/asgi.py，ASGI即异步网关协议接口(Asynchronous Server Gateway Interface)，是一个介于网络协议服务和python应用之间的标准接口，用于处理通用类型的协议。参考自Django官方文档的资料，ASGI是为了支持异步网络服务器和应用而出现的新的python标准，可以将ASGI理解为WSGI协议的扩展 2.Django生命周期了解了Django项目基本组成后，Django的生命周期也基本明朗：前端发起URL请求 ➞ WSGI ➞ Django中间件 ➞ URL路由 ➞ View视图函数 ➞ Model数据库交互 ➞ Template模板进行html渲染 ➞ Django中间件 ➞ WSGI ➞ 前端页面 3.Django项目启动通过在Pycharm终端中执行python manage.py runserver来启动Django服务，系统默认在:8000端口上执行 可以在settings.py中增加ALLOWED_HOSTS的参数，来增加支持的hosts，这里新增一个localhost。也可以自定义端口号等1ALLOWED_HOSTS = ['localhost'] 4.创建Django APP首先创建一个app专门负责处理订单管理系统中对订单销售的增删改查请求 在Pycharm终端中执行python manage.py startapp sales，会创建一个名为sales的app目录，其中包含新的下列文件： 该文件目录其实就是一个python包 5.编辑View视图函数和URL路由打开sales/views.py文件，准备新增一条视图功能：1234from django.http import HttpResponsedef listorders(request): return HttpResponse(\"下面是系统中所有的订单信息......\") 这里将返回HttpResponse对象的字符串参数 接着打开itemproj/urls.py文件，导入视图函数listordes，在列表变量urlpatterns中新增指向View的路由信息。urlpatterns就是Django URL路由的主入口123456789from django.contrib import adminfrom django.urls import pathfrom sales.views import listordersurlpatterns = [ path('admin/', admin.site.urls), path('sales/orders/', listorders),] 启动Django 在实际工程项目中，系统的urls条目复杂多变，放在同一份目录下会显得臃肿且难以维护，查询起来非常困难，不符合后端项目的设计理念，这里就需要将系统的urls做成树形系统，按照urls对应的视图功能将其拆分进对应的URL路由子表当中 比如，这里将所有以sales开头的urls全部放在sales app目录下新创建的子路由sales/urls.py文件中1234567from django.urls import pathfrom . import viewsurlpatterns = [ path('orders/', views.listorders),] 再对主路由文件itemproj/urls.py进行修改12345678from django.contrib import adminfrom django.urls import path, includeurlpatterns = [ path('admin/', admin.site.urls), path('sales/', include('sales.urls')),] 6.创建数据库数据库自然是Django后端开发中的重要内容，Django对其的设置保存在itemproj/settings.py文件中 进入settings.py，找到settings.pyDATABASES设置123456DATABASES = &#123; 'default': &#123; 'ENGINE': 'django.db.backends.sqlite3', 'NAME': BASE_DIR / 'db.sqlite3', &#125;&#125; 可以看到Django默认的数据库引擎为sqlite3，要添加MySql数据库，可以将其设置为：12345678910DATABASES = &#123; 'default':&#123; 'ENGINE':'django.db.backends.mysql', 'NAME':'testdemo1', #所使用的数据库的名字 'USER':'root', #数据库用户 'PASSWORD':'123456', #密码 'HOST':'127.0.0.1', #主机 'PORT':'3306', #端口 &#125;&#125; 执行创建本地数据库的指令python manage.py migrate 默认情况下就会在项目根目录下生成一个db.sqlite3，并在其中自动创建一些Django表 可以在Navicat中将其打开 7.创建表单新建一份common app应用目录，在其中存放一些公共信息 python manage.py startapp common 进入app中common/models.py，添加内容：1234567891011from django.db import modelsclass Customer(models.Model): # 客户名称 name = models.CharField(max_length=200) # 联系电话 phonenumber = models.CharField(max_length=200) # 邮箱 address = models.CharField(max_length=200) 其中的客户Customer类继承自django.db.models.Model，用来定义数据库表格，添加的name、phonenumber和adress分别是表中的3个varchar字段，参数max_length则限制了3个字段的最大长度 接着来到itemproj/settings.py中，添加common app12345678910INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'common.apps.CommonConfig',] 完成后，Django已经真正添加了app用，继续执行命令python manage.py makemigrations common记录对数据库的操作 执行完成后，会在common/migrations.py目录下生成0001_inital.py文件，该脚本就是进行的数据库操作代码 1234567891011121314151617181920from django.db import migrations, modelsclass Migration(migrations.Migration): initial = True dependencies = [ ] operations = [ migrations.CreateModel( name='Customer', fields=[ ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')), ('name', models.CharField(max_length=200)), ('phonenumber', models.CharField(max_length=200)), ('address', models.CharField(max_length=200)), ], ), ] 接下来执行python manage.py migrate去数据库中创建表 8.创建Django管理员Django提供了一个内置的管理员操作页面，可以直接进行对数据库的增删改查工作，将数据库操作变得简单。 首先创建超级管理员账号，执行命令python manage.py createsuperuser 因为是做测试学习使用，所以密码设置得相对简单，直接无视掉警告，同样的，还可以临时取消Django的CSRF(跨站请求伪造)校验，否则后续的HTTP请求都必须携带校验数据 123456789MIDDLEWARE = [ 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.common.CommonMiddleware', #'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware',] 要牢记设置的密码，因为忘记密码后如果想要去数据库里查看，不经过密码学解密的情况下，密码在数据表中是以pbkdf2_sha256算法加密后的密文形式记录的，这在Django后台管理页面当中也会有提示 如果担心忘记密码可以先将密码注释到itemproj/settings.py中123456'''Username (leave blank to use 'elbadaernu9.9'): 9.9Email address: rtl1312@163.comPassword: 00000000Superuser created successfully.''' 然后修改common应用中管理员配置文件common/admin.py，注册自定义的model类，提交给Django12345from django.contrib import adminfrom .models import Customeradmin.site.register(Customer) 启动Django，访问http://localhost:8000/admin，出现管理员登陆界面 登陆后进入到管理员后台 增加客户数据 上述步骤完成以后，进入Navicat，刷新common_customer表，可以看到新增数据已经出现在数据库当中 要将管理员界面设置为中文，可以到到itemproj/settings.py中对中间件加入如下设置：1234567891011MIDDLEWARE = [ 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.common.CommonMiddleware', 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware', #管理员后台界面设置为中文 'django.middleware.locale.LocaleMiddleware',] 9.获取数据先来实现第一个功能，当浏览器访问/sales/customs/时，服务器会返回系统中的所有客户记录 来到sales/views.py中，定义listcustomers函数1234567891011121314from common.models import Customerdef listcustomers(request): qs = Customer.objects.values() # 定义返回字符串 retStr = '' for customer in qs: for name,value in customer.items(): retStr += f'&#123;name&#125; : &#123;value&#125; | ' retStr += '&lt;br&gt;' return HttpResponse(retStr) Customer.objects.values()会返回Django定义的QuerySet对象，将返回包含的全部Customer表记录，每条记录都是一个字典对象 接着添加对/sales/customs/访问url请求的路由12345678910from django.urls import pathfrom . import viewsurlpatterns = [ path('orders/', views.listorders), #这里是新添加的路由信息 path('customers/', views.listcustomers), ] 启动Django，进入对应页面后出现： 因为在实际订单信息中，客户数量庞大，可以对代码新增过滤条件1234567891011121314151617def listcustomers(request): qs = Customer.objects.values() # 检查url中是否有参数phonenumber ph = request.GET.get('phonenumber',None) if ph: qs = qs.filter(phonenumber=ph) # 定义返回字符串 retStr = '' for customer in qs: for name,value in customer.items(): retStr += f'&#123;name&#125; : &#123;value&#125; | ' retStr += '&lt;br&gt;' return HttpResponse(retStr) 10.使用Html展示数据使用Django内置的模板引擎，定义一段Html代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657html_template = '''&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=\"UTF-8\"&gt;&lt;style&gt;table &#123; border-collapse: collapse;&#125;th, td &#123; padding: 8px; text-align: left; border-bottom: 1px solid #ddd;&#125;&lt;/style&gt;&lt;/head&gt; &lt;body&gt; &lt;table&gt; &lt;tr&gt; &lt;th&gt;id&lt;/th&gt; &lt;th&gt;姓名&lt;/th&gt; &lt;th&gt;电话号码&lt;/th&gt; &lt;th&gt;邮箱&lt;/th&gt; &lt;/tr&gt; &#123;% for customer in customers %&#125; &lt;tr&gt; &#123;% for name, value in customer.items %&#125; &lt;td&gt;&#123;&#123; value &#125;&#125;&lt;/td&gt; &#123;% endfor %&#125; &lt;/tr&gt; &#123;% endfor %&#125; &lt;/table&gt; &lt;/body&gt;&lt;/html&gt;'''from django.template import enginesdjango_engine = engines['django']template = django_engine.from_string(html_template)def listcustomers(request): qs = Customer.objects.values() ph = request.GET.get('phonenumber', None) if ph: qs = qs.filter(phonenumber=ph) # 传入渲染模板需要的参数 rendered = template.render(&#123;'customers': qs&#125;) return HttpResponse(rendered) 在实际前后端分离的开发中，Html部分的工作通常交由前端开发人员，后端主要是准备好与前端页面的API接口工作 11.对数据的增删改查1.创建管理应用针对管理员用户的工作，继续创建新应用mgr来处理管相关请求 为了避免面向mgr管理员的系统里mgr/views.py中的函数过于庞大，新创建一份mgr/customer.py来专门处理对customer数据的操作 2.设置URL路由实际的增删改查操作都发生在同一份路由当中 · GET：获取请求 · POST：添加请求 · PUT：修改请求 · DELETE：删除请求 而Django不支持在路由当中添加HTTP请求方法，可以在mgr/customer.py文件中新增函数dispatcher()来管理请求123456789101112131415161718192021222324252627from django.http import JsonResponseimport jsondef dispatcher(request): # 将请求参数统一放入request的params属性中，方便后续处理 # GET请求 参数在url中，通过request对象的GET属性获取 if request.method == 'GET': request.params = request.GET # POST/PUT/DELETE 请求 参数从request对象的body属性中获取 elif request.method in ['POST','PUT','DELETE']: # 接口中POST/PUT/DELETE请求的消息体都是json格式 request.params = json.loads(request.body) # 根据不同的action分派给不同的函数进行处理 action = request.params['action'] if action == 'list_customer': return listcustomers(request) elif action == 'add_customer': return addcustomer(request) elif action == 'modify_customer': return modifycustomer(request) elif action == 'del_customer': return deletecustomer(request) else: return JsonResponse(&#123;'ret': 1, 'msg': '不支持该类型http请求'&#125;) 进入主路由itemproj/urls.py新增1path('api/mgr/', include('mgr.urls')), 然后创建并进入子路由mgr/urls.py添加路径声明1234567from django.urls import pathfrom mgr import customerurlpatterns = [ path('customers', customer.dispatcher),] 这样，如果有来自API请求/api/mgr/customers当中内容，都将经过dispatcher()函数处理 继续完成增删改查功能对应的函数 3.增加客户导入1from common.models import Customer 123456789def addcustomer(request): info = request.params['data'] record = Customer.objects.create(name=info['name'] , phonenumber=info['phonenumber'] , address=info['address']) return JsonResponse(&#123;'ret': 0, 'id':record.id&#125;) 4.删除客户123456789101112131415def deletecustomer(request): customerid = request.params['id'] try: customer = Customer.objects.get(id=customerid) except Customer.DoesNotExist: return &#123; 'ret': 1, 'msg': f'id 为`&#123;customerid&#125;`的客户不存在' &#125; customer.delete() return JsonResponse(&#123;'ret': 0&#125;) 5.修改客户1234567891011121314151617181920212223242526def modifycustomer(request): # 从请求消息中 获取修改客户的信息 # 找到该客户，并且进行修改操作 customerid = request.params['id'] newdata = request.params['newdata'] try: customer = Customer.objects.get(id=customerid) except Customer.DoesNotExist: return &#123; 'ret': 1, 'msg': f'id 为`&#123;customerid&#125;`的客户不存在' &#125; if 'name' in newdata: customer.name = newdata['name'] if 'phonenumber' in newdata: customer.phonenumber = newdata['phonenumber'] if 'address' in newdata: customer.address = newdata['address'] customer.save() return JsonResponse(&#123;'ret': 0&#125;) 6.查询客户12345678def listcustomers(request): # 返回一个QuerySet对象 qs = Customer.objects.values() # 将 QuerySet对象转化为list类型，JSON字符串 retlist = list(qs) return JsonResponse(&#123;'ret': 0, 'retlist': retlist&#125;) 12.测试实际开发过程中可能不能及时收到前端的Html文件，此时就需要对项目功能进行一定测试 可以通过模拟前端的形式，发出HTTP请求，对后端接口进行测试 在项目根目录下新建test文件夹用于存放测试文件，并新建一份test01.py文件用于测试customer数据 导入python的requests库，做接口测试12345import requests,pprintresponse = requests.get('http://localhost:8000/api/mgr/customers?action=list_customer')pprint.pprint(response.json()) 继续创建test_add.py文件用于测试新增用户123456789101112131415161718import requests,pprintpayload = &#123; \"action\":\"add_customer\", \"data\":&#123; \"name\":\"小明\", \"phonenumber\":\"12345678910\", \"address\":\"xiaoming@163.com\" &#125;&#125;response = requests.post('http://localhost:8000/api/mgr/customers',json=payload)pprint.pprint(response.json())response = requests.get('http://localhost:8000/api/mgr/customers?action=list_customer')pprint.pprint(response.json()) 此时数据库中也随之出现了“小明”的记录 13.与前端集成制作前端的Html文件，打包后命名为front_end作为静态文件放进项目的根目录，进入itemproj/urls.py文件中，添加静态文件声明，并在末尾增加代码12345678910from django.contrib import adminfrom django.urls import path, includefrom django.conf.urls.static import staticurlpatterns = [ path('admin/', admin.site.urls), path('sales/', include('sales.urls')), path('api/mgr/', include('mgr.urls')),] + static(\"/\", document_root=\"./front_end\") 最后添加的就是查找前端静态文件的URL路由，一但HTTP请求不是以admin/ sales/ api/开头，Django会访问前端目录下的静态文件 假设与前端文件的集成无误，启动Django，访问http://localhost:8000/mgr/index.html,进入管理员后台，会得到类似这样的成果 14.登陆功能要实现订单管理员的登陆登出功能，继续在mgr文件夹中新建一份sign_in_out.py文件，管理登陆登出 Django内置的app django.contrib.auth已经实现了登陆验证功能，并且在数据库中定义过一张auth_user表，可以直接使用其方法 1234567891011121314151617181920212223242526272829303132333435from django.http import JsonResponsefrom django.contrib.auth import authenticate, login, logout# 登录处理def signin(request): # 从 HTTP POST请求中获取用户名、密码参数 userName = request.POST.get('username') passWord = request.POST.get('password') # 使用Django auth库里的方法校验用户名、密码 user = authenticate(username=userName, password=passWord) # 如果能找到用户，并且密码正确 if user is not None: if user.is_active: if user.is_superuser: login(request, user) # 在session中存入用户类型 request.session['usertype'] = 'mgr' return JsonResponse(&#123;'ret': 0&#125;) else: return JsonResponse(&#123;'ret': 1, 'msg': '请使用管理员账户登录'&#125;) else: return JsonResponse(&#123;'ret': 0, 'msg': '用户已经被禁用'&#125;) # 否则用户名、密码有误 else: return JsonResponse(&#123;'ret': 1, 'msg': '用户名或者密码错误'&#125;)# 登出处理def signout(request): # 使用登出方法 logout(request) return JsonResponse(&#123;'ret': 0&#125;) 再到mgr/urls.py中添加url路由12345678910from django.urls import pathfrom mgr import customerfrom mgr import sign_in_outurlpatterns = [ path('customers', customer.dispatcher), path('signin', sign_in_out.signin), path('signout', sign_in_out.signout),] 到这里登陆功能已经写好，对其进行测试，在test文件夹里新建测试文件test_login.py12345678910import requests,pprintpayload = &#123; 'username': '9.9', 'password': '00000000'&#125;response = requests.post('http://localhost:8000/api/mgr/signin',data=payload)pprint.pprint(response.json()) 如果返回{‘ret’: 0}，表示成功登陆 如果登陆失败，则返回{‘ret’: 1}，并指明失败原因 继续在Django管理员后台增加新的访客用户 到test文件夹里再新建测试文件test_user.py测试普通访客登录12345678910import requests,pprintpayload = &#123; 'username': 'guest', 'password': 'rtl1312693017'&#125;response = requests.post('http://localhost:8000/api/mgr/signin',data=payload)pprint.pprint(response.json()) 系统会提示没有权限访问 最后再通过与前端文件进行集成，得到优化后的登陆界面： 成功登陆后将跳转到之前的客户订单管理系统","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"Django","slug":"Django","permalink":"http://yoursite.com/tags/Django/"}]},{"title":"基于卷积神经网络的面部年龄识别","slug":"基于卷积神经网络的面部年龄识别","date":"2021-05-26T15:13:00.000Z","updated":"2022-06-27T07:41:11.298Z","comments":true,"path":"2021/05/26/基于卷积神经网络的面部年龄识别/","link":"","permalink":"http://yoursite.com/2021/05/26/基于卷积神经网络的面部年龄识别/","excerpt":"1.训练模型和框架1.1 Adience数据集Adience图片集包括Flickr等相册，通过从iPhone5（或更高版本）的智能手机等移动设备自动上传并进行组装，由其作者根据知识共享（CC）许可向公众发布。数据集提供了共计26580张面部照片的数据和基准，旨在尽可能真实地应对实际成像并做出判断。数据的标签有年龄组、性别、户外等，可用于监督学习的人脸识别对年龄的研究。Adience收录的数据信息包括主体外观、动作、噪点、光线等实际情况中进行图像采集时包含的动态变化，具有环境适应性强、应用范围广的特点。 1.2 Caffe框架Caffe（Convolutional Architecture for Fast Feature Embedding）采用CUDA架构，可在CPU和GPU上进行高速运算，是一个兼具了效率、表达和思维模块化的卷积神经网络框架。 Caffe的数据结构以Blobs-Layers-Net的形式存在。 Blobs是Caffe的核心数据格式，提供了统一的内存接口，并且可以在CPU与GPU之间进行数据同步。主要通过四维张量（NumberChannelweight*high）的形式，按照C-contiguous方式（数组的行存储连续且不间断）来存储和交流网络中的权重、激活值、正反向数据。 Layers是Caffe模型的关键内容，是组成神经网络和进行相关计算的基础。所有的Layer层都可以接收底层输入的Blobs，并向高层输出Blobs。Layers每一层都定义三种重要的计算：初始化（Setup）、向前传播（Forward）、向后传播（Backward）。 其包含的运算有： · 1.load data：数据载入 · 2.Convolve filters：卷积层，进行卷积。 · 3.Pooling：池化层，进行池化。 · 4.Nonlinearities：非线性映射运算，即激活函数。 · 5.Inner Products：内积运算。 · 6.Normalize：归一化 · 7.Compute losses：损失函数计算，如softmax、hinge。 Net是一个由一系列连接的Layer层组成的有向无环图（Directed Acyclic Graph，DAG）。caffe会在向前传播或向后传播时，对DAG中的所有层进行记录，确保其准确性。 2.基于卷积神经网络的人脸识别2.1 卷积神经网络架构使用的架构包括3个卷积层、2个全连接层和1个最终输出层。具体定义卷积层如下： 1.Conv1：将内核大小为337的共计96个像素节点的过滤应用于输入第一卷积层中，经过修正线性单元ReLU（激活函数）处理后，池化层采用保留最大值（max-pooling）的规则，选择一个两像素跨度的3*3区域中最大值，进行池化，再经过局部响应归一化层（Local Response Normalization，LRN）。 2.Conv2：上一层的输出（96×28×28）由第二个卷积层进行处理，包括对256个大小为9655的像素过滤。同样的，经过一个修正线性单元ReLU，最大池化层，和一个与之前参数相同的局部响应归一化层。 3.Conv3：第三层卷积层通过对一组384个大小为25633的像素过滤来对256×14×14的 Blob进行处理，接着经过修正线性单元ReLU和一个最大池化层。 再通过下列方式定义完全连接层： 1.第一个完全连接层包含了512个人工神经元，用于接收第三卷积层的输出结果。接着再通过修正线性单元ReLU和Dropout层（防止CNN过拟合）。 2.第二个完全连接层接收第一个完全连接层的512个人工神经元空间大小的输出（同样包含512个人工神经元），再通过修正线性单元ReLU和Dropout层。 3.第三层完全连接层映射最终的分类结果。 最终，最后一个完全连接层的输出会被反馈到为每个类别分配概率的Softmax层，预测其本身通过给定的测试图像的最大概率。 2.2 年龄预测人的面部特征无时不刻发生着微妙变化反映出其年龄的不断增长，在最理想的情况之下，人的面部特征随着人的成长应该表现出正相关的关系，那么年龄估计就是一个广义上的回归问题。然而实际上仅通过回归的方法来判断一个人的年龄是靠不住的，即便一个正常的自然人也很难推断出观察对象的准确实际年龄。 但是人眼可以对观察对象做出一个大致判断，较为准确的预测出对方的年龄所在区间。这样，就对对方的年龄有了初步估计。这样，就可以对年龄区间进行一个分类，以进一步研究人脸和年龄的关系。 Adience数据集将人的年龄划分为了八个类别，分别为：[0-2]、[4-6]、[8-13]、[15-20]、[25-32]、[38-43]、[48-53]、[60 -]。因此，深度神经网络在最终的Softmax层中有8个节点，分别对各年龄段进行分类。12MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)'] Blob输入网络进行年龄的检测，并且年龄检测程序向前传播。123ageNet.setInput(blobs)agepredction = ageNet.forward()age = ageList[agepredction[0].argmax()]","text":"1.训练模型和框架1.1 Adience数据集Adience图片集包括Flickr等相册，通过从iPhone5（或更高版本）的智能手机等移动设备自动上传并进行组装，由其作者根据知识共享（CC）许可向公众发布。数据集提供了共计26580张面部照片的数据和基准，旨在尽可能真实地应对实际成像并做出判断。数据的标签有年龄组、性别、户外等，可用于监督学习的人脸识别对年龄的研究。Adience收录的数据信息包括主体外观、动作、噪点、光线等实际情况中进行图像采集时包含的动态变化，具有环境适应性强、应用范围广的特点。 1.2 Caffe框架Caffe（Convolutional Architecture for Fast Feature Embedding）采用CUDA架构，可在CPU和GPU上进行高速运算，是一个兼具了效率、表达和思维模块化的卷积神经网络框架。 Caffe的数据结构以Blobs-Layers-Net的形式存在。 Blobs是Caffe的核心数据格式，提供了统一的内存接口，并且可以在CPU与GPU之间进行数据同步。主要通过四维张量（NumberChannelweight*high）的形式，按照C-contiguous方式（数组的行存储连续且不间断）来存储和交流网络中的权重、激活值、正反向数据。 Layers是Caffe模型的关键内容，是组成神经网络和进行相关计算的基础。所有的Layer层都可以接收底层输入的Blobs，并向高层输出Blobs。Layers每一层都定义三种重要的计算：初始化（Setup）、向前传播（Forward）、向后传播（Backward）。 其包含的运算有： · 1.load data：数据载入 · 2.Convolve filters：卷积层，进行卷积。 · 3.Pooling：池化层，进行池化。 · 4.Nonlinearities：非线性映射运算，即激活函数。 · 5.Inner Products：内积运算。 · 6.Normalize：归一化 · 7.Compute losses：损失函数计算，如softmax、hinge。 Net是一个由一系列连接的Layer层组成的有向无环图（Directed Acyclic Graph，DAG）。caffe会在向前传播或向后传播时，对DAG中的所有层进行记录，确保其准确性。 2.基于卷积神经网络的人脸识别2.1 卷积神经网络架构使用的架构包括3个卷积层、2个全连接层和1个最终输出层。具体定义卷积层如下： 1.Conv1：将内核大小为337的共计96个像素节点的过滤应用于输入第一卷积层中，经过修正线性单元ReLU（激活函数）处理后，池化层采用保留最大值（max-pooling）的规则，选择一个两像素跨度的3*3区域中最大值，进行池化，再经过局部响应归一化层（Local Response Normalization，LRN）。 2.Conv2：上一层的输出（96×28×28）由第二个卷积层进行处理，包括对256个大小为9655的像素过滤。同样的，经过一个修正线性单元ReLU，最大池化层，和一个与之前参数相同的局部响应归一化层。 3.Conv3：第三层卷积层通过对一组384个大小为25633的像素过滤来对256×14×14的 Blob进行处理，接着经过修正线性单元ReLU和一个最大池化层。 再通过下列方式定义完全连接层： 1.第一个完全连接层包含了512个人工神经元，用于接收第三卷积层的输出结果。接着再通过修正线性单元ReLU和Dropout层（防止CNN过拟合）。 2.第二个完全连接层接收第一个完全连接层的512个人工神经元空间大小的输出（同样包含512个人工神经元），再通过修正线性单元ReLU和Dropout层。 3.第三层完全连接层映射最终的分类结果。 最终，最后一个完全连接层的输出会被反馈到为每个类别分配概率的Softmax层，预测其本身通过给定的测试图像的最大概率。 2.2 年龄预测人的面部特征无时不刻发生着微妙变化反映出其年龄的不断增长，在最理想的情况之下，人的面部特征随着人的成长应该表现出正相关的关系，那么年龄估计就是一个广义上的回归问题。然而实际上仅通过回归的方法来判断一个人的年龄是靠不住的，即便一个正常的自然人也很难推断出观察对象的准确实际年龄。 但是人眼可以对观察对象做出一个大致判断，较为准确的预测出对方的年龄所在区间。这样，就对对方的年龄有了初步估计。这样，就可以对年龄区间进行一个分类，以进一步研究人脸和年龄的关系。 Adience数据集将人的年龄划分为了八个类别，分别为：[0-2]、[4-6]、[8-13]、[15-20]、[25-32]、[38-43]、[48-53]、[60 -]。因此，深度神经网络在最终的Softmax层中有8个节点，分别对各年龄段进行分类。12MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)'] Blob输入网络进行年龄的检测，并且年龄检测程序向前传播。123ageNet.setInput(blobs)agepredction = ageNet.forward()age = ageList[agepredction[0].argmax()] 3.计算机视觉技术与系统的运行测试3.1 OpenCVOpenCV是基于伯克利软件套件（Berkeley Software Distribution，BSD）许可发行的一个开源的跨平台计算机视觉库，其本身由C++语言和C函数编写，同时提供了Python、JAVA、MATLAB等语言的接口，能够运行计算机视觉的一系列算法，对图像进行处理。 要在python环境下使用OpenCV视觉库，可以通过pip执行命令“pip install opencv-python“来安装OpenCV模块。 安装完成以后，在Python中导入模块。 3.2 图像侦测使用OpenCV中的DNN人脸侦测模块对人脸的图像进行侦测和获取，OpenCV为该检测器提供了Caffe实施的16位浮点数版本。其优点有： · 1.在CPU上实时运行。 · 2.即使在严重遮挡下也可以工作。 · 3.能够检测各种比例的面部。 · 4.适用不同的脸部朝向，如上、下、左、右和侧面等。 整个面部检测的功能使用函数“getFaceBox”完成，包含信号量参数“net“、图像参数“frame“和阈值参数“conf_threshold”。将图像转化为blob，其中“blobFromImage”函数用于减均值、图像缩放和进行通道交换（由于opencv中的图像存储都是基于BGR通道，所以需要将原本的RGB通道替换为BGR通道）。1234567import cv2def getFaceBox(net, frame, threshold_conf=0.7): bounding_OpencvDnn = frame.copy() bounding_Height = bounding_OpencvDnn.shape[0] bounding_Width = bounding_OpencvDnn.shape[1] blobs = cv2.dnn.blobFromImage(bounding_OpencvDnn, 1.0, (300, 300), [104, 117, 123], True, False) 侦测人脸的网络向前传递。接着进行人脸识别框bounding box的绘制，设置bounding box的坐标分别为x1、y1，x2、y2。输出检测是一个4-D矩阵，其中i是面部的迭代器，第三维用于遍历检测到的面部数据，第四维包含了每个面的边界框和分数信息。由于识别框的输出坐标会在[0,1]间进行归一化，所以，为了获得正确的bounding box，需要将坐标再乘以原始图像的宽度和高度。12345678910111213net.setInput(blobs)detections = net.forward()bounding_bboxes = []for i in range(detections.shape[2]): confidence = detections[0, 0, i, 2] if confidence &gt; threshold_conf: x1 = int(detections[0, 0, i, 3] * bounding_Width) y1 = int(detections[0, 0, i, 4] * bounding_Height) x2 = int(detections[0, 0, i, 5] * bounding_Width) y2 = int(detections[0, 0, i, 6] * bounding_Height) bounding_bboxes.append([x1, y1, x2, y2]) cv2.rectangle(bounding_OpencvDnn, (x1, y1), (x2, y2), (0, 255, 0), int(round(bounding_Height/150)), 8)return bounding_OpencvDnn, bounding_bboxes 3.3 输入将识别的源图片位置存放在字符串str当中，调用OpenCV中的函数imread()来读取图片，完成图像的输入。并截取文件的路径只保留文件名，方便后续输出。123str = \"C:\\\\Users\\\\elbadaernU9.9\\\\Desktop\\\\test.png\"frame = cv2.imread(str)name = str[31:] 使用OpenCV库，导入网络模型和预训练模型，在Caffe框架下对Adience数据集进行模型的训练。12345faceProto = \"opencv_face_detector.pbtxt\"faceModel = \"opencv_face_detector_uint8.pb\"ageProto = \"age_deploy.prototxt\"ageModel = \"age_net.caffemodel\" 5.4 测试输出最后使用imshow()函数在输入的图像上显示网络的输出，包括识别框bounding box、分类（预测）的最终结果。为了使图像正常显示，需要添加一行代码“cv2.waitKey(1)”，并设置显示的时间为5秒。通过imwrite()函数保存输出的结果。至此，主要的年龄识别系统编写完成。12345label = \"&#123;&#125;,&#123;&#125;\".format(gender, age)cv2.putText(frameFace, label, (bounding_bbox[0], bounding_bbox[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA)cv2.imshow(\"Age &amp; Gender Prediction Demo\", frameFace)cv2.imwrite(\"age-gender-out-&#123;&#125;\".format(args.input),frameFace)time.sleep(5) 5.5 拓展系统初步的年龄估计功能实现以后，开始对系统进行进一步的优化和功能拓展。 由于原系统的图像输入是以保存的图片形式导入的，在实际应用当中没有事先保存源图像文件的情况下就会有诸多不便。所以尝试将图像的输入形式做一个动态的拓展，由静态的图片形式补充为摄像头动态捕捉人像。 通过调用OpenCV启动摄像头的函数，使系统自动捕获人脸，同时保留图片输入的功能（使用Dos命令实现）。12345678cap = cv2.VideoCapture(args.input if args.input else 0)padding = 20while cv2.waitKey(1) &lt; 0: t = time.time() hasFrame, frame = cap.read() if not hasFrame: cv2.waitKey() break 由于Adience数据集同样包含了性别的标签，可以在年龄估计的同时为系统添加性别预测功能的代码。12345678910genderProto = \"gender_deploy.prototxt\"genderModel = \"gender_net.caffemodel\"genderList = ['Male', 'Female']genderNet = cv2.dnn.readNet(genderModel, genderProto)genderNet.setInput(blobs)genderPredction = genderNet.forward()label = \"&#123;&#125;,&#123;&#125;\".format(gender, age) 完成后在Pycharm中将项目封装为.exe可执行文件。 得到最终的运行结果如下： 完整的人脸识别程序代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192import timeimport argparseimport cv2def getFaceBox(net, frame, threshold_conf=0.7): bounding_OpencvDnn = frame.copy() bounding_Height = bounding_OpencvDnn.shape[0] bounding_Width = bounding_OpencvDnn.shape[1] blobs = cv2.dnn.blobFromImage(bounding_OpencvDnn, 1.0, (300, 300), [104, 117, 123], True, False) net.setInput(blobs) detections = net.forward() bounding_bboxes = [] for i in range(detections.shape[2]): confidence = detections[0, 0, i, 2] if confidence &gt; threshold_conf: x1 = int(detections[0, 0, i, 3] * bounding_Width) y1 = int(detections[0, 0, i, 4] * bounding_Height) x2 = int(detections[0, 0, i, 5] * bounding_Width) y2 = int(detections[0, 0, i, 6] * bounding_Height) bounding_bboxes.append([x1, y1, x2, y2]) cv2.rectangle(bounding_OpencvDnn, (x1, y1), (x2, y2), (0, 255, 0), int(round(bounding_Height/150)), 8) return bounding_OpencvDnn, bounding_bboxesparser = argparse.ArgumentParser(description='Use this script to run age and gender recognition using OpenCV.')parser.add_argument('--input', help='Path to input image or video file. Skip this argument to capture frames from a camera.')args = parser.parse_args()faceProto = \"opencv_face_detector.pbtxt\"faceModel = \"opencv_face_detector_uint8.pb\"ageProto = \"age_deploy.prototxt\"ageModel = \"age_net.caffemodel\"genderProto = \"gender_deploy.prototxt\"genderModel = \"gender_net.caffemodel\"MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']genderList = ['Male', 'Female']#genderList.decode(\"utf-8\")# Load networkageNet = cv2.dnn.readNet(ageModel, ageProto)genderNet = cv2.dnn.readNet(genderModel, genderProto)faceNet = cv2.dnn.readNet(faceModel, faceProto)# Open a video file or an image file or a camera streamcap = cv2.VideoCapture(args.input if args.input else 0)padding = 20while cv2.waitKey(1) &lt; 0: # Read frame t = time.time() hasFrame, frame = cap.read() if not hasFrame: cv2.waitKey() break frameFace, bounding_bboxes = getFaceBox(faceNet, frame) if not bounding_bboxes: print(\"No face Detected, Checking next frame\") continue for bounding_bbox in bounding_bboxes: print(\"=====================================Face Found=====================================\") # print(bounding_bbox) face = frame[max(0,bounding_bbox[1]-padding):min(bounding_bbox[3]+padding,frame.shape[0]-1),max(0,bounding_bbox[0]-padding):min(bounding_bbox[2]+padding, frame.shape[1]-1)] blobs = cv2.dnn.blobFromImage(face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False) genderNet.setInput(blobs) genderPredction = genderNet.forward() gender = genderList[genderPredction[0].argmax()] # print(\"Gender Output : &#123;&#125;\".format(genderPredction)) print(\"Gender : &#123;&#125;, conf = &#123;:.3f&#125;\".format(gender, genderPredction[0].max())) ageNet.setInput(blobs) agepredction = ageNet.forward() age = ageList[agepredction[0].argmax()] print(\"Age Output : &#123;&#125;\".format(agepredction)) print(\"Age : &#123;&#125;, conf = &#123;:.3f&#125;\".format(age, agepredction[0].max())) label = \"&#123;&#125;,&#123;&#125;\".format(gender, age) cv2.putText(frameFace, label, (bounding_bbox[0], bounding_bbox[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA) cv2.imshow(\"Age &amp; Gender Prediction Demo\", frameFace) # cv2.imwrite(\"age-gender-out-&#123;&#125;\".format(args.input),frameFace) print(\"time : &#123;:.3f&#125;\".format(time.time() - t)) print(\"=====================================Round Over=====================================\") (本文节选自我的毕业论文，有删改)相关代码和训练集已打包至github：https://github.com/elbadaernU404/AgeGenderPredictionDemo","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"CNN","slug":"CNN","permalink":"http://yoursite.com/tags/CNN/"}]},{"title":"卷积神经网络算法","slug":"卷积神经网络算法","date":"2021-05-25T05:01:35.000Z","updated":"2024-12-06T08:10:51.026Z","comments":true,"path":"2021/05/25/卷积神经网络算法/","link":"","permalink":"http://yoursite.com/2021/05/25/卷积神经网络算法/","excerpt":"神经网络(Neural Networks，NN)是人工智能研究领域的重要组成部分，是大数据与人工智能技术发展的重要动力。 1.卷积神经网络卷积神经网络属于机器学习领域当中的深度学习技术，是一类包含了卷积计算并且具有深度结构的前馈神经网络（Feedforward Neural Networks）。由纽约大学教授Yann LeCun在1998年提出，主要应用于语音识别、计算机视觉、自然语言处理等领域。 其本质上是一个采用了局部连接和权值共享方式的多层感知机，包括一般的卷积神经网络（CNN）和深度卷积神经网络（Deep Convolutional Neural Networks, DCNN），通常不做特意区分。一般的卷积神经网络存在浅层结构，但准确性和表现力相对不足，深度卷积神经网络可以自动从大规模的数据中进行特征学习，且具备将结果向同类型的未知数据泛化的能力，是目前最常用的神经网络技术。 2.卷积神经网络原理2.1 卷积原理从数学的角度出发，分析数学中定义一种运算，称(f*g)(n)为(f，g)的卷积（Convolution）。其连续的定义为： 其离散的定义为： 其连续和离散具有共同的特征：n=τ+(n-τ)，当令x=τ，y=n-τ时，有直线：x+y=n，其图像为如下图： 如果遍历这些直线，就类似于沿直线卷起毛巾的过程。 下面将卷积扩展到二维，有两个R2→R的二元函数f(x，y)和g(x，y)，那么令f和g的卷积就是一个新的R2→R的二元函数c(x，y)： 其含义为将f在(s，t)上的值乘以g在(x-s，y-t)上的值，再遍历从-∞到+∞全部s和t的值，进行f和g积分意义上的加和，最终得到c在(x，y)上的值（二重积分)。 可以看出这种加和是一种加权求和，即以f为权，(x，y)为中心，将f在(s，t)上的值乘以g距离中心(-s，-t)上的值再相加。 其离散的表示形式为： 2.2 神经元神经细胞有众多的树突和一个伸长的轴突，不同神经元之间通过树突连接轴突的方式，并通过神经脉冲的传导进行细胞间交流。具体过程为神经细胞A的轴突连接到神经细胞B的树突，传导神经脉冲，神经细胞A根据其树突传递来的信号决定是否通过轴突来向下一个神经细胞C传递信息。 1943年，美国心理学家W.McCulloch和数学家W.Pitts根据生物神经元、生物电和生物化学的运行机理提出了二值神经元的数学模型，即神经细胞机能的简单化的数理模型，被称作莫克罗-彼特氏神经模型（McCulloch－Pitts′Neuron Model），也叫人工神经元模型。 一个人工神经元模型就是对生物神经元的数学建模： 其中X1、X2、X3、…、Xn分别是从其他神经元传入的输入信号，Wi1、Wi2、Wi3、…、Win分别是对应X1、X2、X3、…、Xn的权重。θ表示一个阈值，或称为偏置（Bias），其设置目的是正确将样本分类。神经元当前的输入信号和偏置相加后产生当前神经元的最终处理信号Net，称为净激活或净激励（Net Activation），并作为激活函数或激励函数（Activation Function）f的输入，即f（net)，最终得到对应的输出yi。形如： 激活函数f（net)的作用是通过加入非线性因素来解决线性模型的表达和分类能力不足的问题。常见的激活函数有： 1.LinearLinear是一个线性激活函数：f(x)=∑i xiwi+b。本质上就是对输入的加权求和再加上偏置，所以相对简单，功能较为单一。 由于多个矩阵进行连乘的结果仍是矩阵，类似的，如果一个神经网络中仅使用线性激活函数，那么就只能学习到线性函数。 2.SigmoidSigmoid是在神经网络研究的初始阶段就被投入了广泛使用的一个经典的非线性激活函数，其表达式是一种逻辑斯蒂函数（Logistic Function）： 其中x=∑i xiwi+b 这个函数在函数的区域内是完全可导的，输出为0到1范围内的正值（被证实不易于神经网络对收敛的学习），并且对于接近0和1的函数部分，导数无限趋近于0，存在明显的梯度衰减问题。所以现阶段对Sigmod函数的应用已经越来越少。 3.Tanh通过其图像可以看出Tanh函数旨在克服Sigmoid函数只能取正值的缺陷，将其设计为关于原点对称的双曲正切（hyperbolic tangent）形式： 其中x=∑i xiwi+b。函数的导数取值范围在0到1之间，优于sigmoid的0至1/4，在一定程度上减轻了梯度的消失。但函数的两端仍然存在梯度饱和问题，且计算变得更为复杂。 Tanh的输入和输出能够保持非线性单调上升和下降的关系，符合BP算法网络的梯度求解，容错性好。 4.ReLU （The Rectified Linear Unit）其公式的表达形式为： 也可以表示为f(x)=max(0,x)。即当输入信号小于0时，函数的输出为0；当输入信号大于0时，函数的输出等于输入值。 ReLU拥有操作简便，耗时少的优点，又具有部分线性的特性，不会出现过饱和现象、减免了梯度消失的问题（函数得到的随机梯度下降法的收敛速度比Sigmoid和Tanh都快）。并且ReLU只需要一个阈值就可以得到激活值，不用和Sigmoid一样进行复杂的指数运算。但其神经元比价脆弱容易失去作用：当神经元接收到非常大的梯度数据流以后，该神经元可能就不再对输入数据进行反馈了，所以在进行训练时要设置一个较小的学习率参数。ReLU是目前用于神经网络领域最常见的激活函数之一。 这就是单个人工神经元的定义。但是早期的单个人工神经元模型甚至无法处理异或等运算，在多个人工神经元发展为神经网络之后逐渐克服了这些问题和种种困难，成为现代人工智能的重要模型。","text":"神经网络(Neural Networks，NN)是人工智能研究领域的重要组成部分，是大数据与人工智能技术发展的重要动力。 1.卷积神经网络卷积神经网络属于机器学习领域当中的深度学习技术，是一类包含了卷积计算并且具有深度结构的前馈神经网络（Feedforward Neural Networks）。由纽约大学教授Yann LeCun在1998年提出，主要应用于语音识别、计算机视觉、自然语言处理等领域。 其本质上是一个采用了局部连接和权值共享方式的多层感知机，包括一般的卷积神经网络（CNN）和深度卷积神经网络（Deep Convolutional Neural Networks, DCNN），通常不做特意区分。一般的卷积神经网络存在浅层结构，但准确性和表现力相对不足，深度卷积神经网络可以自动从大规模的数据中进行特征学习，且具备将结果向同类型的未知数据泛化的能力，是目前最常用的神经网络技术。 2.卷积神经网络原理2.1 卷积原理从数学的角度出发，分析数学中定义一种运算，称(f*g)(n)为(f，g)的卷积（Convolution）。其连续的定义为： 其离散的定义为： 其连续和离散具有共同的特征：n=τ+(n-τ)，当令x=τ，y=n-τ时，有直线：x+y=n，其图像为如下图： 如果遍历这些直线，就类似于沿直线卷起毛巾的过程。 下面将卷积扩展到二维，有两个R2→R的二元函数f(x，y)和g(x，y)，那么令f和g的卷积就是一个新的R2→R的二元函数c(x，y)： 其含义为将f在(s，t)上的值乘以g在(x-s，y-t)上的值，再遍历从-∞到+∞全部s和t的值，进行f和g积分意义上的加和，最终得到c在(x，y)上的值（二重积分)。 可以看出这种加和是一种加权求和，即以f为权，(x，y)为中心，将f在(s，t)上的值乘以g距离中心(-s，-t)上的值再相加。 其离散的表示形式为： 2.2 神经元神经细胞有众多的树突和一个伸长的轴突，不同神经元之间通过树突连接轴突的方式，并通过神经脉冲的传导进行细胞间交流。具体过程为神经细胞A的轴突连接到神经细胞B的树突，传导神经脉冲，神经细胞A根据其树突传递来的信号决定是否通过轴突来向下一个神经细胞C传递信息。 1943年，美国心理学家W.McCulloch和数学家W.Pitts根据生物神经元、生物电和生物化学的运行机理提出了二值神经元的数学模型，即神经细胞机能的简单化的数理模型，被称作莫克罗-彼特氏神经模型（McCulloch－Pitts′Neuron Model），也叫人工神经元模型。 一个人工神经元模型就是对生物神经元的数学建模： 其中X1、X2、X3、…、Xn分别是从其他神经元传入的输入信号，Wi1、Wi2、Wi3、…、Win分别是对应X1、X2、X3、…、Xn的权重。θ表示一个阈值，或称为偏置（Bias），其设置目的是正确将样本分类。神经元当前的输入信号和偏置相加后产生当前神经元的最终处理信号Net，称为净激活或净激励（Net Activation），并作为激活函数或激励函数（Activation Function）f的输入，即f（net)，最终得到对应的输出yi。形如： 激活函数f（net)的作用是通过加入非线性因素来解决线性模型的表达和分类能力不足的问题。常见的激活函数有： 1.LinearLinear是一个线性激活函数：f(x)=∑i xiwi+b。本质上就是对输入的加权求和再加上偏置，所以相对简单，功能较为单一。 由于多个矩阵进行连乘的结果仍是矩阵，类似的，如果一个神经网络中仅使用线性激活函数，那么就只能学习到线性函数。 2.SigmoidSigmoid是在神经网络研究的初始阶段就被投入了广泛使用的一个经典的非线性激活函数，其表达式是一种逻辑斯蒂函数（Logistic Function）： 其中x=∑i xiwi+b 这个函数在函数的区域内是完全可导的，输出为0到1范围内的正值（被证实不易于神经网络对收敛的学习），并且对于接近0和1的函数部分，导数无限趋近于0，存在明显的梯度衰减问题。所以现阶段对Sigmod函数的应用已经越来越少。 3.Tanh通过其图像可以看出Tanh函数旨在克服Sigmoid函数只能取正值的缺陷，将其设计为关于原点对称的双曲正切（hyperbolic tangent）形式： 其中x=∑i xiwi+b。函数的导数取值范围在0到1之间，优于sigmoid的0至1/4，在一定程度上减轻了梯度的消失。但函数的两端仍然存在梯度饱和问题，且计算变得更为复杂。 Tanh的输入和输出能够保持非线性单调上升和下降的关系，符合BP算法网络的梯度求解，容错性好。 4.ReLU （The Rectified Linear Unit）其公式的表达形式为： 也可以表示为f(x)=max(0,x)。即当输入信号小于0时，函数的输出为0；当输入信号大于0时，函数的输出等于输入值。 ReLU拥有操作简便，耗时少的优点，又具有部分线性的特性，不会出现过饱和现象、减免了梯度消失的问题（函数得到的随机梯度下降法的收敛速度比Sigmoid和Tanh都快）。并且ReLU只需要一个阈值就可以得到激活值，不用和Sigmoid一样进行复杂的指数运算。但其神经元比价脆弱容易失去作用：当神经元接收到非常大的梯度数据流以后，该神经元可能就不再对输入数据进行反馈了，所以在进行训练时要设置一个较小的学习率参数。ReLU是目前用于神经网络领域最常见的激活函数之一。 这就是单个人工神经元的定义。但是早期的单个人工神经元模型甚至无法处理异或等运算，在多个人工神经元发展为神经网络之后逐渐克服了这些问题和种种困难，成为现代人工智能的重要模型。 2.3 神经网络神经网络（NN），又叫人工神经网络（Artificial Neural Networks， ANN），是一个历史悠久的计算模型。 进行上述过程的推演，将一个人工神经元的输出作为另一个人工神经元的输入，一定数量的这样的人工神经元就连接成人工神经网络。人工神经网络具有多种拓扑结构，其中“多层全连接前向神经网络”结构相对简便：将输入连接到网络第一层的每个神经元上，前一层的每个神经元的输出再连接到下一层的每个神经元的输入上，这样最后一层神经元的输出就可以作为整个神经网络的输出来使用。 图例为一个3层的神经网络结构，第1层称为输入层，第2层称为隐藏层，第三层称为输出层。其包含了10个输入端（10元向量），第1、2层各有12个神经元，最后一层有6个神经元，那么该结构的神经网络最终输出的就是一个6元向量。 关于整个神经网络的计算可以通过矩阵式表出（单层神经网络的表达式)。假设神经网络拥有n个神经元的第i层收到m个输入，可以表示为下列形式： 如果每层神经元的个数不同，那么，输入/输出的维度也不相同，但整体形式保持一致。O^i是n元的输出向量，由于神经网络的第i层共有n个神经元，那么第i层的输入（即第i-1层的输出)是一个m元向量。权值矩阵w是n x m的矩阵：n个神经元中每个神经元包含m个权值。w乘以第i - 1层输出的m向量，可以得到n向量，再加上n元偏置向量b，对结果的每一个元素施以激活函数f，最后就得到第i层的n元输出向量。 3.卷积神经网络结构3.1 图像的输入和特征提取对于一幅图像，全连接神经网络（Fully Connected Neural Network）可以通过将其平整化为数组，并把像素值作为预测图像中数值的特征。 经典的神经网络模型，需要对整幅目标图像进行读取来将其作为神经网络的输入，即全连接模式。如果目标图像的尺寸非常大，那么在连接过程中的参数过多，会导致数据计算量非常大，使得网络理解图像信息变得更加困难。 类比人类对外部环境的感知过程，通常是一个从局部到全部的过程，图像中的空间关系也类似这样，局部范围内像素之间的联系较为紧密，而距离较远的像素之间相关性较弱。所以，各个神经元不需要对整幅图像进行感知，只需对图像的各部分进行感知，然后在更高层将局部信息拼接完整，这样综合起来同样能够获取全局信息。这样的感知模式就是卷积神经网络模型中降低参数数目的重要方法：局部感受野（Local Receptive Fields）。 假设输入图像为字母X和O的图片，通过局部感受野来提取图像的特征。对于CNN的对比模式，CNN是通过一小块一小块的对比在图像中大致相同的位置找到一些粗略的特征进行匹配。相比较传统的整图逐一对比方式，CNN的小块匹配模式能更好的比较出图像之间的相似性。 以字母x为例，用像素值“1”表示白色，用像素值“-1”表示灰色，可以提取出三个重要的特征：1个中心交叉线，两个斜边对角线： 3.2 卷积层卷积神经网络中的第一层始终是卷积层（除去输入层）。当开始对一张图片进行识别时，CNN并不能立即对需要识别的部分进行准确匹配，而是首先对源文件的每个位置进行尝试性匹配，将匹配的目标特征作为一个过滤器，从而处理掉匹配度低的部分。这个操作就是卷积操作。 在这个字母识别案例当中，要计算一个特征和其在字母“X”的原图上对应的某一小块的结果，可以将两个小块内对应位置的像素值进行乘法运算，再将整个小块内乘法运算的结果累加起来，除以小块内像素点总个数。 这样，不论匹配的两个像素点的颜色均为白色或者灰色，它们的乘积结果1×1和(-1)×(1)都为1，说明每一对能够匹配成功的像素，其结果恒为1。类似的，不能完全匹配的结果乘积则为1×(-1)=-1。相关步骤如下： 通过卷积的计算，第一块特征完成匹配后结果为1。 类似的，将第一块特征与其他部分进行匹配，得到匹配后的结果。 以此类推，对三个特征不断重复上述步骤，通过对每个特征的卷积操作，最终得到一个称为feature map的新二维数组。其中值的含义是越接近1的值表示该对应位置和特征的匹配越完整，越接近-1的值表示该对应位置和特征的反向匹配越完整，而接近0的值表示对应位置没有明显匹配或不相关。 可以预见，当图像的尺寸增大时，每一个卷积层的大小和卷积层的数目呈线性增长的关系，其内部的加法、乘法和除法操作的次数也会随之增加，计算量也会越庞大。 3.3 池化层（Pooling）由于第一层卷积操作的匹配程序计算量较大，如果遇到数据量大的情况会严重影响卷积的效率。为了有效的减少这一步骤的计算，CNN会通过池化层的结构将输入的源图像文件进行必要的简化和缩放，尽可能的减少文件的像素信息，只保留必要的数据信息，减少内存消耗。 池化层通常采用2*2像素的池化区域，按照一定的算法规则保留池化区域中的最大值（max-pooling）、均值（mean-pooling）等，并将结果作为区域的像素。 下图显示了左上角2×2池化区域的max-pooling结果，取该区域的最大值max(0.77,-0.11,-0.11,1.00)，作为池化后的结果。 类似的，池化区域向左移动，取第二块区域的取大值max(0.11,0.33,-0.11,0.33)，作为池化后的结果。 以此类推，得到最终结果如下： 以同样的方式对所有feature map执行相同操作。 最大池化（max-pooling）保留了每一区域内的最大值，也就是相当于保留了这一块区域的最佳匹配结果（值越接近1表示匹配程度越高)。池化操作不会分析区域内具体完成匹配的部分，而是关注是否有某一部分能够匹配上。 通过池化操作，图像被简化，在很大程度上减少了计算量，降低计算机负载。 3.4 激活函数激活函数通过加入非线性因素将卷积层的输出结果做非线性的映射。卷积层常用的激活函数有修正线性单元ReLU函数，其收敛速度较快，且梯度计算简单。ReLU函数的一般表达式近似于max(0，x)，即当x&lt;0时，输出结果恒为0，当x≥0时，输出结果为参数本身。 下图为通过ReLU对feature map的处理，首先取max(0,0.77)，结果为0.77。 类似的，第二个值取max(0,-0.11)，结果为0。 以此类推，最终得到结果如下： 3.5 深度神经网络通过将卷积、激活函数、池化进行组合，得到一层简单的神经网络解构。 将卷积、池化、激活函数经过组合和拓展以后，层数增加，得到深度神经网络的结构。 3.6 全连接层（Fully connected layers）全连接层相当于整个神经网络当中的“分类器”，数据经过卷积、池化、激活函数等深度网络以后，再通过全连接层对结果进行分类，获取识别的结果。由于神经网络技术是对有标签数据的一种监督学习，在模型的训练过程中，得到全连接层的权重，这样再通过模型对目标进行识别时，根据卷积、池化、激活函数等深度网络操作的结果和得到的权重进行加权求和，获取对各分类结果的预测值，选择最优结果即完成预测分类的任务。全连接层可以有多个。 将经过卷积、激活函数、池化后的深度网络的结果串联起来，对字母进行识别判断。 3.7 卷积神经网络再完成了卷积层、池化层、激活函数、深度神经网络、全连接层一系列的操作后，就得到了一个完整的卷积神经网络的结构。整体可划分为两大步骤，步骤一是经过卷积、池化和激活函数的特征提取，步骤二是通过组成深度神经网络和全连接层的特征识别和预测分类。 卷积神经网络在本质上是一种对输入输出映射关系的学习，特点是不需要掌握输入输出之间的明确关系或具体的函数表达式，而是利用其特征提取的模式对卷积网络加以训练，最终网络会具备输入输出之间映射的能力。 （本文节选自我的毕业论文）","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}],"tags":[{"name":"CNN","slug":"CNN","permalink":"http://yoursite.com/tags/CNN/"}]},{"title":"Hadoop集群组件配置","slug":"Hadoop集群组件配置","date":"2019-09-08T18:35:33.000Z","updated":"2024-12-06T08:54:35.878Z","comments":true,"path":"2019/09/09/Hadoop集群组件配置/","link":"","permalink":"http://yoursite.com/2019/09/09/Hadoop集群组件配置/","excerpt":"Spark集群搭建首先在scala官网（http://www.scala-lang.org/files/archive/scala-2.10.4.tgz）下载scala语言并解压缩，配置/etc/profile下scala的环境变量12export SCALA_HOME=/usr/local/scalaexport PATH=$PATH:$&#123;SCALA_HOME&#125;/bin 环境变量成功生效后进行测试，输入15*15,返回225，成功 接下来到spark官网（http://archive.cloudera.com/cdh5/cdh/5/spark-1.5.0-cdh5.6.0.tar.gz)下载Spark包，继续配置环境变量 12export SPARK_HOME=/usr/local/soft/spark-1.6.0-bin-hadoop2.6export PATH=$PATH:$&#123;SPARK_HOME&#125;/bin 完成后继续配置${SPARK_HOME}/conf/spark-env.sh 配置{SPARK_HOME}/conf/ slaves，在vi编辑器中添加node节点,在将程序分发给node节点 12scp -r spark-1.6.0-bin-hadoop2.6. root@node1:/usr/local/soft/scp -r spark-1.6.0-bin-hadoop2.6. root@node2:/usr/local/soft/ 可以通过./start-all.sh命令来启动spark集群 可也有通过节点来启动： · 通过sbin/start-master.sh启动主节点· 通过sbin/start-slave.sh 192.168.218.141:8070等启动node节点 可以执行/bin/run-example SparkPi 10 &gt; Sparkpilog.txt运行spark示例程序 通过jps命令查看启动的java服务，进行验证","text":"Spark集群搭建首先在scala官网（http://www.scala-lang.org/files/archive/scala-2.10.4.tgz）下载scala语言并解压缩，配置/etc/profile下scala的环境变量12export SCALA_HOME=/usr/local/scalaexport PATH=$PATH:$&#123;SCALA_HOME&#125;/bin 环境变量成功生效后进行测试，输入15*15,返回225，成功 接下来到spark官网（http://archive.cloudera.com/cdh5/cdh/5/spark-1.5.0-cdh5.6.0.tar.gz)下载Spark包，继续配置环境变量 12export SPARK_HOME=/usr/local/soft/spark-1.6.0-bin-hadoop2.6export PATH=$PATH:$&#123;SPARK_HOME&#125;/bin 完成后继续配置${SPARK_HOME}/conf/spark-env.sh 配置{SPARK_HOME}/conf/ slaves，在vi编辑器中添加node节点,在将程序分发给node节点 12scp -r spark-1.6.0-bin-hadoop2.6. root@node1:/usr/local/soft/scp -r spark-1.6.0-bin-hadoop2.6. root@node2:/usr/local/soft/ 可以通过./start-all.sh命令来启动spark集群 可也有通过节点来启动： · 通过sbin/start-master.sh启动主节点· 通过sbin/start-slave.sh 192.168.218.141:8070等启动node节点 可以执行/bin/run-example SparkPi 10 &gt; Sparkpilog.txt运行spark示例程序 通过jps命令查看启动的java服务，进行验证 都通过后，可以在本地浏览器输入http://192.168.218.141:8090/，进入到spark服务界面 如果要启动spark shell，可以执行：spark shell：bin/spark-shell 最后通过./stop-all.sh来关闭spark集群 HiveHbaseKafukaSqoopFlinkStrom（未完）","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://yoursite.com/tags/Hadoop/"}]},{"title":"Hadoop入门","slug":"Hadoop入门","date":"2019-09-07T09:17:17.000Z","updated":"2024-12-06T08:53:41.815Z","comments":true,"path":"2019/09/07/Hadoop入门/","link":"","permalink":"http://yoursite.com/2019/09/07/Hadoop入门/","excerpt":"Hadoop是一个用Java编写的Apache开源框架，允许使用简单的编程模型跨计算机集群分布式处理大型数据集。Hadoop框架工作的应用程序在跨计算机集群提供分布式存储和计算的环境中工作。Hadoop旨在从单个服务器扩展到数千个机器，每个都提供本地计算和存储。 Hadoop伪分布式搭建首先创建一台master虚拟机，并克隆node1、node2两台节点机，我的虚拟机集群ip地址分别设置为： · 192.168.218.141 master · 192.168.218.142 node1 · 192.168.218.143 node2 到master中设置好ssh，做好主机名与ip地址的映射，修改/etc/hosts下的配置文件，并将完成的hosts文件复制进节点机中 此时可以打开xshell，也可以继续在虚拟机中进行操作 将hadoop的jar包上传到虚拟机中，配置环境变量，并对hadoop下的core-site.xml、hdfs-site.xml、yarn-site.xml和mapred-site.xml文件进行相关配置 完成以后再将hadoop的安装目录分别拷贝到子节点中，检查是否完成集群文件的配置和部署 启动回到主机中打开hadoop安装目录，执行 12start-dfs.shstart-yarn.sh （老版本的”start-all.sh”会提示已过期） 完成hadoop的启动后，在主机和节点机输入jps查看当前运行的java进程 成功后打开浏览器，输入localhost:50700可以进入hadoop初始界面。","text":"Hadoop是一个用Java编写的Apache开源框架，允许使用简单的编程模型跨计算机集群分布式处理大型数据集。Hadoop框架工作的应用程序在跨计算机集群提供分布式存储和计算的环境中工作。Hadoop旨在从单个服务器扩展到数千个机器，每个都提供本地计算和存储。 Hadoop伪分布式搭建首先创建一台master虚拟机，并克隆node1、node2两台节点机，我的虚拟机集群ip地址分别设置为： · 192.168.218.141 master · 192.168.218.142 node1 · 192.168.218.143 node2 到master中设置好ssh，做好主机名与ip地址的映射，修改/etc/hosts下的配置文件，并将完成的hosts文件复制进节点机中 此时可以打开xshell，也可以继续在虚拟机中进行操作 将hadoop的jar包上传到虚拟机中，配置环境变量，并对hadoop下的core-site.xml、hdfs-site.xml、yarn-site.xml和mapred-site.xml文件进行相关配置 完成以后再将hadoop的安装目录分别拷贝到子节点中，检查是否完成集群文件的配置和部署 启动回到主机中打开hadoop安装目录，执行 12start-dfs.shstart-yarn.sh （老版本的”start-all.sh”会提示已过期） 完成hadoop的启动后，在主机和节点机输入jps查看当前运行的java进程 成功后打开浏览器，输入localhost:50700可以进入hadoop初始界面。 因为我之前已经完成了平台的搭建，dfs中已经存放有一些测试文件，可以进行查看(豆瓣电影数据) MapreduceMapReduce是Hadoop中一套分布式的计算框架,分为Map和Reduce两个部分,Map用于数据的整理,Reduce负责数据的汇总，当hadoop平台搭建完成后，就可以使用内置的mapreduce程序”wordcount”进行测试，其作用是对输入文件的词频进行统计 进入hadoop目录新建一个测试文件夹testfiles，并创建几个txt文本文件12345cd $HADOOP_HOMEmkdir testfilesecho Hello World &gt;&gt; testfiles/test1.txtecho Hello Hadoop &gt;&gt; testfiles/test2.txtecho Hello spark &gt;&gt; testfiles/test3.txt 执行后再到dfs中创建一个inputtestfiles文件夹,并将用于测试的文本文件发送进去 12hadoop fs -mkdir /inputfiletestshadoop fs -put testfiles/* /inputfiletests 找到mapreduce所在的位置，执行12cd $HADOOP_HOME/share/hadoop/mapreducehadoop jar hadoop-mapreduce-examples-2.6.0.jar wordcount /inputfiletests /outputfiletests 执行完成后查看输出路径/outputfiletests，包含了运行成功文件和最终输出文件，打开输出的文件进行查看 12hadoop fs -ls /outputhadoop fs -cat /outputfiletests/part-r-00000 同时在hadoop后台里也会有wordcount的运行结果 最后，关闭hadoop集群 12stop-dfs.shstop-yarn.sh","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://yoursite.com/tags/Hadoop/"}]},{"title":"实习项目：CRM管理系统","slug":"实习项目：CRM管理系统","date":"2019-06-06T05:59:01.000Z","updated":"2022-06-27T07:41:43.666Z","comments":true,"path":"2019/06/06/实习项目：CRM管理系统/","link":"","permalink":"http://yoursite.com/2019/06/06/实习项目：CRM管理系统/","excerpt":"项目文件代码已存储于github：https://github.com/elbadaernU404/projcode 1.前期准备首先完成了MySql、JspStudy的安装，测试了数据库的连接，tomcat服务器的部署，以及配置tomcat的动态更新等操作。接下来主要是做了对Html语法的复习，并尝试对后台登陆界面的编程。在项目文件的根目录下/web中创建register.html文件进行测试，通过do get的方法发送网页请求，以及添加各类可视化元素，完成注册界面。同样在/web中创建login.html文件，通过do post的方法发送网页请求（隐藏显示的细节）完成登陆界面。","text":"项目文件代码已存储于github：https://github.com/elbadaernU404/projcode 1.前期准备首先完成了MySql、JspStudy的安装，测试了数据库的连接，tomcat服务器的部署，以及配置tomcat的动态更新等操作。接下来主要是做了对Html语法的复习，并尝试对后台登陆界面的编程。在项目文件的根目录下/web中创建register.html文件进行测试，通过do get的方法发送网页请求，以及添加各类可视化元素，完成注册界面。同样在/web中创建login.html文件，通过do post的方法发送网页请求（隐藏显示的细节）完成登陆界面。 2.注册功能&amp;服务器与数据库的连接1.在项目目录/src里创建main文件夹，在main文件夹中分别创建controller、dao、model、service、util五个子文件夹，并一一创建相对应的java class文件，在web.xml中定义。 2.在mysql数据库中找到user表，并通过新建查询测试创建用户的查询语句适合当前数据库版本。 3.编写UserServlet.java获取表单的属性，并完成注册功能结束后的跳转步骤，实现register注册页面向login登陆页面的跳转。123456789101112131415161718192021222324252627282930313233343536package main.controller;import main.model.User;import main.service.UserService;import javax.servlet.ServletException;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import java.io.IOException;/** * @author rtl * @site elbadaernu404.github.io * @company suzhou university * @create 2020-06-21 16:40 */public class UserServlet extends HttpServlet&#123; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123;//获取表单中的属性值 String username=req.getParameter(\"name\"); String password=req.getParameter(\"password\");//新建一个user对象 User user=new User(); user.setName(username); user.setPassword(password); //调用service，完成最后的注册功能 UserService userService=new UserService(); userService.addUser(user);//注册完成后，跳转到登录页面 req.getRequestDispatcher(\"/login.jsp\").forward(req,resp); &#125; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; &#125;&#125; 4.编写UserDao.java完成数据库内容的导入，类型为Interface接口类型。12345678910111213package main.dao;import main.model.User;/** * @author rtl * @site elbadaernu404.github.io * @company suzhou university * @create 2020-06-21 16:40 */public interface UserDao &#123; //用户注册的功能 public void addUser(User user);&#125; 5.编写User.java，实现具体的注册功能的写入。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package main.model;/** * @author rtl * @site elbadaernu404.github.io * @company suzhou university * @create 2020-06-21 16:40 */public class User &#123; private int id; private String name; private String password; public User() &#123; &#125; public User(int id, String name, String password) &#123; this.id = id; this.name = name; this.password = password; &#125; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getPassword() &#123; return password; &#125; public void setPassword(String password) &#123; this.password = password; &#125; @Override public String toString() &#123; return \"User&#123;\" + \"id=\" + id + \", name='\" + name + '\\'' + \", password='\" + password + '\\'' + '&#125;'; &#125;&#125; 6.编写UserService.java将用户数据操作到数据库当中，完成tomcat到数据库的映射功能。123456789101112131415161718192021222324252627282930313233package main.service;import main.dao.UserDao;import main.model.User;import main.util.JdbcUtil;import java.sql.Connection;import java.sql.PreparedStatement;import java.sql.SQLException;/** * @author rtl * @site elbadaernu404.github.io * @company suzhou university * @create 2020-06-21 16:40 */public class UserService implements UserDao &#123; PreparedStatement ps=null; @Override public void addUser(User user) &#123; //真正的把用户注册的数据进行操作到数据库中 String sql=\"INSERT into user(name,password) values(?,?)\"; //？是占位符的意思 Connection conn = null; conn = JdbcUtil.getConn(); try &#123; ps= conn.prepareStatement(sql); ps.setString(1,user.getName()); ps.setString(2,user.getPassword()); ps.executeUpdate(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 7.编写JdbcUtil.java，添加本地数据库链接及Jdbc功能，完成Jdbc驱动。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091package main.util;import java.sql.*;public class JdbcUtil &#123; static&#123; try &#123; Class.forName(\"com.mysql.jdbc.Driver\");//加载数据库驱动 &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; &#125; private JdbcUtil() &#123;&#125; public static Connection getConn()&#123; Connection conn = null; try &#123; conn = DriverManager.getConnection(\"jdbc:mysql://localhost:3306/crmpro?useUnicode=true&amp;characterEncoding=utf-8\", \"root\",\"root\");//连接数据库 &#125; catch(SQLException e)&#123; e.printStackTrace(); &#125; return conn; &#125; public static void closeConn(Connection conn)&#123; try &#123; if(conn != null )&#123; conn.close(); conn = null; &#125; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; public static PreparedStatement getPStmt(Connection conn,String sql)&#123; PreparedStatement pstmt = null; try &#123; pstmt = conn.prepareStatement(sql); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; return pstmt; &#125; public static void closeStmt(Statement stmt)&#123; try &#123; if(stmt != null )&#123; stmt.close(); stmt = null; &#125; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; public static ResultSet executeQuery(Statement stmt,String sql)&#123; ResultSet rs = null; try &#123; rs = stmt.executeQuery(sql); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; return rs; &#125; public static void closeRs(ResultSet rs)&#123; try &#123; if(rs != null )&#123; rs.close(); rs = null; &#125; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; public static ResultSet executeQuery(Connection conn,String sql)&#123;//重载 ResultSet rs = null; try &#123; rs = conn.createStatement().executeQuery(sql); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; return rs; &#125;&#125; 8.添加tomcat-jdbc.jar包，完成驱动的功能，使得所有文件可以相互调用，实现注册信息录入本地数据库，注册功能基本完成。 9.运行程序，注册功能正常运行。实现了进行项目与数据库的连接 3.登陆功能1.在src/main/controller中创建LoginServlet.java，编写代码实现从登陆界面通过get方法获取账户和密码，并与数据库现存数据进行对比，如果验证成功则登陆成功，验证失败则登陆无效，转而进行下一步操作。1234567891011121314151617181920212223242526272829303132package main.controller;import main.model.User;import main.service.LoginService;import javax.servlet.ServletException;import javax.servlet.annotation.WebServlet;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import java.io.IOException;import java.util.List;/** * @author rtl * @site elbadaernu404.github.io * @company suzhou university * @create 2020-06-21 16:40 */public class LoginServlet extends HttpServlet &#123; protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; &#125; protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; //从登陆界面获得了用户名和密码 String username = request.getParameter(\"name\"); String password = request.getParameter(\"password\"); //与数据库的数据进行对比 LoginService loginService=new LoginService(); User user=new User(); user.setName(username); user.setPassword(password); List list= loginService.login(user); &#125;&#125; 2.在src/main/dao中创建LoginDao.java，部署接口。12345678910111213package main.dao;import main.model.User;import java.util.List;/** * @author rtl * @site elbadaernu404.github.io * @company suzhou university * @create 2020-06-21 16:40 */public interface LoginDao &#123; //登录功能 public List login(User user);&#125; 3.在.在src/main/service中创建LoginService，编写代码通过jdbc驱动来实现数据库的查询功能，并通过查询语句进行具体的数据库数据的查询操作，其执行查询并遍历结果集。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package main.service;import main.dao.LoginDao;import main.model.User;import main.util.JdbcUtil;import java.sql.Connection;import java.sql.PreparedStatement;import java.sql.ResultSet;import java.sql.SQLException;import java.util.ArrayList;import java.util.List;/** * @author rtl * @site elbadaernu404.github.io * @company suzhou university * @create 2020-06-21 16:40 */public class LoginService implements LoginDao &#123; @Override public List login(User user) &#123; //返回值的类型是一个集合 List list=new ArrayList(); PreparedStatement ps=null; String sql =\"select * from user where name =? and password=?\" ;//jdbc去实现查询功能 Connection conn = null; conn = JdbcUtil.getConn(); try &#123; ps= conn.prepareStatement(sql); ps.setString(1,user.getName()); ps.setString(2,user.getPassword()); ResultSet resultSet =ps.executeQuery();//执行查询语句//遍历这个结果集 while(resultSet.next())&#123; String name=resultSet.getString(2); String password=resultSet.getString(3); list.add(name); list.add(password); &#125; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; return list; &#125;&#125; 4.在LoginServlet.java中添加代码，实现登陆成功向localhost主页面跳转，登陆失败则向register.jsp注册页面跳转。1234567if (list.size()&gt;0)&#123; //用户名密码存在，登陆成功 request.getRequestDispatcher(\"index.jsp\").forward(request,response);&#125;else&#123; //用户名密码不存在，登陆失败 request.getRequestDispatcher(\"register.jsp\").forward(request,response);&#125; LoginServlet.java完整代码如下:12345678910111213141516171819202122232425262728293031323334353637383940package main.controller;import main.model.User;import main.service.LoginService;import javax.servlet.ServletException;import javax.servlet.annotation.WebServlet;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import java.io.IOException;import java.util.List;/** * @author rtl * @site elbadaernu404.github.io * @company suzhou university * @create 2020-06-21 16:40 */public class LoginServlet extends HttpServlet &#123; protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; &#125; protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; //从登陆界面获得了用户名和密码 String username = request.getParameter(\"name\"); String password = request.getParameter(\"password\"); //与数据库的数据进行对比 LoginService loginService=new LoginService(); User user=new User(); user.setName(username); user.setPassword(password); List list= loginService.login(user); if (list.size()&gt;0)&#123; //用户名密码存在，登陆成功 request.getRequestDispatcher(\"index.jsp\").forward(request,response); &#125;else&#123; //用户名密码不存在，登陆失败 request.getRequestDispatcher(\"register.jsp\").forward(request,response); &#125; &#125;&#125; 5.在UserServlet.java中添加代码，实现注册完成向login.jsp登陆页面跳转的功能。12//注册完成后，跳转到登录页面req.getRequestDispatcher(\"/login.jsp\").forward(req,resp); 6.在web.xml中定义以添加上述功能。123456789101112131415161718192021222324252627&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;web-app xmlns=\"http://xmlns.jcp.org/xml/ns/javaee\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd\" version=\"3.1\"&gt; &lt;welcome-file-list&gt; &lt;welcome-file&gt;login.jsp&lt;/welcome-file&gt; &lt;/welcome-file-list&gt; &lt;servlet&gt; &lt;servlet-name&gt;UserServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;main.controller.UserServlet&lt;/servlet-class&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;UserServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/register&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;servlet&gt; &lt;servlet-name&gt;LoginServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;main.controller.LoginServlet&lt;/servlet-class&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;LoginServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/login&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;/web-app&gt; 7.启动tomcat服务器，运行，彻底完成注册及登陆功能及一系列跳转步骤，最后在web.xml中将login.jsp设置为tomcat服务器启动的主界面。 4.EasyUI框架1.进入jquery easyui官网首页http://www.jeasyui.net，进行浏览easyui的相关信息，了解easyui的实例demo及教程。 2.进入easyui的下载界面，选择下载“EasyUI for jQuery”，这里选择免费版本。 3.将下载好的easyui文件解压，重命名后整体复制到/web目录中去，经过一段时间文件导入完毕。 4.修改正确的文件路径，将easyui.css、icon.css、jquery.min.js、jquery.easyui.min.js和demo.css文件一同添加到index.jsp文件的&#60;head&#62;&#60;/head&#62;中。123456789101112&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Insert title here&lt;/title&gt; &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"easyui/themes/bootstrap/easyui.css\"&gt; &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"easyui/themes/icon.css\"&gt; &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"./easyui/demo/demo.css\"&gt; &lt;script type=\"text/javascript\" src=\"easyui/jquery.min.js\"&gt;&lt;/script&gt; &lt;script type=\"text/javascript\" src=\"easyui/jquery.easyui.min.js\"&gt;&lt;/script&gt; ...&lt;/head&gt; 5.通过使用后台的基本框架代码，进行修改index.jsp文件的&#60;body&#62;&#60;/body&#62;部分，完成一个最基本的基于easyui的CRM用户管理系统后台代码。完整的index.jsp文件代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112&lt;%@ page contentType=\"text/html;charset=UTF-8\" language=\"java\" %&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Insert title here&lt;/title&gt; &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"easyui/themes/bootstrap/easyui.css\"&gt; &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"easyui/themes/icon.css\"&gt; &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"./easyui/demo/demo.css\"&gt; &lt;script type=\"text/javascript\" src=\"easyui/jquery.min.js\"&gt;&lt;/script&gt; &lt;script type=\"text/javascript\" src=\"easyui/jquery.easyui.min.js\"&gt;&lt;/script&gt; &lt;style&gt; .westname &#123; text-decoration:none; &#125; &lt;/style&gt; &lt;script type=\"text/javascript\"&gt; $(function()&#123; $('.westname').click(function()&#123; var name = this.innerHTML; var url = this.href; //通过获取的名字创建选项卡 crateTabs(name,url); //让超链接不跳转 return false; &#125;); &#125;); //创建tabs的函数 function crateTabs(name,url)&#123; if($('#tt').tabs('exists',name))&#123; $('#tt').tabs('select',name); &#125;else&#123; $('#tt').tabs('add',&#123; title:name, content:createUrl(url), closable:true &#125;); &#125; &#125; //使用iframe转化里面的数据使之返回的是url里面的数据 function createUrl(url)&#123; return \"&lt;iframe src='\"+url+\"' style='border:0px;width:100%;height:95%;'&gt;&lt;/iframe&gt;\"; &#125; &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div id=\"cc\" class=\"easyui-layout\" data-options=\"fit:true\"&gt; &lt;div data-options=\"region:'north',title:'CRM管理系统',split:true\" style=\"height:100px;background-color: #0081c2\"&gt; &lt;font size=\"6\" color=white&gt;&lt;div&gt; &lt;span&gt; &lt;script type=\"text/javascript\"&gt; var date = new Date(); document.write(date.getFullYear() + \"年\" + (date.getMonth() + 1) + \"月\" + date.getDate() + \"日\" + \" 星期\" + \"日一二三四五六\".charAt(date.getDay())); &lt;/script&gt; &lt;/span&gt; &lt;/div&gt;&lt;/font&gt; &lt;%--&lt;img src=\"img/2.png\" width=\"100% \"height=\"100%\"/&gt;--%&gt; &lt;/div&gt; &lt;div data-options=\"region:'west',title:'系统菜单',split:true\" style=\"width:240px;\"&gt; &lt;div id=\"aa\" class=\"easyui-accordion\" data-options=\"fit:true\"&gt; &lt;div title=\"客户管理\" data-options=\"iconCls:'icon-reload',selected:true\" style=\"overflow: auto; padding: 10px;background-color: #DCDCE2\"&gt; &lt;a href=\"customer.jsp\" class=\"westname\" &gt;客户管理&lt;/a&gt; &lt;br&gt;&lt;a href=\"department.jsp\" class=\"westname\" &gt;部门信息&lt;/a&gt;&lt;/br&gt; &lt;a href=\"employee.jsp\" class=\"westname\" &gt;员工管理&lt;/a&gt; &lt;/div&gt; &lt;div title=\"数据管理\" data-options=\"iconCls:'icon-reload'\" style=\"padding: 10px;background-color: #DCDCE2\"&gt; &lt;a href=\"linkMen.html\" class=\"westname\"&gt;字典目录&lt;/a&gt; &lt;br&gt;&lt;a href=\"linkMen.html\" class=\"westname\"&gt;字典明细&lt;/a&gt;&lt;/br&gt; &lt;/div&gt; &lt;div title=\"员工管理\" data-options=\"iconCls:'icon-reload'\" style=\"padding: 10px;background-color: #DCDCE2\"&gt; &lt;a href=\"#\" class=\"westname\"&gt;员工管理&lt;/a&gt; &lt;/div&gt; &lt;div title=\"订单管理\" data-options=\"iconCls:'icon-reload'\" style=\"padding: 10px;background-color: #DCDCE2\"&gt; &lt;a href=\"#\" class=\"westname\"&gt;订单管理&lt;/a&gt; &lt;/div&gt; &lt;div title=\"地址管理\" data-options=\"iconCls:'icon-reload'\" style=\"padding: 10px;background-color: #DCDCE2\"&gt; &lt;a href=\"#\" class=\"westname\"&gt;地址管理&lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div data-options=\"region:'center',title:''\" style=\"padding:5px;background:#eee;\"&gt; &lt;div id=\"tt\" class=\"easyui-tabs\" data-options=\"fit:true\"&gt; &lt;div align=\"center\" title=\"主页\" data-options=\"closable:true\" style=\"overflow: auto; padding: 20px; display: none;background-color: white;\"&gt; &lt;form id=\"bdfm\" target=\"_blank\" name=\"bdfm\" method=\"get\" action=\"http://www.baidu.com/s\"&gt; &lt;table&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=\"http://www.baidu.com\"&gt; &lt;img src=\"img/baidu.png\"/&gt; &lt;/a&gt; &lt;/td&gt; &lt;td&gt;&lt;br/&gt;&lt;input type=\"text\" id=\"search1\" name=\"word\"/&gt;&lt;/td&gt; &lt;td&gt;&lt;br/&gt;&lt;input type=\"submit\" value=\"搜索\" /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/form&gt; &lt;img src=\"img/1.png\" alt=\"some_text\"&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 6.在web.xml中进一步添加必要信息。 7.启动tomcat服务器，通过验证账号密码进入CRM后台系统，界面基本成形。 8.保持tomcat服务器的启动，在easyui的easyui.css文件中修改框架各成分的颜色，对后台做最初的优化。 5.与数据库的进一步交互1.在src/main/controller中新建GetAllUser.java文件，编写代码实现easyui框架的后台能够获取数据。主要思路为通过将数据库list数据类型转化为json数组，使得数据集合以json的键值对格式返回到easyui前端。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package main.controller;import main.model.User;import main.service.GetUserService;import main.util.UserOperation;import net.sf.json.JSONArray;import net.sf.json.JSONObject;import javax.servlet.ServletException;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import java.io.IOException;import java.io.PrintWriter;import java.util.HashMap;import java.util.List;import java.util.Map;/** * @author rtl * @site elbadaernu404.github.io * @company suzhou university * @create 2020-06-21 16:40 */public class GetAllUser extends HttpServlet &#123; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; super.doGet(req, resp); &#125; @Override protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; PrintWriter out = response.getWriter(); //请求页码、每页显示行数、偏移、总数 int page,rows,offset,total; //获取 String input_page=request.getParameter(\"page\"); page=(input_page==null)?1:Integer.parseInt(input_page);//页码数 String input_rows=request.getParameter(\"rows\"); rows=(input_rows==null)?10:Integer.parseInt(input_rows);//每页多少条数据 offset=(page-1)*rows; UserOperation operation=new UserOperation(); total=operation.selectCount(); List&lt;User&gt; users=operation.selectPage(offset, rows); Map&lt;String, Object&gt; jsonMap = new HashMap&lt;String, Object&gt;();//定义map jsonMap.put(\"total\", total);//total键 存放总记录数，必须的 jsonMap.put(\"rows\", users);//rows键 存放每页记录 list String result = JSONObject.fromObject(jsonMap).toString();//格式化result 一定要是JSONObject out.print(result); out.flush(); out.close(); &#125;&#125; 2.将json的jar包导入idea。 3.在src/main/dao中新建GetUserDao.java接口类型文件，创建显示用户数据的数据库接口。123456789101112package main.dao;import main.model.User;import java.util.List;/** * @author rtl * @site elbadaernu404.github.io * @company suzhou university * @create 2020-06-21 16:40 */public interface GetUserDao &#123; public List&lt;User&gt; GetAllUser();//返回值类型List&lt;User&gt;&#125; 4.在src/main/service中新建GetUserService.java文件，此程序的功能为通过jdbc驱动去实现数据库查询的功能，使得后台可以通过查询语句遍历并返回出结果，是查询数据的具体功能实现。123456789101112131415161718192021222324252627282930313233343536373839404142package main.service;import main.dao.GetUserDao;import main.model.User;import main.util.JdbcUtil;import java.sql.Connection;import java.sql.PreparedStatement;import java.sql.ResultSet;import java.sql.SQLException;import java.util.ArrayList;import java.util.List;/** * @author rtl * @site elbadaernu404.github.io * @company suzhou university * @create 2020-06-21 16:40 */public class GetUserService implements GetUserDao &#123; @Override public List&lt;User&gt; GetAllUser() &#123; List&lt;User&gt; list=new ArrayList(); PreparedStatement ps=null; String sql =\"select * from user \" ;//jdbc去实现查询功能 Connection conn = null; conn = JdbcUtil.getConn(); try &#123; ps= conn.prepareStatement(sql); ResultSet resultSet =ps.executeQuery();//执行查询语句//遍历这个结果集 while(resultSet.next())&#123; int id=resultSet.getInt(1); String name=resultSet.getString(2); String password=resultSet.getString(3); User user=new User(id,name,password); list.add(user); &#125; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; return list; &#125;&#125; 5.在web.xml中添加上述功能映射。12345678&lt;servlet&gt; &lt;servlet-name&gt;GetAllUser&lt;/servlet-name&gt; &lt;servlet-class&gt;main.controller.GetAllUser&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;GetAllUser&lt;/servlet-name&gt; &lt;url-pattern&gt;/getAllUser&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 6.在/web文件夹下新建custom.jsp文件，准备用来显示用户数据。 7.在custom.jsp文件中添加增添用户的按钮控件，编写具体功能。custom.jsp文件中具体代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147&lt;%@ page contentType=\"text/html;charset=UTF-8\" language=\"java\" %&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;客户管理&lt;/title&gt; &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"easyui/themes/bootstrap/easyui.css\"&gt; &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"easyui/themes/icon.css\"&gt; &lt;script type=\"text/javascript\" src=\"easyui/jquery.min.js\"&gt;&lt;/script&gt; &lt;script type=\"text/javascript\" src=\"easyui/jquery.easyui.min.js\"&gt;&lt;/script&gt; &lt;script type=\"text/javascript\" src=\"easyui/locale/easyui-lang-zh_CN.js\"&gt;&lt;/script&gt; &lt;script type=\"text/javascript\"&gt; var url; $(function()&#123; $('#dg').datagrid(&#123; url:'getAllUser', columns:[[ &#123;field:'id',title:'id',width:100&#125;, &#123;field:'name',title:'用户名',width:100&#125;, &#123;field:'password',title:'密码',width:100&#125; ]] , striped:'true', pagination:'true', pagePosition:'bottom', pageNumber:1, pageSize:5, pageList: [3,5,10] &#125;); &#125;); function newUser()&#123;$('#winAdd').window('open');&#125; function save()&#123; //提交表单数据 $('#formAdd').form('submit', &#123; url:\"saveUser\" &#125;); alert('保存成功'); //关闭窗口 $(\"#winAdd\").window(\"close\"); //表格重新加载 $(\"#dg\").datagrid(\"reload\"); &#125; function deleteUser() &#123; var row = $('#dg').datagrid('getSelected'); if (row) &#123; $.messager.confirm(\"系统提示\", \"您确定要删除这条记录吗?\", function (r) &#123; if (r) &#123; $.post('userDelete', &#123;id: row.id&#125;, function (result) &#123; if (result.success) &#123; $.messager.alert(\"系统提示\", \"已成功删除这条记录!\"); $(\"#dg\").datagrid(\"reload\"); &#125; else &#123; $.messager.alert(\"系统提示\", result.errorMsg); &#125; &#125;, 'json'); &#125; &#125;); &#125; &#125; function editUser()&#123; var row=$('#dg').datagrid('getSelected'); if(row)&#123; $(\"#dlg\").dialog('open').dialog('setTitle','编辑用户'); $('#formAdd2').form('load',row);//把表单数据带过来进行填充 url='userSave?id='+row.id; &#125; &#125; function saveUser()&#123; $('#formAdd2').form('submit',&#123; url:url, onSubmit:function()&#123; return $(this).form('validate'); &#125;, &#125;); $.messager.alert(\"系统提示\",\"保存成功\"); $('#dlg').dialog('close'); $(\"#dg\").datagrid(\"reload\"); &#125; &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;table id=\"dg\"&gt; &lt;div id=\"toolbar\"&gt; &lt;a href=\"javascript:void(0)\" class=\"easyui-linkbutton\" iconCls=\"icon-add\" plain=\"true\" onclick=\"newUser()\"&gt;添加用户&lt;/a&gt; &lt;a href=\"javascript:void(0)\" class=\"easyui-linkbutton\" iconCls=\"icon-edit\" plain=\"true\" onclick=\"editUser()\"&gt;编辑用户&lt;/a&gt; &lt;a href=\"javascript:void(0)\" class=\"easyui-linkbutton\" iconCls=\"icon-remove\" plain=\"true\" onclick=\"deleteUser()\"&gt;删除用户&lt;/a&gt; &lt;a href=\"javascript:void(0)\" class=\"easyui-linkbutton\" iconCls=\"icon-search\" plain=\"true\" onclick=\"doSearch()\"&gt;Search&lt;/a&gt; &lt;/div&gt;&lt;/table&gt;&lt;!-- 添加客户的表单，默认是隐藏的 --&gt;&lt;div id=\"winAdd\" class=\"easyui-window\" title=\"添加客户\" style=\"width:600px;height:400px\" data-options=\"iconCls:'icon-save',modal:true,closed:true\"&gt; &lt;form id=\"formAdd\" method=\"post\"&gt; &lt;TABLE cellSpacing=0 cellPadding=5 border=0&gt; &lt;TR&gt; &lt;td&gt;用户名：&lt;/td&gt; &lt;td&gt; &lt;INPUT class=textbox style=\"WIDTH: 180px\" maxLength=50 name=\"name\"&gt; &lt;/td&gt; &lt;td&gt;密码 ：&lt;/td&gt; &lt;td&gt; &lt;INPUT class=textbox dstyle=\"WIDTH: 180px\" maxLength=50 name=\"password\"&gt; &lt;/td&gt; &lt;/TR&gt; &lt;tr&gt; &lt;td rowspan=2&gt; &lt;button id=\"customerBtn\" type=\"button\" onclick=\"save()\"&gt;保存&lt;/button&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/TABLE&gt; &lt;/form&gt;&lt;/div&gt;&lt;!-- 编辑客户的表单，默认是隐藏的 --&gt;&lt;div id=\"dlg\" class=\"easyui-window\" title=\"编辑客户\" style=\"width:600px;height:400px\" data-options=\"iconCls:'icon-save',modal:true,closed:true\"&gt; &lt;form id=\"formAdd2\" method=\"post\"&gt; &lt;TABLE cellSpacing=0 cellPadding=5 border=0&gt; &lt;TR&gt; &lt;td&gt;用户名：&lt;/td&gt; &lt;td&gt; &lt;INPUT class=textbox style=\"WIDTH: 180px\" maxLength=50 name=\"name\"&gt; &lt;/td&gt; &lt;td&gt;密码 ：&lt;/td&gt; &lt;td&gt; &lt;INPUT class=textbox dstyle=\"WIDTH: 180px\" maxLength=50 name=\"password\"&gt; &lt;/td&gt; &lt;/TR&gt; &lt;tr&gt; &lt;td rowspan=2&gt; &lt;button id=\"customerBtn2\" type=\"button\" onclick=\"saveUser()\"&gt;保存&lt;/button&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/TABLE&gt; &lt;/form&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 8.在src/main中创建SaveUser.java文件用于保存添加结果；创建SaveUserDao创建数据库接口；创建SaveUserService.java文件，通过jdbc驱动对数据库进行实际操作，以更新用户状态并保存。 9.启动tomcat服务器测试项目显示和新增用户的运行结果。 6.增删改查1.在/web/custom.jsp文件中新增数据的修改和数据删除两个控件，并编写操作窗口，用于修改和提示，增加项目的可视化。 2.在/src/main/controller中新建UserSave.java文件，编写程序用于实现保存前台传递过来的用户名和密码。 3.在/src/main/controller中新建UserDelete.java文件，编写程序用于实现数据的选中的删除操作。 4.在/src/main/dao中新建UserDeleteDao.java文件，编写java数据库的接口程序，为链接数据库做准备。 5.在/src/main/service中新建UserDeleteService.java文件，编写程序用于实现将数据通过实际的查询语句，操作在数据库中，真正的实现数据的删除操作，并返回给用户成功提示。 6.在web.xml当中新增上述映射。1234567891011121314151617&lt;servlet&gt; &lt;servlet-name&gt;UserDelete&lt;/servlet-name&gt; &lt;servlet-class&gt;main.controller.UserDelete&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;UserDelete&lt;/servlet-name&gt; &lt;url-pattern&gt;/userDelete&lt;/url-pattern&gt;&lt;/servlet-mapping&gt;&lt;servlet&gt; &lt;servlet-name&gt;UserSave&lt;/servlet-name&gt; &lt;servlet-class&gt;main.controller.UserSave&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;UserSave&lt;/servlet-name&gt; &lt;url-pattern&gt;/userSave&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 7.在/src/main/util中新建JsonUtil.java文件，编写具体程序用于使result结果转化为json数组格式。1234567891011121314151617181920212223242526272829303132package main.util;import net.sf.json.JSONArray;import net.sf.json.JSONObject;import java.sql.ResultSet;import java.sql.ResultSetMetaData;/** * @author rtl * @site elbadaernu404.github.io * @company suzhou university * @create 2020-06-21 16:40 */public class JsonUtil &#123; /** * 将result的结果集转化成json数组格式 * @param rs * @return * @throws Exception */ public static JSONArray formatRsToJsonArray(ResultSet rs)throws Exception&#123; ResultSetMetaData md=rs.getMetaData(); int num=md.getColumnCount(); JSONArray array=new JSONArray(); while(rs.next())&#123; JSONObject mapOfColValues=new JSONObject(); for(int i=1;i&lt;=num;i++)&#123; mapOfColValues.put(md.getColumnName(i), rs.getObject(i)); &#125; array.add(mapOfColValues); &#125; return array; &#125;&#125; 8.继续在util文件夹中新建UserOperation.java文件，编写程序通过计算user类在数据库中的总数据条目，把检索结果看成只有一条记录一个字段的表，实现数据的分页功能。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package main.util;import main.model.User;import java.sql.Connection;import java.sql.PreparedStatement;import java.sql.ResultSet;import java.sql.SQLException;import java.util.ArrayList;import java.util.List;/** * @author rtl * @site elbadaernu404.github.io * @company suzhou university * @create 2020-06-21 16:40 */public class UserOperation &#123; public List selectPage(int offset, int size) &#123;//注意返回值null和list.size()==0的区别 PreparedStatement ps=null; ResultSet rs=null; ArrayList&lt;User&gt; list=new ArrayList&lt;User&gt;();//返回值 Connection conn = null; conn = JdbcUtil.getConn(); try &#123; ps=conn.prepareStatement(\"select * from user limit \"+offset+\",\"+size); rs= ps.executeQuery(); while(rs.next())&#123; User one=new User();//返回值中的一个 one.setId(rs.getInt(1)); one.setName(rs.getString(2)); one.setPassword(rs.getString(3)); list.add(one);//添加到列表 &#125; return list; &#125; catch (Exception ex) &#123; return null; &#125; &#125;//计算user类在数据库中总共有多少条数据 public int selectCount()&#123; PreparedStatement ps=null; ResultSet rs=null; ArrayList&lt;User&gt; list=new ArrayList&lt;User&gt;();//返回值 Connection conn = null; conn = JdbcUtil.getConn(); int count=0; try &#123; ps=conn.prepareStatement(\"select count(*) from user \"); rs= ps.executeQuery(); while(rs.next()) &#123;//打印的就是总记录数。把检索结果看成只有一跳记录一个字段的表 count=rs.getInt(1); System.out.println(count); &#125; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125;return count; &#125;&#125; 9.启动tomcat服务器，测试功能的完整。 7.系统修缮1.对CRM后台管理系进行初步设计，按照设计需要在本地数据库中创建更多的表单，填写属性，准备录入CRM管理系统。 2.在本地数据库系统中完成创建部门和员工表，并且完成创建和实施相关的控件、数据库接口、模块、具体功能、查询语句等主要程序的编程。 4.在/web目录中新建部门和员工的jsp文件，补全操作的页面、窗口以及主要函数功能的代码，完成其他数据库表单的jdbc调度功能。部门：123456789101112131415161718192021222324252627282930313233343536373839404142&lt;%@ page contentType=\"text/html;charset=UTF-8\" language=\"java\" %&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;部门信息&lt;/title&gt; &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"easyui/themes/bootstrap/easyui.css\"&gt; &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"easyui/themes/icon.css\"&gt; &lt;script type=\"text/javascript\" src=\"easyui/jquery.min.js\"&gt;&lt;/script&gt; &lt;script type=\"text/javascript\" src=\"easyui/jquery.easyui.min.js\"&gt;&lt;/script&gt; &lt;script type=\"text/javascript\" src=\"easyui/locale/easyui-lang-zh_CN.js\"&gt;&lt;/script&gt; &lt;script type=\"text/javascript\"&gt; var url; $(function()&#123; $('#dg').datagrid(&#123; url:'getAllDepartment', columns:[[ &#123;field:'number',title:'Number',width:100&#125;, &#123;field:'departmentname',title:'部门',width:100&#125;, &#123;field:'departmentfunction',title:'权限',width:100&#125; ]] , striped:'true', pagination:'true', pagePosition:'bottom', pageNumber:1, pageSize:5, pageList: [3,5,10] &#125;); &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;table id=\"dg\"&gt; &lt;div id=\"toolbar\"&gt; &lt;a href=\"javascript:void(0)\" &gt;&lt;/a&gt; &lt;a href=\"javascript:void(0)\" &gt;&lt;/a&gt; &lt;a href=\"javascript:void(0)\" &gt;&lt;/a&gt; &lt;/div&gt;&lt;/table&gt;&lt;/body&gt;&lt;/html&gt; 员工：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177&lt;%@ page contentType=\"text/html;charset=UTF-8\" language=\"java\" %&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;员工管理&lt;/title&gt; &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"easyui/themes/bootstrap/easyui.css\"&gt; &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"easyui/themes/icon.css\"&gt; &lt;script type=\"text/javascript\" src=\"easyui/jquery.min.js\"&gt;&lt;/script&gt; &lt;script type=\"text/javascript\" src=\"easyui/jquery.easyui.min.js\"&gt;&lt;/script&gt; &lt;script type=\"text/javascript\" src=\"easyui/locale/easyui-lang-zh_CN.js\"&gt;&lt;/script&gt; &lt;script type=\"text/javascript\"&gt; var url; $(function()&#123; $('#dg1').datagrid(&#123; url:'getAllEmployee', columns:[[ &#123;field:'id',title:'工号',width:100&#125;, &#123;field:'employeename',title:'姓名',width:100&#125;, &#123;field:'employeedept',title:'部门',width:100&#125;, &#123;field:'employeeemail',title:'联系邮箱',width:100&#125; ]] , striped:'true', pagination:'true', pagePosition:'bottom', pageNumber:1, pageSize:5, pageList: [3,5,10] &#125;); &#125;); function newEmployee()&#123;$('#winAdda').window('open');&#125; function save()&#123; //提交表单数据 $('#formAdda').form('submit', &#123; url:\"saveEmployee\" &#125;); alert('保存成功'); //关闭窗口 $(\"#winAdda\").window(\"close\"); //表格重新加载 $(\"#dg1\").datagrid(\"reload\"); &#125; function search()&#123; //提交表单数据 $('#formAddaa').form('submit', &#123; url:\"EmployeeSearch\" &#125;); alert('查询成功'); //关闭窗口 $(\"#winAddaa\").window(\"close\"); &#125; function deleteEmployee() &#123; var row = $('#dg1').datagrid('getSelected'); if (row) &#123; $.messager.confirm(\"系统提示\", \"确定开除此员工?\", function (r) &#123; if (r) &#123; $.post('employeeDelete', &#123;id: row.id&#125;, function (result) &#123; if (result.success) &#123; $.messager.alert(\"系统提示\", \"成功!\"); $(\"#dg1\").datagrid(\"reload\"); &#125; else &#123; $.messager.alert(\"系统提示\", result.errorMsg); &#125; &#125;, 'json'); &#125; &#125;); &#125; &#125; function editEmployee()&#123; var row=$('#dg1').datagrid('getSelected'); if(row)&#123; $(\"#dlg1\").dialog('open').dialog('setTitle','编辑员工'); $('#formAdda2').form('load',row); url='employeeSave?id='+row.id; &#125; &#125; function searchEmployee()&#123;$('#winAddaa').window('open');&#125; function saveEmployee()&#123; $('#formAdda2').form('submit',&#123; url:url, onSubmit:function()&#123; return $(this).form('validate'); &#125;, &#125;); $.messager.alert(\"系统提示\",\"保存成功\"); $('#dlg1').dialog('close'); $(\"#dg1\").datagrid(\"reload\"); &#125; &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;table id=\"dg1\"&gt; &lt;div id=\"toolbar1\"&gt; &lt;a href=\"javascript:void(0)\" class=\"easyui-linkbutton\" plain=\"true\" onclick=\"newEmployee()\"&gt;添加员工&lt;/a&gt; &lt;a href=\"javascript:void(0)\" class=\"easyui-linkbutton\" plain=\"true\" onclick=\"editEmployee()\"&gt;管理员工&lt;/a&gt; &lt;a href=\"javascript:void(0)\" class=\"easyui-linkbutton\" plain=\"true\" onclick=\"deleteEmployee()\"&gt;开除员工&lt;/a&gt; &lt;a href=\"javascript:void(0)\" class=\"easyui-linkbutton\" iconCls=\"icon-search\" plain=\"true\" onclick=\"searchEmployee()\"&gt;Search&lt;/a&gt; &lt;/div&gt;&lt;/table&gt;&lt;!-- 添加客户的表单，默认是隐藏的 --&gt;&lt;div id=\"winAdda\" class=\"easyui-window\" title=\"添加员工\" style=\"width:600px;height:400px\" data-options=\"iconCls:'icon-save',modal:true,closed:true\"&gt; &lt;form id=\"formAdda\" method=\"post\"&gt; &lt;TABLE cellSpacing=0 cellPadding=5 border=0&gt; &lt;br&gt; &lt;td&gt;姓名：&lt;/td&gt; &lt;td&gt; &lt;INPUT class=textbox style=\"WIDTH: 120px\" maxLength=30 name=\"employeename\"&gt; &lt;/td&gt; &lt;td&gt;部门：&lt;/td&gt; &lt;td&gt; &lt;INPUT class=textbox style=\"WIDTH: 120px\" maxLength=30 name=\"employeedept\"&gt; &lt;/td&gt; &lt;td&gt;邮箱：&lt;/td&gt; &lt;td&gt; &lt;INPUT class=textbox style=\"WIDTH: 120px\" maxLength=30 name=\"employeeemail\"&gt; &lt;/td&gt; &lt;/TR&gt; &lt;tr&gt; &lt;td rowspan=2&gt; &lt;button id=\"employeeBtn\" type=\"button\" onclick=\"save()\"&gt;保存&lt;/button&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/TABLE&gt; &lt;/form&gt;&lt;/div&gt;&lt;!-- 编辑客户的表单，默认是隐藏的 --&gt;&lt;div id=\"dlg1\" class=\"easyui-window\" title=\"编辑员工\" style=\"width:600px;height:400px\" data-options=\"iconCls:'icon-save',modal:true,closed:true\"&gt; &lt;form id=\"formAdda2\" method=\"post\"&gt; &lt;TABLE cellSpacing=0 cellPadding=5 border=0&gt; &lt;TR&gt; &lt;td&gt;姓名：&lt;/td&gt; &lt;td&gt; &lt;INPUT class=textbox style=\"WIDTH: 120px\" maxLength=30 name=\"employeename\"&gt; &lt;/td&gt; &lt;td&gt;部门：&lt;/td&gt; &lt;td&gt; &lt;INPUT class=textbox style=\"WIDTH: 120px\" maxLength=30 name=\"employeedept\"&gt; &lt;/td&gt; &lt;td&gt;邮箱：&lt;/td&gt; &lt;td&gt; &lt;INPUT class=textbox style=\"WIDTH: 120px\" maxLength=30 name=\"employeeemail\"&gt; &lt;/td&gt; &lt;/TR&gt; &lt;tr&gt; &lt;td rowspan=2&gt; &lt;button id=\"employeeBtn2\" type=\"button\" onclick=\"saveEmployee()\"&gt;保存&lt;/button&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/TABLE&gt; &lt;/form&gt;&lt;/div&gt;&lt;div id=\"winAddaa\" class=\"easyui-window\" title=\"搜索员工\" style=\"width:600px;height:400px\" data-options=\"iconCls:'icon-save',modal:true,closed:true\"&gt; &lt;form id=\"formAddaa\" method=\"post\"&gt; &lt;TABLE cellSpacing=0 cellPadding=5 border=0&gt; &lt;br&gt; &lt;td&gt;姓名：&lt;/td&gt; &lt;td&gt; &lt;INPUT class=textbox style=\"WIDTH: 120px\" maxLength=30 name=\"employeename\"&gt; &lt;/td&gt; &lt;/TR&gt; &lt;tr&gt; &lt;td rowspan=2&gt; &lt;button id=\"employeeBtn3\" type=\"button\" onclick=\"search()\"&gt;搜索&lt;/button&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/TABLE&gt; &lt;/form&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 5.数据字典：后台web应用系统中，在一些含有字典值数据情况下，这些数据要显式地展示给用户，就要通过转换字典值数据，然后返回到页面。其实现方法可以是每次通过遍历字典数据然后在展示数据中增加转换后的值，然后将数据返回到页面，也可以通过自定义标签，页面自动转换字典值显示问题，相比第一种解决方案，第二种方式可以通过一个标签处理所有的数据字典，不必像第一种方案那样要每次在后台手动处理。 6.在本地数据库中创建一个数据字典，编程实现CRM管理系统的数据字典目录及字典明细，启动tomcat服务器对目前完成的项目进行测试。 8.客户跟踪的程序以及系统的深度优化1.在本地数据库中新建客户表，并且完成创建和实施相关的控件、数据库接口、模块、具体功能、查询语句等主要程序的编程。成功将客户表添加到系统。 2.为客户表添加增删改查功能，依次补全客户系统的资源池、潜在客户、失败客户、正式客户、流失客户等一系列客户管理系统的核心代码，完成客户跟进功能。 3.在官网下载easyui主题，并通过index.jsp中链接框架主体的代码修改easyui的默认主题，更新ui界面，为easyui设置系统时间模块。 4.收集图片素材，为index主页面添加图片背景，优化区块，同时调整框架各部分的自适应大小。 5.为主界面添加搜索框，同步到百度，并实现通过超链接的跳转。将百度logo的素材图片导入搜索框左侧，完成增强搜索功能的可视化。1234567891011&lt;table&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=\"http://www.baidu.com\"&gt; &lt;img src=\"img/baidu.png\"/&gt; &lt;/a&gt; &lt;/td&gt; &lt;td&gt;&lt;br/&gt;&lt;input type=\"text\" id=\"search1\" name=\"word\"/&gt;&lt;/td&gt; &lt;td&gt;&lt;br/&gt;&lt;input type=\"submit\" value=\"搜索\" /&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt; 6.启动tomcat服务器，测试功能基本齐全、ui完善的后台系统，项目初步完成。 9.关于Maven和ssm框架至此，CRM管理系统基本完成。之后的时间里我对其他框架进行了简单的了解与学习：1.Maven项目对象模型(POM)，可以通过一小段描述信息来管理项目的构建，报告和文档的项目管理工具软件。Maven 除了以程序构建能力为特色之外，还提供高级项目管理工具。由于 Maven 的面向项目的方法，许多 Apache Jakarta 项目发文时使用 Maven，而且公司项目采用 Maven 的比例在持续增长。 2.ssm框架：Spring + Spring MVC + MyBatis。Spring是分层的Java SE/EE应用一站式的轻量级开源框架，以IoC反转控制和AOP面向切面编程为内核，提供了展现层：Spring MVC和持久层：Spring JDBC以及业务层：事务管理等众多的企业级应用技术。Spring MVC 分离了控制器、模型对象、分派器以及处理程序对象的角色，这种分离让它们更容易进行定制。MyBatis 使用简单的 XML或注解用于配置和原始映射，将接口和 Java 的POJOs映射成数据库中的记录。 3.maven与ssm框架的整合，主要包含的依赖有依赖SpringMVC、依赖c3p0、依赖Mybatis，进行配置spring声明式事务管理，至此maven已经成功整合了spring+springmvc+mybatis。通过新建程序和完成映射，完成论文增删改查系统。","categories":[{"name":"个人笔记","slug":"个人笔记","permalink":"http://yoursite.com/categories/个人笔记/"}],"tags":[{"name":"java","slug":"java","permalink":"http://yoursite.com/tags/java/"}]},{"title":"Python爬虫：豆瓣电影数据","slug":"Python爬虫：豆瓣电影数据","date":"2019-05-01T16:39:06.000Z","updated":"2022-06-27T07:38:05.697Z","comments":true,"path":"2019/05/02/Python爬虫：豆瓣电影数据/","link":"","permalink":"http://yoursite.com/2019/05/02/Python爬虫：豆瓣电影数据/","excerpt":"描述· 目标网站：豆瓣电影网https://movie.douban.com/ · 目标数据1：热门高分电影的名称、类型、评分、总评人数等 · 目标数据2：热门电影《少年的你》影评 · 数据存储：csv、txt · 爬虫方法：requests、json、xpath、正则表达式 找到网站json，利用xpath、正则表达式等对豆瓣电影数据进行分页爬取，以及对热门电影《少年的你》全部影评进行爬取","text":"描述· 目标网站：豆瓣电影网https://movie.douban.com/ · 目标数据1：热门高分电影的名称、类型、评分、总评人数等 · 目标数据2：热门电影《少年的你》影评 · 数据存储：csv、txt · 爬虫方法：requests、json、xpath、正则表达式 找到网站json，利用xpath、正则表达式等对豆瓣电影数据进行分页爬取，以及对热门电影《少年的你》全部影评进行爬取 源码main.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101import timeimport xlwtfrom lxml import etreeimport requestsimport jsonimport reheaders = &#123; \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36\"&#125;def processing_data(content_list): workbook = xlwt.Workbook(encoding='utf-8') worksheet = workbook.add_sheet('My Worksheet') for i, content in enumerate(content_list): for x, info in enumerate(content): worksheet.write(i, x, label=info) # 将数据存入excel workbook.save('info.csv')def save_info(content): info = content.xpath(\"//div[@id='info']\")[0] try: name = str(content.xpath('//*[@id=\"content\"]/h1/span[1]/text()')[0]).replace(\"'\", \" \") except: name = \"无\" try: daoyan = str(info.xpath(\"./span[1]/span[2]/a/text()\")[0] if info.xpath(\"./span[1]/span[2]/a/text()\") else None ).replace(\"'\", \" \") except: daoyan = \"无\" try: bianju = str(info.xpath(\"./span[2]/span[2]/a/text()\")[0] if info.xpath(\"./span[2]/span[2]/a/text()\") else None).replace(\"'\", \" \") except: bianju = \"无\" try: zhuyan = '/'.join(info.xpath(\"./span[3]/span[2]/a/text()\")).replace(\"'\", \" \") except: zhuyan = \"无\" try: leixing = '/'.join(info.xpath(\"./span[@property='v:genre']/text()\")).replace(\"'\", \" \") except: leixing = \"无\" try: shangyingshijian= str( '/'.join(info.xpath(\".//span[@property='v:initialReleaseDate']/text()\")).replace(\"'\", \" \"))[0:10] except: shangyingshijian = \"无\" try: shichang = str(info.xpath(\".//span[@property='v:runtime']/text()\")[0]).replace(\"'\", \" \") res1 = str(re.match(r'(.*)', shichang))[37:-1] except: shichang = \"无\" try: pingfen = str(content.xpath('//*[@id=\"interest_sectl\"]/div[1]/div[2]/strong/text()')[0]).replace(\"'\", \" \") except: pingfen = \"无\" try: pingjiarenshu = content.xpath('//*[@id=\"interest_sectl\"]/div[1]/div[2]/div/div[2]/a/span/text()')[0] except: pingjiarenshu = \"无\" print(\"电影名称：\", name) print(\"类型：\", leixing) print(\"评分：\", pingfen) print(\"评价人数：\", pingjiarenshu) print(\"导演：\", daoyan) print(\"编剧：\", bianju) print(\"主演：\", zhuyan) print(\"上映时间：\", shangyingshijian) print(\"时长：\", res1) one_info = [name, leixing, pingfen, pingjiarenshu, daoyan, bianju, zhuyan, shangyingshijian, shichang] all_list.append(one_info) processing_data(all_list)def main(): try: for x in range(0,9999): url = 'https://movie.douban.com/j/new_search_subjects?sort=T&amp;range=0,10&amp;tags=&amp;start='+ str(x) content = requests.get(url, headers=headers) content_json = json.loads(content.text)[\"data\"] for one_info in content_json: one_id = one_info[\"id\"] print(one_id) url2 = \"https://movie.douban.com/subject/%s/\"%one_id # content_html = requests.get(url, headers=headers) html = requests.get(url2, headers=headers) if html.status_code == 200: content = html.content.decode(\"utf-8\") content = etree.HTML(content) save_info(content) time.sleep(1) except: processing_data(all_list)if __name__ == '__main__': all_list = [] main() yping.py1234567891011121314151617181920212223import requestsfrom lxml import etreeheaders=&#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36'&#125;for x in range(0, 480, 20): try: startUrl = 'https://movie.douban.com/subject/30166972/comments?start='+str(x)+'&amp;limit=20&amp;sort=new_score&amp;status=P' r = requests.get(startUrl, headers=headers).content.decode('utf-8') html = etree.HTML(r) for i in range(1, 21): try: comment = str(html.xpath('//*[@id=\"comments\"]/div['+str(i)+']/div[2]/p/span/text()')[0]).replace(\"'\", \" \") print(comment) with open('info.txt', 'a', encoding='utf8') as file: file.write(comment + '\\n') except: continue except: continue 输出","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"xpath","slug":"xpath","permalink":"http://yoursite.com/tags/xpath/"}]},{"title":"Python爬虫：人民日报新闻","slug":"Python爬虫：人民日报新闻","date":"2019-05-01T16:20:51.000Z","updated":"2022-06-27T07:37:27.489Z","comments":true,"path":"2019/05/02/Python爬虫：人民日报新闻/","link":"","permalink":"http://yoursite.com/2019/05/02/Python爬虫：人民日报新闻/","excerpt":"描述· 目标网站：人民日报官网http://paper.people.com.cn/ · 目标数据：新闻的标题、正文等 · 数据存储：txt · 爬虫方法：requests、beautifulsoup4 主要使用了requests和bs4解析库，将指定输入的日期转化为字符串，与对应新闻链接进行连接，对区间内新闻进行爬取","text":"描述· 目标网站：人民日报官网http://paper.people.com.cn/ · 目标数据：新闻的标题、正文等 · 数据存储：txt · 爬虫方法：requests、beautifulsoup4 主要使用了requests和bs4解析库，将指定输入的日期转化为字符串，与对应新闻链接进行连接，对区间内新闻进行爬取 源码注：2021年1月1日起人民日报官网改动，代码已更新(2021)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137import requestsimport bs4import osimport datetimeimport timedef fetchUrl(url): headers = &#123; 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8', 'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36', &#125; r = requests.get(url, headers=headers) r.raise_for_status() r.encoding = r.apparent_encoding return r.textdef getPageList(year, month, day): url = 'http://paper.people.com.cn/rmrb/html/' + year + '-' + month + '/' + day + '/nbs.D110000renmrb_01.htm' html = fetchUrl(url) bsobj = bs4.BeautifulSoup(html, 'html.parser') temp = bsobj.find('div', attrs = &#123;'id': 'pageList'&#125;) if temp: pageList = temp.ul.find_all('div', attrs = &#123;'class': 'right_title-name'&#125;) else: pageList = bsobj.find('div', attrs = &#123;'class': 'swiper-container'&#125;).find_all('div', attrs = &#123;'class': 'swiper-slide'&#125;) linkList = [] for page in pageList: link = page.a[\"href\"] url = 'http://paper.people.com.cn/rmrb/html/' + year + '-' + month + '/' + day + '/' + link linkList.append(url) return linkListdef getTitleList(year, month, day, pageUrl): html = fetchUrl(pageUrl) bsobj = bs4.BeautifulSoup(html, 'html.parser') temp = bsobj.find('div', attrs = &#123;'id': 'titleList'&#125;) if temp: titleList = temp.ul.find_all('li') else: titleList = bsobj.find('ul', attrs = &#123;'class': 'news-list'&#125;).find_all('li') linkList = [] for title in titleList: tempList = title.find_all('a') for temp in tempList: link = temp[\"href\"] if 'nw.D110000renmrb' in link: url = 'http://paper.people.com.cn/rmrb/html/' + year + '-' + month + '/' + day + '/' + link linkList.append(url) return linkListdef getContent(html): bsobj = bs4.BeautifulSoup(html, 'html.parser') title = bsobj.h3.text + '\\n' + bsobj.h1.text + '\\n' + bsobj.h2.text + '\\n' pList = bsobj.find('div', attrs=&#123;'id': 'ozoom'&#125;).find_all('p') content = '' for p in pList: content += p.text + '\\n' resp = title + content return respdef saveFile(content, path, filename): if not os.path.exists(path): os.makedirs(path) with open(path + filename, 'w', encoding='utf-8') as f: f.write(content)def download_rmrb(year, month, day, destdir): pageList = getPageList(year, month, day) for page in pageList: titleList = getTitleList(year, month, day, page) for url in titleList: html = fetchUrl(url) content = getContent(html) temp = url.split('_')[2].split('.')[0].split('-') pageNo = temp[1] titleNo = temp[0] if int(temp[0]) &gt;= 10 else '0' + temp[0] path = destdir + '/' + year + month + day + '/' fileName = year + month + day + '-' + pageNo + '-' + titleNo + '.txt' saveFile(content, path, fileName)def gen_dates(b_date, days): day = datetime.timedelta(days=1) for i in range(days): yield b_date + day * idef get_date_list(beginDate, endDate): start = datetime.datetime.strptime(beginDate, \"%Y%m%d\") end = datetime.datetime.strptime(endDate, \"%Y%m%d\") data = [] for d in gen_dates(start, (end - start).days): data.append(d) return dataif __name__ == '__main__': ''' main（） ''' beginDate = input('新闻开始日期（格式如20210101）:') endDate = input('新闻结束日期（格式如20210102）:') data = get_date_list(beginDate, endDate) for d in data: year = str(d.year) month = str(d.month) if d.month &gt;= 10 else '0' + str(d.month) day = str(d.day) if d.day &gt;= 10 else '0' + str(d.day) download_rmrb(year, month, day, 'data') print(\"完成：\" + year + month + day)'''爬的不多不删注释，小心封ip'''# time.Sleep(5) 输出","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"beautifulsoup","slug":"beautifulsoup","permalink":"http://yoursite.com/tags/beautifulsoup/"}]},{"title":"Python爬虫：获取yuehui163用户资料","slug":"Python爬虫：获取yuehui163用户资料","date":"2019-05-01T14:58:05.000Z","updated":"2022-06-27T07:37:51.457Z","comments":true,"path":"2019/05/01/Python爬虫：获取yuehui163用户资料/","link":"","permalink":"http://yoursite.com/2019/05/01/Python爬虫：获取yuehui163用户资料/","excerpt":"描述· 目标网站：网易https://yuehui.163.com/同城约会网站 · 目标数据：用户的头像、性别、城市、地区、工作、学历等 · 数据存储：png、txt、csv · 爬虫方法：requests、json 通过json格式，对yuehui163网站用户的多页爬取，使用循环和字符串拼接等方式获取详细分页数据","text":"描述· 目标网站：网易https://yuehui.163.com/同城约会网站 · 目标数据：用户的头像、性别、城市、地区、工作、学历等 · 数据存储：png、txt、csv · 爬虫方法：requests、json 通过json格式，对yuehui163网站用户的多页爬取，使用循环和字符串拼接等方式获取详细分页数据 抓包获取网站的json数据(源json很长，以下为一条用户的信息)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192&#123; \"hasAuth\": false, \"vippage\": 1, \"mobileRootUrl\": \"/mobile\", \"viptotal\": 0, \"mainDomain\": \"yuehui.163.com\", \"list\": [&#123; \"age\": 19, \"aim\": 4, \"aimName\": \"知己\", \"albumCount\": 0, \"alg\": \"\", \"auditingTodayMood\": \"\", \"auditingTodayMoodTag\": 0, \"avoirdupois\": 49, \"carNo\": \"\", \"certLevel\": 0, \"city\": 0, \"cityName\": \"北京\", \"constellation\": 3, \"constellationName\": \"白羊座\", \"contact_popo\": \"\", \"contact_qq\": \"\", \"dateCount\": 0, \"dateTheme\": 0, \"degree\": 3, \"degreeName\": \"本科\", \"district\": 0, \"districtName\": \"\", \"eggName\": \"\", \"email\": \"\", \"focusUser\": false, \"fullPhotoUri\": \"https://yuehui2.nosdn.127.net/29/37/10/d6f9fa88660ab618fbd63cc307841eec/704131037/1648550666583\", \"hasCert\": false, \"hasEmail\": false, \"hasMobile\": false, \"hasMobileOrEmail\": false, \"hasPortrait\": 2, \"hasRcmIndex\": -1, \"hasRcmNIndex\": -1, \"house\": 2, \"id\": 704131037, \"income\": 8, \"incomeName\": \"保密\", \"industry\": 40, \"industryName\": \"学生\", \"intro\": \"\", \"isCloseArgue\": 0, \"isDiamondVIP\": false, \"isInsufficentCity\": -1, \"isVip\": 0, \"isVipName\": \"普通\", \"lastLoginTime\": 1651385715089, \"level\": 0, \"marriage\": 0, \"marriageName\": \"未婚\", \"maskPic\": 0, \"memberNo\": \"5665449560\", \"mobileNo\": \"\", \"monthVIP\": false, \"needCrop\": false, \"newVipNames\": \"普通\", \"nick\": \"小希\", \"nickShort\": \"小希\", \"nickShort3\": \"小希\", \"nickShort4\": \"小希\", \"nickShort5\": \"小希\", \"nickShort7\": \"小希\", \"offStr\": \"\", \"online\": 0, \"onlineState\": \"\", \"pageStyle\": 0, \"photoUri\": \"https://yuehui2.nosdn.127.net/29/37/10/d6f9fa88660ab618fbd63cc307841eec/704131037/1648550666583_9_10\", \"photoUri180\": \"https://yuehui2.nosdn.127.net/29/37/10/d6f9fa88660ab618fbd63cc307841eec/704131037/1648550666583?imageView&amp;crop=0_45_403_447&amp;thumbnail=180x200\", \"photoUri195\": \"https://yuehui2.nosdn.127.net/29/37/10/d6f9fa88660ab618fbd63cc307841eec/704131037/1648550666583_3_4_m\", \"photoUri250\": \"https://yuehui2.nosdn.127.net/29/37/10/d6f9fa88660ab618fbd63cc307841eec/704131037/1648550666583?imageView&amp;crop=0_45_403_447&amp;thumbnail=250x278\", \"photoUri90\": \"https://yuehui2.nosdn.127.net/29/37/10/d6f9fa88660ab618fbd63cc307841eec/704131037/1648550666583_9_10?imageView&amp;thumbnail=90x100\", \"pos\": 0, \"province\": 0, \"provinceName\": \"\", \"rank\": 0, \"sex\": 0, \"showScore\": 0, \"stature\": 164, \"todayMoodTag\": 4, \"todaymood\": \"\", \"userType\": 0, \"visitLevel\": 0, \"visitLevelName\": \"\", \"visitLevelName3\": false, \"yhstatid\": \"newsearchuser,user\" &#125; 源码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117import requests#from lxml import etreeimport jsonimport mathheaders=&#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36'&#125;for page in range(0,34): for page2 in range(21): for page3 in range(2): startUrl = 'http://yuehui.163.com/searchusersrcm.do?ajax=1&amp;ageBegin=18&amp;ageEnd=25&amp;aim=-1&amp;marriage=0&amp;mode=4&amp;order=8&amp;province='+str(page)+'&amp;city='+str(page2)+'&amp;district=-1&amp;sex='+str(page3)+'&amp;userTag=0&amp;vippage=-1&amp;searchType=0&amp;page=1&amp;pagesize=81' r = requests.get(startUrl,headers=headers).content.decode('utf-8') r_j=json.loads(r) #print(r_j) aaa=r_j[0] #print(aaa) info=aaa['list'] #print(info) if info==[]: break else: num=1 strttt=\"未知\" for i in info: full_url = i['photoUri'] r2 = requests.get(full_url,headers=headers,stream=True) ID=str(i['id']) name = str(i['nick']) cityname=str(i['cityName']) cityarea=str(i['districtName']) Age=str(i['age']) studylevel=str(i['degreeName']) sense=str(i['marriageName']) job=str(i['industryName']) star=str(i['constellationName']) weight=str(i['avoirdupois']) high=str(i['stature']) salary=str(i['incomeName']) if page3==0: sex='女' else: sex='男' if (str(i['cityName'])==''): cityname=strttt else: cityname = str(i['cityName']) if (str(i['districtName'])==''): cityarea=strttt else: cityarea = str(i['districtName']) if (str(i['age'])==''): Age='未知' else: Age = str(i['age']) if (str(i['degreeName'])==''): studylevel='未知' else: studylevel = str(i['degreeName']) if (str(i['marriageName'])==''): sense='未知' else: sense = str(i['marriageName']) if (str(i['industryName'])==''): job='未知' else: job = str(i['industryName']) if (str(i['constellationName'])==''): star='未知' else: star = str(i['constellationName']) if (str(i['avoirdupois'])==''): weight='未知' else: weight = str(i['avoirdupois']) if (str(i['stature'])==''): high='未知' else: high = str(i['stature']) if (str(i['incomeName'])==''): salary='未知' else: salary = str(i['incomeName']) if (int(i['avoirdupois'])==0|int(i['stature'])==0): bmi='未知' else: bmi = str((float(i['avoirdupois']) / float(i['stature'] ** 2)) * 10000) with open('D:/PycharmProjects/1/yuehuipro/final/'+ ID+'.png','wb') as file: for j in r2.iter_content(1024): file.write(j) with open('D:/PycharmProjects/1/yuehuipro/final/' + ID + '.txt', 'a',encoding='utf8') as file: file.write('id：'+ID+ '\\n' + '=' * 100 + '\\n' + name + '\\n' + '=' * 100 + '\\n' + '年龄：'+Age + '\\n' + '=' * 100 + '\\n '+ '性别：' + sex + '\\n' + '=' * 100 + '\\n '+ '所在城市：'+ cityname +'\\n' + '=' * 100 + '\\n '+ '所在地区：'+ cityarea + '\\n' + '=' * 100 + '\\n '+ '工作：' + job + '\\n' + '=' * 100 + '\\n' + '学历：' + studylevel + '\\n' + '=' * 100 + '\\n' + '感情状况：' + sense + '\\n' + '=' * 100 + '\\n' + '身高：' + high + '\\n' + '=' * 100 + '\\n' + '体重：' + weight + '\\n' + '=' * 100 + '\\n' + '薪资：' + salary + '\\n' + '=' * 100 + '\\n' + '星座：' + star + '\\n' + '=' * 100 + '\\n' + 'BMI：' + bmi + '\\n') with open('info.csv', 'a', encoding='utf8') as file: file.write(' '+ID+' '+name + ' ' + Age + ' ' + cityname + ' ' + cityarea +' ' +sex+' ' + job + ' '+studylevel+' '+sense+' '+star+' '+'身高：'+ high+ ' '+'体重：'+ weight +' '+'薪资：'+ salary + ' '+'BMI：'+' '+bmi+'\\n') if page3==1: print('正在加载男性') else: print('正在加载女性') print('已加载第%d条数据'%num) num+=1 结果以并发方式爬取了共计约10000条数据后手动中止了，效率还是比较慢的，并且没有增加异常判断(比较顺利，没有发生中断的情况)","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"xpath","slug":"xpath","permalink":"http://yoursite.com/tags/xpath/"}]},{"title":"Python多线程基础","slug":"Python多线程基础","date":"2019-04-16T13:00:56.000Z","updated":"2022-06-27T07:36:27.778Z","comments":true,"path":"2019/04/16/Python多线程基础/","link":"","permalink":"http://yoursite.com/2019/04/16/Python多线程基础/","excerpt":"同步和异步同步其实类似于小学时期如何在最短的时间内去完成所有事件的数学题。 假设小明早上7：00起床，刷牙洗脸需要3分钟，烧水需要10分钟，穿衣叠被需要2分钟，吃早餐需要15分钟，收听早间新闻需要20分钟（7点开始），问小明起床做完这些事情至少需要多长时间？ 同步是指调用一旦开始，必须等待返回调用的结果，才能继续下一步操作 那么结果就是，小明一共需要听新闻（20分钟）+穿衣叠被（2分钟）+刷牙洗脸（3分钟）+烧水（10分钟）+吃早餐（15分钟），总共50分钟，一定要迟到了，很笨 异步异步和同步相反，调用一旦开始，调用者无需等待方法的完成，就可以继续执行后续方法，被调用者通过状态体通知调用者，或者使用回调函数处理调用 异步方法下，小明需要打开新闻，开始烧水，穿衣叠被，再洗漱，吃早餐，只需要20分钟，新闻结束，就可以高高兴兴去上学了 并发和并行并发并发是同时具有多个活动的系统，实质是一个或多个物理CPU在若干道程序（或线程）之间多路复用，其实就是n个事件在同一时间线上交替完成：小明做完所有事件花了50分钟 并行并行是真正意义上的不同事件或线程在同一时刻同时执行。并行利用并发使系统运行更快，在操作系统的多个抽象层次运用，是n个事件在同一时刻上同时发生：小明家里雇了4个机器人保姆，1个打开电视后烧水，1个帮小明穿衣叠被，1个帮小明洗漱，最后1个喂小明吃早餐，这样在早间新闻放完前，小明只用了18分钟就完成了所有事件，还多出2分钟时间检查书包 进程和线程进程程序不能自己运行，只有将程序装载进内存，由系统分配资源后才能运行。程序是指令的集合，可以视为指令的静态文本，而进程是指令的执行活动，是动态的行为 进程是操作系统对于一个运行中程序的抽象，是CPU、内存、IO设备的抽象。系统可以同时运行多个进程 线程线程是操作系统进行运算调度的最小单位，被包含于进程中，是进程的实际运作单位。一个进程可以由多个线程组成，线程之间共享代码和数据。 由于在实际网络服务器中对并行的需求，线程成为重要的编程模型： 1.多线程之间比多进程之间更易共享数据。2.线程一般比进程高效。","text":"同步和异步同步其实类似于小学时期如何在最短的时间内去完成所有事件的数学题。 假设小明早上7：00起床，刷牙洗脸需要3分钟，烧水需要10分钟，穿衣叠被需要2分钟，吃早餐需要15分钟，收听早间新闻需要20分钟（7点开始），问小明起床做完这些事情至少需要多长时间？ 同步是指调用一旦开始，必须等待返回调用的结果，才能继续下一步操作 那么结果就是，小明一共需要听新闻（20分钟）+穿衣叠被（2分钟）+刷牙洗脸（3分钟）+烧水（10分钟）+吃早餐（15分钟），总共50分钟，一定要迟到了，很笨 异步异步和同步相反，调用一旦开始，调用者无需等待方法的完成，就可以继续执行后续方法，被调用者通过状态体通知调用者，或者使用回调函数处理调用 异步方法下，小明需要打开新闻，开始烧水，穿衣叠被，再洗漱，吃早餐，只需要20分钟，新闻结束，就可以高高兴兴去上学了 并发和并行并发并发是同时具有多个活动的系统，实质是一个或多个物理CPU在若干道程序（或线程）之间多路复用，其实就是n个事件在同一时间线上交替完成：小明做完所有事件花了50分钟 并行并行是真正意义上的不同事件或线程在同一时刻同时执行。并行利用并发使系统运行更快，在操作系统的多个抽象层次运用，是n个事件在同一时刻上同时发生：小明家里雇了4个机器人保姆，1个打开电视后烧水，1个帮小明穿衣叠被，1个帮小明洗漱，最后1个喂小明吃早餐，这样在早间新闻放完前，小明只用了18分钟就完成了所有事件，还多出2分钟时间检查书包 进程和线程进程程序不能自己运行，只有将程序装载进内存，由系统分配资源后才能运行。程序是指令的集合，可以视为指令的静态文本，而进程是指令的执行活动，是动态的行为 进程是操作系统对于一个运行中程序的抽象，是CPU、内存、IO设备的抽象。系统可以同时运行多个进程 线程线程是操作系统进行运算调度的最小单位，被包含于进程中，是进程的实际运作单位。一个进程可以由多个线程组成，线程之间共享代码和数据。 由于在实际网络服务器中对并行的需求，线程成为重要的编程模型： 1.多线程之间比多进程之间更易共享数据。2.线程一般比进程高效。 python进程创建1234import multiprocessing进程对象 = multiprocessing.Process(target=函数名或当前任务,*args/**kwargs)进程对象.start() 进程编号1234import osos.getpid() #获取当前进程的编号os.getppid() #获取当前进程父进程的编号 主进程往往会等待所有的子进程完成以后才结束 可以设置守护主进程，当主进程结束以后子进程中止1234进程对象 = multiprocessing.Process(target=函数名或当前任务, daemon=True)#或进程对象.daemon = True 举例-文件拷贝程序12345678910111213141516171819202122232425import osimport multiprocessingdef copy_file(file_name, source_dir, dest_dir): source_path = source_dir + '\\\\' + file_name dest_path = dest_dir + '\\\\' + file_name with open(source_path, 'rb') as source_file: with open(dest_path, 'wb') as dest_file: while True: data = source_file.read(1024) if data: dest_file.write(data) else: breakif __name__ == '__main__': source_dir = r'C:\\Users\\elbadaernU9.9\\Desktop\\新建文件夹 (1)' dest_dir= r'F:C:\\Users\\elbadaernU9.9\\Desktop\\新建文件夹 (2)' try: os.mkdir(dest_dir) except: print(\"目标文件夹已经存在\") file_list = os.listdir(source_dir) for file_name in file_list: sub_process = multiprocessing.Process(target=copy_file, args=(file_name, source_dir, dest_dir)) sub_process.start() python线程创建1234import threading线程对象 = threading.Thread(target=函数名或当前任务,*args/**kwargs)线程对象.start() 多线程-文件拷贝程序12345678910111213141516171819202122232425import osimport threadingdef copy_file(file_name, source_dir, dest_dir): source_path = source_dir + '\\\\' + file_name dest_path = dest_dir + '\\\\' + file_name with open(source_path, 'rb') as source_file: with open(dest_path, 'wb') as dest_file: while True: data = source_file.read(1024) if data: dest_file.write(data) else: breakif __name__ == '__main__': source_dir = r'C:\\Users\\elbadaernU9.9\\Desktop\\新建文件夹 (1)' dest_dir= r'F:C:\\Users\\elbadaernU9.9\\Desktop\\新建文件夹 (2)' try: os.mkdir(dest_dir) except: print(\"目标文件夹已经存在\") file_list = os.listdir(source_dir) for file_name in file_list: sub_thread = threading.Thread(target=copy_file, args=(file_name, source_dir, dest_dir)) sub_thread.start() 进程与线程的关系对比进程是操作系统资源分配的基本单位，线程是CPU调度的基本单位。线程依附于进程，不能脱离进程独立存在。一个进程默认提供了一条线程，可以创建多个线程 创建进程的资源开销较大，可以使用CPU多核，线程的资源开销小，只能在单核模式运行","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"http://yoursite.com/tags/多线程/"}]},{"title":"HTTP请求方法","slug":"HTTP请求方法","date":"2019-04-11T08:07:35.000Z","updated":"2022-06-27T07:31:08.481Z","comments":true,"path":"2019/04/11/HTTP请求方法/","link":"","permalink":"http://yoursite.com/2019/04/11/HTTP请求方法/","excerpt":"","text":"GET请求指定的页面信息，并返回实体主体 POST向指定的资源提交数据，请求处理。数据被包含在请求体当中，重新提交页面，导致新资源的建立或已有资源的修改 PUT从客户端向服务器传送数据来取代指定的文档内容 DELETE请求服务器删除指定的页面 HEAD类似GET请求，返回的响应中没有具体内容，仅获取报头 OPTIONS允许客户端查看服务器的内容 TRACE显示服务器收到的请求，用于测试或诊断 CONNECTHTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器 GET方法和POST方法的区别· GET仅请求目标信息，返回/刷新是无害的，POST每次返回/刷新会重新提交数据 · GET可被保存在书签、缓存、历史记录当中，POST不行 · GET相对没有POST安全 · GET参数通过URL进行传递，POST参数存放在Request body当中 · GET请求时，浏览器会将http header和data一起发送除去，服务器响应为200； POST请求时，浏览器会先发送header，服务器响应100 continue，再发送data，服务器响应200（时间消耗相对GET要长） · GET每次产生1个TCP数据包，POST产生两个 · GET对数据的长度有限制（2KB），POST没有限制 · GET仅允许ASCII字符，POST没有限制","categories":[{"name":"Web","slug":"Web","permalink":"http://yoursite.com/categories/Web/"}],"tags":[{"name":"GET/POST...","slug":"GET-POST","permalink":"http://yoursite.com/tags/GET-POST/"}]},{"title":"Python的图片与字符画转换","slug":"Python的图片与字符画转换","date":"2019-04-10T11:27:27.000Z","updated":"2022-06-27T07:36:12.714Z","comments":true,"path":"2019/04/10/Python的图片与字符画转换/","link":"","permalink":"http://yoursite.com/2019/04/10/Python的图片与字符画转换/","excerpt":"","text":"总体思想为：将图片转换为灰度图片后,利用字符的“复杂程度”替换掉图片的内容，字符数组的“复杂程度”由大到小排列即可。 代码（需要pip安装PIL库）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253from PIL import Image import argparse # 命令行输入参数处理 parser = argparse.ArgumentParser() parser.add_argument('file') # 输入文件 parser.add_argument('-o', '--output') # 输出文件 parser.add_argument('--width', type=int, default=80) # 输出字符画宽 parser.add_argument('--height', type=int, default=80) # 输出字符画高 # 获取参数 args = parser.parse_args() IMG = args.fileWIDTH = args.width HEIGHT = args.height OUTPUT = args.output ascii_char = list(\"$@B%8&amp;WM#*oahkbdpqwmZO0QLCJUYXzcvunxrjft/\\|()1&#123;&#125;[]?-_+~&lt;&gt;i!lI;:,\\\"^`'. \") # 将256灰度映射到70个字符上 def get_char(r, b, g, alpha=256): if alpha == 0: return ' ' length = len(ascii_char) gray = int(0.2126 * r + 0.7152 * g + 0.0722 * b) unit = (256.0 + 1)/length return ascii_char[int(gray/unit)] if __name__ == '__main__': im = Image.open(IMG) im = im.resize((WIDTH, HEIGHT), Image.NEAREST) txt = \"\" for i in range(HEIGHT): for j in range(WIDTH): txt += get_char(*im.getpixel((j, i))) txt += '\\n' print (txt) # 字符画输出到文件 if OUTPUT: with open(OUTPUT,'w') as f: f.write(txt) else: with open(\"output.txt\", 'w') as f: f.write(txt) （稿）","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"PIL","slug":"PIL","permalink":"http://yoursite.com/tags/PIL/"}]},{"title":"DOSBox0.74 For MASM","slug":"DOSBox0-74-For-MASM","date":"2019-04-10T11:26:12.000Z","updated":"2022-06-27T07:28:57.569Z","comments":true,"path":"2019/04/10/DOSBox0-74-For-MASM/","link":"","permalink":"http://yoursite.com/2019/04/10/DOSBox0-74-For-MASM/","excerpt":"","text":"由于汇编的MASM是在32位环境下使用的，要在64位Windows环境下使用需要借助工具DOSBox。 DOSBox-0.74下载地址：[https://pan.baidu.com/s/1wiAv0pwUhRavkM51pmdJCQ]（提取码：m0kx） MASM下载MASM文件夹内至少要包含debug/exe2bin/link/masm四个.exe文件。地址：[https://pan.baidu.com/s/1MC75x01gKBxZwxmBuYq6fA]（提取码：4zu0） 使用将DOSBox-0.74放进MASM文件夹内，找到DOSBox.exe文件双击打开，输入mount d d:\\path,path可以为你的masm目录。如果MASM文件夹下有编写好的.asm汇编指令，可以直接通过命令d:进入MASM，再输入masm，进入调试，输入.asm文件名称（可不写扩展名），依次link，debug完成。效果如图：","categories":[{"name":"汇编","slug":"汇编","permalink":"http://yoursite.com/categories/汇编/"}],"tags":[{"name":"MASM","slug":"MASM","permalink":"http://yoursite.com/tags/MASM/"}]},{"title":"Just for joy---打地鼠小游戏","slug":"Just-for-joy-打地鼠小游戏","date":"2018-12-20T02:59:34.000Z","updated":"2022-06-27T07:32:33.122Z","comments":true,"path":"2018/12/20/Just-for-joy-打地鼠小游戏/","link":"","permalink":"http://yoursite.com/2018/12/20/Just-for-joy-打地鼠小游戏/","excerpt":"","text":"Just For Joy 官网（模拟）获取更多JFJ咨询，请关注我们的官方网站Created by elbadaernU9.9（inc） powered by hexo 2018. 打地鼠小游戏1快来和你的小伙伴们来试试我们全新推出的休闲小游戏：MouseHit（打地鼠），游戏仅售0$! 购买链接： 12链接: https://pan.baidu.com/s/1LSMij2q31pJXMwmXImponA 1提取码：90t6 提要我们的游戏基于Qt环境开发，可以实现调整游戏难度等各项基础功能，除此以外，我们为游戏设置了更多的可玩性因素！现在点击两个大红色问号，来获得作弊的快感！你同样可以进行不同的游戏模式，我们目前开发出来经典模式和闯关两种模式，快来测试你在闯关模式中能闯到第几轮呢？？？赶紧和你的小伙伴们一起来挑战吧！ （注，以上内容为演出效果，有时间将具体代码补至Github）","categories":[{"name":"个人笔记","slug":"个人笔记","permalink":"http://yoursite.com/categories/个人笔记/"}],"tags":[{"name":"QT","slug":"QT","permalink":"http://yoursite.com/tags/QT/"}]},{"title":"D3:SVG随机树与时钟等","slug":"D3-SVG随机树与时钟等","date":"2018-08-07T13:18:40.000Z","updated":"2022-06-27T07:27:58.434Z","comments":true,"path":"2018/08/07/D3-SVG随机树与时钟等/","link":"","permalink":"http://yoursite.com/2018/08/07/D3-SVG随机树与时钟等/","excerpt":"","text":"基本功能及说明：1.点击index.html文件，进入主页选择相应的功能。 2.选择svg盆栽随机树：基于D3和SVG生成的盆栽随机树，通过点击“Start Growing!”按钮，来实现盆栽的生长过程，同时更新按钮，继续点击“Growing More”继续生成更多的随机树子叶；最后通过点击“Replant”恢复花盆状态，可以进行重新生成；通过单机“Back”按钮，回到index主页面 3.选择SVG时钟：基于SVG和TweenmaxGSAP的时钟模型，用SVG来绘制时刻表形状，并通过GSAP来实现时钟指针转动的动态效果。进入主页以后会自动匹配系统时间，主要通过Tweenmax的GSAP动画形成环形时钟。浏览后可以通过点击“Back”按钮，回到系统index主界面，完成交互 用途本文件内容可拆分用于装饰博客等视讯网站框架，增加功能性和可读性 使用的库和工具jquery-2.1.1.min.js、TweenMax、D3.js、SVG 额外的Svg.css文件用于存放index.html的主界面信息，增强可视化。 其他https://github.com/elbadaernU404/d3","categories":[{"name":"前端","slug":"前端","permalink":"http://yoursite.com/categories/前端/"}],"tags":[{"name":"D3-SVG","slug":"D3-SVG","permalink":"http://yoursite.com/tags/D3-SVG/"}]},{"title":"Python爬虫相关","slug":"Python爬虫相关","date":"2018-07-29T11:25:10.000Z","updated":"2022-06-27T07:39:26.379Z","comments":true,"path":"2018/07/29/Python爬虫相关/","link":"","permalink":"http://yoursite.com/2018/07/29/Python爬虫相关/","excerpt":"进程和线程并发和并行并发是同时具有多个活动的系统，实质是一个或多个物理CPU在若干道程序（或线程）之间多路复用。并行是真正意义上的不同事件或线程在同一时刻同时执行。并行利用并发使系统运行更快，在操作系统的多个抽象层次运用。 进程程序不能自己运行，只有将程序装载进内存，由系统分配资源后才能运行。程序是指令的集合，可以视为指令的静态文本，而进程是指令的执行活动，是动态的行为。进程是操作系统对于一个运行中程序的抽象，是CPU、内存、IO设备的抽象。系统可以同时运行多个进程。 线程线程是操作系统进行运算调度的最小单位，被包含于进程中，是进程的实际运作单位。一个进程可以由多个线程组成，线程之间共享代码和数据。由于在实际网络服务器中对并行的需求，线程成为重要的编程模型：1.多线程之间比多进程之间更易共享数据。2.线程一般比进程高效。","text":"进程和线程并发和并行并发是同时具有多个活动的系统，实质是一个或多个物理CPU在若干道程序（或线程）之间多路复用。并行是真正意义上的不同事件或线程在同一时刻同时执行。并行利用并发使系统运行更快，在操作系统的多个抽象层次运用。 进程程序不能自己运行，只有将程序装载进内存，由系统分配资源后才能运行。程序是指令的集合，可以视为指令的静态文本，而进程是指令的执行活动，是动态的行为。进程是操作系统对于一个运行中程序的抽象，是CPU、内存、IO设备的抽象。系统可以同时运行多个进程。 线程线程是操作系统进行运算调度的最小单位，被包含于进程中，是进程的实际运作单位。一个进程可以由多个线程组成，线程之间共享代码和数据。由于在实际网络服务器中对并行的需求，线程成为重要的编程模型：1.多线程之间比多进程之间更易共享数据。2.线程一般比进程高效。 深度优先和广度优先在爬虫系统中，待抓取URL队列是很重要的一部分，待抓取URL队列中的URL以什么样的顺序排列也是一个很重要的问题，因为这涉及到先抓取哪个页面，后抓取哪个页面。而决定这些URL排列顺序的方法，叫做抓取策略。以下是两种常用的策略：深度优先、广度优先。 深度优先深度优先是指网络爬虫会从起始页开始，一个链接一个链接跟踪下去，处理完这条线路之后再转入下一个起始页，继续追踪链接。深度优先的爬取的顺序式：A-B-D-E-I-C-F-G-H （递归）算法（伪代码形式）： 广度优先广度优先，是指将新下载网页发现的链接直接插入到待抓取URL队列的末尾，也就是指网络爬虫会先抓取起始页中的所有网页，然后在选择其中的一个连接网页，继续抓取在此网页中链接的所有网页。广度优先的爬取顺序为：A-B-C-D-E-F-G-H-I (队列)算法（伪代码形式）： Cookie和SessionCookie简单的说就是当用户通过http协议访问一个服务器的时候，这个服务器会将一些Name/Value键值对返回给客户端浏览器，并将这些数据加上一些限制条件。在条件符合时，这个用户下次再访问服务器的时候，数据又被完整的带给服务器。因为http是一种无状态协议，用户首次访问web站点的时候，服务器对用户一无所知。而Cookie就像是服务器给每个来访问的用户贴的标签，而这些标签就是对来访问的客户端的独有的身份的一个标识，这里就如同每个人的身份证一样，带着你的个人信息。而当一个客户端第一次连接过来的时候，服务端就会给他打一个标签，这里就如同给你发了一个身份证，当你下载带着这个身份证来的时候，服务器就知道你是谁了。所以Cookie是存在客户端的，这里其实就是在你的浏览器中。 Cookie分类Cookie主要分为两种：会话Cookie：不设置过期时间，保存在浏览器的内存中，关闭浏览器，Cookie便被销毁普通Cookie:设置了过期时间，保存在硬盘上 Session上面我们知道了Cookie可以让服务器端跟踪每个客户端的访问，但是每次客户端的访问都必须传回这些 Cookie，如果 Cookie 很多，这无形地增加了客户端与服务端的数据传输量，而 Session 的出现正是为了解决这个问题。同一个客户端每次和服务端交互时，不需要每次都传回所有的Cookie值，而是只要传回一个ID这个ID是客户端第一次访问服务器的时候生成的，而且每个客户端是唯一的。这样每个客户端就有了一个唯一的ID，客户端只要传回这个ID就行了，这个ID通常是NANE为JSESIONID的一个Cookie。所以Session其实是利用Cookie进行信息处理的。12345678910import reimport requestsresault = requests.get(&apos;http://120.24.86.145:8002/qiumingshan/&apos;)re = re.requests(r&apos;^&lt;div&gt;(.*)=\\?;&lt;/div&gt;$&apos;, resault.text,re.S|re.M|re.I)value =&#123;&apos;value&apos;: eval(re.group(1))&#125;resault = requests.post(&apos;http://120.24.86.145:8002/qiumingshan/&apos;, data=value)print(&apos;The Resault is:&apos;,eval(re.group(1)))print(resault.text) 这样是错误的，会话无法维持。正确写法如下：1234567891011import reimport requestssearch = requests.session()resault = search.get(&apos;http://120.24.86.145:8002/qiumingshan/&apos;)re = re.search(r&apos;^&lt;div&gt;(.*)=\\?;&lt;/div&gt;$&apos;, resault.text,re.S|re.M|re.I)value =&#123;&apos;value&apos;: eval(re.group(1))&#125;resault = search.post(&apos;http://120.24.86.145:8002/qiumingshan/&apos;, data=value)print(&apos;The Resault is:&apos;,eval(re.group(1)))print(resault.text) 用HTML测试网站httpbin.org举例：1234import requestsrequests.get(&quot;http://httpbin.org/cookies/set/number/123456&quot;)response = requests.get(&quot;http://httpbin.org/cookies&quot;)print(response.text) 结果：12345import requestss = requests.Session()s.get(&quot;http://httpbin.org/cookies/set/number/123456&quot;)response = s.get(&quot;http://httpbin.org/cookies&quot;)print(response.text) 结果：对于Cookie，因为人为的可分析本地Cookie进行Cookie欺骗，Session相对安全。而Session会一段时间内保持在服务器中，访问量越多越占用服务器性能。如果主要减轻服务器压力，Cookie相对合适。 HTML中form表单登录页面会有一个form表单提交，form表单的功能是用于搜集不同类型的用户输入的内容。有了表单，网页的内容可以由用户自己创建。获取authenticity_token信息后，可以获得Cookie信息，分析登录包获取提交地址，利用Scrapy爬虫框架等一步步实现通过爬虫程序登录网站。","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}]},{"title":"Python正则表达式和requests爬虫","slug":"Python正则表达式和requests爬虫","date":"2018-07-20T00:23:31.000Z","updated":"2022-06-27T07:39:39.354Z","comments":true,"path":"2018/07/20/Python正则表达式和requests爬虫/","link":"","permalink":"http://yoursite.com/2018/07/20/Python正则表达式和requests爬虫/","excerpt":"Some people,when confronted with a problem,think,”I know,I’ll use regular expressions.”Now they have two problems. ----Jamie Zawinski 正则表达式正则表达式是可匹配文本片段的模式。最简单的正则表达式为普通字符串，与自己匹配。python模块re提供了其对正则表达式的支持。 requests模块Requests是用Python语言编写，基于urllib，采用Apache2Licensed开源协议的HTTP库，是一个需要单独安装的第三方库。通过pip执行指令：1pip install requests 来安装requests模块。py3版本安装时自带pip。如果报错，可能需要对pip进行更新，执行：1python -m pip install -U pip 来对pip安装更新，需要注意的是这里需要将python添加进环境变量才可以执行。这样就完成requests模块的安装。","text":"Some people,when confronted with a problem,think,”I know,I’ll use regular expressions.”Now they have two problems. ----Jamie Zawinski 正则表达式正则表达式是可匹配文本片段的模式。最简单的正则表达式为普通字符串，与自己匹配。python模块re提供了其对正则表达式的支持。 requests模块Requests是用Python语言编写，基于urllib，采用Apache2Licensed开源协议的HTTP库，是一个需要单独安装的第三方库。通过pip执行指令：1pip install requests 来安装requests模块。py3版本安装时自带pip。如果报错，可能需要对pip进行更新，执行：1python -m pip install -U pip 来对pip安装更新，需要注意的是这里需要将python添加进环境变量才可以执行。这样就完成requests模块的安装。 脚本接下来便可以尝试利用正则表达式结合requests爬虫来写一个脚本用于爬取网页信息，以一道简单的CTF题为例：http://120.24.86.145:8002/qiumingshan/打开页面后要求2s内计算算术式的值，多次刷新后得到提示用post传递value参数：首先引用模块正则表达式和requests爬虫：1import re 1import requests 接下来使用Session对象来确保对话的持续，使服务器知道请求均为同一PC端发出。使用get获得网页源码中的内容：1search = requests.session() 1resault = search.get(&apos;http://120.24.86.145:8002/qiumingshan/&apos;) 接下来通过正则表达式来提取需要利用的信息：1re = re.search(r&apos;^&lt;div&gt;(.*)=\\?;&lt;/div&gt;$&apos;, resault.text,re.S|re.M|re.I) 在正则表达式中，“.”为通配符，与除换行符外的其他字符都匹配，“.*”表示通配符可出现0、1或多次。从网页中我们可以看到每次算术式都在改变，但是最后的“=?”是不变的，所以对最后的“=?”保留原样，并使用反斜杠\\进行转义：“=\\?”（正则表达式中问号代表0或1，是特殊字符），而为了表示re模块需要的单个反斜杠，通常需要用两个反斜杠来进行转义，包涵解释器的转义和re模块执行的转义。所以在表达式的开始我使用了原始字符串的标志r来取消了解释器反斜杠的使用。由脱字符^来开始，打开链接，F12进行审查元素，可以看到需要利用到的部分包括在HTML标签“&lt;div&gt;…&lt;/div&gt; ”中：将主体算术部分用小括号“()”来进行编组，此时“.*”为编组1。以美元符号“$”来表示字符串的末尾，用resault.text将字符串返回。re.S、re.M、re.I的作用分别为以“.”代替任何字符，多行模式，以及忽略大小写，按位或运算符“|”在这里表示这些flags定义全部成立。1value =&#123;&apos;value&apos;: eval(re.group(1))&#125; 定义一个字典value，其键值分别为’value’和算术式的结果。eval函数用于计算字符串表示的python表达式的值并返回结果，即计算出编组1中算术式的值，并作为值与’value’匹配。发送post请求：1resault = search.post(&apos;http://120.24.86.145:8002/qiumingshan/&apos;, data=value) post请求的参数以data关键字参数来传递，而data参数传递字典，这里传递的便是字典’value’。显示结果：12print(&apos;The Resault is:&apos;，eval(re.group(1)))print(resault.text) 这样解题的脚本便完成了。在IDLE中运行，可以得到flag。","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}]},{"title":"海龟绘图法（Python）","slug":"海龟绘图法（Python）","date":"2018-07-15T09:26:57.000Z","updated":"2022-06-27T07:40:34.929Z","comments":true,"path":"2018/07/15/海龟绘图法（Python）/","link":"","permalink":"http://yoursite.com/2018/07/15/海龟绘图法（Python）/","excerpt":"","text":"这里是python中的一个绘图模块:海龟绘图(turtle)。此模块可以让操作者绘制图形，而非打印文本。其他命令参考：https://docs.python.org/3/library/turtle.html绘图方法:参考搜索turtle graphis 最简单的示例:import turtlet=turtle.Pen()t.reset()for x in range(100): t.forward(x) t.left(90)效果图:这里需要注意的是，存储文件时文件名称不能以turtle命名，否则解释器将会报错。 之后可以尝试结合随机函数和随机语句来绘制较为复杂的图形:import turtle as tfrom random import randintt.TurtleScreen._RUNNING = True #使程序于IDLE中顺利运行t.speed(0) #速度最快t.bgcolor(“black”) #背景色为黑色t.setpos(-25, 25) #起始位置t.colormode(255) #颜色模式为真彩色i = 0 #赋哑值for a in range(0,200): #绘制花瓣 r = randint(0, 255) g = randint(0, 255) b = randint(0, 255) t.pencolor(r, g, b) t.forward(50 + i) t.right(85) t.forward(i) t.left(30) i += 1t.left(90) #绘制花杆t.pensize(width=10)t.pencolor(“green”)t.speed(10)t.forward(250)t.done()效果图： 可以进一步来设计一个小游戏:import turtle as tfrom random import randint t.TurtleScreen._RUNNING = Truet.setup(width=750, height=350, startx=None, starty=None) #界面大小设置t.hideturtle() #隐藏海龟画笔t.color(“orange”)t.penup()t.setpos(0, 0)myfont = (“隶书”, 16, “normal”) #设置字体target = randint(1, 100)t.write(“一个1到100之间的整数，猜猜看：”, font=myfont)guess = 0while target!=guess in range(0,100): guess = t.simpledialog.askinteger(“猜数游戏”, “请输入一个整数：”) if guess == target: answer = “Congratulations!” t.bgcolor(“black”) elif not guess: t.clear() t.write(“Give Up,Game Over.”, font=myfont) break elif guess &gt; target: answer = “你猜大了，再猜一次：” else: answer = “你猜小了，再猜一次：” t.clear() t.write(answer, font=myfont)t.done()初始界面:胜利界面:放弃界面:","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"海龟绘图","slug":"海龟绘图","permalink":"http://yoursite.com/tags/海龟绘图/"}]},{"title":"一些.vbs脚本文件(自娱自乐向)","slug":"一些-vbs脚本文件-自娱自乐向","date":"2018-07-01T11:57:57.000Z","updated":"2022-06-27T07:42:42.213Z","comments":true,"path":"2018/07/01/一些-vbs脚本文件-自娱自乐向/","link":"","permalink":"http://yoursite.com/2018/07/01/一些-vbs脚本文件-自娱自乐向/","excerpt":"","text":".vbs我记得初识basic语言好像要回溯到2014年以前，初中时代，当时考试的重心主要是放在读程序逻辑上，几个语句，像loop…until至今印象深刻。记录几个简单的.vbs脚本，计算机可以直接执行。 1.消息框招呼脚本记事本编写’msgbox “Hello，9.9”‘，将文件后缀.txt修改为.vbs即可，双击运行后弹出消息框，可以将此脚本设置为开机启动，恩，硬核自己给自己打个招呼来自娱自乐。（另，在室友电脑编写后首末添加do/loop可以让程序进入死循环，讯息内容可以随意发挥，控制台关闭程序即可） 2.语音脚本同样的，在记事本编写’CreateObject(“SAPI.SpVoice”).Speak “Hello,9.9”‘，调整扩展名为.vbs，运行后得到系统语言提示。同样的，可以将其设置为开机启动，配合消息框食用效果更佳（假装是电脑给自己打招呼）。 3.无限刷新：1234set ws=createobject(&quot;wscript.shell&quot;)dows.sendkeys&quot;&#123;f5&#125;&quot;loop 无限Alt+f4123456dim WSHshellset WSHshell=wscript.createobject(&quot;wscript.shell&quot;)dowscript.sleep 2000WSHshell.SendKeys&quot;%&#123;f4&#125;&quot;loop QQ消息自动发送1234567891011On Error Resume Next Dim fd,ye set fd=createobject(&quot;wscript.shell&quot;) for i=1 to 10 //循环次数wscript.sleep 100 //循环间隔，越大越慢fd.AppActivate(&quot;Name&quot;) //发送对象的昵称fd.sendKeys &quot;^v&quot; //打印复制的内容fd.sendKeys ifd.sendKeys &quot;%s&quot; next wscript.quit （注：以上脚本慎用，玩笑适可而止。） 添加插曲来自伟大的360爸爸发出的木马警告（我日我自己lol）","categories":[{"name":"脚本","slug":"脚本","permalink":"http://yoursite.com/categories/脚本/"}],"tags":[{"name":"vb","slug":"vb","permalink":"http://yoursite.com/tags/vb/"}]},{"title":"2018年上实训记录","slug":"2018年上实训记录","date":"2018-06-04T08:59:38.000Z","updated":"2022-06-27T07:33:02.826Z","comments":true,"path":"2018/06/04/2018年上实训记录/","link":"","permalink":"http://yoursite.com/2018/06/04/2018年上实训记录/","excerpt":"（持续更新ing…） Day1.所谓实训的大目标只有一个俄罗斯方块的设计，工具为Codeblocks。实际上挺无聊，上午上机的环境之前都有配置过，简单调试了下，坑爹的是不知情的情况下安装了什么伽卡他卡我卡大家卡的流氓学生端课堂工具（已灭口，下图1附加方法）。下午随进度写了几行代码，仅仅调整了光标以及颜色等的设置，然后就（结束了？？）emmmmm，GG，懵逼中水掉一天，还是顺带学些其他的玩意吧XD。不多吐槽，这玩意虽然常见，代码千千万万但到自己手里毕竟也是第一次做，踏实些。","text":"（持续更新ing…） Day1.所谓实训的大目标只有一个俄罗斯方块的设计，工具为Codeblocks。实际上挺无聊，上午上机的环境之前都有配置过，简单调试了下，坑爹的是不知情的情况下安装了什么伽卡他卡我卡大家卡的流氓学生端课堂工具（已灭口，下图1附加方法）。下午随进度写了几行代码，仅仅调整了光标以及颜色等的设置，然后就（结束了？？）emmmmm，GG，懵逼中水掉一天，还是顺带学些其他的玩意吧XD。不多吐槽，这玩意虽然常见，代码千千万万但到自己手里毕竟也是第一次做，踏实些。 Day2.收录上午的代码：1/Headers/day1.h #ifndef DAY1_H_INCLUDED #define DAY1_H_INCLUDED #include&lt;stdio.h&gt; #include&lt;stdlib.h&gt; #include&lt;string.h&gt; #include&lt;time.h&gt; #include&lt;conio.h&gt; #include&lt;windows.h&gt; #include&lt;stdbool.h&gt; typedef struct TetriManager{ unsigned int pool[28];//游戏池 int x;//当前方块横坐标，此处坐标为X轴左上角横坐标 int y;//当前方块纵坐标，此处坐标为Y轴左上角纵坐标 int type[3];//当前、下一个和下下一个方块的类型 int orientation[3];//当前、下一个和下下一个方块的旋转状态 unsigned score;//得分 unsigned erasedCount[4];//消行数 unsigned erasedTotle;//消失行总数 unsigned tetrisTotal;//方块总数 bool dead;//游戏是否结束}Manager; typedef struct TetrisControl{ bool pause;//暂停 bool clockwise;//旋转方向：顺时针为true int direction;//移动方向：0向左运动，1向右运动 int color[28][16]}Control; HANDLE Output;void printPrompting();void gotoxyWithFullwidth(short x,short y); #endif // DAY1_H_INCLUDED1/Sources/main.c int main(){ Output=GetStdHandle(STD_OUTPUT_HANDLE); printPrompting();} void gotoxyWithFullwidth(short x,short y)//进行全角定位{ static COORD cd; cd.X=(x*2); cd.Y=y; SetConsoleCursorPosition(Output,cd);} void printPrompting()//显示提示信息{ SetConsoleTextAttribute(Output,11); gotoxyWithFullwidth(28,12); printf(“控制：”);} 收录下午的代码（主要是利用循环增添提示信息等）：","categories":[{"name":"个人笔记","slug":"个人笔记","permalink":"http://yoursite.com/categories/个人笔记/"}],"tags":[{"name":"C","slug":"C","permalink":"http://yoursite.com/tags/C/"}]},{"title":"ISCC2018的部分WP","slug":"ISCC2018的部分WP","date":"2018-05-07T12:03:21.000Z","updated":"2022-06-27T07:31:28.450Z","comments":true,"path":"2018/05/07/ISCC2018的部分WP/","link":"","permalink":"http://yoursite.com/2018/05/07/ISCC2018的部分WP/","excerpt":"（持续更新…） 1.要求输入比服务器上大的数字，随便输入几个9，发现最多只能输入三位数，直接F12找到maxlength=3将3修改为4再随意填入一个四位数即可。 2.打开地址后是一份PHP代码：由代码读出需要使strcmp函数值为0，应该是要构造出长度相等的数组。构造payload，在连接后添入/?password[]=a，返回flag。 3.用Burpsuite抓包，GO一次后得到提示在Request添加X-Forwarded-For:127.0.0.1即可。","text":"（持续更新…） 1.要求输入比服务器上大的数字，随便输入几个9，发现最多只能输入三位数，直接F12找到maxlength=3将3修改为4再随意填入一个四位数即可。 2.打开地址后是一份PHP代码：由代码读出需要使strcmp函数值为0，应该是要构造出长度相等的数组。构造payload，在连接后添入/?password[]=a，返回flag。 3.用Burpsuite抓包，GO一次后得到提示在Request添加X-Forwarded-For:127.0.0.1即可。 4.百度了一下XSS：又称CSS (Cross Site Script)，跨站脚本攻击。所以链接后的那一部分应该是被植入了脚本并隐藏进链接中。解码后根据+v+了解到是utf-7编码，解码后得到key的内容，填入得flag。 5.仅文本要求给出正确的账号和密码。查看代码：&lt;?phperror_reporting(0);$flag = “***“;if(isset($_GET[‘username’])){ if (0 == strcasecmp($flag,$_GET[‘username’])){ $a = fla; echo “very good!Username is right”; } else{ print ‘Username is not right‘;}}elseprint ‘Please give me username or password!’;if (isset($_GET[‘password’])){ if (is_numeric($_GET[‘password’])){ if (strlen($_GET[‘password’]) &lt; 4){ if ($_GET[‘password’] &gt; 999){ $b = g; print ‘very good!Password is right‘; }else print ‘Password too little‘; }else print ‘Password too long‘; }else print ‘Password is not numeric‘;}if ($a.$b == “flag”) print $flag;?&gt;用户名为Flag，密码要求小于4个字节并大于999，按照strcasecmp遇数组返回null=0，同时用16进制3e8来代替1000即可（实际上3e7和3e6对应的999以及998也可以通过，可能是题目设计得不够严谨）。构造http://118.190.152.202:8017/index.php?username[]=&amp;password=3E8即显示Flag。 6.一只黑色的手指向下方，可能在暗示些什么。用tweakpng打开后得到提示：按要求修改402e2d95为b92fc0cd并没有实际效果。了解到crc错误，可能是手指下方截掉某些部分导致图片不完整。尝试修改了高度1f4（16进制500）为3e8（1000）后，可以看到Flag。 7.基于培根密码的秘密电报，根据密码表即可破译。 8.“有趣的ISCC”，在图片中隐藏了内容。直接用记事本打开图片，发现文本最后的编码：? # 9 2 ; &amp; # 1 1 7 ; &amp; # 4 8 ; &amp; # 4 8 ; &amp; # 5 4 ; &amp; # 5 4 ; &amp; # 9 2 ; &amp; # 1 1 7 ; &amp; # 4 8 ; &amp; # 4 8 ; &amp; # 5 4 ; &amp; # 9 9 ; &amp; # 9 2 ; &amp; # 1 1 7 ; &amp; # 4 8 ; &amp; # 4 8 ; &amp; # 5 4 ; &amp; # 4 9 ; &amp; # 9 2 ; &amp; # 1 1 7 ; &amp; # 4 8 ; &amp; # 4 8 ; &amp; # 5 4 ; &amp; # 5 5 ; &amp; # 9 2 ; &amp; # 1 1 7 ; &amp; # 4 8 ; &amp; # 4 8 ; &amp; # 5 5 ; &amp; # 9 8 ; &amp; # 9 2 ; &amp; # 1 1 7 ; &amp; # 4 8 ; &amp; # 4 8 ; &amp; # 5 4 ; &amp; # 5 7 ; &amp; # 9 2 ; &amp; # 1 1 7 ; &amp; # 4 8 ; &amp; # 4 8 ; &amp; # 5 5 ; &amp; # 5 1 ; &amp; # 9 2 ; &amp; # 1 1 7 ; &amp; # 4 8 ; &amp; # 4 8 ; &amp; # 5 4 ; &amp; # 5 1 ; &amp; # 9 2 ; &amp; # 1 1 7 ; &amp; # 4 8 ; &amp; # 4 8 ; &amp; # 5 4 ; &amp; # 5 1 ; &amp; # 9 2 ; &amp; # 1 1 7 ; &amp; # 4 8 ; &amp; # 4 8 ; &amp; # 5 0 ; &amp; # 4 8 ; &amp; # 9 2 ; &amp; # 1 1 7 ; &amp; # 4 8 ; &amp; # 4 8 ; &amp; # 5 4 ; &amp; # 5 7 ; &amp; # 9 2 ; &amp; # 1 1 7 ; &amp; # 4 8 ; &amp; # 4 8 ; &amp; # 5 5 ; &amp; # 5 1 ; &amp; # 9 2 ; &amp; # 1 1 7 ; &amp; # 4 8 ; &amp; # 4 8 ; &amp; # 5 0 ; &amp; # 4 8 ; &amp; # 9 2 ; &amp; # 1 1 7 ; &amp; # 4 8 ; &amp; # 4 8 ; &amp; # 5 4 ; &amp; # 5 4 ; &amp; # 9 2 ; &amp; # 1 1 7 ; &amp; # 4 8 ; &amp; # 4 8 ; &amp; # 5 5 ; &amp; # 5 3 ; &amp; # 9 2 ; &amp; # 1 1 7 ; &amp; # 4 8 ; &amp; # 4 8 ; &amp; # 5 4 ; &amp; # 1 0 1 ; &amp; # 9 2 ; &amp; # 1 1 7 ; &amp; # 4 8 ; &amp; # 4 8 ; &amp; # 5 5 ; &amp; # 1 0 0 ;一开始拿去Unicode解码没有任何结果，弄了半天后才删掉了字符间的空格……整理好得到这样一段：?#92;&#117;&#48;&#48;&#54;&#54;&#92;&#117;&#48;&#48;&#54;&#99;&#92;&#117;&amp;#4 8;&#48;&#54;&#49;&#92;&#117;&#48;&#48;&#54;&#55;&#92;&#117;&#48;&#48;&#55;&#98;&#92;&#117;&#48;&#48;&#54;&#57; &#92;&#117;&#48;&#48;&#55;&#51;&#92;&#117;&#48;&#48;&#54;&#51;&#92;&#117;&#48;&#48;&#54;&#51;&#92;&#117;&#48;&#48;&#50;&#48;&#92;&#117;&#48;&#48;&#54;&#57;&#92;&#117;&#48;&#48;&#55;&#51;&#92;&#117;&#48;&#48;&#50;&#48;&#92;&#117;&#48;&#48;&#54;&#54;&#92;&#117;&#48;&#48;&#55;&#53;&#92;&#117;&#48;&#48;&#54;&#101;&#92;&#117;&#48;&#48;&#55;&#100;再解码： 9.凯撒十三世在学会使用键盘后，向你扔了一串字符：“ebdgc697g95w3”，猜猜它吧。凯撒密码，在相关网站解出了25组结果，试了前2组后发现都是错的，索性直接一组一组按加密次数还原，发现第13次解密得到的roqtp697t95j3加密13次后正好是ebdgc697g95w3（这很凯撒十三世…）。填入提示仍然错误。想了很久，也尝试了12，14的结果期间错了一万次，后来才发现得到的结果本身也是个键盘密码，瞎弄了很久，期间又错了一万次，到最后才弄明白是每个单词再对应其原始键盘的下一行开始慢慢得到Flagyougotme。令人智熄。 10.佛系CTF…打开附件下载后得到的页面：末尾两个“=”拿去Base64解码，反复解码最后获得：U2FsdGVkX183BPnBd50ynIRM3o8YLmwHaoi8b8QvfVdFHCEwG9iwp4hJHznrl7d4%0AB5rKClEyYVtx6uZFIKtCXo71fR9Mcf6b0EzejhZ4pnhnJOl+zrZVlV0T9NUA+u1z%0AiN+jkpb6ERH86j7t45v4Mpe+j1gCpvaQgoKC0Oaa5kc%3D了解到AES加密后解密得到文字：很懵逼，我佛慈悲，在解密网站集锦Ctrl+F输入“佛”后居然还真的有种与佛论禅加密。依照提示加上佛曰之后得到了Flag：","categories":[{"name":"信息安全","slug":"信息安全","permalink":"http://yoursite.com/categories/信息安全/"}],"tags":[{"name":"ctf","slug":"ctf","permalink":"http://yoursite.com/tags/ctf/"}]},{"title":"关于QQ坦白说JSON源破解对方身份的方法","slug":"关于QQ坦白说JSON源破解对方身份的方法","date":"2018-04-03T05:06:04.000Z","updated":"2022-06-27T07:40:45.713Z","comments":true,"path":"2018/04/03/关于QQ坦白说JSON源破解对方身份的方法/","link":"","permalink":"http://yoursite.com/2018/04/03/关于QQ坦白说JSON源破解对方身份的方法/","excerpt":"","text":"这个问题在4月2日晚上七点左右已经被修复，存在时间不长。随着被修复的问题还有很多，墨水不多，这里不多说。首先通过https://ti.qq.com/cgi-node/honest-say/receive/mine ，可以打开自己的坦白说JSON源，然后发现EncodeUin后的加密代码，如图：有大佬破解出了：对照解码后便可以得到发送者的QQ号码……恩 ，很可怕。当然问题很严重，修复也很快，现在只能被开发者调侃。怎么说呢，就当愚人节菜单吧，在匿名的世界里皮一下也得适当一些233。","categories":[{"name":"信息安全","slug":"信息安全","permalink":"http://yoursite.com/categories/信息安全/"}],"tags":[{"name":"QQ坦白说","slug":"QQ坦白说","permalink":"http://yoursite.com/tags/QQ坦白说/"}]},{"title":"CTF-Tests(3)","slug":"CTF-Tests-3","date":"2018-03-31T13:02:45.000Z","updated":"2022-06-27T07:27:23.769Z","comments":true,"path":"2018/03/31/CTF-Tests-3/","link":"","permalink":"http://yoursite.com/2018/03/31/CTF-Tests-3/","excerpt":"1.2018年0ctf签到题根据提示要求加入freenode网站的IRC(Internet Relay Chat)0ctf2018聊天室来获取flag。初次进入chat.freenode.net，网页没有反应，推测端口问题或者是网络节点问题。之后我在进入freenode.net后，按照说明使用了6667~6669的端口，均不能成功，于是打开手机VPN，等待之后成功进入聊天页面，输入给定的#0ctf2018进入聊天室，得到flag。","text":"1.2018年0ctf签到题根据提示要求加入freenode网站的IRC(Internet Relay Chat)0ctf2018聊天室来获取flag。初次进入chat.freenode.net，网页没有反应，推测端口问题或者是网络节点问题。之后我在进入freenode.net后，按照说明使用了6667~6669的端口，均不能成功，于是打开手机VPN，等待之后成功进入聊天页面，输入给定的#0ctf2018进入聊天室，得到flag。 2.进入后尝试admin登陆，用BP字典Passworld爆破密码，结果长度均一致，转用注册功能注册admin，提示已经被注册，所以还是要用admin登陆来获得flag。根据提示创建admin,在其后增加若干空格后进入，得到flag。 3.要求2秒内进行计算。用PY脚本：import requestsimport reurl = ‘http://120.24.86.145:8002/qiumingshan/&#39;s = requests.Session()source = s.get(url)expression = re.search(r’(\\d+[+-*])+(\\d+)’, source.text).group()result = eval(expression)post = {‘value’: result}print(s.post(url, data = post).text)运行后可得到flag。 4.用BP抓包后得到flag的信息，从末尾的“=”得知应该是一个base64编码，解码后得到的结果继续解码用Py脚本解出flag。import requests import base64url =’http://120.24.86.145:8002/web6/&#39;r =requests.session()headers = r.get(url).headerskey = base64.b64decode(base64.b64decode(headers[‘flag’]).split(‘:’)[1])data={‘margin’:key}print r.post(url=url,data=data).content 5.CTF故事汇……BP抓包后发现文本中的链接是个真的链接，打开后发现是php中md5()函数漏洞和strcmp()函数漏洞的利用，MD5不能处理数组，有时也可以用数组绕过。同时strcmp（）函数也可用数组绕过，构造payload，进入http://47.93.190.246:49162/?v1[]=1&amp;v2[]=2&amp;v3[]=1 ，出现flag。","categories":[{"name":"信息安全","slug":"信息安全","permalink":"http://yoursite.com/categories/信息安全/"}],"tags":[{"name":"ctf","slug":"ctf","permalink":"http://yoursite.com/tags/ctf/"}]},{"title":"一些密码学解密网站","slug":"一些密码学解密网站","date":"2018-03-29T14:53:00.000Z","updated":"2022-06-27T07:43:01.465Z","comments":true,"path":"2018/03/29/一些密码学解密网站/","link":"","permalink":"http://yoursite.com/2018/03/29/一些密码学解密网站/","excerpt":"","text":"资料来源网络，持续更新。URL[http://tool.chinaz.com/tools/urlencode.aspx]; JSFuck[http://discogscounter.getfreehosting.co.uk/js-noalnum.php?i=1]; Base64[http://www1.tc711.com/tool/BASE64.htm]; Base64(img)[http://www.vgot.net/test/image2base64.php]; Base32[https://tools.deamwork.com/crypt/decrypt/base32decode.html]; String[http://www.5ixuexiwang.com/str/];","categories":[{"name":"信息安全","slug":"信息安全","permalink":"http://yoursite.com/categories/信息安全/"}],"tags":[{"name":"工具站","slug":"工具站","permalink":"http://yoursite.com/tags/工具站/"}]},{"title":"CTF-Tests(2)","slug":"CTF-Tests-2","date":"2018-03-29T13:34:58.000Z","updated":"2022-06-27T07:27:07.978Z","comments":true,"path":"2018/03/29/CTF-Tests-2/","link":"","permalink":"http://yoursite.com/2018/03/29/CTF-Tests-2/","excerpt":"","text":"仍然是5道普通的CTF。 1.页面会不断弹出”FLAG就在这里”、”来找找吧”这样的信息。在强行关闭弹窗后，审查元素，得到如下的编码，弄清后在网站[http://tool.chinaz.com/tools/urlencode.aspx] Unicode解码得到FLAG。 2.根据提示打开源代码，找到之后同样按照提示进行解码后便得到FLAG。 3.了解到同样是一个编码后，根据提示JSFuck(呃…)解码，会得到FLAG。 4.题目叫头等舱，而打开后提示什么都没有，审查元素以及查看源后无果，用Burpsuite抓包，得到Flag。 5.只有这些内容，尝试从语法入手后添加链接，回车后会得到FLAG。","categories":[{"name":"信息安全","slug":"信息安全","permalink":"http://yoursite.com/categories/信息安全/"}],"tags":[{"name":"ctf","slug":"ctf","permalink":"http://yoursite.com/tags/ctf/"}]},{"title":"CTF-Tests","slug":"CTF-Tests","date":"2018-03-22T12:30:54.000Z","updated":"2022-06-27T07:26:50.323Z","comments":true,"path":"2018/03/22/CTF-Tests/","link":"","permalink":"http://yoursite.com/2018/03/22/CTF-Tests/","excerpt":"","text":"Burpsuite、IDA、OD等工具安装完成后，我开始尝试在百度等教程下去做简单的CTF。这里是5个简单的例子。 1.第一个是叫hello的程序，使用OD打开后，利用搜索功能查找字符串，直接得到了key：WelcomeToKanXueCtf2017，输入，完成。 2.这是一个关于网页验证码的问题，很简单的两位数相加求和，但尝试输入时发现了问题所在：只能输入一个字符。F12开启审查元素，找到maxlength=”1”，很明显问题出在这里，在1后面加上两个0之后，输入结果，得到Flag。 3.宽带信息泄露，好厉害的样子，下一个…Ummmmm，找到.bin的文件，用RouterPassView工具打开它，然后得到username，也就是提示的Flag，输入通过。 4.审题名字叫做停不下来…恩确实停不下来，页面不断自动有规律的刷新，刷出不同的结果，打开Burpsuite，放进Repeater之后开始按照题目刷新的规律多次发送访问包，得到了Flag。 5.要求提交一个.php文件，我在Wamp里拿出之前的HelloWorld，提交进去之后提示不是图片文件，emmmm，将后缀改为.png，再次提交时丢进BurpSuite，在其中再将.png改作.php后，上传至Repeater得到Flag。ps，实际上尝试去解这些CTF时，基本每一道题都会在审查元素里无获后，尝试各种工具，遇到各种障碍以及看不懂的情况，懵逼良久。也有很多题目完成一部分后开始一知半解，始终得不到Flag，想解出某些密码，在审查元素中找到一些线索后却不能开展下去。时间很短，路还很长。","categories":[{"name":"信息安全","slug":"信息安全","permalink":"http://yoursite.com/categories/信息安全/"}],"tags":[{"name":"ctf","slug":"ctf","permalink":"http://yoursite.com/tags/ctf/"}]},{"title":"BurpSuite","slug":"BurpSuite","date":"2018-03-22T12:25:03.000Z","updated":"2022-06-27T07:26:31.089Z","comments":true,"path":"2018/03/22/BurpSuite/","link":"","permalink":"http://yoursite.com/2018/03/22/BurpSuite/","excerpt":"","text":"最近开始弄CTF。学习信息安全，自我意识到仅仅只靠php这些语言和编译器是远远不足的，各式各样的工具同样发挥着非常重要的作用。之前就听说过打嗝，但是实装BurpSuite则是不久前再度被安利才进行，过程仍然一波三折。我发现很多所谓的教程更多完全只是提供了完整的具体思路，完全没有考虑过真正初次接触者的感受，很多细节是非常令人头痛的，我觉得探索更多应该放在实践之中，而不是在初学之中。BurpSuite是一款基于Java环境下的Java程序，那么在安装Burp之前需要先配置Java环境，正巧我还没有接触过Java，所以先搭载了jdk，安装完毕，之后下载了52破解平台的Burp专业版（之前在官网还下载过直装普通版，专业版花费有些高），习惯性解压，得到两个文件夹。然后晕了，恩？Burp怎么打开？虽然没有应用程序，但也应该有可执行的部分，将文件夹逐个打开后是许许多多.Class文件。初步推测Java环境未能搭建成功，Win+R，开启CMD，输入Java -version，结果确实是提示’Java不是内部或外部命令，也不是可运行的程序或批处理文件’，因为之前装nodejs时出过类似问题，第一时间想到的是版本不符，于是我去Java官网将Jdk最新版本、8u161、8u162都尝试安装，期间也尝试了Burp的各个版本，之后在52确定了Burp1.7和jdk161完全没有问题，开始在其他方面查找问题。询问大佬之后才意识到jdk的安装是有门道的，于是我开始查找教程一步步安装jdk，发现一直遗漏了关键的步骤，就是在最后手动配置Java环境变量（这玩意还真不是安装包一步到位的……）一番处理后，打开CMD，Java安装成功。接下来我重新解压了Burp，尴尬的是仍然是两个文件夹，摆在我的面前。一番尝试无果，回归懵逼状态。又是一番搜索之后，无意中了解到.jar与.zip文件之间的孽缘，原来那两个“压缩包”可以直接打开运行……安装算是成功了，按照52的教程一步步顺利的打开了BurpSuite。而配置完成后，这两天开始实践CTF时在Burp中再一次遇到了瓶颈，无论如何刷新网页，Burp都不会自己抓包，也是，虽然我设置了Burp的端口127.0.0.1：8082，但是和服务器的链接应该还是有待协调，最后在局域里添加进准备好端口，并在Firefox里设置好代理，终于，BurpSuite开始在浏览器的刷新下自动抓包。到这里差不多全部完成了。大体而言就是这样，过程拖泥带水，不过结果还算近人意。下一次安装工具时希望能稍微顺利一些。可能我碰上的细节问题也会是很多人第一次处理的麻烦所在，虽然讨厌，但只要步骤正确，终究能够完成，这或许是计算机的魅力所在。@elbadaernU 2018.3.22","categories":[{"name":"信息安全","slug":"信息安全","permalink":"http://yoursite.com/categories/信息安全/"}],"tags":[{"name":"BurpSuite","slug":"BurpSuite","permalink":"http://yoursite.com/tags/BurpSuite/"}]},{"title":"Update-log","slug":"Update-log","date":"2018-03-05T09:33:54.000Z","updated":"2022-06-22T03:02:18.064Z","comments":true,"path":"2018/03/05/Update-log/","link":"","permalink":"http://yoursite.com/2018/03/05/Update-log/","excerpt":"","text":"2018.3.5 更新日志：11.替换第二张首页幻灯片； 12.音乐更改为手动播放； 13.标题颜色由随机修改为蓝色； 14.删除“打赏”功能； 1Ps：刚刚过完年，最近复习C和学习Python，计划之后完成页面扩展、增加音乐和视频内容等更新.","categories":[],"tags":[]},{"title":"我的第一个PHP实例","slug":"我的第一个PHP实例","date":"2018-02-03T09:51:38.000Z","updated":"2022-06-27T07:42:01.072Z","comments":true,"path":"2018/02/03/我的第一个PHP实例/","link":"","permalink":"http://yoursite.com/2018/02/03/我的第一个PHP实例/","excerpt":"","text":"开始我是本周，18年1月末到18年2月初开始入门学习PHP。原本计划先从Python开始入手学习，但是网上预定的相关书籍发货日期较迟，又实在不喜欢电子书的阅览方式，身边正好有PHP的工具书，索性开始尝试。 首先作为第一步当然是配置相关软件，PHP相关的编译工具有Dreamweaver、ZendStudio、PhpStorm、Notepad++、EditPlus等多种。我的系统为Win10，于Windows环境下首先安装了Adobe的Dreamweaver CC。 相关编译器布置完成后，我开始在Windows环境下配置WampServer。 一点问题Wamp运行失败的第一次尝试解决相关安装完成后，打开Wamp，图标显示橙色，第一次使用没有在意。我将浏览器打开输入了localhost，结果是无法访问。工具书并没有给出相关解释，于是我首先猜想这个localhost的默认端口可能与某个软件的端口发生了冲突。百度之后找到了httpd.conf和wampmanager两个文件，将默认端口80修改为82。但结果仍旧是无法打开localhost。 Wamp运行失败的第二次尝试解决于是我怀疑这些常规端口可能都被占用了。搜索到了一名博主的类似解决方案，他将端口修改为Allow from all，解决了问题，于是我按照他的修改打开了.conf文件，Ctrl+f却搜索不到他的文件内容。经过对比后我发现可能是基于软件版本的不同文件内容已被修改过。我手动补全他的文件再打开Wamp，仍然是橙色。 Wamp运行失败的第三次尝试解决我开始确信不是大多数人“端口被占用”的问题。百度到控制台指令netstat -ano后我查看了每个被应用的端口，80并没有被占用，开始尝试在其他角度解决问题。这时我发现了Wamp的提示：服务器离线，2/3个进程再运行。于是我猜想会不会Wamp在安装时发生错误，有部分未被安装上。于是在卸载Wamp之后我重新安装了一次，结果仍然是橙色，服务器离线，2/3个服务在运行。但是会不会是文件修改错误，反思了一下是才安装的软件，仅仅修改了端口又重新修改回去，不可能是软件自身的问题。此时此刻我仍旧一头雾水。 Wamp运行失败的第四次尝试解决不甘心的打开了电脑管理中的服务，开始着手去查到底是哪项服务没有被打开。这回终于发现了，是Apache没有开启服务。我尝试手动打开，但被提示Windows无法打开这项服务，于是我终于意识到问题出在Apache的故障。我开始思考会不会是Apache的版本不兼容，想着要不要重新安装另一个版本来替换时，我在百度里发现了一条线索，路径问题。因为我的Wamp安装在路径cd /d/编译器/PHP/WampSever中，我在想会不会是“编译器”这个中文路径的锅。因为MySQL也在同一路径但是没有出任何问题，于是我想可能性应该很小。但抱着破罐子破摔的心态我还是将组件又全部卸载安装了一次，并将路径中的“编译器”修改为了“Compiler”。这一次，出乎意料而令人哭笑不得的是，打开Wamp服务后它终于从红色变成橙色，即刻转为绿色。打开localhost，成功进入，大功告成。 我的第一个PHP实例之后我顺利的打开Dreamweaver，用最基本的php语法写下了和初学C语言时一样的那句我很喜欢的话：echo &quot;Hello World&quot;; 保存进提前设定好的文件夹路径后，我回到Wamp，在localhost后添加了.php文件的路径，并打开。至此完成。 感想A.12或许在问题中挣扎确实很累，但是解决掉它之后人总会体验到名为执着的幸福。往往很多问题实际上是出在细节，平静下来，细嗅蔷薇，要拆掉的不一定是高楼大厦，只是底层被掰弯的一根螺丝钉。 B.1既然选择计算机这条道路，那么遇问题就应该及时明确问题、找出原因、对症下药，不忘举一反三，使人感到快乐。 就到这里结束吧。实际上这也算作是我的第一条正式写成的博客。感谢浏览。 @elbadaernU --2018.2.3.","categories":[{"name":"个人笔记","slug":"个人笔记","permalink":"http://yoursite.com/categories/个人笔记/"}],"tags":[{"name":"php","slug":"php","permalink":"http://yoursite.com/tags/php/"}]},{"title":"Start test","slug":"Start-test","date":"2018-01-25T06:20:21.000Z","updated":"2022-06-22T03:02:18.064Z","comments":true,"path":"2018/01/25/Start-test/","link":"","permalink":"http://yoursite.com/2018/01/25/Start-test/","excerpt":"","text":"@2018.1Okeydokey.Finished.","categories":[],"tags":[]}],"categories":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/算法/"},{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"实例分割","slug":"实例分割","permalink":"http://yoursite.com/categories/实例分割/"},{"name":"地址归一","slug":"地址归一","permalink":"http://yoursite.com/categories/地址归一/"},{"name":"流媒体","slug":"流媒体","permalink":"http://yoursite.com/categories/流媒体/"},{"name":"关键点检测","slug":"关键点检测","permalink":"http://yoursite.com/categories/关键点检测/"},{"name":"目标检测","slug":"目标检测","permalink":"http://yoursite.com/categories/目标检测/"},{"name":"NetworkX","slug":"NetworkX","permalink":"http://yoursite.com/categories/NetworkX/"},{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"},{"name":"GPT","slug":"GPT","permalink":"http://yoursite.com/categories/GPT/"},{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"},{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://yoursite.com/categories/计算机视觉/"},{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/categories/Docker/"},{"name":"Web","slug":"Web","permalink":"http://yoursite.com/categories/Web/"},{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"},{"name":"个人笔记","slug":"个人笔记","permalink":"http://yoursite.com/categories/个人笔记/"},{"name":"汇编","slug":"汇编","permalink":"http://yoursite.com/categories/汇编/"},{"name":"前端","slug":"前端","permalink":"http://yoursite.com/categories/前端/"},{"name":"脚本","slug":"脚本","permalink":"http://yoursite.com/categories/脚本/"},{"name":"信息安全","slug":"信息安全","permalink":"http://yoursite.com/categories/信息安全/"}],"tags":[{"name":"OCR","slug":"OCR","permalink":"http://yoursite.com/tags/OCR/"},{"name":"paddle","slug":"paddle","permalink":"http://yoursite.com/tags/paddle/"},{"name":"KIE","slug":"KIE","permalink":"http://yoursite.com/tags/KIE/"},{"name":"doccano","slug":"doccano","permalink":"http://yoursite.com/tags/doccano/"},{"name":"ERNIE-UIE","slug":"ERNIE-UIE","permalink":"http://yoursite.com/tags/ERNIE-UIE/"},{"name":"yolov8","slug":"yolov8","permalink":"http://yoursite.com/tags/yolov8/"},{"name":"opencv","slug":"opencv","permalink":"http://yoursite.com/tags/opencv/"},{"name":"BERT","slug":"BERT","permalink":"http://yoursite.com/tags/BERT/"},{"name":"GRU","slug":"GRU","permalink":"http://yoursite.com/tags/GRU/"},{"name":"RTSP","slug":"RTSP","permalink":"http://yoursite.com/tags/RTSP/"},{"name":"nebula","slug":"nebula","permalink":"http://yoursite.com/tags/nebula/"},{"name":"图算法","slug":"图算法","permalink":"http://yoursite.com/tags/图算法/"},{"name":"networkx","slug":"networkx","permalink":"http://yoursite.com/tags/networkx/"},{"name":"VGG","slug":"VGG","permalink":"http://yoursite.com/tags/VGG/"},{"name":"卷积神经网络","slug":"卷积神经网络","permalink":"http://yoursite.com/tags/卷积神经网络/"},{"name":"AlexNet","slug":"AlexNet","permalink":"http://yoursite.com/tags/AlexNet/"},{"name":"LeNet","slug":"LeNet","permalink":"http://yoursite.com/tags/LeNet/"},{"name":"GoogLeNet","slug":"GoogLeNet","permalink":"http://yoursite.com/tags/GoogLeNet/"},{"name":"ResNet","slug":"ResNet","permalink":"http://yoursite.com/tags/ResNet/"},{"name":"linux工具","slug":"linux工具","permalink":"http://yoursite.com/tags/linux工具/"},{"name":"LibreOffice","slug":"LibreOffice","permalink":"http://yoursite.com/tags/LibreOffice/"},{"name":"langchain-chatchat","slug":"langchain-chatchat","permalink":"http://yoursite.com/tags/langchain-chatchat/"},{"name":"chat-glm","slug":"chat-glm","permalink":"http://yoursite.com/tags/chat-glm/"},{"name":"baichuan","slug":"baichuan","permalink":"http://yoursite.com/tags/baichuan/"},{"name":"database","slug":"database","permalink":"http://yoursite.com/tags/database/"},{"name":"paddlepaddle","slug":"paddlepaddle","permalink":"http://yoursite.com/tags/paddlepaddle/"},{"name":"LiveGBS","slug":"LiveGBS","permalink":"http://yoursite.com/tags/LiveGBS/"},{"name":"nebua","slug":"nebua","permalink":"http://yoursite.com/tags/nebua/"},{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"},{"name":"elastic search","slug":"elastic-search","permalink":"http://yoursite.com/tags/elastic-search/"},{"name":"pyspark","slug":"pyspark","permalink":"http://yoursite.com/tags/pyspark/"},{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"},{"name":"-database -nebula","slug":"database-nebula","permalink":"http://yoursite.com/tags/database-nebula/"},{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/tags/Docker/"},{"name":"fastapi","slug":"fastapi","permalink":"http://yoursite.com/tags/fastapi/"},{"name":"clickhouse","slug":"clickhouse","permalink":"http://yoursite.com/tags/clickhouse/"},{"name":"FastApi","slug":"FastApi","permalink":"http://yoursite.com/tags/FastApi/"},{"name":"scrapy","slug":"scrapy","permalink":"http://yoursite.com/tags/scrapy/"},{"name":"numpy、pandas","slug":"numpy、pandas","permalink":"http://yoursite.com/tags/numpy、pandas/"},{"name":"Django","slug":"Django","permalink":"http://yoursite.com/tags/Django/"},{"name":"CNN","slug":"CNN","permalink":"http://yoursite.com/tags/CNN/"},{"name":"Hadoop","slug":"Hadoop","permalink":"http://yoursite.com/tags/Hadoop/"},{"name":"java","slug":"java","permalink":"http://yoursite.com/tags/java/"},{"name":"xpath","slug":"xpath","permalink":"http://yoursite.com/tags/xpath/"},{"name":"beautifulsoup","slug":"beautifulsoup","permalink":"http://yoursite.com/tags/beautifulsoup/"},{"name":"多线程","slug":"多线程","permalink":"http://yoursite.com/tags/多线程/"},{"name":"GET/POST...","slug":"GET-POST","permalink":"http://yoursite.com/tags/GET-POST/"},{"name":"PIL","slug":"PIL","permalink":"http://yoursite.com/tags/PIL/"},{"name":"MASM","slug":"MASM","permalink":"http://yoursite.com/tags/MASM/"},{"name":"QT","slug":"QT","permalink":"http://yoursite.com/tags/QT/"},{"name":"D3-SVG","slug":"D3-SVG","permalink":"http://yoursite.com/tags/D3-SVG/"},{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"},{"name":"海龟绘图","slug":"海龟绘图","permalink":"http://yoursite.com/tags/海龟绘图/"},{"name":"vb","slug":"vb","permalink":"http://yoursite.com/tags/vb/"},{"name":"C","slug":"C","permalink":"http://yoursite.com/tags/C/"},{"name":"ctf","slug":"ctf","permalink":"http://yoursite.com/tags/ctf/"},{"name":"QQ坦白说","slug":"QQ坦白说","permalink":"http://yoursite.com/tags/QQ坦白说/"},{"name":"工具站","slug":"工具站","permalink":"http://yoursite.com/tags/工具站/"},{"name":"BurpSuite","slug":"BurpSuite","permalink":"http://yoursite.com/tags/BurpSuite/"},{"name":"php","slug":"php","permalink":"http://yoursite.com/tags/php/"}]}